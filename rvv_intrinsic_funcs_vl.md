<!--NOTE: This file is generated by rvv_intrinsic_gen.py-->

### [RVV C extension types](rvv-intrinsic-rfc.md#data-types):

**Prototypes:**
``` C
vint8m1_t
vint8m2_t
vint8m4_t
vint8m8_t
vint16m1_t
vint16m2_t
vint16m4_t
vint16m8_t
vint32m1_t
vint32m2_t
vint32m4_t
vint32m8_t
vint64m1_t
vint64m2_t
vint64m4_t
vint64m8_t
vuint8m1_t
vuint8m2_t
vuint8m4_t
vuint8m8_t
vuint16m1_t
vuint16m2_t
vuint16m4_t
vuint16m8_t
vuint32m1_t
vuint32m2_t
vuint32m4_t
vuint32m8_t
vuint64m1_t
vuint64m2_t
vuint64m4_t
vuint64m8_t
vfloat16m1_t
vfloat16m2_t
vfloat16m4_t
vfloat16m8_t
vfloat32m1_t
vfloat32m2_t
vfloat32m4_t
vfloat32m8_t
vfloat64m1_t
vfloat64m2_t
vfloat64m4_t
vfloat64m8_t
```
### [RVV C Extension Types for Segment Load/Store](rvv-intrinsic-api.md#types-for-segment-loadstore):

**Prototypes:**
``` C
vint8m1x2_t
vint8m1x3_t
vint8m1x4_t
vint8m1x5_t
vint8m1x6_t
vint8m1x7_t
vint8m1x8_t
vint8m2x2_t
vint8m2x3_t
vint8m2x4_t
vint8m4x2_t
vint16m1x2_t
vint16m1x3_t
vint16m1x4_t
vint16m1x5_t
vint16m1x6_t
vint16m1x7_t
vint16m1x8_t
vint16m2x2_t
vint16m2x3_t
vint16m2x4_t
vint16m4x2_t
vint32m1x2_t
vint32m1x3_t
vint32m1x4_t
vint32m1x5_t
vint32m1x6_t
vint32m1x7_t
vint32m1x8_t
vint32m2x2_t
vint32m2x3_t
vint32m2x4_t
vint32m4x2_t
vint64m1x2_t
vint64m1x3_t
vint64m1x4_t
vint64m1x5_t
vint64m1x6_t
vint64m1x7_t
vint64m1x8_t
vint64m2x2_t
vint64m2x3_t
vint64m2x4_t
vint64m4x2_t
vuint8m1x2_t
vuint8m1x3_t
vuint8m1x4_t
vuint8m1x5_t
vuint8m1x6_t
vuint8m1x7_t
vuint8m1x8_t
vuint8m2x2_t
vuint8m2x3_t
vuint8m2x4_t
vuint8m4x2_t
vuint16m1x2_t
vuint16m1x3_t
vuint16m1x4_t
vuint16m1x5_t
vuint16m1x6_t
vuint16m1x7_t
vuint16m1x8_t
vuint16m2x2_t
vuint16m2x3_t
vuint16m2x4_t
vuint16m4x2_t
vuint32m1x2_t
vuint32m1x3_t
vuint32m1x4_t
vuint32m1x5_t
vuint32m1x6_t
vuint32m1x7_t
vuint32m1x8_t
vuint32m2x2_t
vuint32m2x3_t
vuint32m2x4_t
vuint32m4x2_t
vuint64m1x2_t
vuint64m1x3_t
vuint64m1x4_t
vuint64m1x5_t
vuint64m1x6_t
vuint64m1x7_t
vuint64m1x8_t
vuint64m2x2_t
vuint64m2x3_t
vuint64m2x4_t
vuint64m4x2_t
vfloat16m1x2_t
vfloat16m1x3_t
vfloat16m1x4_t
vfloat16m1x5_t
vfloat16m1x6_t
vfloat16m1x7_t
vfloat16m1x8_t
vfloat16m2x2_t
vfloat16m2x3_t
vfloat16m2x4_t
vfloat16m4x2_t
vfloat32m1x2_t
vfloat32m1x3_t
vfloat32m1x4_t
vfloat32m1x5_t
vfloat32m1x6_t
vfloat32m1x7_t
vfloat32m1x8_t
vfloat32m2x2_t
vfloat32m2x3_t
vfloat32m2x4_t
vfloat32m4x2_t
vfloat64m1x2_t
vfloat64m1x3_t
vfloat64m1x4_t
vfloat64m1x5_t
vfloat64m1x6_t
vfloat64m1x7_t
vfloat64m1x8_t
vfloat64m2x2_t
vfloat64m2x3_t
vfloat64m2x4_t
vfloat64m4x2_t
```
### [RVV C extension mask types](rvv-intrinsic-rfc.md#mask-types):
- The Syntax is `vbool<MLEN>_t`
 - `vbool1_t`
 - `vbool2_t`
 - `vbool4_t`
 - `vbool8_t`
 - `vbool16_t`
 - `vbool32_t`
 - `vbool64_t`

# RVV intrinsic Functions:

## Configuration-Setting Functions:

### [Set `vl` and `vtype` Functions](rvv-intrinsic-api.md#set-vl-and-vtype-functions):

**Prototypes:**
``` C
_VL_T vsetvl_e8m1 (size_t avl);
_VL_T vsetvl_e8m2 (size_t avl);
_VL_T vsetvl_e8m4 (size_t avl);
_VL_T vsetvl_e8m8 (size_t avl);
_VL_T vsetvl_e16m1 (size_t avl);
_VL_T vsetvl_e16m2 (size_t avl);
_VL_T vsetvl_e16m4 (size_t avl);
_VL_T vsetvl_e16m8 (size_t avl);
_VL_T vsetvl_e32m1 (size_t avl);
_VL_T vsetvl_e32m2 (size_t avl);
_VL_T vsetvl_e32m4 (size_t avl);
_VL_T vsetvl_e32m8 (size_t avl);
_VL_T vsetvl_e64m1 (size_t avl);
_VL_T vsetvl_e64m2 (size_t avl);
_VL_T vsetvl_e64m4 (size_t avl);
_VL_T vsetvl_e64m8 (size_t avl);
```
### [Set the vl to VLMAX with specific vtype](rvv-intrinsic-api.md#set-vl-to-vlmax-with-specific-vtype):

**Prototypes:**
``` C
_VL_T vsetvlmax_e8m1 ();
_VL_T vsetvlmax_e8m2 ();
_VL_T vsetvlmax_e8m4 ();
_VL_T vsetvlmax_e8m8 ();
_VL_T vsetvlmax_e16m1 ();
_VL_T vsetvlmax_e16m2 ();
_VL_T vsetvlmax_e16m4 ();
_VL_T vsetvlmax_e16m8 ();
_VL_T vsetvlmax_e32m1 ();
_VL_T vsetvlmax_e32m2 ();
_VL_T vsetvlmax_e32m4 ();
_VL_T vsetvlmax_e32m8 ();
_VL_T vsetvlmax_e64m1 ();
_VL_T vsetvlmax_e64m2 ();
_VL_T vsetvlmax_e64m4 ();
_VL_T vsetvlmax_e64m8 ();
```
### [Read the vl](rvv-intrinsic-api.md#read-vl-value):

**Prototypes:**
``` C
size_t vreadvl ();
```
## Vector Loads and Stores Functions:

### [Vector Unit-Stride Load Functions](rvv-intrinsic-api.md#74-vector-unit-stride-operations):

**Prototypes:**
``` C
vint8m1_t vlb_v_i8m1_vl (const int8_t *base, _VL_T vl);
vint8m2_t vlb_v_i8m2_vl (const int8_t *base, _VL_T vl);
vint8m4_t vlb_v_i8m4_vl (const int8_t *base, _VL_T vl);
vint8m8_t vlb_v_i8m8_vl (const int8_t *base, _VL_T vl);
vint16m1_t vlb_v_i16m1_vl (const int8_t *base, _VL_T vl);
vint16m2_t vlb_v_i16m2_vl (const int8_t *base, _VL_T vl);
vint16m4_t vlb_v_i16m4_vl (const int8_t *base, _VL_T vl);
vint16m8_t vlb_v_i16m8_vl (const int8_t *base, _VL_T vl);
vint32m1_t vlb_v_i32m1_vl (const int8_t *base, _VL_T vl);
vint32m2_t vlb_v_i32m2_vl (const int8_t *base, _VL_T vl);
vint32m4_t vlb_v_i32m4_vl (const int8_t *base, _VL_T vl);
vint32m8_t vlb_v_i32m8_vl (const int8_t *base, _VL_T vl);
vint64m1_t vlb_v_i64m1_vl (const int8_t *base, _VL_T vl);
vint64m2_t vlb_v_i64m2_vl (const int8_t *base, _VL_T vl);
vint64m4_t vlb_v_i64m4_vl (const int8_t *base, _VL_T vl);
vint64m8_t vlb_v_i64m8_vl (const int8_t *base, _VL_T vl);
vuint8m1_t vlbu_v_u8m1_vl (const uint8_t *base, _VL_T vl);
vuint8m2_t vlbu_v_u8m2_vl (const uint8_t *base, _VL_T vl);
vuint8m4_t vlbu_v_u8m4_vl (const uint8_t *base, _VL_T vl);
vuint8m8_t vlbu_v_u8m8_vl (const uint8_t *base, _VL_T vl);
vuint16m1_t vlbu_v_u16m1_vl (const uint8_t *base, _VL_T vl);
vuint16m2_t vlbu_v_u16m2_vl (const uint8_t *base, _VL_T vl);
vuint16m4_t vlbu_v_u16m4_vl (const uint8_t *base, _VL_T vl);
vuint16m8_t vlbu_v_u16m8_vl (const uint8_t *base, _VL_T vl);
vuint32m1_t vlbu_v_u32m1_vl (const uint8_t *base, _VL_T vl);
vuint32m2_t vlbu_v_u32m2_vl (const uint8_t *base, _VL_T vl);
vuint32m4_t vlbu_v_u32m4_vl (const uint8_t *base, _VL_T vl);
vuint32m8_t vlbu_v_u32m8_vl (const uint8_t *base, _VL_T vl);
vuint64m1_t vlbu_v_u64m1_vl (const uint8_t *base, _VL_T vl);
vuint64m2_t vlbu_v_u64m2_vl (const uint8_t *base, _VL_T vl);
vuint64m4_t vlbu_v_u64m4_vl (const uint8_t *base, _VL_T vl);
vuint64m8_t vlbu_v_u64m8_vl (const uint8_t *base, _VL_T vl);
vint16m1_t vlh_v_i16m1_vl (const int16_t *base, _VL_T vl);
vint16m2_t vlh_v_i16m2_vl (const int16_t *base, _VL_T vl);
vint16m4_t vlh_v_i16m4_vl (const int16_t *base, _VL_T vl);
vint16m8_t vlh_v_i16m8_vl (const int16_t *base, _VL_T vl);
vint32m1_t vlh_v_i32m1_vl (const int16_t *base, _VL_T vl);
vint32m2_t vlh_v_i32m2_vl (const int16_t *base, _VL_T vl);
vint32m4_t vlh_v_i32m4_vl (const int16_t *base, _VL_T vl);
vint32m8_t vlh_v_i32m8_vl (const int16_t *base, _VL_T vl);
vint64m1_t vlh_v_i64m1_vl (const int16_t *base, _VL_T vl);
vint64m2_t vlh_v_i64m2_vl (const int16_t *base, _VL_T vl);
vint64m4_t vlh_v_i64m4_vl (const int16_t *base, _VL_T vl);
vint64m8_t vlh_v_i64m8_vl (const int16_t *base, _VL_T vl);
vuint16m1_t vlhu_v_u16m1_vl (const uint16_t *base, _VL_T vl);
vuint16m2_t vlhu_v_u16m2_vl (const uint16_t *base, _VL_T vl);
vuint16m4_t vlhu_v_u16m4_vl (const uint16_t *base, _VL_T vl);
vuint16m8_t vlhu_v_u16m8_vl (const uint16_t *base, _VL_T vl);
vuint32m1_t vlhu_v_u32m1_vl (const uint16_t *base, _VL_T vl);
vuint32m2_t vlhu_v_u32m2_vl (const uint16_t *base, _VL_T vl);
vuint32m4_t vlhu_v_u32m4_vl (const uint16_t *base, _VL_T vl);
vuint32m8_t vlhu_v_u32m8_vl (const uint16_t *base, _VL_T vl);
vuint64m1_t vlhu_v_u64m1_vl (const uint16_t *base, _VL_T vl);
vuint64m2_t vlhu_v_u64m2_vl (const uint16_t *base, _VL_T vl);
vuint64m4_t vlhu_v_u64m4_vl (const uint16_t *base, _VL_T vl);
vuint64m8_t vlhu_v_u64m8_vl (const uint16_t *base, _VL_T vl);
vint32m1_t vlw_v_i32m1_vl (const int32_t *base, _VL_T vl);
vint32m2_t vlw_v_i32m2_vl (const int32_t *base, _VL_T vl);
vint32m4_t vlw_v_i32m4_vl (const int32_t *base, _VL_T vl);
vint32m8_t vlw_v_i32m8_vl (const int32_t *base, _VL_T vl);
vint64m1_t vlw_v_i64m1_vl (const int32_t *base, _VL_T vl);
vint64m2_t vlw_v_i64m2_vl (const int32_t *base, _VL_T vl);
vint64m4_t vlw_v_i64m4_vl (const int32_t *base, _VL_T vl);
vint64m8_t vlw_v_i64m8_vl (const int32_t *base, _VL_T vl);
vuint32m1_t vlwu_v_u32m1_vl (const uint32_t *base, _VL_T vl);
vuint32m2_t vlwu_v_u32m2_vl (const uint32_t *base, _VL_T vl);
vuint32m4_t vlwu_v_u32m4_vl (const uint32_t *base, _VL_T vl);
vuint32m8_t vlwu_v_u32m8_vl (const uint32_t *base, _VL_T vl);
vuint64m1_t vlwu_v_u64m1_vl (const uint32_t *base, _VL_T vl);
vuint64m2_t vlwu_v_u64m2_vl (const uint32_t *base, _VL_T vl);
vuint64m4_t vlwu_v_u64m4_vl (const uint32_t *base, _VL_T vl);
vuint64m8_t vlwu_v_u64m8_vl (const uint32_t *base, _VL_T vl);
vint8m1_t vle_v_i8m1_vl (const int8_t *base, _VL_T vl);
vint8m2_t vle_v_i8m2_vl (const int8_t *base, _VL_T vl);
vint8m4_t vle_v_i8m4_vl (const int8_t *base, _VL_T vl);
vint8m8_t vle_v_i8m8_vl (const int8_t *base, _VL_T vl);
vint16m1_t vle_v_i16m1_vl (const int16_t *base, _VL_T vl);
vint16m2_t vle_v_i16m2_vl (const int16_t *base, _VL_T vl);
vint16m4_t vle_v_i16m4_vl (const int16_t *base, _VL_T vl);
vint16m8_t vle_v_i16m8_vl (const int16_t *base, _VL_T vl);
vint32m1_t vle_v_i32m1_vl (const int32_t *base, _VL_T vl);
vint32m2_t vle_v_i32m2_vl (const int32_t *base, _VL_T vl);
vint32m4_t vle_v_i32m4_vl (const int32_t *base, _VL_T vl);
vint32m8_t vle_v_i32m8_vl (const int32_t *base, _VL_T vl);
vint64m1_t vle_v_i64m1_vl (const int64_t *base, _VL_T vl);
vint64m2_t vle_v_i64m2_vl (const int64_t *base, _VL_T vl);
vint64m4_t vle_v_i64m4_vl (const int64_t *base, _VL_T vl);
vint64m8_t vle_v_i64m8_vl (const int64_t *base, _VL_T vl);
vuint8m1_t vle_v_u8m1_vl (const uint8_t *base, _VL_T vl);
vuint8m2_t vle_v_u8m2_vl (const uint8_t *base, _VL_T vl);
vuint8m4_t vle_v_u8m4_vl (const uint8_t *base, _VL_T vl);
vuint8m8_t vle_v_u8m8_vl (const uint8_t *base, _VL_T vl);
vuint16m1_t vle_v_u16m1_vl (const uint16_t *base, _VL_T vl);
vuint16m2_t vle_v_u16m2_vl (const uint16_t *base, _VL_T vl);
vuint16m4_t vle_v_u16m4_vl (const uint16_t *base, _VL_T vl);
vuint16m8_t vle_v_u16m8_vl (const uint16_t *base, _VL_T vl);
vuint32m1_t vle_v_u32m1_vl (const uint32_t *base, _VL_T vl);
vuint32m2_t vle_v_u32m2_vl (const uint32_t *base, _VL_T vl);
vuint32m4_t vle_v_u32m4_vl (const uint32_t *base, _VL_T vl);
vuint32m8_t vle_v_u32m8_vl (const uint32_t *base, _VL_T vl);
vuint64m1_t vle_v_u64m1_vl (const uint64_t *base, _VL_T vl);
vuint64m2_t vle_v_u64m2_vl (const uint64_t *base, _VL_T vl);
vuint64m4_t vle_v_u64m4_vl (const uint64_t *base, _VL_T vl);
vuint64m8_t vle_v_u64m8_vl (const uint64_t *base, _VL_T vl);
vfloat16m1_t vle_v_f16m1_vl (const float16_t *base, _VL_T vl);
vfloat16m2_t vle_v_f16m2_vl (const float16_t *base, _VL_T vl);
vfloat16m4_t vle_v_f16m4_vl (const float16_t *base, _VL_T vl);
vfloat16m8_t vle_v_f16m8_vl (const float16_t *base, _VL_T vl);
vfloat32m1_t vle_v_f32m1_vl (const float32_t *base, _VL_T vl);
vfloat32m2_t vle_v_f32m2_vl (const float32_t *base, _VL_T vl);
vfloat32m4_t vle_v_f32m4_vl (const float32_t *base, _VL_T vl);
vfloat32m8_t vle_v_f32m8_vl (const float32_t *base, _VL_T vl);
vfloat64m1_t vle_v_f64m1_vl (const float64_t *base, _VL_T vl);
vfloat64m2_t vle_v_f64m2_vl (const float64_t *base, _VL_T vl);
vfloat64m4_t vle_v_f64m4_vl (const float64_t *base, _VL_T vl);
vfloat64m8_t vle_v_f64m8_vl (const float64_t *base, _VL_T vl);
// masked functions
vint8m1_t vlb_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m2_t vlb_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, _VL_T vl);
vint8m4_t vlb_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, _VL_T vl);
vint8m8_t vlb_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, _VL_T vl);
vint16m1_t vlb_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int8_t *base, _VL_T vl);
vint16m2_t vlb_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int8_t *base, _VL_T vl);
vint16m4_t vlb_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int8_t *base, _VL_T vl);
vint16m8_t vlb_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int8_t *base, _VL_T vl);
vint32m1_t vlb_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int8_t *base, _VL_T vl);
vint32m2_t vlb_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int8_t *base, _VL_T vl);
vint32m4_t vlb_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int8_t *base, _VL_T vl);
vint32m8_t vlb_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int8_t *base, _VL_T vl);
vint64m1_t vlb_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int8_t *base, _VL_T vl);
vint64m2_t vlb_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int8_t *base, _VL_T vl);
vint64m4_t vlb_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int8_t *base, _VL_T vl);
vint64m8_t vlb_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int8_t *base, _VL_T vl);
vuint8m1_t vlbu_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m2_t vlbu_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m4_t vlbu_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m8_t vlbu_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m1_t vlbu_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m2_t vlbu_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m4_t vlbu_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m8_t vlbu_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m1_t vlbu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m2_t vlbu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m4_t vlbu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m8_t vlbu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m1_t vlbu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m2_t vlbu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m4_t vlbu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m8_t vlbu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint8_t *base, _VL_T vl);
vint16m1_t vlh_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m2_t vlh_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, _VL_T vl);
vint16m4_t vlh_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, _VL_T vl);
vint16m8_t vlh_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, _VL_T vl);
vint32m1_t vlh_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int16_t *base, _VL_T vl);
vint32m2_t vlh_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int16_t *base, _VL_T vl);
vint32m4_t vlh_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int16_t *base, _VL_T vl);
vint32m8_t vlh_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int16_t *base, _VL_T vl);
vint64m1_t vlh_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int16_t *base, _VL_T vl);
vint64m2_t vlh_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int16_t *base, _VL_T vl);
vint64m4_t vlh_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int16_t *base, _VL_T vl);
vint64m8_t vlh_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int16_t *base, _VL_T vl);
vuint16m1_t vlhu_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m2_t vlhu_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m4_t vlhu_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m8_t vlhu_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m1_t vlhu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m2_t vlhu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m4_t vlhu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m8_t vlhu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m1_t vlhu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m2_t vlhu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m4_t vlhu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m8_t vlhu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint16_t *base, _VL_T vl);
vint32m1_t vlw_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m2_t vlw_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, _VL_T vl);
vint32m4_t vlw_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, _VL_T vl);
vint32m8_t vlw_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, _VL_T vl);
vint64m1_t vlw_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int32_t *base, _VL_T vl);
vint64m2_t vlw_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int32_t *base, _VL_T vl);
vint64m4_t vlw_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int32_t *base, _VL_T vl);
vint64m8_t vlw_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int32_t *base, _VL_T vl);
vuint32m1_t vlwu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m2_t vlwu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m4_t vlwu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m8_t vlwu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m1_t vlwu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m2_t vlwu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m4_t vlwu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m8_t vlwu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint32_t *base, _VL_T vl);
vint8m1_t vle_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m2_t vle_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, _VL_T vl);
vint8m4_t vle_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, _VL_T vl);
vint8m8_t vle_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, _VL_T vl);
vint16m1_t vle_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m2_t vle_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, _VL_T vl);
vint16m4_t vle_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, _VL_T vl);
vint16m8_t vle_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, _VL_T vl);
vint32m1_t vle_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m2_t vle_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, _VL_T vl);
vint32m4_t vle_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, _VL_T vl);
vint32m8_t vle_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, _VL_T vl);
vint64m1_t vle_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m2_t vle_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, _VL_T vl);
vint64m4_t vle_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int64_t *base, _VL_T vl);
vint64m8_t vle_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int64_t *base, _VL_T vl);
vuint8m1_t vle_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m2_t vle_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m4_t vle_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m8_t vle_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m1_t vle_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m2_t vle_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m4_t vle_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m8_t vle_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m1_t vle_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m2_t vle_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m4_t vle_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m8_t vle_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m1_t vle_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m2_t vle_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m4_t vle_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m8_t vle_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint64_t *base, _VL_T vl);
vfloat16m1_t vle_v_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m2_t vle_v_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m4_t vle_v_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m8_t vle_v_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, const float16_t *base, _VL_T vl);
vfloat32m1_t vle_v_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m2_t vle_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m4_t vle_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m8_t vle_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, const float32_t *base, _VL_T vl);
vfloat64m1_t vle_v_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m2_t vle_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m4_t vle_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m8_t vle_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, const float64_t *base, _VL_T vl);
```
### [Vector Unit-Stride Store Functions](rvv-intrinsic-api.md#74-vector-unit-stride-operations):

**Prototypes:**
``` C
void vsb_v_i8m1_vl (int8_t *base, vint8m1_t value, _VL_T vl);
void vsb_v_i8m2_vl (int8_t *base, vint8m2_t value, _VL_T vl);
void vsb_v_i8m4_vl (int8_t *base, vint8m4_t value, _VL_T vl);
void vsb_v_i8m8_vl (int8_t *base, vint8m8_t value, _VL_T vl);
void vsb_v_i16m1_vl (int8_t *base, vint16m1_t value, _VL_T vl);
void vsb_v_i16m2_vl (int8_t *base, vint16m2_t value, _VL_T vl);
void vsb_v_i16m4_vl (int8_t *base, vint16m4_t value, _VL_T vl);
void vsb_v_i16m8_vl (int8_t *base, vint16m8_t value, _VL_T vl);
void vsb_v_i32m1_vl (int8_t *base, vint32m1_t value, _VL_T vl);
void vsb_v_i32m2_vl (int8_t *base, vint32m2_t value, _VL_T vl);
void vsb_v_i32m4_vl (int8_t *base, vint32m4_t value, _VL_T vl);
void vsb_v_i32m8_vl (int8_t *base, vint32m8_t value, _VL_T vl);
void vsb_v_i64m1_vl (int8_t *base, vint64m1_t value, _VL_T vl);
void vsb_v_i64m2_vl (int8_t *base, vint64m2_t value, _VL_T vl);
void vsb_v_i64m4_vl (int8_t *base, vint64m4_t value, _VL_T vl);
void vsb_v_i64m8_vl (int8_t *base, vint64m8_t value, _VL_T vl);
void vsb_v_u8m1_vl (uint8_t *base, vuint8m1_t value, _VL_T vl);
void vsb_v_u8m2_vl (uint8_t *base, vuint8m2_t value, _VL_T vl);
void vsb_v_u8m4_vl (uint8_t *base, vuint8m4_t value, _VL_T vl);
void vsb_v_u8m8_vl (uint8_t *base, vuint8m8_t value, _VL_T vl);
void vsb_v_u16m1_vl (uint8_t *base, vuint16m1_t value, _VL_T vl);
void vsb_v_u16m2_vl (uint8_t *base, vuint16m2_t value, _VL_T vl);
void vsb_v_u16m4_vl (uint8_t *base, vuint16m4_t value, _VL_T vl);
void vsb_v_u16m8_vl (uint8_t *base, vuint16m8_t value, _VL_T vl);
void vsb_v_u32m1_vl (uint8_t *base, vuint32m1_t value, _VL_T vl);
void vsb_v_u32m2_vl (uint8_t *base, vuint32m2_t value, _VL_T vl);
void vsb_v_u32m4_vl (uint8_t *base, vuint32m4_t value, _VL_T vl);
void vsb_v_u32m8_vl (uint8_t *base, vuint32m8_t value, _VL_T vl);
void vsb_v_u64m1_vl (uint8_t *base, vuint64m1_t value, _VL_T vl);
void vsb_v_u64m2_vl (uint8_t *base, vuint64m2_t value, _VL_T vl);
void vsb_v_u64m4_vl (uint8_t *base, vuint64m4_t value, _VL_T vl);
void vsb_v_u64m8_vl (uint8_t *base, vuint64m8_t value, _VL_T vl);
void vsh_v_i16m1_vl (int16_t *base, vint16m1_t value, _VL_T vl);
void vsh_v_i16m2_vl (int16_t *base, vint16m2_t value, _VL_T vl);
void vsh_v_i16m4_vl (int16_t *base, vint16m4_t value, _VL_T vl);
void vsh_v_i16m8_vl (int16_t *base, vint16m8_t value, _VL_T vl);
void vsh_v_i32m1_vl (int16_t *base, vint32m1_t value, _VL_T vl);
void vsh_v_i32m2_vl (int16_t *base, vint32m2_t value, _VL_T vl);
void vsh_v_i32m4_vl (int16_t *base, vint32m4_t value, _VL_T vl);
void vsh_v_i32m8_vl (int16_t *base, vint32m8_t value, _VL_T vl);
void vsh_v_i64m1_vl (int16_t *base, vint64m1_t value, _VL_T vl);
void vsh_v_i64m2_vl (int16_t *base, vint64m2_t value, _VL_T vl);
void vsh_v_i64m4_vl (int16_t *base, vint64m4_t value, _VL_T vl);
void vsh_v_i64m8_vl (int16_t *base, vint64m8_t value, _VL_T vl);
void vsh_v_u16m1_vl (uint16_t *base, vuint16m1_t value, _VL_T vl);
void vsh_v_u16m2_vl (uint16_t *base, vuint16m2_t value, _VL_T vl);
void vsh_v_u16m4_vl (uint16_t *base, vuint16m4_t value, _VL_T vl);
void vsh_v_u16m8_vl (uint16_t *base, vuint16m8_t value, _VL_T vl);
void vsh_v_u32m1_vl (uint16_t *base, vuint32m1_t value, _VL_T vl);
void vsh_v_u32m2_vl (uint16_t *base, vuint32m2_t value, _VL_T vl);
void vsh_v_u32m4_vl (uint16_t *base, vuint32m4_t value, _VL_T vl);
void vsh_v_u32m8_vl (uint16_t *base, vuint32m8_t value, _VL_T vl);
void vsh_v_u64m1_vl (uint16_t *base, vuint64m1_t value, _VL_T vl);
void vsh_v_u64m2_vl (uint16_t *base, vuint64m2_t value, _VL_T vl);
void vsh_v_u64m4_vl (uint16_t *base, vuint64m4_t value, _VL_T vl);
void vsh_v_u64m8_vl (uint16_t *base, vuint64m8_t value, _VL_T vl);
void vsw_v_i32m1_vl (int32_t *base, vint32m1_t value, _VL_T vl);
void vsw_v_i32m2_vl (int32_t *base, vint32m2_t value, _VL_T vl);
void vsw_v_i32m4_vl (int32_t *base, vint32m4_t value, _VL_T vl);
void vsw_v_i32m8_vl (int32_t *base, vint32m8_t value, _VL_T vl);
void vsw_v_i64m1_vl (int32_t *base, vint64m1_t value, _VL_T vl);
void vsw_v_i64m2_vl (int32_t *base, vint64m2_t value, _VL_T vl);
void vsw_v_i64m4_vl (int32_t *base, vint64m4_t value, _VL_T vl);
void vsw_v_i64m8_vl (int32_t *base, vint64m8_t value, _VL_T vl);
void vsw_v_u32m1_vl (uint32_t *base, vuint32m1_t value, _VL_T vl);
void vsw_v_u32m2_vl (uint32_t *base, vuint32m2_t value, _VL_T vl);
void vsw_v_u32m4_vl (uint32_t *base, vuint32m4_t value, _VL_T vl);
void vsw_v_u32m8_vl (uint32_t *base, vuint32m8_t value, _VL_T vl);
void vsw_v_u64m1_vl (uint32_t *base, vuint64m1_t value, _VL_T vl);
void vsw_v_u64m2_vl (uint32_t *base, vuint64m2_t value, _VL_T vl);
void vsw_v_u64m4_vl (uint32_t *base, vuint64m4_t value, _VL_T vl);
void vsw_v_u64m8_vl (uint32_t *base, vuint64m8_t value, _VL_T vl);
void vse_v_i8m1_vl (int8_t *base, vint8m1_t value, _VL_T vl);
void vse_v_i8m2_vl (int8_t *base, vint8m2_t value, _VL_T vl);
void vse_v_i8m4_vl (int8_t *base, vint8m4_t value, _VL_T vl);
void vse_v_i8m8_vl (int8_t *base, vint8m8_t value, _VL_T vl);
void vse_v_i16m1_vl (int16_t *base, vint16m1_t value, _VL_T vl);
void vse_v_i16m2_vl (int16_t *base, vint16m2_t value, _VL_T vl);
void vse_v_i16m4_vl (int16_t *base, vint16m4_t value, _VL_T vl);
void vse_v_i16m8_vl (int16_t *base, vint16m8_t value, _VL_T vl);
void vse_v_i32m1_vl (int32_t *base, vint32m1_t value, _VL_T vl);
void vse_v_i32m2_vl (int32_t *base, vint32m2_t value, _VL_T vl);
void vse_v_i32m4_vl (int32_t *base, vint32m4_t value, _VL_T vl);
void vse_v_i32m8_vl (int32_t *base, vint32m8_t value, _VL_T vl);
void vse_v_i64m1_vl (int64_t *base, vint64m1_t value, _VL_T vl);
void vse_v_i64m2_vl (int64_t *base, vint64m2_t value, _VL_T vl);
void vse_v_i64m4_vl (int64_t *base, vint64m4_t value, _VL_T vl);
void vse_v_i64m8_vl (int64_t *base, vint64m8_t value, _VL_T vl);
void vse_v_u8m1_vl (uint8_t *base, vuint8m1_t value, _VL_T vl);
void vse_v_u8m2_vl (uint8_t *base, vuint8m2_t value, _VL_T vl);
void vse_v_u8m4_vl (uint8_t *base, vuint8m4_t value, _VL_T vl);
void vse_v_u8m8_vl (uint8_t *base, vuint8m8_t value, _VL_T vl);
void vse_v_u16m1_vl (uint16_t *base, vuint16m1_t value, _VL_T vl);
void vse_v_u16m2_vl (uint16_t *base, vuint16m2_t value, _VL_T vl);
void vse_v_u16m4_vl (uint16_t *base, vuint16m4_t value, _VL_T vl);
void vse_v_u16m8_vl (uint16_t *base, vuint16m8_t value, _VL_T vl);
void vse_v_u32m1_vl (uint32_t *base, vuint32m1_t value, _VL_T vl);
void vse_v_u32m2_vl (uint32_t *base, vuint32m2_t value, _VL_T vl);
void vse_v_u32m4_vl (uint32_t *base, vuint32m4_t value, _VL_T vl);
void vse_v_u32m8_vl (uint32_t *base, vuint32m8_t value, _VL_T vl);
void vse_v_u64m1_vl (uint64_t *base, vuint64m1_t value, _VL_T vl);
void vse_v_u64m2_vl (uint64_t *base, vuint64m2_t value, _VL_T vl);
void vse_v_u64m4_vl (uint64_t *base, vuint64m4_t value, _VL_T vl);
void vse_v_u64m8_vl (uint64_t *base, vuint64m8_t value, _VL_T vl);
void vse_v_f16m1_vl (float16_t *base, vfloat16m1_t value, _VL_T vl);
void vse_v_f16m2_vl (float16_t *base, vfloat16m2_t value, _VL_T vl);
void vse_v_f16m4_vl (float16_t *base, vfloat16m4_t value, _VL_T vl);
void vse_v_f16m8_vl (float16_t *base, vfloat16m8_t value, _VL_T vl);
void vse_v_f32m1_vl (float32_t *base, vfloat32m1_t value, _VL_T vl);
void vse_v_f32m2_vl (float32_t *base, vfloat32m2_t value, _VL_T vl);
void vse_v_f32m4_vl (float32_t *base, vfloat32m4_t value, _VL_T vl);
void vse_v_f32m8_vl (float32_t *base, vfloat32m8_t value, _VL_T vl);
void vse_v_f64m1_vl (float64_t *base, vfloat64m1_t value, _VL_T vl);
void vse_v_f64m2_vl (float64_t *base, vfloat64m2_t value, _VL_T vl);
void vse_v_f64m4_vl (float64_t *base, vfloat64m4_t value, _VL_T vl);
void vse_v_f64m8_vl (float64_t *base, vfloat64m8_t value, _VL_T vl);
// masked functions
void vsb_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vint8m1_t value, _VL_T vl);
void vsb_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vint8m2_t value, _VL_T vl);
void vsb_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vint8m4_t value, _VL_T vl);
void vsb_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vint8m8_t value, _VL_T vl);
void vsb_v_i16m1_m_vl (vbool16_t mask, int8_t *base, vint16m1_t value, _VL_T vl);
void vsb_v_i16m2_m_vl (vbool8_t mask, int8_t *base, vint16m2_t value, _VL_T vl);
void vsb_v_i16m4_m_vl (vbool4_t mask, int8_t *base, vint16m4_t value, _VL_T vl);
void vsb_v_i16m8_m_vl (vbool2_t mask, int8_t *base, vint16m8_t value, _VL_T vl);
void vsb_v_i32m1_m_vl (vbool32_t mask, int8_t *base, vint32m1_t value, _VL_T vl);
void vsb_v_i32m2_m_vl (vbool16_t mask, int8_t *base, vint32m2_t value, _VL_T vl);
void vsb_v_i32m4_m_vl (vbool8_t mask, int8_t *base, vint32m4_t value, _VL_T vl);
void vsb_v_i32m8_m_vl (vbool4_t mask, int8_t *base, vint32m8_t value, _VL_T vl);
void vsb_v_i64m1_m_vl (vbool64_t mask, int8_t *base, vint64m1_t value, _VL_T vl);
void vsb_v_i64m2_m_vl (vbool32_t mask, int8_t *base, vint64m2_t value, _VL_T vl);
void vsb_v_i64m4_m_vl (vbool16_t mask, int8_t *base, vint64m4_t value, _VL_T vl);
void vsb_v_i64m8_m_vl (vbool8_t mask, int8_t *base, vint64m8_t value, _VL_T vl);
void vsb_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t value, _VL_T vl);
void vsb_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t value, _VL_T vl);
void vsb_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t value, _VL_T vl);
void vsb_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t value, _VL_T vl);
void vsb_v_u16m1_m_vl (vbool16_t mask, uint8_t *base, vuint16m1_t value, _VL_T vl);
void vsb_v_u16m2_m_vl (vbool8_t mask, uint8_t *base, vuint16m2_t value, _VL_T vl);
void vsb_v_u16m4_m_vl (vbool4_t mask, uint8_t *base, vuint16m4_t value, _VL_T vl);
void vsb_v_u16m8_m_vl (vbool2_t mask, uint8_t *base, vuint16m8_t value, _VL_T vl);
void vsb_v_u32m1_m_vl (vbool32_t mask, uint8_t *base, vuint32m1_t value, _VL_T vl);
void vsb_v_u32m2_m_vl (vbool16_t mask, uint8_t *base, vuint32m2_t value, _VL_T vl);
void vsb_v_u32m4_m_vl (vbool8_t mask, uint8_t *base, vuint32m4_t value, _VL_T vl);
void vsb_v_u32m8_m_vl (vbool4_t mask, uint8_t *base, vuint32m8_t value, _VL_T vl);
void vsb_v_u64m1_m_vl (vbool64_t mask, uint8_t *base, vuint64m1_t value, _VL_T vl);
void vsb_v_u64m2_m_vl (vbool32_t mask, uint8_t *base, vuint64m2_t value, _VL_T vl);
void vsb_v_u64m4_m_vl (vbool16_t mask, uint8_t *base, vuint64m4_t value, _VL_T vl);
void vsb_v_u64m8_m_vl (vbool8_t mask, uint8_t *base, vuint64m8_t value, _VL_T vl);
void vsh_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vint16m1_t value, _VL_T vl);
void vsh_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vint16m2_t value, _VL_T vl);
void vsh_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vint16m4_t value, _VL_T vl);
void vsh_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vint16m8_t value, _VL_T vl);
void vsh_v_i32m1_m_vl (vbool32_t mask, int16_t *base, vint32m1_t value, _VL_T vl);
void vsh_v_i32m2_m_vl (vbool16_t mask, int16_t *base, vint32m2_t value, _VL_T vl);
void vsh_v_i32m4_m_vl (vbool8_t mask, int16_t *base, vint32m4_t value, _VL_T vl);
void vsh_v_i32m8_m_vl (vbool4_t mask, int16_t *base, vint32m8_t value, _VL_T vl);
void vsh_v_i64m1_m_vl (vbool64_t mask, int16_t *base, vint64m1_t value, _VL_T vl);
void vsh_v_i64m2_m_vl (vbool32_t mask, int16_t *base, vint64m2_t value, _VL_T vl);
void vsh_v_i64m4_m_vl (vbool16_t mask, int16_t *base, vint64m4_t value, _VL_T vl);
void vsh_v_i64m8_m_vl (vbool8_t mask, int16_t *base, vint64m8_t value, _VL_T vl);
void vsh_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t value, _VL_T vl);
void vsh_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t value, _VL_T vl);
void vsh_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t value, _VL_T vl);
void vsh_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t value, _VL_T vl);
void vsh_v_u32m1_m_vl (vbool32_t mask, uint16_t *base, vuint32m1_t value, _VL_T vl);
void vsh_v_u32m2_m_vl (vbool16_t mask, uint16_t *base, vuint32m2_t value, _VL_T vl);
void vsh_v_u32m4_m_vl (vbool8_t mask, uint16_t *base, vuint32m4_t value, _VL_T vl);
void vsh_v_u32m8_m_vl (vbool4_t mask, uint16_t *base, vuint32m8_t value, _VL_T vl);
void vsh_v_u64m1_m_vl (vbool64_t mask, uint16_t *base, vuint64m1_t value, _VL_T vl);
void vsh_v_u64m2_m_vl (vbool32_t mask, uint16_t *base, vuint64m2_t value, _VL_T vl);
void vsh_v_u64m4_m_vl (vbool16_t mask, uint16_t *base, vuint64m4_t value, _VL_T vl);
void vsh_v_u64m8_m_vl (vbool8_t mask, uint16_t *base, vuint64m8_t value, _VL_T vl);
void vsw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vint32m1_t value, _VL_T vl);
void vsw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vint32m2_t value, _VL_T vl);
void vsw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vint32m4_t value, _VL_T vl);
void vsw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vint32m8_t value, _VL_T vl);
void vsw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vint64m1_t value, _VL_T vl);
void vsw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vint64m2_t value, _VL_T vl);
void vsw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vint64m4_t value, _VL_T vl);
void vsw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vint64m8_t value, _VL_T vl);
void vsw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t value, _VL_T vl);
void vsw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t value, _VL_T vl);
void vsw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t value, _VL_T vl);
void vsw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t value, _VL_T vl);
void vsw_v_u64m1_m_vl (vbool64_t mask, uint32_t *base, vuint64m1_t value, _VL_T vl);
void vsw_v_u64m2_m_vl (vbool32_t mask, uint32_t *base, vuint64m2_t value, _VL_T vl);
void vsw_v_u64m4_m_vl (vbool16_t mask, uint32_t *base, vuint64m4_t value, _VL_T vl);
void vsw_v_u64m8_m_vl (vbool8_t mask, uint32_t *base, vuint64m8_t value, _VL_T vl);
void vse_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vint8m1_t value, _VL_T vl);
void vse_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vint8m2_t value, _VL_T vl);
void vse_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vint8m4_t value, _VL_T vl);
void vse_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vint8m8_t value, _VL_T vl);
void vse_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vint16m1_t value, _VL_T vl);
void vse_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vint16m2_t value, _VL_T vl);
void vse_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vint16m4_t value, _VL_T vl);
void vse_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vint16m8_t value, _VL_T vl);
void vse_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vint32m1_t value, _VL_T vl);
void vse_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vint32m2_t value, _VL_T vl);
void vse_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vint32m4_t value, _VL_T vl);
void vse_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vint32m8_t value, _VL_T vl);
void vse_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vint64m1_t value, _VL_T vl);
void vse_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vint64m2_t value, _VL_T vl);
void vse_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vint64m4_t value, _VL_T vl);
void vse_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vint64m8_t value, _VL_T vl);
void vse_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t value, _VL_T vl);
void vse_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t value, _VL_T vl);
void vse_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t value, _VL_T vl);
void vse_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t value, _VL_T vl);
void vse_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t value, _VL_T vl);
void vse_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t value, _VL_T vl);
void vse_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t value, _VL_T vl);
void vse_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t value, _VL_T vl);
void vse_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t value, _VL_T vl);
void vse_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t value, _VL_T vl);
void vse_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t value, _VL_T vl);
void vse_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t value, _VL_T vl);
void vse_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t value, _VL_T vl);
void vse_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t value, _VL_T vl);
void vse_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t value, _VL_T vl);
void vse_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t value, _VL_T vl);
void vse_v_f16m1_m_vl (vbool16_t mask, float16_t *base, vfloat16m1_t value, _VL_T vl);
void vse_v_f16m2_m_vl (vbool8_t mask, float16_t *base, vfloat16m2_t value, _VL_T vl);
void vse_v_f16m4_m_vl (vbool4_t mask, float16_t *base, vfloat16m4_t value, _VL_T vl);
void vse_v_f16m8_m_vl (vbool2_t mask, float16_t *base, vfloat16m8_t value, _VL_T vl);
void vse_v_f32m1_m_vl (vbool32_t mask, float32_t *base, vfloat32m1_t value, _VL_T vl);
void vse_v_f32m2_m_vl (vbool16_t mask, float32_t *base, vfloat32m2_t value, _VL_T vl);
void vse_v_f32m4_m_vl (vbool8_t mask, float32_t *base, vfloat32m4_t value, _VL_T vl);
void vse_v_f32m8_m_vl (vbool4_t mask, float32_t *base, vfloat32m8_t value, _VL_T vl);
void vse_v_f64m1_m_vl (vbool64_t mask, float64_t *base, vfloat64m1_t value, _VL_T vl);
void vse_v_f64m2_m_vl (vbool32_t mask, float64_t *base, vfloat64m2_t value, _VL_T vl);
void vse_v_f64m4_m_vl (vbool16_t mask, float64_t *base, vfloat64m4_t value, _VL_T vl);
void vse_v_f64m8_m_vl (vbool8_t mask, float64_t *base, vfloat64m8_t value, _VL_T vl);
```
### [Vector Strided Load Functions](rvv-intrinsic-api.md#75-vector-strided-loadstore-operations):

**Prototypes:**
``` C
vint8m1_t vlsb_v_i8m1_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2_t vlsb_v_i8m2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m4_t vlsb_v_i8m4_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m8_t vlsb_v_i8m8_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1_t vlsb_v_i16m1_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2_t vlsb_v_i16m2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4_t vlsb_v_i16m4_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m8_t vlsb_v_i16m8_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlsb_v_i32m1_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlsb_v_i32m2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlsb_v_i32m4_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlsb_v_i32m8_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlsb_v_i64m1_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlsb_v_i64m2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlsb_v_i64m4_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlsb_v_i64m8_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1_t vlsbu_v_u8m1_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2_t vlsbu_v_u8m2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m4_t vlsbu_v_u8m4_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m8_t vlsbu_v_u8m8_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1_t vlsbu_v_u16m1_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2_t vlsbu_v_u16m2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4_t vlsbu_v_u16m4_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m8_t vlsbu_v_u16m8_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlsbu_v_u32m1_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlsbu_v_u32m2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlsbu_v_u32m4_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlsbu_v_u32m8_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlsbu_v_u64m1_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlsbu_v_u64m2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlsbu_v_u64m4_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlsbu_v_u64m8_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1_t vlsh_v_i16m1_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2_t vlsh_v_i16m2_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4_t vlsh_v_i16m4_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m8_t vlsh_v_i16m8_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlsh_v_i32m1_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlsh_v_i32m2_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlsh_v_i32m4_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlsh_v_i32m8_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlsh_v_i64m1_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlsh_v_i64m2_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlsh_v_i64m4_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlsh_v_i64m8_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1_t vlshu_v_u16m1_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2_t vlshu_v_u16m2_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4_t vlshu_v_u16m4_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m8_t vlshu_v_u16m8_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlshu_v_u32m1_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlshu_v_u32m2_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlshu_v_u32m4_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlshu_v_u32m8_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlshu_v_u64m1_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlshu_v_u64m2_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlshu_v_u64m4_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlshu_v_u64m8_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlsw_v_i32m1_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlsw_v_i32m2_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlsw_v_i32m4_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlsw_v_i32m8_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlsw_v_i64m1_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlsw_v_i64m2_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlsw_v_i64m4_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlsw_v_i64m8_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlswu_v_u32m1_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlswu_v_u32m2_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlswu_v_u32m4_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlswu_v_u32m8_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlswu_v_u64m1_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlswu_v_u64m2_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlswu_v_u64m4_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlswu_v_u64m8_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1_t vlse_v_i8m1_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2_t vlse_v_i8m2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m4_t vlse_v_i8m4_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m8_t vlse_v_i8m8_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1_t vlse_v_i16m1_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2_t vlse_v_i16m2_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4_t vlse_v_i16m4_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m8_t vlse_v_i16m8_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlse_v_i32m1_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlse_v_i32m2_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlse_v_i32m4_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlse_v_i32m8_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlse_v_i64m1_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlse_v_i64m2_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlse_v_i64m4_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlse_v_i64m8_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1_t vlse_v_u8m1_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2_t vlse_v_u8m2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m4_t vlse_v_u8m4_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m8_t vlse_v_u8m8_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1_t vlse_v_u16m1_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2_t vlse_v_u16m2_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4_t vlse_v_u16m4_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m8_t vlse_v_u16m8_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlse_v_u32m1_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlse_v_u32m2_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlse_v_u32m4_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlse_v_u32m8_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlse_v_u64m1_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlse_v_u64m2_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlse_v_u64m4_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlse_v_u64m8_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1_t vlse_v_f16m1_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2_t vlse_v_f16m2_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m4_t vlse_v_f16m4_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m8_t vlse_v_f16m8_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1_t vlse_v_f32m1_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2_t vlse_v_f32m2_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m4_t vlse_v_f32m4_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m8_t vlse_v_f32m8_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1_t vlse_v_f64m1_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2_t vlse_v_f64m2_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m4_t vlse_v_f64m4_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m8_t vlse_v_f64m8_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
// masked functions
vint8m1_t vlsb_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2_t vlsb_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m4_t vlsb_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m8_t vlsb_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1_t vlsb_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2_t vlsb_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4_t vlsb_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m8_t vlsb_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlsb_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlsb_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlsb_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlsb_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlsb_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlsb_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlsb_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlsb_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1_t vlsbu_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2_t vlsbu_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m4_t vlsbu_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m8_t vlsbu_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1_t vlsbu_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2_t vlsbu_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4_t vlsbu_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m8_t vlsbu_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlsbu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlsbu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlsbu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlsbu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlsbu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlsbu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlsbu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlsbu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1_t vlsh_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2_t vlsh_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4_t vlsh_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m8_t vlsh_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlsh_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlsh_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlsh_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlsh_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlsh_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlsh_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlsh_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlsh_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1_t vlshu_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2_t vlshu_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4_t vlshu_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m8_t vlshu_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlshu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlshu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlshu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlshu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlshu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlshu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlshu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlshu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlsw_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlsw_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlsw_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlsw_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlsw_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlsw_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlsw_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlsw_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlswu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlswu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlswu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlswu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlswu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlswu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlswu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlswu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1_t vlse_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2_t vlse_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m4_t vlse_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m8_t vlse_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1_t vlse_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2_t vlse_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4_t vlse_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m8_t vlse_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1_t vlse_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2_t vlse_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4_t vlse_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m8_t vlse_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1_t vlse_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2_t vlse_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4_t vlse_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m8_t vlse_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1_t vlse_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2_t vlse_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m4_t vlse_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m8_t vlse_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1_t vlse_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2_t vlse_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4_t vlse_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m8_t vlse_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1_t vlse_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2_t vlse_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4_t vlse_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m8_t vlse_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1_t vlse_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2_t vlse_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4_t vlse_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m8_t vlse_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1_t vlse_v_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2_t vlse_v_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m4_t vlse_v_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m8_t vlse_v_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1_t vlse_v_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2_t vlse_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m4_t vlse_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m8_t vlse_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1_t vlse_v_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2_t vlse_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m4_t vlse_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m8_t vlse_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
```
### [Vector Strided Store Functions](rvv-intrinsic-api.md#75-vector-strided-loadstore-operations):

**Prototypes:**
``` C
void vssb_v_i8m1_vl (int8_t *base, ptrdiff_t bstride, vint8m1_t value, _VL_T vl);
void vssb_v_i8m2_vl (int8_t *base, ptrdiff_t bstride, vint8m2_t value, _VL_T vl);
void vssb_v_i8m4_vl (int8_t *base, ptrdiff_t bstride, vint8m4_t value, _VL_T vl);
void vssb_v_i8m8_vl (int8_t *base, ptrdiff_t bstride, vint8m8_t value, _VL_T vl);
void vssb_v_i16m1_vl (int8_t *base, ptrdiff_t bstride, vint16m1_t value, _VL_T vl);
void vssb_v_i16m2_vl (int8_t *base, ptrdiff_t bstride, vint16m2_t value, _VL_T vl);
void vssb_v_i16m4_vl (int8_t *base, ptrdiff_t bstride, vint16m4_t value, _VL_T vl);
void vssb_v_i16m8_vl (int8_t *base, ptrdiff_t bstride, vint16m8_t value, _VL_T vl);
void vssb_v_i32m1_vl (int8_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vssb_v_i32m2_vl (int8_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vssb_v_i32m4_vl (int8_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vssb_v_i32m8_vl (int8_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vssb_v_i64m1_vl (int8_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vssb_v_i64m2_vl (int8_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vssb_v_i64m4_vl (int8_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vssb_v_i64m8_vl (int8_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vssb_v_u8m1_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1_t value, _VL_T vl);
void vssb_v_u8m2_vl (uint8_t *base, ptrdiff_t bstride, vuint8m2_t value, _VL_T vl);
void vssb_v_u8m4_vl (uint8_t *base, ptrdiff_t bstride, vuint8m4_t value, _VL_T vl);
void vssb_v_u8m8_vl (uint8_t *base, ptrdiff_t bstride, vuint8m8_t value, _VL_T vl);
void vssb_v_u16m1_vl (uint8_t *base, ptrdiff_t bstride, vuint16m1_t value, _VL_T vl);
void vssb_v_u16m2_vl (uint8_t *base, ptrdiff_t bstride, vuint16m2_t value, _VL_T vl);
void vssb_v_u16m4_vl (uint8_t *base, ptrdiff_t bstride, vuint16m4_t value, _VL_T vl);
void vssb_v_u16m8_vl (uint8_t *base, ptrdiff_t bstride, vuint16m8_t value, _VL_T vl);
void vssb_v_u32m1_vl (uint8_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vssb_v_u32m2_vl (uint8_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vssb_v_u32m4_vl (uint8_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vssb_v_u32m8_vl (uint8_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vssb_v_u64m1_vl (uint8_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vssb_v_u64m2_vl (uint8_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vssb_v_u64m4_vl (uint8_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vssb_v_u64m8_vl (uint8_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vssh_v_i16m1_vl (int16_t *base, ptrdiff_t bstride, vint16m1_t value, _VL_T vl);
void vssh_v_i16m2_vl (int16_t *base, ptrdiff_t bstride, vint16m2_t value, _VL_T vl);
void vssh_v_i16m4_vl (int16_t *base, ptrdiff_t bstride, vint16m4_t value, _VL_T vl);
void vssh_v_i16m8_vl (int16_t *base, ptrdiff_t bstride, vint16m8_t value, _VL_T vl);
void vssh_v_i32m1_vl (int16_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vssh_v_i32m2_vl (int16_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vssh_v_i32m4_vl (int16_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vssh_v_i32m8_vl (int16_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vssh_v_i64m1_vl (int16_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vssh_v_i64m2_vl (int16_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vssh_v_i64m4_vl (int16_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vssh_v_i64m8_vl (int16_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vssh_v_u16m1_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1_t value, _VL_T vl);
void vssh_v_u16m2_vl (uint16_t *base, ptrdiff_t bstride, vuint16m2_t value, _VL_T vl);
void vssh_v_u16m4_vl (uint16_t *base, ptrdiff_t bstride, vuint16m4_t value, _VL_T vl);
void vssh_v_u16m8_vl (uint16_t *base, ptrdiff_t bstride, vuint16m8_t value, _VL_T vl);
void vssh_v_u32m1_vl (uint16_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vssh_v_u32m2_vl (uint16_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vssh_v_u32m4_vl (uint16_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vssh_v_u32m8_vl (uint16_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vssh_v_u64m1_vl (uint16_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vssh_v_u64m2_vl (uint16_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vssh_v_u64m4_vl (uint16_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vssh_v_u64m8_vl (uint16_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vssw_v_i32m1_vl (int32_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vssw_v_i32m2_vl (int32_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vssw_v_i32m4_vl (int32_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vssw_v_i32m8_vl (int32_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vssw_v_i64m1_vl (int32_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vssw_v_i64m2_vl (int32_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vssw_v_i64m4_vl (int32_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vssw_v_i64m8_vl (int32_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vssw_v_u32m1_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vssw_v_u32m2_vl (uint32_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vssw_v_u32m4_vl (uint32_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vssw_v_u32m8_vl (uint32_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vssw_v_u64m1_vl (uint32_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vssw_v_u64m2_vl (uint32_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vssw_v_u64m4_vl (uint32_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vssw_v_u64m8_vl (uint32_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vsse_v_i8m1_vl (int8_t *base, ptrdiff_t bstride, vint8m1_t value, _VL_T vl);
void vsse_v_i8m2_vl (int8_t *base, ptrdiff_t bstride, vint8m2_t value, _VL_T vl);
void vsse_v_i8m4_vl (int8_t *base, ptrdiff_t bstride, vint8m4_t value, _VL_T vl);
void vsse_v_i8m8_vl (int8_t *base, ptrdiff_t bstride, vint8m8_t value, _VL_T vl);
void vsse_v_i16m1_vl (int16_t *base, ptrdiff_t bstride, vint16m1_t value, _VL_T vl);
void vsse_v_i16m2_vl (int16_t *base, ptrdiff_t bstride, vint16m2_t value, _VL_T vl);
void vsse_v_i16m4_vl (int16_t *base, ptrdiff_t bstride, vint16m4_t value, _VL_T vl);
void vsse_v_i16m8_vl (int16_t *base, ptrdiff_t bstride, vint16m8_t value, _VL_T vl);
void vsse_v_i32m1_vl (int32_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vsse_v_i32m2_vl (int32_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vsse_v_i32m4_vl (int32_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vsse_v_i32m8_vl (int32_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vsse_v_i64m1_vl (int64_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vsse_v_i64m2_vl (int64_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vsse_v_i64m4_vl (int64_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vsse_v_i64m8_vl (int64_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vsse_v_u8m1_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1_t value, _VL_T vl);
void vsse_v_u8m2_vl (uint8_t *base, ptrdiff_t bstride, vuint8m2_t value, _VL_T vl);
void vsse_v_u8m4_vl (uint8_t *base, ptrdiff_t bstride, vuint8m4_t value, _VL_T vl);
void vsse_v_u8m8_vl (uint8_t *base, ptrdiff_t bstride, vuint8m8_t value, _VL_T vl);
void vsse_v_u16m1_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1_t value, _VL_T vl);
void vsse_v_u16m2_vl (uint16_t *base, ptrdiff_t bstride, vuint16m2_t value, _VL_T vl);
void vsse_v_u16m4_vl (uint16_t *base, ptrdiff_t bstride, vuint16m4_t value, _VL_T vl);
void vsse_v_u16m8_vl (uint16_t *base, ptrdiff_t bstride, vuint16m8_t value, _VL_T vl);
void vsse_v_u32m1_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vsse_v_u32m2_vl (uint32_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vsse_v_u32m4_vl (uint32_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vsse_v_u32m8_vl (uint32_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vsse_v_u64m1_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vsse_v_u64m2_vl (uint64_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vsse_v_u64m4_vl (uint64_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vsse_v_u64m8_vl (uint64_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vsse_v_f16m1_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1_t value, _VL_T vl);
void vsse_v_f16m2_vl (float16_t *base, ptrdiff_t bstride, vfloat16m2_t value, _VL_T vl);
void vsse_v_f16m4_vl (float16_t *base, ptrdiff_t bstride, vfloat16m4_t value, _VL_T vl);
void vsse_v_f16m8_vl (float16_t *base, ptrdiff_t bstride, vfloat16m8_t value, _VL_T vl);
void vsse_v_f32m1_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1_t value, _VL_T vl);
void vsse_v_f32m2_vl (float32_t *base, ptrdiff_t bstride, vfloat32m2_t value, _VL_T vl);
void vsse_v_f32m4_vl (float32_t *base, ptrdiff_t bstride, vfloat32m4_t value, _VL_T vl);
void vsse_v_f32m8_vl (float32_t *base, ptrdiff_t bstride, vfloat32m8_t value, _VL_T vl);
void vsse_v_f64m1_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1_t value, _VL_T vl);
void vsse_v_f64m2_vl (float64_t *base, ptrdiff_t bstride, vfloat64m2_t value, _VL_T vl);
void vsse_v_f64m4_vl (float64_t *base, ptrdiff_t bstride, vfloat64m4_t value, _VL_T vl);
void vsse_v_f64m8_vl (float64_t *base, ptrdiff_t bstride, vfloat64m8_t value, _VL_T vl);
// masked functions
void vssb_v_i8m1_m_vl (vbool8_t mask, int8_t *base, ptrdiff_t bstride, vint8m1_t value, _VL_T vl);
void vssb_v_i8m2_m_vl (vbool4_t mask, int8_t *base, ptrdiff_t bstride, vint8m2_t value, _VL_T vl);
void vssb_v_i8m4_m_vl (vbool2_t mask, int8_t *base, ptrdiff_t bstride, vint8m4_t value, _VL_T vl);
void vssb_v_i8m8_m_vl (vbool1_t mask, int8_t *base, ptrdiff_t bstride, vint8m8_t value, _VL_T vl);
void vssb_v_i16m1_m_vl (vbool16_t mask, int8_t *base, ptrdiff_t bstride, vint16m1_t value, _VL_T vl);
void vssb_v_i16m2_m_vl (vbool8_t mask, int8_t *base, ptrdiff_t bstride, vint16m2_t value, _VL_T vl);
void vssb_v_i16m4_m_vl (vbool4_t mask, int8_t *base, ptrdiff_t bstride, vint16m4_t value, _VL_T vl);
void vssb_v_i16m8_m_vl (vbool2_t mask, int8_t *base, ptrdiff_t bstride, vint16m8_t value, _VL_T vl);
void vssb_v_i32m1_m_vl (vbool32_t mask, int8_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vssb_v_i32m2_m_vl (vbool16_t mask, int8_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vssb_v_i32m4_m_vl (vbool8_t mask, int8_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vssb_v_i32m8_m_vl (vbool4_t mask, int8_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vssb_v_i64m1_m_vl (vbool64_t mask, int8_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vssb_v_i64m2_m_vl (vbool32_t mask, int8_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vssb_v_i64m4_m_vl (vbool16_t mask, int8_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vssb_v_i64m8_m_vl (vbool8_t mask, int8_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vssb_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m1_t value, _VL_T vl);
void vssb_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m2_t value, _VL_T vl);
void vssb_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m4_t value, _VL_T vl);
void vssb_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m8_t value, _VL_T vl);
void vssb_v_u16m1_m_vl (vbool16_t mask, uint8_t *base, ptrdiff_t bstride, vuint16m1_t value, _VL_T vl);
void vssb_v_u16m2_m_vl (vbool8_t mask, uint8_t *base, ptrdiff_t bstride, vuint16m2_t value, _VL_T vl);
void vssb_v_u16m4_m_vl (vbool4_t mask, uint8_t *base, ptrdiff_t bstride, vuint16m4_t value, _VL_T vl);
void vssb_v_u16m8_m_vl (vbool2_t mask, uint8_t *base, ptrdiff_t bstride, vuint16m8_t value, _VL_T vl);
void vssb_v_u32m1_m_vl (vbool32_t mask, uint8_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vssb_v_u32m2_m_vl (vbool16_t mask, uint8_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vssb_v_u32m4_m_vl (vbool8_t mask, uint8_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vssb_v_u32m8_m_vl (vbool4_t mask, uint8_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vssb_v_u64m1_m_vl (vbool64_t mask, uint8_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vssb_v_u64m2_m_vl (vbool32_t mask, uint8_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vssb_v_u64m4_m_vl (vbool16_t mask, uint8_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vssb_v_u64m8_m_vl (vbool8_t mask, uint8_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vssh_v_i16m1_m_vl (vbool16_t mask, int16_t *base, ptrdiff_t bstride, vint16m1_t value, _VL_T vl);
void vssh_v_i16m2_m_vl (vbool8_t mask, int16_t *base, ptrdiff_t bstride, vint16m2_t value, _VL_T vl);
void vssh_v_i16m4_m_vl (vbool4_t mask, int16_t *base, ptrdiff_t bstride, vint16m4_t value, _VL_T vl);
void vssh_v_i16m8_m_vl (vbool2_t mask, int16_t *base, ptrdiff_t bstride, vint16m8_t value, _VL_T vl);
void vssh_v_i32m1_m_vl (vbool32_t mask, int16_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vssh_v_i32m2_m_vl (vbool16_t mask, int16_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vssh_v_i32m4_m_vl (vbool8_t mask, int16_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vssh_v_i32m8_m_vl (vbool4_t mask, int16_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vssh_v_i64m1_m_vl (vbool64_t mask, int16_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vssh_v_i64m2_m_vl (vbool32_t mask, int16_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vssh_v_i64m4_m_vl (vbool16_t mask, int16_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vssh_v_i64m8_m_vl (vbool8_t mask, int16_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vssh_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m1_t value, _VL_T vl);
void vssh_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m2_t value, _VL_T vl);
void vssh_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m4_t value, _VL_T vl);
void vssh_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m8_t value, _VL_T vl);
void vssh_v_u32m1_m_vl (vbool32_t mask, uint16_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vssh_v_u32m2_m_vl (vbool16_t mask, uint16_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vssh_v_u32m4_m_vl (vbool8_t mask, uint16_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vssh_v_u32m8_m_vl (vbool4_t mask, uint16_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vssh_v_u64m1_m_vl (vbool64_t mask, uint16_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vssh_v_u64m2_m_vl (vbool32_t mask, uint16_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vssh_v_u64m4_m_vl (vbool16_t mask, uint16_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vssh_v_u64m8_m_vl (vbool8_t mask, uint16_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vssw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vssw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vssw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vssw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vssw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vssw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vssw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vssw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vssw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vssw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vssw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vssw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vssw_v_u64m1_m_vl (vbool64_t mask, uint32_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vssw_v_u64m2_m_vl (vbool32_t mask, uint32_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vssw_v_u64m4_m_vl (vbool16_t mask, uint32_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vssw_v_u64m8_m_vl (vbool8_t mask, uint32_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vsse_v_i8m1_m_vl (vbool8_t mask, int8_t *base, ptrdiff_t bstride, vint8m1_t value, _VL_T vl);
void vsse_v_i8m2_m_vl (vbool4_t mask, int8_t *base, ptrdiff_t bstride, vint8m2_t value, _VL_T vl);
void vsse_v_i8m4_m_vl (vbool2_t mask, int8_t *base, ptrdiff_t bstride, vint8m4_t value, _VL_T vl);
void vsse_v_i8m8_m_vl (vbool1_t mask, int8_t *base, ptrdiff_t bstride, vint8m8_t value, _VL_T vl);
void vsse_v_i16m1_m_vl (vbool16_t mask, int16_t *base, ptrdiff_t bstride, vint16m1_t value, _VL_T vl);
void vsse_v_i16m2_m_vl (vbool8_t mask, int16_t *base, ptrdiff_t bstride, vint16m2_t value, _VL_T vl);
void vsse_v_i16m4_m_vl (vbool4_t mask, int16_t *base, ptrdiff_t bstride, vint16m4_t value, _VL_T vl);
void vsse_v_i16m8_m_vl (vbool2_t mask, int16_t *base, ptrdiff_t bstride, vint16m8_t value, _VL_T vl);
void vsse_v_i32m1_m_vl (vbool32_t mask, int32_t *base, ptrdiff_t bstride, vint32m1_t value, _VL_T vl);
void vsse_v_i32m2_m_vl (vbool16_t mask, int32_t *base, ptrdiff_t bstride, vint32m2_t value, _VL_T vl);
void vsse_v_i32m4_m_vl (vbool8_t mask, int32_t *base, ptrdiff_t bstride, vint32m4_t value, _VL_T vl);
void vsse_v_i32m8_m_vl (vbool4_t mask, int32_t *base, ptrdiff_t bstride, vint32m8_t value, _VL_T vl);
void vsse_v_i64m1_m_vl (vbool64_t mask, int64_t *base, ptrdiff_t bstride, vint64m1_t value, _VL_T vl);
void vsse_v_i64m2_m_vl (vbool32_t mask, int64_t *base, ptrdiff_t bstride, vint64m2_t value, _VL_T vl);
void vsse_v_i64m4_m_vl (vbool16_t mask, int64_t *base, ptrdiff_t bstride, vint64m4_t value, _VL_T vl);
void vsse_v_i64m8_m_vl (vbool8_t mask, int64_t *base, ptrdiff_t bstride, vint64m8_t value, _VL_T vl);
void vsse_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m1_t value, _VL_T vl);
void vsse_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m2_t value, _VL_T vl);
void vsse_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m4_t value, _VL_T vl);
void vsse_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, ptrdiff_t bstride, vuint8m8_t value, _VL_T vl);
void vsse_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m1_t value, _VL_T vl);
void vsse_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m2_t value, _VL_T vl);
void vsse_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m4_t value, _VL_T vl);
void vsse_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, ptrdiff_t bstride, vuint16m8_t value, _VL_T vl);
void vsse_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m1_t value, _VL_T vl);
void vsse_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m2_t value, _VL_T vl);
void vsse_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m4_t value, _VL_T vl);
void vsse_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, ptrdiff_t bstride, vuint32m8_t value, _VL_T vl);
void vsse_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, ptrdiff_t bstride, vuint64m1_t value, _VL_T vl);
void vsse_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, ptrdiff_t bstride, vuint64m2_t value, _VL_T vl);
void vsse_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, ptrdiff_t bstride, vuint64m4_t value, _VL_T vl);
void vsse_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, ptrdiff_t bstride, vuint64m8_t value, _VL_T vl);
void vsse_v_f16m1_m_vl (vbool16_t mask, float16_t *base, ptrdiff_t bstride, vfloat16m1_t value, _VL_T vl);
void vsse_v_f16m2_m_vl (vbool8_t mask, float16_t *base, ptrdiff_t bstride, vfloat16m2_t value, _VL_T vl);
void vsse_v_f16m4_m_vl (vbool4_t mask, float16_t *base, ptrdiff_t bstride, vfloat16m4_t value, _VL_T vl);
void vsse_v_f16m8_m_vl (vbool2_t mask, float16_t *base, ptrdiff_t bstride, vfloat16m8_t value, _VL_T vl);
void vsse_v_f32m1_m_vl (vbool32_t mask, float32_t *base, ptrdiff_t bstride, vfloat32m1_t value, _VL_T vl);
void vsse_v_f32m2_m_vl (vbool16_t mask, float32_t *base, ptrdiff_t bstride, vfloat32m2_t value, _VL_T vl);
void vsse_v_f32m4_m_vl (vbool8_t mask, float32_t *base, ptrdiff_t bstride, vfloat32m4_t value, _VL_T vl);
void vsse_v_f32m8_m_vl (vbool4_t mask, float32_t *base, ptrdiff_t bstride, vfloat32m8_t value, _VL_T vl);
void vsse_v_f64m1_m_vl (vbool64_t mask, float64_t *base, ptrdiff_t bstride, vfloat64m1_t value, _VL_T vl);
void vsse_v_f64m2_m_vl (vbool32_t mask, float64_t *base, ptrdiff_t bstride, vfloat64m2_t value, _VL_T vl);
void vsse_v_f64m4_m_vl (vbool16_t mask, float64_t *base, ptrdiff_t bstride, vfloat64m4_t value, _VL_T vl);
void vsse_v_f64m8_m_vl (vbool8_t mask, float64_t *base, ptrdiff_t bstride, vfloat64m8_t value, _VL_T vl);
```
### [Vector Indexed Load Functions](rvv-intrinsic-api.md#76-vector-indexed-loadstore-operations):

**Prototypes:**
``` C
vint8m1_t vlxb_v_i8m1_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m2_t vlxb_v_i8m2_vl (const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m4_t vlxb_v_i8m4_vl (const int8_t *base, vuint8m4_t bindex, _VL_T vl);
vint8m8_t vlxb_v_i8m8_vl (const int8_t *base, vuint8m8_t bindex, _VL_T vl);
vint16m1_t vlxb_v_i16m1_vl (const int8_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2_t vlxb_v_i16m2_vl (const int8_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4_t vlxb_v_i16m4_vl (const int8_t *base, vuint16m4_t bindex, _VL_T vl);
vint16m8_t vlxb_v_i16m8_vl (const int8_t *base, vuint16m8_t bindex, _VL_T vl);
vint32m1_t vlxb_v_i32m1_vl (const int8_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxb_v_i32m2_vl (const int8_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxb_v_i32m4_vl (const int8_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxb_v_i32m8_vl (const int8_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxb_v_i64m1_vl (const int8_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxb_v_i64m2_vl (const int8_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxb_v_i64m4_vl (const int8_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxb_v_i64m8_vl (const int8_t *base, vuint64m8_t bindex, _VL_T vl);
vuint8m1_t vlxbu_v_u8m1_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m2_t vlxbu_v_u8m2_vl (const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m4_t vlxbu_v_u8m4_vl (const uint8_t *base, vuint8m4_t bindex, _VL_T vl);
vuint8m8_t vlxbu_v_u8m8_vl (const uint8_t *base, vuint8m8_t bindex, _VL_T vl);
vuint16m1_t vlxbu_v_u16m1_vl (const uint8_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2_t vlxbu_v_u16m2_vl (const uint8_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4_t vlxbu_v_u16m4_vl (const uint8_t *base, vuint16m4_t bindex, _VL_T vl);
vuint16m8_t vlxbu_v_u16m8_vl (const uint8_t *base, vuint16m8_t bindex, _VL_T vl);
vuint32m1_t vlxbu_v_u32m1_vl (const uint8_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxbu_v_u32m2_vl (const uint8_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxbu_v_u32m4_vl (const uint8_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxbu_v_u32m8_vl (const uint8_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxbu_v_u64m1_vl (const uint8_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxbu_v_u64m2_vl (const uint8_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxbu_v_u64m4_vl (const uint8_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxbu_v_u64m8_vl (const uint8_t *base, vuint64m8_t bindex, _VL_T vl);
vint16m1_t vlxh_v_i16m1_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2_t vlxh_v_i16m2_vl (const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4_t vlxh_v_i16m4_vl (const int16_t *base, vuint16m4_t bindex, _VL_T vl);
vint16m8_t vlxh_v_i16m8_vl (const int16_t *base, vuint16m8_t bindex, _VL_T vl);
vint32m1_t vlxh_v_i32m1_vl (const int16_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxh_v_i32m2_vl (const int16_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxh_v_i32m4_vl (const int16_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxh_v_i32m8_vl (const int16_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxh_v_i64m1_vl (const int16_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxh_v_i64m2_vl (const int16_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxh_v_i64m4_vl (const int16_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxh_v_i64m8_vl (const int16_t *base, vuint64m8_t bindex, _VL_T vl);
vuint16m1_t vlxhu_v_u16m1_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2_t vlxhu_v_u16m2_vl (const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4_t vlxhu_v_u16m4_vl (const uint16_t *base, vuint16m4_t bindex, _VL_T vl);
vuint16m8_t vlxhu_v_u16m8_vl (const uint16_t *base, vuint16m8_t bindex, _VL_T vl);
vuint32m1_t vlxhu_v_u32m1_vl (const uint16_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxhu_v_u32m2_vl (const uint16_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxhu_v_u32m4_vl (const uint16_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxhu_v_u32m8_vl (const uint16_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxhu_v_u64m1_vl (const uint16_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxhu_v_u64m2_vl (const uint16_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxhu_v_u64m4_vl (const uint16_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxhu_v_u64m8_vl (const uint16_t *base, vuint64m8_t bindex, _VL_T vl);
vint32m1_t vlxw_v_i32m1_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxw_v_i32m2_vl (const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxw_v_i32m4_vl (const int32_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxw_v_i32m8_vl (const int32_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxw_v_i64m1_vl (const int32_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxw_v_i64m2_vl (const int32_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxw_v_i64m4_vl (const int32_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxw_v_i64m8_vl (const int32_t *base, vuint64m8_t bindex, _VL_T vl);
vuint32m1_t vlxwu_v_u32m1_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxwu_v_u32m2_vl (const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxwu_v_u32m4_vl (const uint32_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxwu_v_u32m8_vl (const uint32_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxwu_v_u64m1_vl (const uint32_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxwu_v_u64m2_vl (const uint32_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxwu_v_u64m4_vl (const uint32_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxwu_v_u64m8_vl (const uint32_t *base, vuint64m8_t bindex, _VL_T vl);
vint8m1_t vlxe_v_i8m1_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m2_t vlxe_v_i8m2_vl (const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m4_t vlxe_v_i8m4_vl (const int8_t *base, vuint8m4_t bindex, _VL_T vl);
vint8m8_t vlxe_v_i8m8_vl (const int8_t *base, vuint8m8_t bindex, _VL_T vl);
vint16m1_t vlxe_v_i16m1_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2_t vlxe_v_i16m2_vl (const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4_t vlxe_v_i16m4_vl (const int16_t *base, vuint16m4_t bindex, _VL_T vl);
vint16m8_t vlxe_v_i16m8_vl (const int16_t *base, vuint16m8_t bindex, _VL_T vl);
vint32m1_t vlxe_v_i32m1_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxe_v_i32m2_vl (const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxe_v_i32m4_vl (const int32_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxe_v_i32m8_vl (const int32_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxe_v_i64m1_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxe_v_i64m2_vl (const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxe_v_i64m4_vl (const int64_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxe_v_i64m8_vl (const int64_t *base, vuint64m8_t bindex, _VL_T vl);
vuint8m1_t vlxe_v_u8m1_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m2_t vlxe_v_u8m2_vl (const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m4_t vlxe_v_u8m4_vl (const uint8_t *base, vuint8m4_t bindex, _VL_T vl);
vuint8m8_t vlxe_v_u8m8_vl (const uint8_t *base, vuint8m8_t bindex, _VL_T vl);
vuint16m1_t vlxe_v_u16m1_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2_t vlxe_v_u16m2_vl (const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4_t vlxe_v_u16m4_vl (const uint16_t *base, vuint16m4_t bindex, _VL_T vl);
vuint16m8_t vlxe_v_u16m8_vl (const uint16_t *base, vuint16m8_t bindex, _VL_T vl);
vuint32m1_t vlxe_v_u32m1_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxe_v_u32m2_vl (const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxe_v_u32m4_vl (const uint32_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxe_v_u32m8_vl (const uint32_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxe_v_u64m1_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxe_v_u64m2_vl (const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxe_v_u64m4_vl (const uint64_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxe_v_u64m8_vl (const uint64_t *base, vuint64m8_t bindex, _VL_T vl);
vfloat16m1_t vlxe_v_f16m1_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m2_t vlxe_v_f16m2_vl (const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m4_t vlxe_v_f16m4_vl (const float16_t *base, vuint16m4_t bindex, _VL_T vl);
vfloat16m8_t vlxe_v_f16m8_vl (const float16_t *base, vuint16m8_t bindex, _VL_T vl);
vfloat32m1_t vlxe_v_f32m1_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m2_t vlxe_v_f32m2_vl (const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m4_t vlxe_v_f32m4_vl (const float32_t *base, vuint32m4_t bindex, _VL_T vl);
vfloat32m8_t vlxe_v_f32m8_vl (const float32_t *base, vuint32m8_t bindex, _VL_T vl);
vfloat64m1_t vlxe_v_f64m1_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m2_t vlxe_v_f64m2_vl (const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m4_t vlxe_v_f64m4_vl (const float64_t *base, vuint64m4_t bindex, _VL_T vl);
vfloat64m8_t vlxe_v_f64m8_vl (const float64_t *base, vuint64m8_t bindex, _VL_T vl);
// masked functions
vint8m1_t vlxb_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m2_t vlxb_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m4_t vlxb_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, vuint8m4_t bindex, _VL_T vl);
vint8m8_t vlxb_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, vuint8m8_t bindex, _VL_T vl);
vint16m1_t vlxb_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int8_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2_t vlxb_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int8_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4_t vlxb_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int8_t *base, vuint16m4_t bindex, _VL_T vl);
vint16m8_t vlxb_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int8_t *base, vuint16m8_t bindex, _VL_T vl);
vint32m1_t vlxb_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int8_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxb_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int8_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxb_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int8_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxb_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int8_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxb_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int8_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxb_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int8_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxb_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int8_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxb_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int8_t *base, vuint64m8_t bindex, _VL_T vl);
vuint8m1_t vlxbu_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m2_t vlxbu_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m4_t vlxbu_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, vuint8m4_t bindex, _VL_T vl);
vuint8m8_t vlxbu_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, vuint8m8_t bindex, _VL_T vl);
vuint16m1_t vlxbu_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint8_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2_t vlxbu_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint8_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4_t vlxbu_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint8_t *base, vuint16m4_t bindex, _VL_T vl);
vuint16m8_t vlxbu_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint8_t *base, vuint16m8_t bindex, _VL_T vl);
vuint32m1_t vlxbu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint8_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxbu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint8_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxbu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint8_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxbu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint8_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxbu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint8_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxbu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint8_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxbu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint8_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxbu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint8_t *base, vuint64m8_t bindex, _VL_T vl);
vint16m1_t vlxh_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2_t vlxh_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4_t vlxh_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, vuint16m4_t bindex, _VL_T vl);
vint16m8_t vlxh_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, vuint16m8_t bindex, _VL_T vl);
vint32m1_t vlxh_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int16_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxh_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int16_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxh_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int16_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxh_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int16_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxh_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int16_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxh_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int16_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxh_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int16_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxh_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int16_t *base, vuint64m8_t bindex, _VL_T vl);
vuint16m1_t vlxhu_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2_t vlxhu_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4_t vlxhu_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, vuint16m4_t bindex, _VL_T vl);
vuint16m8_t vlxhu_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, vuint16m8_t bindex, _VL_T vl);
vuint32m1_t vlxhu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint16_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxhu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint16_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxhu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint16_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxhu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint16_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxhu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint16_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxhu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint16_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxhu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint16_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxhu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint16_t *base, vuint64m8_t bindex, _VL_T vl);
vint32m1_t vlxw_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxw_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxw_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxw_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxw_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int32_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxw_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int32_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxw_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int32_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxw_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int32_t *base, vuint64m8_t bindex, _VL_T vl);
vuint32m1_t vlxwu_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxwu_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxwu_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxwu_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxwu_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint32_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxwu_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint32_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxwu_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint32_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxwu_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint32_t *base, vuint64m8_t bindex, _VL_T vl);
vint8m1_t vlxe_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m2_t vlxe_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m4_t vlxe_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, vuint8m4_t bindex, _VL_T vl);
vint8m8_t vlxe_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, vuint8m8_t bindex, _VL_T vl);
vint16m1_t vlxe_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2_t vlxe_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4_t vlxe_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, vuint16m4_t bindex, _VL_T vl);
vint16m8_t vlxe_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, vuint16m8_t bindex, _VL_T vl);
vint32m1_t vlxe_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2_t vlxe_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4_t vlxe_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, vuint32m4_t bindex, _VL_T vl);
vint32m8_t vlxe_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, vuint32m8_t bindex, _VL_T vl);
vint64m1_t vlxe_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2_t vlxe_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4_t vlxe_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int64_t *base, vuint64m4_t bindex, _VL_T vl);
vint64m8_t vlxe_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int64_t *base, vuint64m8_t bindex, _VL_T vl);
vuint8m1_t vlxe_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m2_t vlxe_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m4_t vlxe_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, vuint8m4_t bindex, _VL_T vl);
vuint8m8_t vlxe_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, vuint8m8_t bindex, _VL_T vl);
vuint16m1_t vlxe_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2_t vlxe_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4_t vlxe_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, vuint16m4_t bindex, _VL_T vl);
vuint16m8_t vlxe_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, vuint16m8_t bindex, _VL_T vl);
vuint32m1_t vlxe_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2_t vlxe_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4_t vlxe_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, vuint32m4_t bindex, _VL_T vl);
vuint32m8_t vlxe_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, vuint32m8_t bindex, _VL_T vl);
vuint64m1_t vlxe_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2_t vlxe_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4_t vlxe_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint64_t *base, vuint64m4_t bindex, _VL_T vl);
vuint64m8_t vlxe_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint64_t *base, vuint64m8_t bindex, _VL_T vl);
vfloat16m1_t vlxe_v_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m2_t vlxe_v_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m4_t vlxe_v_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, const float16_t *base, vuint16m4_t bindex, _VL_T vl);
vfloat16m8_t vlxe_v_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, const float16_t *base, vuint16m8_t bindex, _VL_T vl);
vfloat32m1_t vlxe_v_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m2_t vlxe_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m4_t vlxe_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, const float32_t *base, vuint32m4_t bindex, _VL_T vl);
vfloat32m8_t vlxe_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, const float32_t *base, vuint32m8_t bindex, _VL_T vl);
vfloat64m1_t vlxe_v_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m2_t vlxe_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m4_t vlxe_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, const float64_t *base, vuint64m4_t bindex, _VL_T vl);
vfloat64m8_t vlxe_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, const float64_t *base, vuint64m8_t bindex, _VL_T vl);
```
### [Vector Indexed Store Functions](rvv-intrinsic-api.md#76-vector-indexed-loadstore-operations):

**Prototypes:**
``` C
void vsxb_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
void vsxb_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
void vsxb_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
void vsxb_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
void vsxb_v_i16m1_vl (int8_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsxb_v_i16m2_vl (int8_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsxb_v_i16m4_vl (int8_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsxb_v_i16m8_vl (int8_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsxb_v_i32m1_vl (int8_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxb_v_i32m2_vl (int8_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxb_v_i32m4_vl (int8_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxb_v_i32m8_vl (int8_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxb_v_i64m1_vl (int8_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxb_v_i64m2_vl (int8_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxb_v_i64m4_vl (int8_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxb_v_i64m8_vl (int8_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxb_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
void vsxb_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
void vsxb_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
void vsxb_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
void vsxb_v_u16m1_vl (uint8_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsxb_v_u16m2_vl (uint8_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsxb_v_u16m4_vl (uint8_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsxb_v_u16m8_vl (uint8_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsxb_v_u32m1_vl (uint8_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxb_v_u32m2_vl (uint8_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxb_v_u32m4_vl (uint8_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxb_v_u32m8_vl (uint8_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxb_v_u64m1_vl (uint8_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxb_v_u64m2_vl (uint8_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxb_v_u64m4_vl (uint8_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxb_v_u64m8_vl (uint8_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxh_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsxh_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsxh_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsxh_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsxh_v_i32m1_vl (int16_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxh_v_i32m2_vl (int16_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxh_v_i32m4_vl (int16_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxh_v_i32m8_vl (int16_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxh_v_i64m1_vl (int16_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxh_v_i64m2_vl (int16_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxh_v_i64m4_vl (int16_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxh_v_i64m8_vl (int16_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxh_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsxh_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsxh_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsxh_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsxh_v_u32m1_vl (uint16_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxh_v_u32m2_vl (uint16_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxh_v_u32m4_vl (uint16_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxh_v_u32m8_vl (uint16_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxh_v_u64m1_vl (uint16_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxh_v_u64m2_vl (uint16_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxh_v_u64m4_vl (uint16_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxh_v_u64m8_vl (uint16_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxw_v_u64m1_vl (uint32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxw_v_u64m2_vl (uint32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxw_v_u64m4_vl (uint32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxw_v_u64m8_vl (uint32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxe_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
void vsxe_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
void vsxe_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
void vsxe_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
void vsxe_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsxe_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsxe_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsxe_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsxe_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxe_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxe_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxe_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxe_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxe_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxe_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxe_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxe_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
void vsxe_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
void vsxe_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
void vsxe_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
void vsxe_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsxe_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsxe_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsxe_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsxe_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxe_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxe_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxe_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxe_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxe_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxe_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxe_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxe_v_f16m1_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1_t value, _VL_T vl);
void vsxe_v_f16m2_vl (float16_t *base, vuint16m2_t bindex, vfloat16m2_t value, _VL_T vl);
void vsxe_v_f16m4_vl (float16_t *base, vuint16m4_t bindex, vfloat16m4_t value, _VL_T vl);
void vsxe_v_f16m8_vl (float16_t *base, vuint16m8_t bindex, vfloat16m8_t value, _VL_T vl);
void vsxe_v_f32m1_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1_t value, _VL_T vl);
void vsxe_v_f32m2_vl (float32_t *base, vuint32m2_t bindex, vfloat32m2_t value, _VL_T vl);
void vsxe_v_f32m4_vl (float32_t *base, vuint32m4_t bindex, vfloat32m4_t value, _VL_T vl);
void vsxe_v_f32m8_vl (float32_t *base, vuint32m8_t bindex, vfloat32m8_t value, _VL_T vl);
void vsxe_v_f64m1_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1_t value, _VL_T vl);
void vsxe_v_f64m2_vl (float64_t *base, vuint64m2_t bindex, vfloat64m2_t value, _VL_T vl);
void vsxe_v_f64m4_vl (float64_t *base, vuint64m4_t bindex, vfloat64m4_t value, _VL_T vl);
void vsxe_v_f64m8_vl (float64_t *base, vuint64m8_t bindex, vfloat64m8_t value, _VL_T vl);
void vsuxe_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
void vsuxe_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
void vsuxe_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
void vsuxe_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
void vsuxe_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsuxe_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsuxe_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsuxe_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsuxe_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsuxe_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsuxe_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsuxe_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsuxe_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsuxe_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsuxe_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsuxe_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsuxe_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
void vsuxe_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
void vsuxe_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
void vsuxe_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
void vsuxe_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsuxe_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsuxe_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsuxe_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsuxe_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsuxe_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsuxe_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsuxe_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsuxe_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsuxe_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsuxe_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsuxe_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsuxe_v_f16m1_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1_t value, _VL_T vl);
void vsuxe_v_f16m2_vl (float16_t *base, vuint16m2_t bindex, vfloat16m2_t value, _VL_T vl);
void vsuxe_v_f16m4_vl (float16_t *base, vuint16m4_t bindex, vfloat16m4_t value, _VL_T vl);
void vsuxe_v_f16m8_vl (float16_t *base, vuint16m8_t bindex, vfloat16m8_t value, _VL_T vl);
void vsuxe_v_f32m1_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1_t value, _VL_T vl);
void vsuxe_v_f32m2_vl (float32_t *base, vuint32m2_t bindex, vfloat32m2_t value, _VL_T vl);
void vsuxe_v_f32m4_vl (float32_t *base, vuint32m4_t bindex, vfloat32m4_t value, _VL_T vl);
void vsuxe_v_f32m8_vl (float32_t *base, vuint32m8_t bindex, vfloat32m8_t value, _VL_T vl);
void vsuxe_v_f64m1_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1_t value, _VL_T vl);
void vsuxe_v_f64m2_vl (float64_t *base, vuint64m2_t bindex, vfloat64m2_t value, _VL_T vl);
void vsuxe_v_f64m4_vl (float64_t *base, vuint64m4_t bindex, vfloat64m4_t value, _VL_T vl);
void vsuxe_v_f64m8_vl (float64_t *base, vuint64m8_t bindex, vfloat64m8_t value, _VL_T vl);
// masked functions
void vsxb_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
void vsxb_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
void vsxb_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
void vsxb_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
void vsxb_v_i16m1_m_vl (vbool16_t mask, int8_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsxb_v_i16m2_m_vl (vbool8_t mask, int8_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsxb_v_i16m4_m_vl (vbool4_t mask, int8_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsxb_v_i16m8_m_vl (vbool2_t mask, int8_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsxb_v_i32m1_m_vl (vbool32_t mask, int8_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxb_v_i32m2_m_vl (vbool16_t mask, int8_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxb_v_i32m4_m_vl (vbool8_t mask, int8_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxb_v_i32m8_m_vl (vbool4_t mask, int8_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxb_v_i64m1_m_vl (vbool64_t mask, int8_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxb_v_i64m2_m_vl (vbool32_t mask, int8_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxb_v_i64m4_m_vl (vbool16_t mask, int8_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxb_v_i64m8_m_vl (vbool8_t mask, int8_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxb_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
void vsxb_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
void vsxb_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
void vsxb_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
void vsxb_v_u16m1_m_vl (vbool16_t mask, uint8_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsxb_v_u16m2_m_vl (vbool8_t mask, uint8_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsxb_v_u16m4_m_vl (vbool4_t mask, uint8_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsxb_v_u16m8_m_vl (vbool2_t mask, uint8_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsxb_v_u32m1_m_vl (vbool32_t mask, uint8_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxb_v_u32m2_m_vl (vbool16_t mask, uint8_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxb_v_u32m4_m_vl (vbool8_t mask, uint8_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxb_v_u32m8_m_vl (vbool4_t mask, uint8_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxb_v_u64m1_m_vl (vbool64_t mask, uint8_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxb_v_u64m2_m_vl (vbool32_t mask, uint8_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxb_v_u64m4_m_vl (vbool16_t mask, uint8_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxb_v_u64m8_m_vl (vbool8_t mask, uint8_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxh_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsxh_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsxh_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsxh_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsxh_v_i32m1_m_vl (vbool32_t mask, int16_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxh_v_i32m2_m_vl (vbool16_t mask, int16_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxh_v_i32m4_m_vl (vbool8_t mask, int16_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxh_v_i32m8_m_vl (vbool4_t mask, int16_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxh_v_i64m1_m_vl (vbool64_t mask, int16_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxh_v_i64m2_m_vl (vbool32_t mask, int16_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxh_v_i64m4_m_vl (vbool16_t mask, int16_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxh_v_i64m8_m_vl (vbool8_t mask, int16_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxh_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsxh_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsxh_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsxh_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsxh_v_u32m1_m_vl (vbool32_t mask, uint16_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxh_v_u32m2_m_vl (vbool16_t mask, uint16_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxh_v_u32m4_m_vl (vbool8_t mask, uint16_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxh_v_u32m8_m_vl (vbool4_t mask, uint16_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxh_v_u64m1_m_vl (vbool64_t mask, uint16_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxh_v_u64m2_m_vl (vbool32_t mask, uint16_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxh_v_u64m4_m_vl (vbool16_t mask, uint16_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxh_v_u64m8_m_vl (vbool8_t mask, uint16_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxw_v_u64m1_m_vl (vbool64_t mask, uint32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxw_v_u64m2_m_vl (vbool32_t mask, uint32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxw_v_u64m4_m_vl (vbool16_t mask, uint32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxw_v_u64m8_m_vl (vbool8_t mask, uint32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxe_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
void vsxe_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
void vsxe_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
void vsxe_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
void vsxe_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsxe_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsxe_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsxe_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsxe_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsxe_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsxe_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsxe_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsxe_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsxe_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsxe_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsxe_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsxe_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
void vsxe_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
void vsxe_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
void vsxe_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
void vsxe_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsxe_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsxe_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsxe_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsxe_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsxe_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsxe_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsxe_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsxe_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsxe_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsxe_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsxe_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsxe_v_f16m1_m_vl (vbool16_t mask, float16_t *base, vuint16m1_t bindex, vfloat16m1_t value, _VL_T vl);
void vsxe_v_f16m2_m_vl (vbool8_t mask, float16_t *base, vuint16m2_t bindex, vfloat16m2_t value, _VL_T vl);
void vsxe_v_f16m4_m_vl (vbool4_t mask, float16_t *base, vuint16m4_t bindex, vfloat16m4_t value, _VL_T vl);
void vsxe_v_f16m8_m_vl (vbool2_t mask, float16_t *base, vuint16m8_t bindex, vfloat16m8_t value, _VL_T vl);
void vsxe_v_f32m1_m_vl (vbool32_t mask, float32_t *base, vuint32m1_t bindex, vfloat32m1_t value, _VL_T vl);
void vsxe_v_f32m2_m_vl (vbool16_t mask, float32_t *base, vuint32m2_t bindex, vfloat32m2_t value, _VL_T vl);
void vsxe_v_f32m4_m_vl (vbool8_t mask, float32_t *base, vuint32m4_t bindex, vfloat32m4_t value, _VL_T vl);
void vsxe_v_f32m8_m_vl (vbool4_t mask, float32_t *base, vuint32m8_t bindex, vfloat32m8_t value, _VL_T vl);
void vsxe_v_f64m1_m_vl (vbool64_t mask, float64_t *base, vuint64m1_t bindex, vfloat64m1_t value, _VL_T vl);
void vsxe_v_f64m2_m_vl (vbool32_t mask, float64_t *base, vuint64m2_t bindex, vfloat64m2_t value, _VL_T vl);
void vsxe_v_f64m4_m_vl (vbool16_t mask, float64_t *base, vuint64m4_t bindex, vfloat64m4_t value, _VL_T vl);
void vsxe_v_f64m8_m_vl (vbool8_t mask, float64_t *base, vuint64m8_t bindex, vfloat64m8_t value, _VL_T vl);
void vsuxe_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
void vsuxe_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
void vsuxe_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
void vsuxe_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
void vsuxe_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
void vsuxe_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
void vsuxe_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
void vsuxe_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
void vsuxe_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
void vsuxe_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
void vsuxe_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
void vsuxe_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
void vsuxe_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
void vsuxe_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
void vsuxe_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
void vsuxe_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
void vsuxe_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
void vsuxe_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
void vsuxe_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
void vsuxe_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
void vsuxe_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
void vsuxe_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
void vsuxe_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
void vsuxe_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
void vsuxe_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
void vsuxe_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
void vsuxe_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
void vsuxe_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
void vsuxe_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
void vsuxe_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
void vsuxe_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
void vsuxe_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
void vsuxe_v_f16m1_m_vl (vbool16_t mask, float16_t *base, vuint16m1_t bindex, vfloat16m1_t value, _VL_T vl);
void vsuxe_v_f16m2_m_vl (vbool8_t mask, float16_t *base, vuint16m2_t bindex, vfloat16m2_t value, _VL_T vl);
void vsuxe_v_f16m4_m_vl (vbool4_t mask, float16_t *base, vuint16m4_t bindex, vfloat16m4_t value, _VL_T vl);
void vsuxe_v_f16m8_m_vl (vbool2_t mask, float16_t *base, vuint16m8_t bindex, vfloat16m8_t value, _VL_T vl);
void vsuxe_v_f32m1_m_vl (vbool32_t mask, float32_t *base, vuint32m1_t bindex, vfloat32m1_t value, _VL_T vl);
void vsuxe_v_f32m2_m_vl (vbool16_t mask, float32_t *base, vuint32m2_t bindex, vfloat32m2_t value, _VL_T vl);
void vsuxe_v_f32m4_m_vl (vbool8_t mask, float32_t *base, vuint32m4_t bindex, vfloat32m4_t value, _VL_T vl);
void vsuxe_v_f32m8_m_vl (vbool4_t mask, float32_t *base, vuint32m8_t bindex, vfloat32m8_t value, _VL_T vl);
void vsuxe_v_f64m1_m_vl (vbool64_t mask, float64_t *base, vuint64m1_t bindex, vfloat64m1_t value, _VL_T vl);
void vsuxe_v_f64m2_m_vl (vbool32_t mask, float64_t *base, vuint64m2_t bindex, vfloat64m2_t value, _VL_T vl);
void vsuxe_v_f64m4_m_vl (vbool16_t mask, float64_t *base, vuint64m4_t bindex, vfloat64m4_t value, _VL_T vl);
void vsuxe_v_f64m8_m_vl (vbool8_t mask, float64_t *base, vuint64m8_t bindex, vfloat64m8_t value, _VL_T vl);
```
### [Unit-stride Fault-Only-First Loads Functions](rvv-intrinsic-api.md#77-unit-stride-fault-only-first-loads-operations):

**Prototypes:**
``` C
vint8m1_t vlbff_v_i8m1_vl (const int8_t *base, _VL_T vl);
vint8m2_t vlbff_v_i8m2_vl (const int8_t *base, _VL_T vl);
vint8m4_t vlbff_v_i8m4_vl (const int8_t *base, _VL_T vl);
vint8m8_t vlbff_v_i8m8_vl (const int8_t *base, _VL_T vl);
vint16m1_t vlbff_v_i16m1_vl (const int8_t *base, _VL_T vl);
vint16m2_t vlbff_v_i16m2_vl (const int8_t *base, _VL_T vl);
vint16m4_t vlbff_v_i16m4_vl (const int8_t *base, _VL_T vl);
vint16m8_t vlbff_v_i16m8_vl (const int8_t *base, _VL_T vl);
vint32m1_t vlbff_v_i32m1_vl (const int8_t *base, _VL_T vl);
vint32m2_t vlbff_v_i32m2_vl (const int8_t *base, _VL_T vl);
vint32m4_t vlbff_v_i32m4_vl (const int8_t *base, _VL_T vl);
vint32m8_t vlbff_v_i32m8_vl (const int8_t *base, _VL_T vl);
vint64m1_t vlbff_v_i64m1_vl (const int8_t *base, _VL_T vl);
vint64m2_t vlbff_v_i64m2_vl (const int8_t *base, _VL_T vl);
vint64m4_t vlbff_v_i64m4_vl (const int8_t *base, _VL_T vl);
vint64m8_t vlbff_v_i64m8_vl (const int8_t *base, _VL_T vl);
vuint8m1_t vlbuff_v_u8m1_vl (const uint8_t *base, _VL_T vl);
vuint8m2_t vlbuff_v_u8m2_vl (const uint8_t *base, _VL_T vl);
vuint8m4_t vlbuff_v_u8m4_vl (const uint8_t *base, _VL_T vl);
vuint8m8_t vlbuff_v_u8m8_vl (const uint8_t *base, _VL_T vl);
vuint16m1_t vlbuff_v_u16m1_vl (const uint8_t *base, _VL_T vl);
vuint16m2_t vlbuff_v_u16m2_vl (const uint8_t *base, _VL_T vl);
vuint16m4_t vlbuff_v_u16m4_vl (const uint8_t *base, _VL_T vl);
vuint16m8_t vlbuff_v_u16m8_vl (const uint8_t *base, _VL_T vl);
vuint32m1_t vlbuff_v_u32m1_vl (const uint8_t *base, _VL_T vl);
vuint32m2_t vlbuff_v_u32m2_vl (const uint8_t *base, _VL_T vl);
vuint32m4_t vlbuff_v_u32m4_vl (const uint8_t *base, _VL_T vl);
vuint32m8_t vlbuff_v_u32m8_vl (const uint8_t *base, _VL_T vl);
vuint64m1_t vlbuff_v_u64m1_vl (const uint8_t *base, _VL_T vl);
vuint64m2_t vlbuff_v_u64m2_vl (const uint8_t *base, _VL_T vl);
vuint64m4_t vlbuff_v_u64m4_vl (const uint8_t *base, _VL_T vl);
vuint64m8_t vlbuff_v_u64m8_vl (const uint8_t *base, _VL_T vl);
vint16m1_t vlhff_v_i16m1_vl (const int16_t *base, _VL_T vl);
vint16m2_t vlhff_v_i16m2_vl (const int16_t *base, _VL_T vl);
vint16m4_t vlhff_v_i16m4_vl (const int16_t *base, _VL_T vl);
vint16m8_t vlhff_v_i16m8_vl (const int16_t *base, _VL_T vl);
vint32m1_t vlhff_v_i32m1_vl (const int16_t *base, _VL_T vl);
vint32m2_t vlhff_v_i32m2_vl (const int16_t *base, _VL_T vl);
vint32m4_t vlhff_v_i32m4_vl (const int16_t *base, _VL_T vl);
vint32m8_t vlhff_v_i32m8_vl (const int16_t *base, _VL_T vl);
vint64m1_t vlhff_v_i64m1_vl (const int16_t *base, _VL_T vl);
vint64m2_t vlhff_v_i64m2_vl (const int16_t *base, _VL_T vl);
vint64m4_t vlhff_v_i64m4_vl (const int16_t *base, _VL_T vl);
vint64m8_t vlhff_v_i64m8_vl (const int16_t *base, _VL_T vl);
vuint16m1_t vlhuff_v_u16m1_vl (const uint16_t *base, _VL_T vl);
vuint16m2_t vlhuff_v_u16m2_vl (const uint16_t *base, _VL_T vl);
vuint16m4_t vlhuff_v_u16m4_vl (const uint16_t *base, _VL_T vl);
vuint16m8_t vlhuff_v_u16m8_vl (const uint16_t *base, _VL_T vl);
vuint32m1_t vlhuff_v_u32m1_vl (const uint16_t *base, _VL_T vl);
vuint32m2_t vlhuff_v_u32m2_vl (const uint16_t *base, _VL_T vl);
vuint32m4_t vlhuff_v_u32m4_vl (const uint16_t *base, _VL_T vl);
vuint32m8_t vlhuff_v_u32m8_vl (const uint16_t *base, _VL_T vl);
vuint64m1_t vlhuff_v_u64m1_vl (const uint16_t *base, _VL_T vl);
vuint64m2_t vlhuff_v_u64m2_vl (const uint16_t *base, _VL_T vl);
vuint64m4_t vlhuff_v_u64m4_vl (const uint16_t *base, _VL_T vl);
vuint64m8_t vlhuff_v_u64m8_vl (const uint16_t *base, _VL_T vl);
vint32m1_t vlwff_v_i32m1_vl (const int32_t *base, _VL_T vl);
vint32m2_t vlwff_v_i32m2_vl (const int32_t *base, _VL_T vl);
vint32m4_t vlwff_v_i32m4_vl (const int32_t *base, _VL_T vl);
vint32m8_t vlwff_v_i32m8_vl (const int32_t *base, _VL_T vl);
vint64m1_t vlwff_v_i64m1_vl (const int32_t *base, _VL_T vl);
vint64m2_t vlwff_v_i64m2_vl (const int32_t *base, _VL_T vl);
vint64m4_t vlwff_v_i64m4_vl (const int32_t *base, _VL_T vl);
vint64m8_t vlwff_v_i64m8_vl (const int32_t *base, _VL_T vl);
vuint32m1_t vlwuff_v_u32m1_vl (const uint32_t *base, _VL_T vl);
vuint32m2_t vlwuff_v_u32m2_vl (const uint32_t *base, _VL_T vl);
vuint32m4_t vlwuff_v_u32m4_vl (const uint32_t *base, _VL_T vl);
vuint32m8_t vlwuff_v_u32m8_vl (const uint32_t *base, _VL_T vl);
vuint64m1_t vlwuff_v_u64m1_vl (const uint32_t *base, _VL_T vl);
vuint64m2_t vlwuff_v_u64m2_vl (const uint32_t *base, _VL_T vl);
vuint64m4_t vlwuff_v_u64m4_vl (const uint32_t *base, _VL_T vl);
vuint64m8_t vlwuff_v_u64m8_vl (const uint32_t *base, _VL_T vl);
vint8m1_t vleff_v_i8m1_vl (const int8_t *base, _VL_T vl);
vint8m2_t vleff_v_i8m2_vl (const int8_t *base, _VL_T vl);
vint8m4_t vleff_v_i8m4_vl (const int8_t *base, _VL_T vl);
vint8m8_t vleff_v_i8m8_vl (const int8_t *base, _VL_T vl);
vint16m1_t vleff_v_i16m1_vl (const int16_t *base, _VL_T vl);
vint16m2_t vleff_v_i16m2_vl (const int16_t *base, _VL_T vl);
vint16m4_t vleff_v_i16m4_vl (const int16_t *base, _VL_T vl);
vint16m8_t vleff_v_i16m8_vl (const int16_t *base, _VL_T vl);
vint32m1_t vleff_v_i32m1_vl (const int32_t *base, _VL_T vl);
vint32m2_t vleff_v_i32m2_vl (const int32_t *base, _VL_T vl);
vint32m4_t vleff_v_i32m4_vl (const int32_t *base, _VL_T vl);
vint32m8_t vleff_v_i32m8_vl (const int32_t *base, _VL_T vl);
vint64m1_t vleff_v_i64m1_vl (const int64_t *base, _VL_T vl);
vint64m2_t vleff_v_i64m2_vl (const int64_t *base, _VL_T vl);
vint64m4_t vleff_v_i64m4_vl (const int64_t *base, _VL_T vl);
vint64m8_t vleff_v_i64m8_vl (const int64_t *base, _VL_T vl);
vuint8m1_t vleff_v_u8m1_vl (const uint8_t *base, _VL_T vl);
vuint8m2_t vleff_v_u8m2_vl (const uint8_t *base, _VL_T vl);
vuint8m4_t vleff_v_u8m4_vl (const uint8_t *base, _VL_T vl);
vuint8m8_t vleff_v_u8m8_vl (const uint8_t *base, _VL_T vl);
vuint16m1_t vleff_v_u16m1_vl (const uint16_t *base, _VL_T vl);
vuint16m2_t vleff_v_u16m2_vl (const uint16_t *base, _VL_T vl);
vuint16m4_t vleff_v_u16m4_vl (const uint16_t *base, _VL_T vl);
vuint16m8_t vleff_v_u16m8_vl (const uint16_t *base, _VL_T vl);
vuint32m1_t vleff_v_u32m1_vl (const uint32_t *base, _VL_T vl);
vuint32m2_t vleff_v_u32m2_vl (const uint32_t *base, _VL_T vl);
vuint32m4_t vleff_v_u32m4_vl (const uint32_t *base, _VL_T vl);
vuint32m8_t vleff_v_u32m8_vl (const uint32_t *base, _VL_T vl);
vuint64m1_t vleff_v_u64m1_vl (const uint64_t *base, _VL_T vl);
vuint64m2_t vleff_v_u64m2_vl (const uint64_t *base, _VL_T vl);
vuint64m4_t vleff_v_u64m4_vl (const uint64_t *base, _VL_T vl);
vuint64m8_t vleff_v_u64m8_vl (const uint64_t *base, _VL_T vl);
vfloat16m1_t vleff_v_f16m1_vl (const float16_t *base, _VL_T vl);
vfloat16m2_t vleff_v_f16m2_vl (const float16_t *base, _VL_T vl);
vfloat16m4_t vleff_v_f16m4_vl (const float16_t *base, _VL_T vl);
vfloat16m8_t vleff_v_f16m8_vl (const float16_t *base, _VL_T vl);
vfloat32m1_t vleff_v_f32m1_vl (const float32_t *base, _VL_T vl);
vfloat32m2_t vleff_v_f32m2_vl (const float32_t *base, _VL_T vl);
vfloat32m4_t vleff_v_f32m4_vl (const float32_t *base, _VL_T vl);
vfloat32m8_t vleff_v_f32m8_vl (const float32_t *base, _VL_T vl);
vfloat64m1_t vleff_v_f64m1_vl (const float64_t *base, _VL_T vl);
vfloat64m2_t vleff_v_f64m2_vl (const float64_t *base, _VL_T vl);
vfloat64m4_t vleff_v_f64m4_vl (const float64_t *base, _VL_T vl);
vfloat64m8_t vleff_v_f64m8_vl (const float64_t *base, _VL_T vl);
// masked functions
vint8m1_t vlbff_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m2_t vlbff_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, _VL_T vl);
vint8m4_t vlbff_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, _VL_T vl);
vint8m8_t vlbff_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, _VL_T vl);
vint16m1_t vlbff_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int8_t *base, _VL_T vl);
vint16m2_t vlbff_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int8_t *base, _VL_T vl);
vint16m4_t vlbff_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int8_t *base, _VL_T vl);
vint16m8_t vlbff_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int8_t *base, _VL_T vl);
vint32m1_t vlbff_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int8_t *base, _VL_T vl);
vint32m2_t vlbff_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int8_t *base, _VL_T vl);
vint32m4_t vlbff_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int8_t *base, _VL_T vl);
vint32m8_t vlbff_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int8_t *base, _VL_T vl);
vint64m1_t vlbff_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int8_t *base, _VL_T vl);
vint64m2_t vlbff_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int8_t *base, _VL_T vl);
vint64m4_t vlbff_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int8_t *base, _VL_T vl);
vint64m8_t vlbff_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int8_t *base, _VL_T vl);
vuint8m1_t vlbuff_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m2_t vlbuff_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m4_t vlbuff_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m8_t vlbuff_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m1_t vlbuff_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m2_t vlbuff_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m4_t vlbuff_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m8_t vlbuff_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m1_t vlbuff_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m2_t vlbuff_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m4_t vlbuff_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint32m8_t vlbuff_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m1_t vlbuff_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m2_t vlbuff_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m4_t vlbuff_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint64m8_t vlbuff_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint8_t *base, _VL_T vl);
vint16m1_t vlhff_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m2_t vlhff_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, _VL_T vl);
vint16m4_t vlhff_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, _VL_T vl);
vint16m8_t vlhff_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, _VL_T vl);
vint32m1_t vlhff_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int16_t *base, _VL_T vl);
vint32m2_t vlhff_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int16_t *base, _VL_T vl);
vint32m4_t vlhff_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int16_t *base, _VL_T vl);
vint32m8_t vlhff_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int16_t *base, _VL_T vl);
vint64m1_t vlhff_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int16_t *base, _VL_T vl);
vint64m2_t vlhff_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int16_t *base, _VL_T vl);
vint64m4_t vlhff_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int16_t *base, _VL_T vl);
vint64m8_t vlhff_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int16_t *base, _VL_T vl);
vuint16m1_t vlhuff_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m2_t vlhuff_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m4_t vlhuff_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m8_t vlhuff_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m1_t vlhuff_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m2_t vlhuff_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m4_t vlhuff_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m8_t vlhuff_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m1_t vlhuff_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m2_t vlhuff_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m4_t vlhuff_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint64m8_t vlhuff_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint16_t *base, _VL_T vl);
vint32m1_t vlwff_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m2_t vlwff_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, _VL_T vl);
vint32m4_t vlwff_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, _VL_T vl);
vint32m8_t vlwff_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, _VL_T vl);
vint64m1_t vlwff_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int32_t *base, _VL_T vl);
vint64m2_t vlwff_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int32_t *base, _VL_T vl);
vint64m4_t vlwff_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int32_t *base, _VL_T vl);
vint64m8_t vlwff_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int32_t *base, _VL_T vl);
vuint32m1_t vlwuff_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m2_t vlwuff_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m4_t vlwuff_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m8_t vlwuff_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m1_t vlwuff_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m2_t vlwuff_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m4_t vlwuff_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m8_t vlwuff_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint32_t *base, _VL_T vl);
vint8m1_t vleff_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m2_t vleff_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, _VL_T vl);
vint8m4_t vleff_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, _VL_T vl);
vint8m8_t vleff_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, const int8_t *base, _VL_T vl);
vint16m1_t vleff_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m2_t vleff_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, _VL_T vl);
vint16m4_t vleff_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, _VL_T vl);
vint16m8_t vleff_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, const int16_t *base, _VL_T vl);
vint32m1_t vleff_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m2_t vleff_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, _VL_T vl);
vint32m4_t vleff_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, _VL_T vl);
vint32m8_t vleff_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, const int32_t *base, _VL_T vl);
vint64m1_t vleff_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m2_t vleff_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, _VL_T vl);
vint64m4_t vleff_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int64_t *base, _VL_T vl);
vint64m8_t vleff_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, const int64_t *base, _VL_T vl);
vuint8m1_t vleff_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m2_t vleff_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m4_t vleff_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m8_t vleff_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m1_t vleff_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m2_t vleff_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m4_t vleff_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m8_t vleff_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m1_t vleff_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m2_t vleff_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m4_t vleff_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m8_t vleff_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m1_t vleff_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m2_t vleff_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m4_t vleff_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m8_t vleff_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, const uint64_t *base, _VL_T vl);
vfloat16m1_t vleff_v_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m2_t vleff_v_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m4_t vleff_v_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m8_t vleff_v_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, const float16_t *base, _VL_T vl);
vfloat32m1_t vleff_v_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m2_t vleff_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m4_t vleff_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m8_t vleff_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, const float32_t *base, _VL_T vl);
vfloat64m1_t vleff_v_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m2_t vleff_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m4_t vleff_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m8_t vleff_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, const float64_t *base, _VL_T vl);
```
## Vector Load/Store Segment Instructions (Zvlsseg):

### [Vector Unit-Stride Segment Load Functions]():

**Prototypes:**
``` C
vint8m1x2_t vlseg2e_v_i8m1x2_vl (const int8_t *base, _VL_T vl);
vint8m1x3_t vlseg3e_v_i8m1x3_vl (const int8_t *base, _VL_T vl);
vint8m1x4_t vlseg4e_v_i8m1x4_vl (const int8_t *base, _VL_T vl);
vint8m1x5_t vlseg5e_v_i8m1x5_vl (const int8_t *base, _VL_T vl);
vint8m1x6_t vlseg6e_v_i8m1x6_vl (const int8_t *base, _VL_T vl);
vint8m1x7_t vlseg7e_v_i8m1x7_vl (const int8_t *base, _VL_T vl);
vint8m1x8_t vlseg8e_v_i8m1x8_vl (const int8_t *base, _VL_T vl);
vint8m2x2_t vlseg2e_v_i8m2x2_vl (const int8_t *base, _VL_T vl);
vint8m2x3_t vlseg3e_v_i8m2x3_vl (const int8_t *base, _VL_T vl);
vint8m2x4_t vlseg4e_v_i8m2x4_vl (const int8_t *base, _VL_T vl);
vint8m4x2_t vlseg2e_v_i8m4x2_vl (const int8_t *base, _VL_T vl);
vint16m1x2_t vlseg2e_v_i16m1x2_vl (const int16_t *base, _VL_T vl);
vint16m1x3_t vlseg3e_v_i16m1x3_vl (const int16_t *base, _VL_T vl);
vint16m1x4_t vlseg4e_v_i16m1x4_vl (const int16_t *base, _VL_T vl);
vint16m1x5_t vlseg5e_v_i16m1x5_vl (const int16_t *base, _VL_T vl);
vint16m1x6_t vlseg6e_v_i16m1x6_vl (const int16_t *base, _VL_T vl);
vint16m1x7_t vlseg7e_v_i16m1x7_vl (const int16_t *base, _VL_T vl);
vint16m1x8_t vlseg8e_v_i16m1x8_vl (const int16_t *base, _VL_T vl);
vint16m2x2_t vlseg2e_v_i16m2x2_vl (const int16_t *base, _VL_T vl);
vint16m2x3_t vlseg3e_v_i16m2x3_vl (const int16_t *base, _VL_T vl);
vint16m2x4_t vlseg4e_v_i16m2x4_vl (const int16_t *base, _VL_T vl);
vint16m4x2_t vlseg2e_v_i16m4x2_vl (const int16_t *base, _VL_T vl);
vint32m1x2_t vlseg2e_v_i32m1x2_vl (const int32_t *base, _VL_T vl);
vint32m1x3_t vlseg3e_v_i32m1x3_vl (const int32_t *base, _VL_T vl);
vint32m1x4_t vlseg4e_v_i32m1x4_vl (const int32_t *base, _VL_T vl);
vint32m1x5_t vlseg5e_v_i32m1x5_vl (const int32_t *base, _VL_T vl);
vint32m1x6_t vlseg6e_v_i32m1x6_vl (const int32_t *base, _VL_T vl);
vint32m1x7_t vlseg7e_v_i32m1x7_vl (const int32_t *base, _VL_T vl);
vint32m1x8_t vlseg8e_v_i32m1x8_vl (const int32_t *base, _VL_T vl);
vint32m2x2_t vlseg2e_v_i32m2x2_vl (const int32_t *base, _VL_T vl);
vint32m2x3_t vlseg3e_v_i32m2x3_vl (const int32_t *base, _VL_T vl);
vint32m2x4_t vlseg4e_v_i32m2x4_vl (const int32_t *base, _VL_T vl);
vint32m4x2_t vlseg2e_v_i32m4x2_vl (const int32_t *base, _VL_T vl);
vint64m1x2_t vlseg2e_v_i64m1x2_vl (const int64_t *base, _VL_T vl);
vint64m1x3_t vlseg3e_v_i64m1x3_vl (const int64_t *base, _VL_T vl);
vint64m1x4_t vlseg4e_v_i64m1x4_vl (const int64_t *base, _VL_T vl);
vint64m1x5_t vlseg5e_v_i64m1x5_vl (const int64_t *base, _VL_T vl);
vint64m1x6_t vlseg6e_v_i64m1x6_vl (const int64_t *base, _VL_T vl);
vint64m1x7_t vlseg7e_v_i64m1x7_vl (const int64_t *base, _VL_T vl);
vint64m1x8_t vlseg8e_v_i64m1x8_vl (const int64_t *base, _VL_T vl);
vint64m2x2_t vlseg2e_v_i64m2x2_vl (const int64_t *base, _VL_T vl);
vint64m2x3_t vlseg3e_v_i64m2x3_vl (const int64_t *base, _VL_T vl);
vint64m2x4_t vlseg4e_v_i64m2x4_vl (const int64_t *base, _VL_T vl);
vint64m4x2_t vlseg2e_v_i64m4x2_vl (const int64_t *base, _VL_T vl);
vuint8m1x2_t vlseg2e_v_u8m1x2_vl (const uint8_t *base, _VL_T vl);
vuint8m1x3_t vlseg3e_v_u8m1x3_vl (const uint8_t *base, _VL_T vl);
vuint8m1x4_t vlseg4e_v_u8m1x4_vl (const uint8_t *base, _VL_T vl);
vuint8m1x5_t vlseg5e_v_u8m1x5_vl (const uint8_t *base, _VL_T vl);
vuint8m1x6_t vlseg6e_v_u8m1x6_vl (const uint8_t *base, _VL_T vl);
vuint8m1x7_t vlseg7e_v_u8m1x7_vl (const uint8_t *base, _VL_T vl);
vuint8m1x8_t vlseg8e_v_u8m1x8_vl (const uint8_t *base, _VL_T vl);
vuint8m2x2_t vlseg2e_v_u8m2x2_vl (const uint8_t *base, _VL_T vl);
vuint8m2x3_t vlseg3e_v_u8m2x3_vl (const uint8_t *base, _VL_T vl);
vuint8m2x4_t vlseg4e_v_u8m2x4_vl (const uint8_t *base, _VL_T vl);
vuint8m4x2_t vlseg2e_v_u8m4x2_vl (const uint8_t *base, _VL_T vl);
vuint16m1x2_t vlseg2e_v_u16m1x2_vl (const uint16_t *base, _VL_T vl);
vuint16m1x3_t vlseg3e_v_u16m1x3_vl (const uint16_t *base, _VL_T vl);
vuint16m1x4_t vlseg4e_v_u16m1x4_vl (const uint16_t *base, _VL_T vl);
vuint16m1x5_t vlseg5e_v_u16m1x5_vl (const uint16_t *base, _VL_T vl);
vuint16m1x6_t vlseg6e_v_u16m1x6_vl (const uint16_t *base, _VL_T vl);
vuint16m1x7_t vlseg7e_v_u16m1x7_vl (const uint16_t *base, _VL_T vl);
vuint16m1x8_t vlseg8e_v_u16m1x8_vl (const uint16_t *base, _VL_T vl);
vuint16m2x2_t vlseg2e_v_u16m2x2_vl (const uint16_t *base, _VL_T vl);
vuint16m2x3_t vlseg3e_v_u16m2x3_vl (const uint16_t *base, _VL_T vl);
vuint16m2x4_t vlseg4e_v_u16m2x4_vl (const uint16_t *base, _VL_T vl);
vuint16m4x2_t vlseg2e_v_u16m4x2_vl (const uint16_t *base, _VL_T vl);
vuint32m1x2_t vlseg2e_v_u32m1x2_vl (const uint32_t *base, _VL_T vl);
vuint32m1x3_t vlseg3e_v_u32m1x3_vl (const uint32_t *base, _VL_T vl);
vuint32m1x4_t vlseg4e_v_u32m1x4_vl (const uint32_t *base, _VL_T vl);
vuint32m1x5_t vlseg5e_v_u32m1x5_vl (const uint32_t *base, _VL_T vl);
vuint32m1x6_t vlseg6e_v_u32m1x6_vl (const uint32_t *base, _VL_T vl);
vuint32m1x7_t vlseg7e_v_u32m1x7_vl (const uint32_t *base, _VL_T vl);
vuint32m1x8_t vlseg8e_v_u32m1x8_vl (const uint32_t *base, _VL_T vl);
vuint32m2x2_t vlseg2e_v_u32m2x2_vl (const uint32_t *base, _VL_T vl);
vuint32m2x3_t vlseg3e_v_u32m2x3_vl (const uint32_t *base, _VL_T vl);
vuint32m2x4_t vlseg4e_v_u32m2x4_vl (const uint32_t *base, _VL_T vl);
vuint32m4x2_t vlseg2e_v_u32m4x2_vl (const uint32_t *base, _VL_T vl);
vuint64m1x2_t vlseg2e_v_u64m1x2_vl (const uint64_t *base, _VL_T vl);
vuint64m1x3_t vlseg3e_v_u64m1x3_vl (const uint64_t *base, _VL_T vl);
vuint64m1x4_t vlseg4e_v_u64m1x4_vl (const uint64_t *base, _VL_T vl);
vuint64m1x5_t vlseg5e_v_u64m1x5_vl (const uint64_t *base, _VL_T vl);
vuint64m1x6_t vlseg6e_v_u64m1x6_vl (const uint64_t *base, _VL_T vl);
vuint64m1x7_t vlseg7e_v_u64m1x7_vl (const uint64_t *base, _VL_T vl);
vuint64m1x8_t vlseg8e_v_u64m1x8_vl (const uint64_t *base, _VL_T vl);
vuint64m2x2_t vlseg2e_v_u64m2x2_vl (const uint64_t *base, _VL_T vl);
vuint64m2x3_t vlseg3e_v_u64m2x3_vl (const uint64_t *base, _VL_T vl);
vuint64m2x4_t vlseg4e_v_u64m2x4_vl (const uint64_t *base, _VL_T vl);
vuint64m4x2_t vlseg2e_v_u64m4x2_vl (const uint64_t *base, _VL_T vl);
vfloat16m1x2_t vlseg2e_v_f16m1x2_vl (const float16_t *base, _VL_T vl);
vfloat16m1x3_t vlseg3e_v_f16m1x3_vl (const float16_t *base, _VL_T vl);
vfloat16m1x4_t vlseg4e_v_f16m1x4_vl (const float16_t *base, _VL_T vl);
vfloat16m1x5_t vlseg5e_v_f16m1x5_vl (const float16_t *base, _VL_T vl);
vfloat16m1x6_t vlseg6e_v_f16m1x6_vl (const float16_t *base, _VL_T vl);
vfloat16m1x7_t vlseg7e_v_f16m1x7_vl (const float16_t *base, _VL_T vl);
vfloat16m1x8_t vlseg8e_v_f16m1x8_vl (const float16_t *base, _VL_T vl);
vfloat16m2x2_t vlseg2e_v_f16m2x2_vl (const float16_t *base, _VL_T vl);
vfloat16m2x3_t vlseg3e_v_f16m2x3_vl (const float16_t *base, _VL_T vl);
vfloat16m2x4_t vlseg4e_v_f16m2x4_vl (const float16_t *base, _VL_T vl);
vfloat16m4x2_t vlseg2e_v_f16m4x2_vl (const float16_t *base, _VL_T vl);
vfloat32m1x2_t vlseg2e_v_f32m1x2_vl (const float32_t *base, _VL_T vl);
vfloat32m1x3_t vlseg3e_v_f32m1x3_vl (const float32_t *base, _VL_T vl);
vfloat32m1x4_t vlseg4e_v_f32m1x4_vl (const float32_t *base, _VL_T vl);
vfloat32m1x5_t vlseg5e_v_f32m1x5_vl (const float32_t *base, _VL_T vl);
vfloat32m1x6_t vlseg6e_v_f32m1x6_vl (const float32_t *base, _VL_T vl);
vfloat32m1x7_t vlseg7e_v_f32m1x7_vl (const float32_t *base, _VL_T vl);
vfloat32m1x8_t vlseg8e_v_f32m1x8_vl (const float32_t *base, _VL_T vl);
vfloat32m2x2_t vlseg2e_v_f32m2x2_vl (const float32_t *base, _VL_T vl);
vfloat32m2x3_t vlseg3e_v_f32m2x3_vl (const float32_t *base, _VL_T vl);
vfloat32m2x4_t vlseg4e_v_f32m2x4_vl (const float32_t *base, _VL_T vl);
vfloat32m4x2_t vlseg2e_v_f32m4x2_vl (const float32_t *base, _VL_T vl);
vfloat64m1x2_t vlseg2e_v_f64m1x2_vl (const float64_t *base, _VL_T vl);
vfloat64m1x3_t vlseg3e_v_f64m1x3_vl (const float64_t *base, _VL_T vl);
vfloat64m1x4_t vlseg4e_v_f64m1x4_vl (const float64_t *base, _VL_T vl);
vfloat64m1x5_t vlseg5e_v_f64m1x5_vl (const float64_t *base, _VL_T vl);
vfloat64m1x6_t vlseg6e_v_f64m1x6_vl (const float64_t *base, _VL_T vl);
vfloat64m1x7_t vlseg7e_v_f64m1x7_vl (const float64_t *base, _VL_T vl);
vfloat64m1x8_t vlseg8e_v_f64m1x8_vl (const float64_t *base, _VL_T vl);
vfloat64m2x2_t vlseg2e_v_f64m2x2_vl (const float64_t *base, _VL_T vl);
vfloat64m2x3_t vlseg3e_v_f64m2x3_vl (const float64_t *base, _VL_T vl);
vfloat64m2x4_t vlseg4e_v_f64m2x4_vl (const float64_t *base, _VL_T vl);
vfloat64m4x2_t vlseg2e_v_f64m4x2_vl (const float64_t *base, _VL_T vl);
// masked functions
vint8m1x2_t vlseg2e_v_i8m1x2_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m1x3_t vlseg3e_v_i8m1x3_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m1x4_t vlseg4e_v_i8m1x4_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m1x5_t vlseg5e_v_i8m1x5_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m1x6_t vlseg6e_v_i8m1x6_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m1x7_t vlseg7e_v_i8m1x7_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m1x8_t vlseg8e_v_i8m1x8_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, _VL_T vl);
vint8m2x2_t vlseg2e_v_i8m2x2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, _VL_T vl);
vint8m2x3_t vlseg3e_v_i8m2x3_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, _VL_T vl);
vint8m2x4_t vlseg4e_v_i8m2x4_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, _VL_T vl);
vint8m4x2_t vlseg2e_v_i8m4x2_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, _VL_T vl);
vint16m1x2_t vlseg2e_v_i16m1x2_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m1x3_t vlseg3e_v_i16m1x3_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m1x4_t vlseg4e_v_i16m1x4_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m1x5_t vlseg5e_v_i16m1x5_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m1x6_t vlseg6e_v_i16m1x6_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m1x7_t vlseg7e_v_i16m1x7_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m1x8_t vlseg8e_v_i16m1x8_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, _VL_T vl);
vint16m2x2_t vlseg2e_v_i16m2x2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, _VL_T vl);
vint16m2x3_t vlseg3e_v_i16m2x3_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, _VL_T vl);
vint16m2x4_t vlseg4e_v_i16m2x4_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, _VL_T vl);
vint16m4x2_t vlseg2e_v_i16m4x2_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, _VL_T vl);
vint32m1x2_t vlseg2e_v_i32m1x2_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m1x3_t vlseg3e_v_i32m1x3_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m1x4_t vlseg4e_v_i32m1x4_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m1x5_t vlseg5e_v_i32m1x5_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m1x6_t vlseg6e_v_i32m1x6_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m1x7_t vlseg7e_v_i32m1x7_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m1x8_t vlseg8e_v_i32m1x8_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, _VL_T vl);
vint32m2x2_t vlseg2e_v_i32m2x2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, _VL_T vl);
vint32m2x3_t vlseg3e_v_i32m2x3_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, _VL_T vl);
vint32m2x4_t vlseg4e_v_i32m2x4_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, _VL_T vl);
vint32m4x2_t vlseg2e_v_i32m4x2_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, _VL_T vl);
vint64m1x2_t vlseg2e_v_i64m1x2_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m1x3_t vlseg3e_v_i64m1x3_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m1x4_t vlseg4e_v_i64m1x4_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m1x5_t vlseg5e_v_i64m1x5_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m1x6_t vlseg6e_v_i64m1x6_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m1x7_t vlseg7e_v_i64m1x7_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m1x8_t vlseg8e_v_i64m1x8_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, _VL_T vl);
vint64m2x2_t vlseg2e_v_i64m2x2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, _VL_T vl);
vint64m2x3_t vlseg3e_v_i64m2x3_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, _VL_T vl);
vint64m2x4_t vlseg4e_v_i64m2x4_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, _VL_T vl);
vint64m4x2_t vlseg2e_v_i64m4x2_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int64_t *base, _VL_T vl);
vuint8m1x2_t vlseg2e_v_u8m1x2_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m1x3_t vlseg3e_v_u8m1x3_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m1x4_t vlseg4e_v_u8m1x4_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m1x5_t vlseg5e_v_u8m1x5_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m1x6_t vlseg6e_v_u8m1x6_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m1x7_t vlseg7e_v_u8m1x7_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m1x8_t vlseg8e_v_u8m1x8_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m2x2_t vlseg2e_v_u8m2x2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m2x3_t vlseg3e_v_u8m2x3_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m2x4_t vlseg4e_v_u8m2x4_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, _VL_T vl);
vuint8m4x2_t vlseg2e_v_u8m4x2_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, _VL_T vl);
vuint16m1x2_t vlseg2e_v_u16m1x2_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m1x3_t vlseg3e_v_u16m1x3_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m1x4_t vlseg4e_v_u16m1x4_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m1x5_t vlseg5e_v_u16m1x5_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m1x6_t vlseg6e_v_u16m1x6_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m1x7_t vlseg7e_v_u16m1x7_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m1x8_t vlseg8e_v_u16m1x8_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m2x2_t vlseg2e_v_u16m2x2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m2x3_t vlseg3e_v_u16m2x3_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m2x4_t vlseg4e_v_u16m2x4_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, _VL_T vl);
vuint16m4x2_t vlseg2e_v_u16m4x2_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, _VL_T vl);
vuint32m1x2_t vlseg2e_v_u32m1x2_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m1x3_t vlseg3e_v_u32m1x3_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m1x4_t vlseg4e_v_u32m1x4_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m1x5_t vlseg5e_v_u32m1x5_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m1x6_t vlseg6e_v_u32m1x6_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m1x7_t vlseg7e_v_u32m1x7_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m1x8_t vlseg8e_v_u32m1x8_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m2x2_t vlseg2e_v_u32m2x2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m2x3_t vlseg3e_v_u32m2x3_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m2x4_t vlseg4e_v_u32m2x4_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, _VL_T vl);
vuint32m4x2_t vlseg2e_v_u32m4x2_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, _VL_T vl);
vuint64m1x2_t vlseg2e_v_u64m1x2_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m1x3_t vlseg3e_v_u64m1x3_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m1x4_t vlseg4e_v_u64m1x4_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m1x5_t vlseg5e_v_u64m1x5_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m1x6_t vlseg6e_v_u64m1x6_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m1x7_t vlseg7e_v_u64m1x7_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m1x8_t vlseg8e_v_u64m1x8_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m2x2_t vlseg2e_v_u64m2x2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m2x3_t vlseg3e_v_u64m2x3_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m2x4_t vlseg4e_v_u64m2x4_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, _VL_T vl);
vuint64m4x2_t vlseg2e_v_u64m4x2_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint64_t *base, _VL_T vl);
vfloat16m1x2_t vlseg2e_v_f16m1x2_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m1x3_t vlseg3e_v_f16m1x3_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m1x4_t vlseg4e_v_f16m1x4_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m1x5_t vlseg5e_v_f16m1x5_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m1x6_t vlseg6e_v_f16m1x6_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m1x7_t vlseg7e_v_f16m1x7_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m1x8_t vlseg8e_v_f16m1x8_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m2x2_t vlseg2e_v_f16m2x2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m2x3_t vlseg3e_v_f16m2x3_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m2x4_t vlseg4e_v_f16m2x4_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, _VL_T vl);
vfloat16m4x2_t vlseg2e_v_f16m4x2_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, const float16_t *base, _VL_T vl);
vfloat32m1x2_t vlseg2e_v_f32m1x2_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m1x3_t vlseg3e_v_f32m1x3_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m1x4_t vlseg4e_v_f32m1x4_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m1x5_t vlseg5e_v_f32m1x5_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m1x6_t vlseg6e_v_f32m1x6_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m1x7_t vlseg7e_v_f32m1x7_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m1x8_t vlseg8e_v_f32m1x8_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m2x2_t vlseg2e_v_f32m2x2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m2x3_t vlseg3e_v_f32m2x3_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m2x4_t vlseg4e_v_f32m2x4_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, _VL_T vl);
vfloat32m4x2_t vlseg2e_v_f32m4x2_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, const float32_t *base, _VL_T vl);
vfloat64m1x2_t vlseg2e_v_f64m1x2_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m1x3_t vlseg3e_v_f64m1x3_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m1x4_t vlseg4e_v_f64m1x4_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m1x5_t vlseg5e_v_f64m1x5_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m1x6_t vlseg6e_v_f64m1x6_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m1x7_t vlseg7e_v_f64m1x7_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m1x8_t vlseg8e_v_f64m1x8_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m2x2_t vlseg2e_v_f64m2x2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m2x3_t vlseg3e_v_f64m2x3_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m2x4_t vlseg4e_v_f64m2x4_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, _VL_T vl);
vfloat64m4x2_t vlseg2e_v_f64m4x2_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, const float64_t *base, _VL_T vl);
```
### [Vector Unit-Stride Segment Store Functions]():

**Prototypes:**
``` C
void vsseg2e_v_i8m1x2_vl (int8_t *base, vint8m1x2_t value, _VL_T vl);
void vsseg3e_v_i8m1x3_vl (int8_t *base, vint8m1x3_t value, _VL_T vl);
void vsseg4e_v_i8m1x4_vl (int8_t *base, vint8m1x4_t value, _VL_T vl);
void vsseg5e_v_i8m1x5_vl (int8_t *base, vint8m1x5_t value, _VL_T vl);
void vsseg6e_v_i8m1x6_vl (int8_t *base, vint8m1x6_t value, _VL_T vl);
void vsseg7e_v_i8m1x7_vl (int8_t *base, vint8m1x7_t value, _VL_T vl);
void vsseg8e_v_i8m1x8_vl (int8_t *base, vint8m1x8_t value, _VL_T vl);
void vsseg2e_v_i8m2x2_vl (int8_t *base, vint8m2x2_t value, _VL_T vl);
void vsseg3e_v_i8m2x3_vl (int8_t *base, vint8m2x3_t value, _VL_T vl);
void vsseg4e_v_i8m2x4_vl (int8_t *base, vint8m2x4_t value, _VL_T vl);
void vsseg2e_v_i8m4x2_vl (int8_t *base, vint8m4x2_t value, _VL_T vl);
void vsseg2e_v_i16m1x2_vl (int16_t *base, vint16m1x2_t value, _VL_T vl);
void vsseg3e_v_i16m1x3_vl (int16_t *base, vint16m1x3_t value, _VL_T vl);
void vsseg4e_v_i16m1x4_vl (int16_t *base, vint16m1x4_t value, _VL_T vl);
void vsseg5e_v_i16m1x5_vl (int16_t *base, vint16m1x5_t value, _VL_T vl);
void vsseg6e_v_i16m1x6_vl (int16_t *base, vint16m1x6_t value, _VL_T vl);
void vsseg7e_v_i16m1x7_vl (int16_t *base, vint16m1x7_t value, _VL_T vl);
void vsseg8e_v_i16m1x8_vl (int16_t *base, vint16m1x8_t value, _VL_T vl);
void vsseg2e_v_i16m2x2_vl (int16_t *base, vint16m2x2_t value, _VL_T vl);
void vsseg3e_v_i16m2x3_vl (int16_t *base, vint16m2x3_t value, _VL_T vl);
void vsseg4e_v_i16m2x4_vl (int16_t *base, vint16m2x4_t value, _VL_T vl);
void vsseg2e_v_i16m4x2_vl (int16_t *base, vint16m4x2_t value, _VL_T vl);
void vsseg2e_v_i32m1x2_vl (int32_t *base, vint32m1x2_t value, _VL_T vl);
void vsseg3e_v_i32m1x3_vl (int32_t *base, vint32m1x3_t value, _VL_T vl);
void vsseg4e_v_i32m1x4_vl (int32_t *base, vint32m1x4_t value, _VL_T vl);
void vsseg5e_v_i32m1x5_vl (int32_t *base, vint32m1x5_t value, _VL_T vl);
void vsseg6e_v_i32m1x6_vl (int32_t *base, vint32m1x6_t value, _VL_T vl);
void vsseg7e_v_i32m1x7_vl (int32_t *base, vint32m1x7_t value, _VL_T vl);
void vsseg8e_v_i32m1x8_vl (int32_t *base, vint32m1x8_t value, _VL_T vl);
void vsseg2e_v_i32m2x2_vl (int32_t *base, vint32m2x2_t value, _VL_T vl);
void vsseg3e_v_i32m2x3_vl (int32_t *base, vint32m2x3_t value, _VL_T vl);
void vsseg4e_v_i32m2x4_vl (int32_t *base, vint32m2x4_t value, _VL_T vl);
void vsseg2e_v_i32m4x2_vl (int32_t *base, vint32m4x2_t value, _VL_T vl);
void vsseg2e_v_i64m1x2_vl (int64_t *base, vint64m1x2_t value, _VL_T vl);
void vsseg3e_v_i64m1x3_vl (int64_t *base, vint64m1x3_t value, _VL_T vl);
void vsseg4e_v_i64m1x4_vl (int64_t *base, vint64m1x4_t value, _VL_T vl);
void vsseg5e_v_i64m1x5_vl (int64_t *base, vint64m1x5_t value, _VL_T vl);
void vsseg6e_v_i64m1x6_vl (int64_t *base, vint64m1x6_t value, _VL_T vl);
void vsseg7e_v_i64m1x7_vl (int64_t *base, vint64m1x7_t value, _VL_T vl);
void vsseg8e_v_i64m1x8_vl (int64_t *base, vint64m1x8_t value, _VL_T vl);
void vsseg2e_v_i64m2x2_vl (int64_t *base, vint64m2x2_t value, _VL_T vl);
void vsseg3e_v_i64m2x3_vl (int64_t *base, vint64m2x3_t value, _VL_T vl);
void vsseg4e_v_i64m2x4_vl (int64_t *base, vint64m2x4_t value, _VL_T vl);
void vsseg2e_v_i64m4x2_vl (int64_t *base, vint64m4x2_t value, _VL_T vl);
void vsseg2e_v_u8m1x2_vl (uint8_t *base, vuint8m1x2_t value, _VL_T vl);
void vsseg3e_v_u8m1x3_vl (uint8_t *base, vuint8m1x3_t value, _VL_T vl);
void vsseg4e_v_u8m1x4_vl (uint8_t *base, vuint8m1x4_t value, _VL_T vl);
void vsseg5e_v_u8m1x5_vl (uint8_t *base, vuint8m1x5_t value, _VL_T vl);
void vsseg6e_v_u8m1x6_vl (uint8_t *base, vuint8m1x6_t value, _VL_T vl);
void vsseg7e_v_u8m1x7_vl (uint8_t *base, vuint8m1x7_t value, _VL_T vl);
void vsseg8e_v_u8m1x8_vl (uint8_t *base, vuint8m1x8_t value, _VL_T vl);
void vsseg2e_v_u8m2x2_vl (uint8_t *base, vuint8m2x2_t value, _VL_T vl);
void vsseg3e_v_u8m2x3_vl (uint8_t *base, vuint8m2x3_t value, _VL_T vl);
void vsseg4e_v_u8m2x4_vl (uint8_t *base, vuint8m2x4_t value, _VL_T vl);
void vsseg2e_v_u8m4x2_vl (uint8_t *base, vuint8m4x2_t value, _VL_T vl);
void vsseg2e_v_u16m1x2_vl (uint16_t *base, vuint16m1x2_t value, _VL_T vl);
void vsseg3e_v_u16m1x3_vl (uint16_t *base, vuint16m1x3_t value, _VL_T vl);
void vsseg4e_v_u16m1x4_vl (uint16_t *base, vuint16m1x4_t value, _VL_T vl);
void vsseg5e_v_u16m1x5_vl (uint16_t *base, vuint16m1x5_t value, _VL_T vl);
void vsseg6e_v_u16m1x6_vl (uint16_t *base, vuint16m1x6_t value, _VL_T vl);
void vsseg7e_v_u16m1x7_vl (uint16_t *base, vuint16m1x7_t value, _VL_T vl);
void vsseg8e_v_u16m1x8_vl (uint16_t *base, vuint16m1x8_t value, _VL_T vl);
void vsseg2e_v_u16m2x2_vl (uint16_t *base, vuint16m2x2_t value, _VL_T vl);
void vsseg3e_v_u16m2x3_vl (uint16_t *base, vuint16m2x3_t value, _VL_T vl);
void vsseg4e_v_u16m2x4_vl (uint16_t *base, vuint16m2x4_t value, _VL_T vl);
void vsseg2e_v_u16m4x2_vl (uint16_t *base, vuint16m4x2_t value, _VL_T vl);
void vsseg2e_v_u32m1x2_vl (uint32_t *base, vuint32m1x2_t value, _VL_T vl);
void vsseg3e_v_u32m1x3_vl (uint32_t *base, vuint32m1x3_t value, _VL_T vl);
void vsseg4e_v_u32m1x4_vl (uint32_t *base, vuint32m1x4_t value, _VL_T vl);
void vsseg5e_v_u32m1x5_vl (uint32_t *base, vuint32m1x5_t value, _VL_T vl);
void vsseg6e_v_u32m1x6_vl (uint32_t *base, vuint32m1x6_t value, _VL_T vl);
void vsseg7e_v_u32m1x7_vl (uint32_t *base, vuint32m1x7_t value, _VL_T vl);
void vsseg8e_v_u32m1x8_vl (uint32_t *base, vuint32m1x8_t value, _VL_T vl);
void vsseg2e_v_u32m2x2_vl (uint32_t *base, vuint32m2x2_t value, _VL_T vl);
void vsseg3e_v_u32m2x3_vl (uint32_t *base, vuint32m2x3_t value, _VL_T vl);
void vsseg4e_v_u32m2x4_vl (uint32_t *base, vuint32m2x4_t value, _VL_T vl);
void vsseg2e_v_u32m4x2_vl (uint32_t *base, vuint32m4x2_t value, _VL_T vl);
void vsseg2e_v_u64m1x2_vl (uint64_t *base, vuint64m1x2_t value, _VL_T vl);
void vsseg3e_v_u64m1x3_vl (uint64_t *base, vuint64m1x3_t value, _VL_T vl);
void vsseg4e_v_u64m1x4_vl (uint64_t *base, vuint64m1x4_t value, _VL_T vl);
void vsseg5e_v_u64m1x5_vl (uint64_t *base, vuint64m1x5_t value, _VL_T vl);
void vsseg6e_v_u64m1x6_vl (uint64_t *base, vuint64m1x6_t value, _VL_T vl);
void vsseg7e_v_u64m1x7_vl (uint64_t *base, vuint64m1x7_t value, _VL_T vl);
void vsseg8e_v_u64m1x8_vl (uint64_t *base, vuint64m1x8_t value, _VL_T vl);
void vsseg2e_v_u64m2x2_vl (uint64_t *base, vuint64m2x2_t value, _VL_T vl);
void vsseg3e_v_u64m2x3_vl (uint64_t *base, vuint64m2x3_t value, _VL_T vl);
void vsseg4e_v_u64m2x4_vl (uint64_t *base, vuint64m2x4_t value, _VL_T vl);
void vsseg2e_v_u64m4x2_vl (uint64_t *base, vuint64m4x2_t value, _VL_T vl);
void vsseg2e_v_f16m1x2_vl (float16_t *base, vfloat16m1x2_t value, _VL_T vl);
void vsseg3e_v_f16m1x3_vl (float16_t *base, vfloat16m1x3_t value, _VL_T vl);
void vsseg4e_v_f16m1x4_vl (float16_t *base, vfloat16m1x4_t value, _VL_T vl);
void vsseg5e_v_f16m1x5_vl (float16_t *base, vfloat16m1x5_t value, _VL_T vl);
void vsseg6e_v_f16m1x6_vl (float16_t *base, vfloat16m1x6_t value, _VL_T vl);
void vsseg7e_v_f16m1x7_vl (float16_t *base, vfloat16m1x7_t value, _VL_T vl);
void vsseg8e_v_f16m1x8_vl (float16_t *base, vfloat16m1x8_t value, _VL_T vl);
void vsseg2e_v_f16m2x2_vl (float16_t *base, vfloat16m2x2_t value, _VL_T vl);
void vsseg3e_v_f16m2x3_vl (float16_t *base, vfloat16m2x3_t value, _VL_T vl);
void vsseg4e_v_f16m2x4_vl (float16_t *base, vfloat16m2x4_t value, _VL_T vl);
void vsseg2e_v_f16m4x2_vl (float16_t *base, vfloat16m4x2_t value, _VL_T vl);
void vsseg2e_v_f32m1x2_vl (float32_t *base, vfloat32m1x2_t value, _VL_T vl);
void vsseg3e_v_f32m1x3_vl (float32_t *base, vfloat32m1x3_t value, _VL_T vl);
void vsseg4e_v_f32m1x4_vl (float32_t *base, vfloat32m1x4_t value, _VL_T vl);
void vsseg5e_v_f32m1x5_vl (float32_t *base, vfloat32m1x5_t value, _VL_T vl);
void vsseg6e_v_f32m1x6_vl (float32_t *base, vfloat32m1x6_t value, _VL_T vl);
void vsseg7e_v_f32m1x7_vl (float32_t *base, vfloat32m1x7_t value, _VL_T vl);
void vsseg8e_v_f32m1x8_vl (float32_t *base, vfloat32m1x8_t value, _VL_T vl);
void vsseg2e_v_f32m2x2_vl (float32_t *base, vfloat32m2x2_t value, _VL_T vl);
void vsseg3e_v_f32m2x3_vl (float32_t *base, vfloat32m2x3_t value, _VL_T vl);
void vsseg4e_v_f32m2x4_vl (float32_t *base, vfloat32m2x4_t value, _VL_T vl);
void vsseg2e_v_f32m4x2_vl (float32_t *base, vfloat32m4x2_t value, _VL_T vl);
void vsseg2e_v_f64m1x2_vl (float64_t *base, vfloat64m1x2_t value, _VL_T vl);
void vsseg3e_v_f64m1x3_vl (float64_t *base, vfloat64m1x3_t value, _VL_T vl);
void vsseg4e_v_f64m1x4_vl (float64_t *base, vfloat64m1x4_t value, _VL_T vl);
void vsseg5e_v_f64m1x5_vl (float64_t *base, vfloat64m1x5_t value, _VL_T vl);
void vsseg6e_v_f64m1x6_vl (float64_t *base, vfloat64m1x6_t value, _VL_T vl);
void vsseg7e_v_f64m1x7_vl (float64_t *base, vfloat64m1x7_t value, _VL_T vl);
void vsseg8e_v_f64m1x8_vl (float64_t *base, vfloat64m1x8_t value, _VL_T vl);
void vsseg2e_v_f64m2x2_vl (float64_t *base, vfloat64m2x2_t value, _VL_T vl);
void vsseg3e_v_f64m2x3_vl (float64_t *base, vfloat64m2x3_t value, _VL_T vl);
void vsseg4e_v_f64m2x4_vl (float64_t *base, vfloat64m2x4_t value, _VL_T vl);
void vsseg2e_v_f64m4x2_vl (float64_t *base, vfloat64m4x2_t value, _VL_T vl);
// masked functions
void vsseg2e_v_i8m1x2_m_vl (int8_t *base, vbool8_t mask, vint8m1x2_t value, _VL_T vl);
void vsseg3e_v_i8m1x3_m_vl (int8_t *base, vbool8_t mask, vint8m1x3_t value, _VL_T vl);
void vsseg4e_v_i8m1x4_m_vl (int8_t *base, vbool8_t mask, vint8m1x4_t value, _VL_T vl);
void vsseg5e_v_i8m1x5_m_vl (int8_t *base, vbool8_t mask, vint8m1x5_t value, _VL_T vl);
void vsseg6e_v_i8m1x6_m_vl (int8_t *base, vbool8_t mask, vint8m1x6_t value, _VL_T vl);
void vsseg7e_v_i8m1x7_m_vl (int8_t *base, vbool8_t mask, vint8m1x7_t value, _VL_T vl);
void vsseg8e_v_i8m1x8_m_vl (int8_t *base, vbool8_t mask, vint8m1x8_t value, _VL_T vl);
void vsseg2e_v_i8m2x2_m_vl (int8_t *base, vbool4_t mask, vint8m2x2_t value, _VL_T vl);
void vsseg3e_v_i8m2x3_m_vl (int8_t *base, vbool4_t mask, vint8m2x3_t value, _VL_T vl);
void vsseg4e_v_i8m2x4_m_vl (int8_t *base, vbool4_t mask, vint8m2x4_t value, _VL_T vl);
void vsseg2e_v_i8m4x2_m_vl (int8_t *base, vbool2_t mask, vint8m4x2_t value, _VL_T vl);
void vsseg2e_v_i16m1x2_m_vl (int16_t *base, vbool16_t mask, vint16m1x2_t value, _VL_T vl);
void vsseg3e_v_i16m1x3_m_vl (int16_t *base, vbool16_t mask, vint16m1x3_t value, _VL_T vl);
void vsseg4e_v_i16m1x4_m_vl (int16_t *base, vbool16_t mask, vint16m1x4_t value, _VL_T vl);
void vsseg5e_v_i16m1x5_m_vl (int16_t *base, vbool16_t mask, vint16m1x5_t value, _VL_T vl);
void vsseg6e_v_i16m1x6_m_vl (int16_t *base, vbool16_t mask, vint16m1x6_t value, _VL_T vl);
void vsseg7e_v_i16m1x7_m_vl (int16_t *base, vbool16_t mask, vint16m1x7_t value, _VL_T vl);
void vsseg8e_v_i16m1x8_m_vl (int16_t *base, vbool16_t mask, vint16m1x8_t value, _VL_T vl);
void vsseg2e_v_i16m2x2_m_vl (int16_t *base, vbool8_t mask, vint16m2x2_t value, _VL_T vl);
void vsseg3e_v_i16m2x3_m_vl (int16_t *base, vbool8_t mask, vint16m2x3_t value, _VL_T vl);
void vsseg4e_v_i16m2x4_m_vl (int16_t *base, vbool8_t mask, vint16m2x4_t value, _VL_T vl);
void vsseg2e_v_i16m4x2_m_vl (int16_t *base, vbool4_t mask, vint16m4x2_t value, _VL_T vl);
void vsseg2e_v_i32m1x2_m_vl (int32_t *base, vbool32_t mask, vint32m1x2_t value, _VL_T vl);
void vsseg3e_v_i32m1x3_m_vl (int32_t *base, vbool32_t mask, vint32m1x3_t value, _VL_T vl);
void vsseg4e_v_i32m1x4_m_vl (int32_t *base, vbool32_t mask, vint32m1x4_t value, _VL_T vl);
void vsseg5e_v_i32m1x5_m_vl (int32_t *base, vbool32_t mask, vint32m1x5_t value, _VL_T vl);
void vsseg6e_v_i32m1x6_m_vl (int32_t *base, vbool32_t mask, vint32m1x6_t value, _VL_T vl);
void vsseg7e_v_i32m1x7_m_vl (int32_t *base, vbool32_t mask, vint32m1x7_t value, _VL_T vl);
void vsseg8e_v_i32m1x8_m_vl (int32_t *base, vbool32_t mask, vint32m1x8_t value, _VL_T vl);
void vsseg2e_v_i32m2x2_m_vl (int32_t *base, vbool16_t mask, vint32m2x2_t value, _VL_T vl);
void vsseg3e_v_i32m2x3_m_vl (int32_t *base, vbool16_t mask, vint32m2x3_t value, _VL_T vl);
void vsseg4e_v_i32m2x4_m_vl (int32_t *base, vbool16_t mask, vint32m2x4_t value, _VL_T vl);
void vsseg2e_v_i32m4x2_m_vl (int32_t *base, vbool8_t mask, vint32m4x2_t value, _VL_T vl);
void vsseg2e_v_i64m1x2_m_vl (int64_t *base, vbool64_t mask, vint64m1x2_t value, _VL_T vl);
void vsseg3e_v_i64m1x3_m_vl (int64_t *base, vbool64_t mask, vint64m1x3_t value, _VL_T vl);
void vsseg4e_v_i64m1x4_m_vl (int64_t *base, vbool64_t mask, vint64m1x4_t value, _VL_T vl);
void vsseg5e_v_i64m1x5_m_vl (int64_t *base, vbool64_t mask, vint64m1x5_t value, _VL_T vl);
void vsseg6e_v_i64m1x6_m_vl (int64_t *base, vbool64_t mask, vint64m1x6_t value, _VL_T vl);
void vsseg7e_v_i64m1x7_m_vl (int64_t *base, vbool64_t mask, vint64m1x7_t value, _VL_T vl);
void vsseg8e_v_i64m1x8_m_vl (int64_t *base, vbool64_t mask, vint64m1x8_t value, _VL_T vl);
void vsseg2e_v_i64m2x2_m_vl (int64_t *base, vbool32_t mask, vint64m2x2_t value, _VL_T vl);
void vsseg3e_v_i64m2x3_m_vl (int64_t *base, vbool32_t mask, vint64m2x3_t value, _VL_T vl);
void vsseg4e_v_i64m2x4_m_vl (int64_t *base, vbool32_t mask, vint64m2x4_t value, _VL_T vl);
void vsseg2e_v_i64m4x2_m_vl (int64_t *base, vbool16_t mask, vint64m4x2_t value, _VL_T vl);
void vsseg2e_v_u8m1x2_m_vl (uint8_t *base, vbool8_t mask, vuint8m1x2_t value, _VL_T vl);
void vsseg3e_v_u8m1x3_m_vl (uint8_t *base, vbool8_t mask, vuint8m1x3_t value, _VL_T vl);
void vsseg4e_v_u8m1x4_m_vl (uint8_t *base, vbool8_t mask, vuint8m1x4_t value, _VL_T vl);
void vsseg5e_v_u8m1x5_m_vl (uint8_t *base, vbool8_t mask, vuint8m1x5_t value, _VL_T vl);
void vsseg6e_v_u8m1x6_m_vl (uint8_t *base, vbool8_t mask, vuint8m1x6_t value, _VL_T vl);
void vsseg7e_v_u8m1x7_m_vl (uint8_t *base, vbool8_t mask, vuint8m1x7_t value, _VL_T vl);
void vsseg8e_v_u8m1x8_m_vl (uint8_t *base, vbool8_t mask, vuint8m1x8_t value, _VL_T vl);
void vsseg2e_v_u8m2x2_m_vl (uint8_t *base, vbool4_t mask, vuint8m2x2_t value, _VL_T vl);
void vsseg3e_v_u8m2x3_m_vl (uint8_t *base, vbool4_t mask, vuint8m2x3_t value, _VL_T vl);
void vsseg4e_v_u8m2x4_m_vl (uint8_t *base, vbool4_t mask, vuint8m2x4_t value, _VL_T vl);
void vsseg2e_v_u8m4x2_m_vl (uint8_t *base, vbool2_t mask, vuint8m4x2_t value, _VL_T vl);
void vsseg2e_v_u16m1x2_m_vl (uint16_t *base, vbool16_t mask, vuint16m1x2_t value, _VL_T vl);
void vsseg3e_v_u16m1x3_m_vl (uint16_t *base, vbool16_t mask, vuint16m1x3_t value, _VL_T vl);
void vsseg4e_v_u16m1x4_m_vl (uint16_t *base, vbool16_t mask, vuint16m1x4_t value, _VL_T vl);
void vsseg5e_v_u16m1x5_m_vl (uint16_t *base, vbool16_t mask, vuint16m1x5_t value, _VL_T vl);
void vsseg6e_v_u16m1x6_m_vl (uint16_t *base, vbool16_t mask, vuint16m1x6_t value, _VL_T vl);
void vsseg7e_v_u16m1x7_m_vl (uint16_t *base, vbool16_t mask, vuint16m1x7_t value, _VL_T vl);
void vsseg8e_v_u16m1x8_m_vl (uint16_t *base, vbool16_t mask, vuint16m1x8_t value, _VL_T vl);
void vsseg2e_v_u16m2x2_m_vl (uint16_t *base, vbool8_t mask, vuint16m2x2_t value, _VL_T vl);
void vsseg3e_v_u16m2x3_m_vl (uint16_t *base, vbool8_t mask, vuint16m2x3_t value, _VL_T vl);
void vsseg4e_v_u16m2x4_m_vl (uint16_t *base, vbool8_t mask, vuint16m2x4_t value, _VL_T vl);
void vsseg2e_v_u16m4x2_m_vl (uint16_t *base, vbool4_t mask, vuint16m4x2_t value, _VL_T vl);
void vsseg2e_v_u32m1x2_m_vl (uint32_t *base, vbool32_t mask, vuint32m1x2_t value, _VL_T vl);
void vsseg3e_v_u32m1x3_m_vl (uint32_t *base, vbool32_t mask, vuint32m1x3_t value, _VL_T vl);
void vsseg4e_v_u32m1x4_m_vl (uint32_t *base, vbool32_t mask, vuint32m1x4_t value, _VL_T vl);
void vsseg5e_v_u32m1x5_m_vl (uint32_t *base, vbool32_t mask, vuint32m1x5_t value, _VL_T vl);
void vsseg6e_v_u32m1x6_m_vl (uint32_t *base, vbool32_t mask, vuint32m1x6_t value, _VL_T vl);
void vsseg7e_v_u32m1x7_m_vl (uint32_t *base, vbool32_t mask, vuint32m1x7_t value, _VL_T vl);
void vsseg8e_v_u32m1x8_m_vl (uint32_t *base, vbool32_t mask, vuint32m1x8_t value, _VL_T vl);
void vsseg2e_v_u32m2x2_m_vl (uint32_t *base, vbool16_t mask, vuint32m2x2_t value, _VL_T vl);
void vsseg3e_v_u32m2x3_m_vl (uint32_t *base, vbool16_t mask, vuint32m2x3_t value, _VL_T vl);
void vsseg4e_v_u32m2x4_m_vl (uint32_t *base, vbool16_t mask, vuint32m2x4_t value, _VL_T vl);
void vsseg2e_v_u32m4x2_m_vl (uint32_t *base, vbool8_t mask, vuint32m4x2_t value, _VL_T vl);
void vsseg2e_v_u64m1x2_m_vl (uint64_t *base, vbool64_t mask, vuint64m1x2_t value, _VL_T vl);
void vsseg3e_v_u64m1x3_m_vl (uint64_t *base, vbool64_t mask, vuint64m1x3_t value, _VL_T vl);
void vsseg4e_v_u64m1x4_m_vl (uint64_t *base, vbool64_t mask, vuint64m1x4_t value, _VL_T vl);
void vsseg5e_v_u64m1x5_m_vl (uint64_t *base, vbool64_t mask, vuint64m1x5_t value, _VL_T vl);
void vsseg6e_v_u64m1x6_m_vl (uint64_t *base, vbool64_t mask, vuint64m1x6_t value, _VL_T vl);
void vsseg7e_v_u64m1x7_m_vl (uint64_t *base, vbool64_t mask, vuint64m1x7_t value, _VL_T vl);
void vsseg8e_v_u64m1x8_m_vl (uint64_t *base, vbool64_t mask, vuint64m1x8_t value, _VL_T vl);
void vsseg2e_v_u64m2x2_m_vl (uint64_t *base, vbool32_t mask, vuint64m2x2_t value, _VL_T vl);
void vsseg3e_v_u64m2x3_m_vl (uint64_t *base, vbool32_t mask, vuint64m2x3_t value, _VL_T vl);
void vsseg4e_v_u64m2x4_m_vl (uint64_t *base, vbool32_t mask, vuint64m2x4_t value, _VL_T vl);
void vsseg2e_v_u64m4x2_m_vl (uint64_t *base, vbool16_t mask, vuint64m4x2_t value, _VL_T vl);
void vsseg2e_v_f16m1x2_m_vl (float16_t *base, vbool16_t mask, vfloat16m1x2_t value, _VL_T vl);
void vsseg3e_v_f16m1x3_m_vl (float16_t *base, vbool16_t mask, vfloat16m1x3_t value, _VL_T vl);
void vsseg4e_v_f16m1x4_m_vl (float16_t *base, vbool16_t mask, vfloat16m1x4_t value, _VL_T vl);
void vsseg5e_v_f16m1x5_m_vl (float16_t *base, vbool16_t mask, vfloat16m1x5_t value, _VL_T vl);
void vsseg6e_v_f16m1x6_m_vl (float16_t *base, vbool16_t mask, vfloat16m1x6_t value, _VL_T vl);
void vsseg7e_v_f16m1x7_m_vl (float16_t *base, vbool16_t mask, vfloat16m1x7_t value, _VL_T vl);
void vsseg8e_v_f16m1x8_m_vl (float16_t *base, vbool16_t mask, vfloat16m1x8_t value, _VL_T vl);
void vsseg2e_v_f16m2x2_m_vl (float16_t *base, vbool8_t mask, vfloat16m2x2_t value, _VL_T vl);
void vsseg3e_v_f16m2x3_m_vl (float16_t *base, vbool8_t mask, vfloat16m2x3_t value, _VL_T vl);
void vsseg4e_v_f16m2x4_m_vl (float16_t *base, vbool8_t mask, vfloat16m2x4_t value, _VL_T vl);
void vsseg2e_v_f16m4x2_m_vl (float16_t *base, vbool4_t mask, vfloat16m4x2_t value, _VL_T vl);
void vsseg2e_v_f32m1x2_m_vl (float32_t *base, vbool32_t mask, vfloat32m1x2_t value, _VL_T vl);
void vsseg3e_v_f32m1x3_m_vl (float32_t *base, vbool32_t mask, vfloat32m1x3_t value, _VL_T vl);
void vsseg4e_v_f32m1x4_m_vl (float32_t *base, vbool32_t mask, vfloat32m1x4_t value, _VL_T vl);
void vsseg5e_v_f32m1x5_m_vl (float32_t *base, vbool32_t mask, vfloat32m1x5_t value, _VL_T vl);
void vsseg6e_v_f32m1x6_m_vl (float32_t *base, vbool32_t mask, vfloat32m1x6_t value, _VL_T vl);
void vsseg7e_v_f32m1x7_m_vl (float32_t *base, vbool32_t mask, vfloat32m1x7_t value, _VL_T vl);
void vsseg8e_v_f32m1x8_m_vl (float32_t *base, vbool32_t mask, vfloat32m1x8_t value, _VL_T vl);
void vsseg2e_v_f32m2x2_m_vl (float32_t *base, vbool16_t mask, vfloat32m2x2_t value, _VL_T vl);
void vsseg3e_v_f32m2x3_m_vl (float32_t *base, vbool16_t mask, vfloat32m2x3_t value, _VL_T vl);
void vsseg4e_v_f32m2x4_m_vl (float32_t *base, vbool16_t mask, vfloat32m2x4_t value, _VL_T vl);
void vsseg2e_v_f32m4x2_m_vl (float32_t *base, vbool8_t mask, vfloat32m4x2_t value, _VL_T vl);
void vsseg2e_v_f64m1x2_m_vl (float64_t *base, vbool64_t mask, vfloat64m1x2_t value, _VL_T vl);
void vsseg3e_v_f64m1x3_m_vl (float64_t *base, vbool64_t mask, vfloat64m1x3_t value, _VL_T vl);
void vsseg4e_v_f64m1x4_m_vl (float64_t *base, vbool64_t mask, vfloat64m1x4_t value, _VL_T vl);
void vsseg5e_v_f64m1x5_m_vl (float64_t *base, vbool64_t mask, vfloat64m1x5_t value, _VL_T vl);
void vsseg6e_v_f64m1x6_m_vl (float64_t *base, vbool64_t mask, vfloat64m1x6_t value, _VL_T vl);
void vsseg7e_v_f64m1x7_m_vl (float64_t *base, vbool64_t mask, vfloat64m1x7_t value, _VL_T vl);
void vsseg8e_v_f64m1x8_m_vl (float64_t *base, vbool64_t mask, vfloat64m1x8_t value, _VL_T vl);
void vsseg2e_v_f64m2x2_m_vl (float64_t *base, vbool32_t mask, vfloat64m2x2_t value, _VL_T vl);
void vsseg3e_v_f64m2x3_m_vl (float64_t *base, vbool32_t mask, vfloat64m2x3_t value, _VL_T vl);
void vsseg4e_v_f64m2x4_m_vl (float64_t *base, vbool32_t mask, vfloat64m2x4_t value, _VL_T vl);
void vsseg2e_v_f64m4x2_m_vl (float64_t *base, vbool16_t mask, vfloat64m4x2_t value, _VL_T vl);
```
### [Vector Strided Segment Load Functions]():

**Prototypes:**
``` C
vint8m1x2_t vlsseg2e_v_i8m1x2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x3_t vlsseg3e_v_i8m1x3_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x4_t vlsseg4e_v_i8m1x4_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x5_t vlsseg5e_v_i8m1x5_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x6_t vlsseg6e_v_i8m1x6_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x7_t vlsseg7e_v_i8m1x7_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x8_t vlsseg8e_v_i8m1x8_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2x2_t vlsseg2e_v_i8m2x2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2x3_t vlsseg3e_v_i8m2x3_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2x4_t vlsseg4e_v_i8m2x4_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m4x2_t vlsseg2e_v_i8m4x2_vl (const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x2_t vlsseg2e_v_i16m1x2_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x3_t vlsseg3e_v_i16m1x3_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x4_t vlsseg4e_v_i16m1x4_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x5_t vlsseg5e_v_i16m1x5_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x6_t vlsseg6e_v_i16m1x6_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x7_t vlsseg7e_v_i16m1x7_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x8_t vlsseg8e_v_i16m1x8_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2x2_t vlsseg2e_v_i16m2x2_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2x3_t vlsseg3e_v_i16m2x3_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2x4_t vlsseg4e_v_i16m2x4_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4x2_t vlsseg2e_v_i16m4x2_vl (const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x2_t vlsseg2e_v_i32m1x2_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x3_t vlsseg3e_v_i32m1x3_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x4_t vlsseg4e_v_i32m1x4_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x5_t vlsseg5e_v_i32m1x5_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x6_t vlsseg6e_v_i32m1x6_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x7_t vlsseg7e_v_i32m1x7_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x8_t vlsseg8e_v_i32m1x8_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2x2_t vlsseg2e_v_i32m2x2_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2x3_t vlsseg3e_v_i32m2x3_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2x4_t vlsseg4e_v_i32m2x4_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4x2_t vlsseg2e_v_i32m4x2_vl (const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x2_t vlsseg2e_v_i64m1x2_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x3_t vlsseg3e_v_i64m1x3_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x4_t vlsseg4e_v_i64m1x4_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x5_t vlsseg5e_v_i64m1x5_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x6_t vlsseg6e_v_i64m1x6_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x7_t vlsseg7e_v_i64m1x7_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x8_t vlsseg8e_v_i64m1x8_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2x2_t vlsseg2e_v_i64m2x2_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2x3_t vlsseg3e_v_i64m2x3_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2x4_t vlsseg4e_v_i64m2x4_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4x2_t vlsseg2e_v_i64m4x2_vl (const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x2_t vlsseg2e_v_u8m1x2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x3_t vlsseg3e_v_u8m1x3_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x4_t vlsseg4e_v_u8m1x4_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x5_t vlsseg5e_v_u8m1x5_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x6_t vlsseg6e_v_u8m1x6_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x7_t vlsseg7e_v_u8m1x7_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x8_t vlsseg8e_v_u8m1x8_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2x2_t vlsseg2e_v_u8m2x2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2x3_t vlsseg3e_v_u8m2x3_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2x4_t vlsseg4e_v_u8m2x4_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m4x2_t vlsseg2e_v_u8m4x2_vl (const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x2_t vlsseg2e_v_u16m1x2_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x3_t vlsseg3e_v_u16m1x3_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x4_t vlsseg4e_v_u16m1x4_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x5_t vlsseg5e_v_u16m1x5_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x6_t vlsseg6e_v_u16m1x6_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x7_t vlsseg7e_v_u16m1x7_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x8_t vlsseg8e_v_u16m1x8_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2x2_t vlsseg2e_v_u16m2x2_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2x3_t vlsseg3e_v_u16m2x3_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2x4_t vlsseg4e_v_u16m2x4_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4x2_t vlsseg2e_v_u16m4x2_vl (const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x2_t vlsseg2e_v_u32m1x2_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x3_t vlsseg3e_v_u32m1x3_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x4_t vlsseg4e_v_u32m1x4_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x5_t vlsseg5e_v_u32m1x5_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x6_t vlsseg6e_v_u32m1x6_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x7_t vlsseg7e_v_u32m1x7_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x8_t vlsseg8e_v_u32m1x8_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2x2_t vlsseg2e_v_u32m2x2_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2x3_t vlsseg3e_v_u32m2x3_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2x4_t vlsseg4e_v_u32m2x4_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4x2_t vlsseg2e_v_u32m4x2_vl (const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x2_t vlsseg2e_v_u64m1x2_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x3_t vlsseg3e_v_u64m1x3_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x4_t vlsseg4e_v_u64m1x4_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x5_t vlsseg5e_v_u64m1x5_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x6_t vlsseg6e_v_u64m1x6_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x7_t vlsseg7e_v_u64m1x7_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x8_t vlsseg8e_v_u64m1x8_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2x2_t vlsseg2e_v_u64m2x2_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2x3_t vlsseg3e_v_u64m2x3_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2x4_t vlsseg4e_v_u64m2x4_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4x2_t vlsseg2e_v_u64m4x2_vl (const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x2_t vlsseg2e_v_f16m1x2_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x3_t vlsseg3e_v_f16m1x3_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x4_t vlsseg4e_v_f16m1x4_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x5_t vlsseg5e_v_f16m1x5_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x6_t vlsseg6e_v_f16m1x6_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x7_t vlsseg7e_v_f16m1x7_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x8_t vlsseg8e_v_f16m1x8_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2x2_t vlsseg2e_v_f16m2x2_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2x3_t vlsseg3e_v_f16m2x3_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2x4_t vlsseg4e_v_f16m2x4_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m4x2_t vlsseg2e_v_f16m4x2_vl (const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x2_t vlsseg2e_v_f32m1x2_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x3_t vlsseg3e_v_f32m1x3_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x4_t vlsseg4e_v_f32m1x4_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x5_t vlsseg5e_v_f32m1x5_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x6_t vlsseg6e_v_f32m1x6_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x7_t vlsseg7e_v_f32m1x7_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x8_t vlsseg8e_v_f32m1x8_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2x2_t vlsseg2e_v_f32m2x2_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2x3_t vlsseg3e_v_f32m2x3_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2x4_t vlsseg4e_v_f32m2x4_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m4x2_t vlsseg2e_v_f32m4x2_vl (const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x2_t vlsseg2e_v_f64m1x2_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x3_t vlsseg3e_v_f64m1x3_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x4_t vlsseg4e_v_f64m1x4_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x5_t vlsseg5e_v_f64m1x5_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x6_t vlsseg6e_v_f64m1x6_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x7_t vlsseg7e_v_f64m1x7_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x8_t vlsseg8e_v_f64m1x8_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2x2_t vlsseg2e_v_f64m2x2_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2x3_t vlsseg3e_v_f64m2x3_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2x4_t vlsseg4e_v_f64m2x4_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m4x2_t vlsseg2e_v_f64m4x2_vl (const float64_t *base, ptrdiff_t bstride, _VL_T vl);
// masked functions
vint8m1x2_t vlsseg2e_v_i8m1x2_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x3_t vlsseg3e_v_i8m1x3_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x4_t vlsseg4e_v_i8m1x4_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x5_t vlsseg5e_v_i8m1x5_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x6_t vlsseg6e_v_i8m1x6_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x7_t vlsseg7e_v_i8m1x7_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m1x8_t vlsseg8e_v_i8m1x8_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2x2_t vlsseg2e_v_i8m2x2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2x3_t vlsseg3e_v_i8m2x3_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m2x4_t vlsseg4e_v_i8m2x4_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint8m4x2_t vlsseg2e_v_i8m4x2_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x2_t vlsseg2e_v_i16m1x2_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x3_t vlsseg3e_v_i16m1x3_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x4_t vlsseg4e_v_i16m1x4_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x5_t vlsseg5e_v_i16m1x5_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x6_t vlsseg6e_v_i16m1x6_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x7_t vlsseg7e_v_i16m1x7_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m1x8_t vlsseg8e_v_i16m1x8_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2x2_t vlsseg2e_v_i16m2x2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2x3_t vlsseg3e_v_i16m2x3_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m2x4_t vlsseg4e_v_i16m2x4_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint16m4x2_t vlsseg2e_v_i16m4x2_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x2_t vlsseg2e_v_i32m1x2_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x3_t vlsseg3e_v_i32m1x3_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x4_t vlsseg4e_v_i32m1x4_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x5_t vlsseg5e_v_i32m1x5_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x6_t vlsseg6e_v_i32m1x6_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x7_t vlsseg7e_v_i32m1x7_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m1x8_t vlsseg8e_v_i32m1x8_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2x2_t vlsseg2e_v_i32m2x2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2x3_t vlsseg3e_v_i32m2x3_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m2x4_t vlsseg4e_v_i32m2x4_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint32m4x2_t vlsseg2e_v_i32m4x2_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x2_t vlsseg2e_v_i64m1x2_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x3_t vlsseg3e_v_i64m1x3_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x4_t vlsseg4e_v_i64m1x4_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x5_t vlsseg5e_v_i64m1x5_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x6_t vlsseg6e_v_i64m1x6_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x7_t vlsseg7e_v_i64m1x7_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m1x8_t vlsseg8e_v_i64m1x8_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2x2_t vlsseg2e_v_i64m2x2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2x3_t vlsseg3e_v_i64m2x3_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m2x4_t vlsseg4e_v_i64m2x4_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vint64m4x2_t vlsseg2e_v_i64m4x2_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x2_t vlsseg2e_v_u8m1x2_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x3_t vlsseg3e_v_u8m1x3_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x4_t vlsseg4e_v_u8m1x4_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x5_t vlsseg5e_v_u8m1x5_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x6_t vlsseg6e_v_u8m1x6_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x7_t vlsseg7e_v_u8m1x7_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m1x8_t vlsseg8e_v_u8m1x8_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2x2_t vlsseg2e_v_u8m2x2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2x3_t vlsseg3e_v_u8m2x3_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m2x4_t vlsseg4e_v_u8m2x4_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint8m4x2_t vlsseg2e_v_u8m4x2_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x2_t vlsseg2e_v_u16m1x2_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x3_t vlsseg3e_v_u16m1x3_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x4_t vlsseg4e_v_u16m1x4_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x5_t vlsseg5e_v_u16m1x5_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x6_t vlsseg6e_v_u16m1x6_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x7_t vlsseg7e_v_u16m1x7_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m1x8_t vlsseg8e_v_u16m1x8_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2x2_t vlsseg2e_v_u16m2x2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2x3_t vlsseg3e_v_u16m2x3_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m2x4_t vlsseg4e_v_u16m2x4_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint16m4x2_t vlsseg2e_v_u16m4x2_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x2_t vlsseg2e_v_u32m1x2_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x3_t vlsseg3e_v_u32m1x3_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x4_t vlsseg4e_v_u32m1x4_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x5_t vlsseg5e_v_u32m1x5_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x6_t vlsseg6e_v_u32m1x6_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x7_t vlsseg7e_v_u32m1x7_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m1x8_t vlsseg8e_v_u32m1x8_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2x2_t vlsseg2e_v_u32m2x2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2x3_t vlsseg3e_v_u32m2x3_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m2x4_t vlsseg4e_v_u32m2x4_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint32m4x2_t vlsseg2e_v_u32m4x2_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x2_t vlsseg2e_v_u64m1x2_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x3_t vlsseg3e_v_u64m1x3_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x4_t vlsseg4e_v_u64m1x4_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x5_t vlsseg5e_v_u64m1x5_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x6_t vlsseg6e_v_u64m1x6_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x7_t vlsseg7e_v_u64m1x7_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m1x8_t vlsseg8e_v_u64m1x8_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2x2_t vlsseg2e_v_u64m2x2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2x3_t vlsseg3e_v_u64m2x3_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m2x4_t vlsseg4e_v_u64m2x4_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vuint64m4x2_t vlsseg2e_v_u64m4x2_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x2_t vlsseg2e_v_f16m1x2_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x3_t vlsseg3e_v_f16m1x3_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x4_t vlsseg4e_v_f16m1x4_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x5_t vlsseg5e_v_f16m1x5_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x6_t vlsseg6e_v_f16m1x6_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x7_t vlsseg7e_v_f16m1x7_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m1x8_t vlsseg8e_v_f16m1x8_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2x2_t vlsseg2e_v_f16m2x2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2x3_t vlsseg3e_v_f16m2x3_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m2x4_t vlsseg4e_v_f16m2x4_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat16m4x2_t vlsseg2e_v_f16m4x2_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, const float16_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x2_t vlsseg2e_v_f32m1x2_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x3_t vlsseg3e_v_f32m1x3_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x4_t vlsseg4e_v_f32m1x4_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x5_t vlsseg5e_v_f32m1x5_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x6_t vlsseg6e_v_f32m1x6_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x7_t vlsseg7e_v_f32m1x7_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m1x8_t vlsseg8e_v_f32m1x8_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2x2_t vlsseg2e_v_f32m2x2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2x3_t vlsseg3e_v_f32m2x3_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m2x4_t vlsseg4e_v_f32m2x4_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat32m4x2_t vlsseg2e_v_f32m4x2_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, const float32_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x2_t vlsseg2e_v_f64m1x2_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x3_t vlsseg3e_v_f64m1x3_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x4_t vlsseg4e_v_f64m1x4_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x5_t vlsseg5e_v_f64m1x5_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x6_t vlsseg6e_v_f64m1x6_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x7_t vlsseg7e_v_f64m1x7_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m1x8_t vlsseg8e_v_f64m1x8_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2x2_t vlsseg2e_v_f64m2x2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2x3_t vlsseg3e_v_f64m2x3_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m2x4_t vlsseg4e_v_f64m2x4_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
vfloat64m4x2_t vlsseg2e_v_f64m4x2_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, const float64_t *base, ptrdiff_t bstride, _VL_T vl);
```
### [Vector Strided Segment Store Functions]():

**Prototypes:**
``` C
void vssseg2e_v_i8m1x2_vl (int8_t *base, ptrdiff_t bstride, vint8m1x2_t value, _VL_T vl);
void vssseg3e_v_i8m1x3_vl (int8_t *base, ptrdiff_t bstride, vint8m1x3_t value, _VL_T vl);
void vssseg4e_v_i8m1x4_vl (int8_t *base, ptrdiff_t bstride, vint8m1x4_t value, _VL_T vl);
void vssseg5e_v_i8m1x5_vl (int8_t *base, ptrdiff_t bstride, vint8m1x5_t value, _VL_T vl);
void vssseg6e_v_i8m1x6_vl (int8_t *base, ptrdiff_t bstride, vint8m1x6_t value, _VL_T vl);
void vssseg7e_v_i8m1x7_vl (int8_t *base, ptrdiff_t bstride, vint8m1x7_t value, _VL_T vl);
void vssseg8e_v_i8m1x8_vl (int8_t *base, ptrdiff_t bstride, vint8m1x8_t value, _VL_T vl);
void vssseg2e_v_i8m2x2_vl (int8_t *base, ptrdiff_t bstride, vint8m2x2_t value, _VL_T vl);
void vssseg3e_v_i8m2x3_vl (int8_t *base, ptrdiff_t bstride, vint8m2x3_t value, _VL_T vl);
void vssseg4e_v_i8m2x4_vl (int8_t *base, ptrdiff_t bstride, vint8m2x4_t value, _VL_T vl);
void vssseg2e_v_i8m4x2_vl (int8_t *base, ptrdiff_t bstride, vint8m4x2_t value, _VL_T vl);
void vssseg2e_v_i16m1x2_vl (int16_t *base, ptrdiff_t bstride, vint16m1x2_t value, _VL_T vl);
void vssseg3e_v_i16m1x3_vl (int16_t *base, ptrdiff_t bstride, vint16m1x3_t value, _VL_T vl);
void vssseg4e_v_i16m1x4_vl (int16_t *base, ptrdiff_t bstride, vint16m1x4_t value, _VL_T vl);
void vssseg5e_v_i16m1x5_vl (int16_t *base, ptrdiff_t bstride, vint16m1x5_t value, _VL_T vl);
void vssseg6e_v_i16m1x6_vl (int16_t *base, ptrdiff_t bstride, vint16m1x6_t value, _VL_T vl);
void vssseg7e_v_i16m1x7_vl (int16_t *base, ptrdiff_t bstride, vint16m1x7_t value, _VL_T vl);
void vssseg8e_v_i16m1x8_vl (int16_t *base, ptrdiff_t bstride, vint16m1x8_t value, _VL_T vl);
void vssseg2e_v_i16m2x2_vl (int16_t *base, ptrdiff_t bstride, vint16m2x2_t value, _VL_T vl);
void vssseg3e_v_i16m2x3_vl (int16_t *base, ptrdiff_t bstride, vint16m2x3_t value, _VL_T vl);
void vssseg4e_v_i16m2x4_vl (int16_t *base, ptrdiff_t bstride, vint16m2x4_t value, _VL_T vl);
void vssseg2e_v_i16m4x2_vl (int16_t *base, ptrdiff_t bstride, vint16m4x2_t value, _VL_T vl);
void vssseg2e_v_i32m1x2_vl (int32_t *base, ptrdiff_t bstride, vint32m1x2_t value, _VL_T vl);
void vssseg3e_v_i32m1x3_vl (int32_t *base, ptrdiff_t bstride, vint32m1x3_t value, _VL_T vl);
void vssseg4e_v_i32m1x4_vl (int32_t *base, ptrdiff_t bstride, vint32m1x4_t value, _VL_T vl);
void vssseg5e_v_i32m1x5_vl (int32_t *base, ptrdiff_t bstride, vint32m1x5_t value, _VL_T vl);
void vssseg6e_v_i32m1x6_vl (int32_t *base, ptrdiff_t bstride, vint32m1x6_t value, _VL_T vl);
void vssseg7e_v_i32m1x7_vl (int32_t *base, ptrdiff_t bstride, vint32m1x7_t value, _VL_T vl);
void vssseg8e_v_i32m1x8_vl (int32_t *base, ptrdiff_t bstride, vint32m1x8_t value, _VL_T vl);
void vssseg2e_v_i32m2x2_vl (int32_t *base, ptrdiff_t bstride, vint32m2x2_t value, _VL_T vl);
void vssseg3e_v_i32m2x3_vl (int32_t *base, ptrdiff_t bstride, vint32m2x3_t value, _VL_T vl);
void vssseg4e_v_i32m2x4_vl (int32_t *base, ptrdiff_t bstride, vint32m2x4_t value, _VL_T vl);
void vssseg2e_v_i32m4x2_vl (int32_t *base, ptrdiff_t bstride, vint32m4x2_t value, _VL_T vl);
void vssseg2e_v_i64m1x2_vl (int64_t *base, ptrdiff_t bstride, vint64m1x2_t value, _VL_T vl);
void vssseg3e_v_i64m1x3_vl (int64_t *base, ptrdiff_t bstride, vint64m1x3_t value, _VL_T vl);
void vssseg4e_v_i64m1x4_vl (int64_t *base, ptrdiff_t bstride, vint64m1x4_t value, _VL_T vl);
void vssseg5e_v_i64m1x5_vl (int64_t *base, ptrdiff_t bstride, vint64m1x5_t value, _VL_T vl);
void vssseg6e_v_i64m1x6_vl (int64_t *base, ptrdiff_t bstride, vint64m1x6_t value, _VL_T vl);
void vssseg7e_v_i64m1x7_vl (int64_t *base, ptrdiff_t bstride, vint64m1x7_t value, _VL_T vl);
void vssseg8e_v_i64m1x8_vl (int64_t *base, ptrdiff_t bstride, vint64m1x8_t value, _VL_T vl);
void vssseg2e_v_i64m2x2_vl (int64_t *base, ptrdiff_t bstride, vint64m2x2_t value, _VL_T vl);
void vssseg3e_v_i64m2x3_vl (int64_t *base, ptrdiff_t bstride, vint64m2x3_t value, _VL_T vl);
void vssseg4e_v_i64m2x4_vl (int64_t *base, ptrdiff_t bstride, vint64m2x4_t value, _VL_T vl);
void vssseg2e_v_i64m4x2_vl (int64_t *base, ptrdiff_t bstride, vint64m4x2_t value, _VL_T vl);
void vssseg2e_v_u8m1x2_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1x2_t value, _VL_T vl);
void vssseg3e_v_u8m1x3_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1x3_t value, _VL_T vl);
void vssseg4e_v_u8m1x4_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1x4_t value, _VL_T vl);
void vssseg5e_v_u8m1x5_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1x5_t value, _VL_T vl);
void vssseg6e_v_u8m1x6_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1x6_t value, _VL_T vl);
void vssseg7e_v_u8m1x7_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1x7_t value, _VL_T vl);
void vssseg8e_v_u8m1x8_vl (uint8_t *base, ptrdiff_t bstride, vuint8m1x8_t value, _VL_T vl);
void vssseg2e_v_u8m2x2_vl (uint8_t *base, ptrdiff_t bstride, vuint8m2x2_t value, _VL_T vl);
void vssseg3e_v_u8m2x3_vl (uint8_t *base, ptrdiff_t bstride, vuint8m2x3_t value, _VL_T vl);
void vssseg4e_v_u8m2x4_vl (uint8_t *base, ptrdiff_t bstride, vuint8m2x4_t value, _VL_T vl);
void vssseg2e_v_u8m4x2_vl (uint8_t *base, ptrdiff_t bstride, vuint8m4x2_t value, _VL_T vl);
void vssseg2e_v_u16m1x2_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1x2_t value, _VL_T vl);
void vssseg3e_v_u16m1x3_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1x3_t value, _VL_T vl);
void vssseg4e_v_u16m1x4_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1x4_t value, _VL_T vl);
void vssseg5e_v_u16m1x5_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1x5_t value, _VL_T vl);
void vssseg6e_v_u16m1x6_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1x6_t value, _VL_T vl);
void vssseg7e_v_u16m1x7_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1x7_t value, _VL_T vl);
void vssseg8e_v_u16m1x8_vl (uint16_t *base, ptrdiff_t bstride, vuint16m1x8_t value, _VL_T vl);
void vssseg2e_v_u16m2x2_vl (uint16_t *base, ptrdiff_t bstride, vuint16m2x2_t value, _VL_T vl);
void vssseg3e_v_u16m2x3_vl (uint16_t *base, ptrdiff_t bstride, vuint16m2x3_t value, _VL_T vl);
void vssseg4e_v_u16m2x4_vl (uint16_t *base, ptrdiff_t bstride, vuint16m2x4_t value, _VL_T vl);
void vssseg2e_v_u16m4x2_vl (uint16_t *base, ptrdiff_t bstride, vuint16m4x2_t value, _VL_T vl);
void vssseg2e_v_u32m1x2_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1x2_t value, _VL_T vl);
void vssseg3e_v_u32m1x3_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1x3_t value, _VL_T vl);
void vssseg4e_v_u32m1x4_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1x4_t value, _VL_T vl);
void vssseg5e_v_u32m1x5_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1x5_t value, _VL_T vl);
void vssseg6e_v_u32m1x6_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1x6_t value, _VL_T vl);
void vssseg7e_v_u32m1x7_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1x7_t value, _VL_T vl);
void vssseg8e_v_u32m1x8_vl (uint32_t *base, ptrdiff_t bstride, vuint32m1x8_t value, _VL_T vl);
void vssseg2e_v_u32m2x2_vl (uint32_t *base, ptrdiff_t bstride, vuint32m2x2_t value, _VL_T vl);
void vssseg3e_v_u32m2x3_vl (uint32_t *base, ptrdiff_t bstride, vuint32m2x3_t value, _VL_T vl);
void vssseg4e_v_u32m2x4_vl (uint32_t *base, ptrdiff_t bstride, vuint32m2x4_t value, _VL_T vl);
void vssseg2e_v_u32m4x2_vl (uint32_t *base, ptrdiff_t bstride, vuint32m4x2_t value, _VL_T vl);
void vssseg2e_v_u64m1x2_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1x2_t value, _VL_T vl);
void vssseg3e_v_u64m1x3_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1x3_t value, _VL_T vl);
void vssseg4e_v_u64m1x4_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1x4_t value, _VL_T vl);
void vssseg5e_v_u64m1x5_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1x5_t value, _VL_T vl);
void vssseg6e_v_u64m1x6_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1x6_t value, _VL_T vl);
void vssseg7e_v_u64m1x7_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1x7_t value, _VL_T vl);
void vssseg8e_v_u64m1x8_vl (uint64_t *base, ptrdiff_t bstride, vuint64m1x8_t value, _VL_T vl);
void vssseg2e_v_u64m2x2_vl (uint64_t *base, ptrdiff_t bstride, vuint64m2x2_t value, _VL_T vl);
void vssseg3e_v_u64m2x3_vl (uint64_t *base, ptrdiff_t bstride, vuint64m2x3_t value, _VL_T vl);
void vssseg4e_v_u64m2x4_vl (uint64_t *base, ptrdiff_t bstride, vuint64m2x4_t value, _VL_T vl);
void vssseg2e_v_u64m4x2_vl (uint64_t *base, ptrdiff_t bstride, vuint64m4x2_t value, _VL_T vl);
void vssseg2e_v_f16m1x2_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1x2_t value, _VL_T vl);
void vssseg3e_v_f16m1x3_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1x3_t value, _VL_T vl);
void vssseg4e_v_f16m1x4_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1x4_t value, _VL_T vl);
void vssseg5e_v_f16m1x5_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1x5_t value, _VL_T vl);
void vssseg6e_v_f16m1x6_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1x6_t value, _VL_T vl);
void vssseg7e_v_f16m1x7_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1x7_t value, _VL_T vl);
void vssseg8e_v_f16m1x8_vl (float16_t *base, ptrdiff_t bstride, vfloat16m1x8_t value, _VL_T vl);
void vssseg2e_v_f16m2x2_vl (float16_t *base, ptrdiff_t bstride, vfloat16m2x2_t value, _VL_T vl);
void vssseg3e_v_f16m2x3_vl (float16_t *base, ptrdiff_t bstride, vfloat16m2x3_t value, _VL_T vl);
void vssseg4e_v_f16m2x4_vl (float16_t *base, ptrdiff_t bstride, vfloat16m2x4_t value, _VL_T vl);
void vssseg2e_v_f16m4x2_vl (float16_t *base, ptrdiff_t bstride, vfloat16m4x2_t value, _VL_T vl);
void vssseg2e_v_f32m1x2_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1x2_t value, _VL_T vl);
void vssseg3e_v_f32m1x3_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1x3_t value, _VL_T vl);
void vssseg4e_v_f32m1x4_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1x4_t value, _VL_T vl);
void vssseg5e_v_f32m1x5_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1x5_t value, _VL_T vl);
void vssseg6e_v_f32m1x6_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1x6_t value, _VL_T vl);
void vssseg7e_v_f32m1x7_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1x7_t value, _VL_T vl);
void vssseg8e_v_f32m1x8_vl (float32_t *base, ptrdiff_t bstride, vfloat32m1x8_t value, _VL_T vl);
void vssseg2e_v_f32m2x2_vl (float32_t *base, ptrdiff_t bstride, vfloat32m2x2_t value, _VL_T vl);
void vssseg3e_v_f32m2x3_vl (float32_t *base, ptrdiff_t bstride, vfloat32m2x3_t value, _VL_T vl);
void vssseg4e_v_f32m2x4_vl (float32_t *base, ptrdiff_t bstride, vfloat32m2x4_t value, _VL_T vl);
void vssseg2e_v_f32m4x2_vl (float32_t *base, ptrdiff_t bstride, vfloat32m4x2_t value, _VL_T vl);
void vssseg2e_v_f64m1x2_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1x2_t value, _VL_T vl);
void vssseg3e_v_f64m1x3_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1x3_t value, _VL_T vl);
void vssseg4e_v_f64m1x4_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1x4_t value, _VL_T vl);
void vssseg5e_v_f64m1x5_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1x5_t value, _VL_T vl);
void vssseg6e_v_f64m1x6_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1x6_t value, _VL_T vl);
void vssseg7e_v_f64m1x7_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1x7_t value, _VL_T vl);
void vssseg8e_v_f64m1x8_vl (float64_t *base, ptrdiff_t bstride, vfloat64m1x8_t value, _VL_T vl);
void vssseg2e_v_f64m2x2_vl (float64_t *base, ptrdiff_t bstride, vfloat64m2x2_t value, _VL_T vl);
void vssseg3e_v_f64m2x3_vl (float64_t *base, ptrdiff_t bstride, vfloat64m2x3_t value, _VL_T vl);
void vssseg4e_v_f64m2x4_vl (float64_t *base, ptrdiff_t bstride, vfloat64m2x4_t value, _VL_T vl);
void vssseg2e_v_f64m4x2_vl (float64_t *base, ptrdiff_t bstride, vfloat64m4x2_t value, _VL_T vl);
// masked functions
void vssseg2e_v_i8m1x2_m_vl (int8_t *base, ptrdiff_t bstride, vbool8_t mask, vint8m1x2_t value, _VL_T vl);
void vssseg3e_v_i8m1x3_m_vl (int8_t *base, ptrdiff_t bstride, vbool8_t mask, vint8m1x3_t value, _VL_T vl);
void vssseg4e_v_i8m1x4_m_vl (int8_t *base, ptrdiff_t bstride, vbool8_t mask, vint8m1x4_t value, _VL_T vl);
void vssseg5e_v_i8m1x5_m_vl (int8_t *base, ptrdiff_t bstride, vbool8_t mask, vint8m1x5_t value, _VL_T vl);
void vssseg6e_v_i8m1x6_m_vl (int8_t *base, ptrdiff_t bstride, vbool8_t mask, vint8m1x6_t value, _VL_T vl);
void vssseg7e_v_i8m1x7_m_vl (int8_t *base, ptrdiff_t bstride, vbool8_t mask, vint8m1x7_t value, _VL_T vl);
void vssseg8e_v_i8m1x8_m_vl (int8_t *base, ptrdiff_t bstride, vbool8_t mask, vint8m1x8_t value, _VL_T vl);
void vssseg2e_v_i8m2x2_m_vl (int8_t *base, ptrdiff_t bstride, vbool4_t mask, vint8m2x2_t value, _VL_T vl);
void vssseg3e_v_i8m2x3_m_vl (int8_t *base, ptrdiff_t bstride, vbool4_t mask, vint8m2x3_t value, _VL_T vl);
void vssseg4e_v_i8m2x4_m_vl (int8_t *base, ptrdiff_t bstride, vbool4_t mask, vint8m2x4_t value, _VL_T vl);
void vssseg2e_v_i8m4x2_m_vl (int8_t *base, ptrdiff_t bstride, vbool2_t mask, vint8m4x2_t value, _VL_T vl);
void vssseg2e_v_i16m1x2_m_vl (int16_t *base, ptrdiff_t bstride, vbool16_t mask, vint16m1x2_t value, _VL_T vl);
void vssseg3e_v_i16m1x3_m_vl (int16_t *base, ptrdiff_t bstride, vbool16_t mask, vint16m1x3_t value, _VL_T vl);
void vssseg4e_v_i16m1x4_m_vl (int16_t *base, ptrdiff_t bstride, vbool16_t mask, vint16m1x4_t value, _VL_T vl);
void vssseg5e_v_i16m1x5_m_vl (int16_t *base, ptrdiff_t bstride, vbool16_t mask, vint16m1x5_t value, _VL_T vl);
void vssseg6e_v_i16m1x6_m_vl (int16_t *base, ptrdiff_t bstride, vbool16_t mask, vint16m1x6_t value, _VL_T vl);
void vssseg7e_v_i16m1x7_m_vl (int16_t *base, ptrdiff_t bstride, vbool16_t mask, vint16m1x7_t value, _VL_T vl);
void vssseg8e_v_i16m1x8_m_vl (int16_t *base, ptrdiff_t bstride, vbool16_t mask, vint16m1x8_t value, _VL_T vl);
void vssseg2e_v_i16m2x2_m_vl (int16_t *base, ptrdiff_t bstride, vbool8_t mask, vint16m2x2_t value, _VL_T vl);
void vssseg3e_v_i16m2x3_m_vl (int16_t *base, ptrdiff_t bstride, vbool8_t mask, vint16m2x3_t value, _VL_T vl);
void vssseg4e_v_i16m2x4_m_vl (int16_t *base, ptrdiff_t bstride, vbool8_t mask, vint16m2x4_t value, _VL_T vl);
void vssseg2e_v_i16m4x2_m_vl (int16_t *base, ptrdiff_t bstride, vbool4_t mask, vint16m4x2_t value, _VL_T vl);
void vssseg2e_v_i32m1x2_m_vl (int32_t *base, ptrdiff_t bstride, vbool32_t mask, vint32m1x2_t value, _VL_T vl);
void vssseg3e_v_i32m1x3_m_vl (int32_t *base, ptrdiff_t bstride, vbool32_t mask, vint32m1x3_t value, _VL_T vl);
void vssseg4e_v_i32m1x4_m_vl (int32_t *base, ptrdiff_t bstride, vbool32_t mask, vint32m1x4_t value, _VL_T vl);
void vssseg5e_v_i32m1x5_m_vl (int32_t *base, ptrdiff_t bstride, vbool32_t mask, vint32m1x5_t value, _VL_T vl);
void vssseg6e_v_i32m1x6_m_vl (int32_t *base, ptrdiff_t bstride, vbool32_t mask, vint32m1x6_t value, _VL_T vl);
void vssseg7e_v_i32m1x7_m_vl (int32_t *base, ptrdiff_t bstride, vbool32_t mask, vint32m1x7_t value, _VL_T vl);
void vssseg8e_v_i32m1x8_m_vl (int32_t *base, ptrdiff_t bstride, vbool32_t mask, vint32m1x8_t value, _VL_T vl);
void vssseg2e_v_i32m2x2_m_vl (int32_t *base, ptrdiff_t bstride, vbool16_t mask, vint32m2x2_t value, _VL_T vl);
void vssseg3e_v_i32m2x3_m_vl (int32_t *base, ptrdiff_t bstride, vbool16_t mask, vint32m2x3_t value, _VL_T vl);
void vssseg4e_v_i32m2x4_m_vl (int32_t *base, ptrdiff_t bstride, vbool16_t mask, vint32m2x4_t value, _VL_T vl);
void vssseg2e_v_i32m4x2_m_vl (int32_t *base, ptrdiff_t bstride, vbool8_t mask, vint32m4x2_t value, _VL_T vl);
void vssseg2e_v_i64m1x2_m_vl (int64_t *base, ptrdiff_t bstride, vbool64_t mask, vint64m1x2_t value, _VL_T vl);
void vssseg3e_v_i64m1x3_m_vl (int64_t *base, ptrdiff_t bstride, vbool64_t mask, vint64m1x3_t value, _VL_T vl);
void vssseg4e_v_i64m1x4_m_vl (int64_t *base, ptrdiff_t bstride, vbool64_t mask, vint64m1x4_t value, _VL_T vl);
void vssseg5e_v_i64m1x5_m_vl (int64_t *base, ptrdiff_t bstride, vbool64_t mask, vint64m1x5_t value, _VL_T vl);
void vssseg6e_v_i64m1x6_m_vl (int64_t *base, ptrdiff_t bstride, vbool64_t mask, vint64m1x6_t value, _VL_T vl);
void vssseg7e_v_i64m1x7_m_vl (int64_t *base, ptrdiff_t bstride, vbool64_t mask, vint64m1x7_t value, _VL_T vl);
void vssseg8e_v_i64m1x8_m_vl (int64_t *base, ptrdiff_t bstride, vbool64_t mask, vint64m1x8_t value, _VL_T vl);
void vssseg2e_v_i64m2x2_m_vl (int64_t *base, ptrdiff_t bstride, vbool32_t mask, vint64m2x2_t value, _VL_T vl);
void vssseg3e_v_i64m2x3_m_vl (int64_t *base, ptrdiff_t bstride, vbool32_t mask, vint64m2x3_t value, _VL_T vl);
void vssseg4e_v_i64m2x4_m_vl (int64_t *base, ptrdiff_t bstride, vbool32_t mask, vint64m2x4_t value, _VL_T vl);
void vssseg2e_v_i64m4x2_m_vl (int64_t *base, ptrdiff_t bstride, vbool16_t mask, vint64m4x2_t value, _VL_T vl);
void vssseg2e_v_u8m1x2_m_vl (uint8_t *base, ptrdiff_t bstride, vbool8_t mask, vuint8m1x2_t value, _VL_T vl);
void vssseg3e_v_u8m1x3_m_vl (uint8_t *base, ptrdiff_t bstride, vbool8_t mask, vuint8m1x3_t value, _VL_T vl);
void vssseg4e_v_u8m1x4_m_vl (uint8_t *base, ptrdiff_t bstride, vbool8_t mask, vuint8m1x4_t value, _VL_T vl);
void vssseg5e_v_u8m1x5_m_vl (uint8_t *base, ptrdiff_t bstride, vbool8_t mask, vuint8m1x5_t value, _VL_T vl);
void vssseg6e_v_u8m1x6_m_vl (uint8_t *base, ptrdiff_t bstride, vbool8_t mask, vuint8m1x6_t value, _VL_T vl);
void vssseg7e_v_u8m1x7_m_vl (uint8_t *base, ptrdiff_t bstride, vbool8_t mask, vuint8m1x7_t value, _VL_T vl);
void vssseg8e_v_u8m1x8_m_vl (uint8_t *base, ptrdiff_t bstride, vbool8_t mask, vuint8m1x8_t value, _VL_T vl);
void vssseg2e_v_u8m2x2_m_vl (uint8_t *base, ptrdiff_t bstride, vbool4_t mask, vuint8m2x2_t value, _VL_T vl);
void vssseg3e_v_u8m2x3_m_vl (uint8_t *base, ptrdiff_t bstride, vbool4_t mask, vuint8m2x3_t value, _VL_T vl);
void vssseg4e_v_u8m2x4_m_vl (uint8_t *base, ptrdiff_t bstride, vbool4_t mask, vuint8m2x4_t value, _VL_T vl);
void vssseg2e_v_u8m4x2_m_vl (uint8_t *base, ptrdiff_t bstride, vbool2_t mask, vuint8m4x2_t value, _VL_T vl);
void vssseg2e_v_u16m1x2_m_vl (uint16_t *base, ptrdiff_t bstride, vbool16_t mask, vuint16m1x2_t value, _VL_T vl);
void vssseg3e_v_u16m1x3_m_vl (uint16_t *base, ptrdiff_t bstride, vbool16_t mask, vuint16m1x3_t value, _VL_T vl);
void vssseg4e_v_u16m1x4_m_vl (uint16_t *base, ptrdiff_t bstride, vbool16_t mask, vuint16m1x4_t value, _VL_T vl);
void vssseg5e_v_u16m1x5_m_vl (uint16_t *base, ptrdiff_t bstride, vbool16_t mask, vuint16m1x5_t value, _VL_T vl);
void vssseg6e_v_u16m1x6_m_vl (uint16_t *base, ptrdiff_t bstride, vbool16_t mask, vuint16m1x6_t value, _VL_T vl);
void vssseg7e_v_u16m1x7_m_vl (uint16_t *base, ptrdiff_t bstride, vbool16_t mask, vuint16m1x7_t value, _VL_T vl);
void vssseg8e_v_u16m1x8_m_vl (uint16_t *base, ptrdiff_t bstride, vbool16_t mask, vuint16m1x8_t value, _VL_T vl);
void vssseg2e_v_u16m2x2_m_vl (uint16_t *base, ptrdiff_t bstride, vbool8_t mask, vuint16m2x2_t value, _VL_T vl);
void vssseg3e_v_u16m2x3_m_vl (uint16_t *base, ptrdiff_t bstride, vbool8_t mask, vuint16m2x3_t value, _VL_T vl);
void vssseg4e_v_u16m2x4_m_vl (uint16_t *base, ptrdiff_t bstride, vbool8_t mask, vuint16m2x4_t value, _VL_T vl);
void vssseg2e_v_u16m4x2_m_vl (uint16_t *base, ptrdiff_t bstride, vbool4_t mask, vuint16m4x2_t value, _VL_T vl);
void vssseg2e_v_u32m1x2_m_vl (uint32_t *base, ptrdiff_t bstride, vbool32_t mask, vuint32m1x2_t value, _VL_T vl);
void vssseg3e_v_u32m1x3_m_vl (uint32_t *base, ptrdiff_t bstride, vbool32_t mask, vuint32m1x3_t value, _VL_T vl);
void vssseg4e_v_u32m1x4_m_vl (uint32_t *base, ptrdiff_t bstride, vbool32_t mask, vuint32m1x4_t value, _VL_T vl);
void vssseg5e_v_u32m1x5_m_vl (uint32_t *base, ptrdiff_t bstride, vbool32_t mask, vuint32m1x5_t value, _VL_T vl);
void vssseg6e_v_u32m1x6_m_vl (uint32_t *base, ptrdiff_t bstride, vbool32_t mask, vuint32m1x6_t value, _VL_T vl);
void vssseg7e_v_u32m1x7_m_vl (uint32_t *base, ptrdiff_t bstride, vbool32_t mask, vuint32m1x7_t value, _VL_T vl);
void vssseg8e_v_u32m1x8_m_vl (uint32_t *base, ptrdiff_t bstride, vbool32_t mask, vuint32m1x8_t value, _VL_T vl);
void vssseg2e_v_u32m2x2_m_vl (uint32_t *base, ptrdiff_t bstride, vbool16_t mask, vuint32m2x2_t value, _VL_T vl);
void vssseg3e_v_u32m2x3_m_vl (uint32_t *base, ptrdiff_t bstride, vbool16_t mask, vuint32m2x3_t value, _VL_T vl);
void vssseg4e_v_u32m2x4_m_vl (uint32_t *base, ptrdiff_t bstride, vbool16_t mask, vuint32m2x4_t value, _VL_T vl);
void vssseg2e_v_u32m4x2_m_vl (uint32_t *base, ptrdiff_t bstride, vbool8_t mask, vuint32m4x2_t value, _VL_T vl);
void vssseg2e_v_u64m1x2_m_vl (uint64_t *base, ptrdiff_t bstride, vbool64_t mask, vuint64m1x2_t value, _VL_T vl);
void vssseg3e_v_u64m1x3_m_vl (uint64_t *base, ptrdiff_t bstride, vbool64_t mask, vuint64m1x3_t value, _VL_T vl);
void vssseg4e_v_u64m1x4_m_vl (uint64_t *base, ptrdiff_t bstride, vbool64_t mask, vuint64m1x4_t value, _VL_T vl);
void vssseg5e_v_u64m1x5_m_vl (uint64_t *base, ptrdiff_t bstride, vbool64_t mask, vuint64m1x5_t value, _VL_T vl);
void vssseg6e_v_u64m1x6_m_vl (uint64_t *base, ptrdiff_t bstride, vbool64_t mask, vuint64m1x6_t value, _VL_T vl);
void vssseg7e_v_u64m1x7_m_vl (uint64_t *base, ptrdiff_t bstride, vbool64_t mask, vuint64m1x7_t value, _VL_T vl);
void vssseg8e_v_u64m1x8_m_vl (uint64_t *base, ptrdiff_t bstride, vbool64_t mask, vuint64m1x8_t value, _VL_T vl);
void vssseg2e_v_u64m2x2_m_vl (uint64_t *base, ptrdiff_t bstride, vbool32_t mask, vuint64m2x2_t value, _VL_T vl);
void vssseg3e_v_u64m2x3_m_vl (uint64_t *base, ptrdiff_t bstride, vbool32_t mask, vuint64m2x3_t value, _VL_T vl);
void vssseg4e_v_u64m2x4_m_vl (uint64_t *base, ptrdiff_t bstride, vbool32_t mask, vuint64m2x4_t value, _VL_T vl);
void vssseg2e_v_u64m4x2_m_vl (uint64_t *base, ptrdiff_t bstride, vbool16_t mask, vuint64m4x2_t value, _VL_T vl);
void vssseg2e_v_f16m1x2_m_vl (float16_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat16m1x2_t value, _VL_T vl);
void vssseg3e_v_f16m1x3_m_vl (float16_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat16m1x3_t value, _VL_T vl);
void vssseg4e_v_f16m1x4_m_vl (float16_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat16m1x4_t value, _VL_T vl);
void vssseg5e_v_f16m1x5_m_vl (float16_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat16m1x5_t value, _VL_T vl);
void vssseg6e_v_f16m1x6_m_vl (float16_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat16m1x6_t value, _VL_T vl);
void vssseg7e_v_f16m1x7_m_vl (float16_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat16m1x7_t value, _VL_T vl);
void vssseg8e_v_f16m1x8_m_vl (float16_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat16m1x8_t value, _VL_T vl);
void vssseg2e_v_f16m2x2_m_vl (float16_t *base, ptrdiff_t bstride, vbool8_t mask, vfloat16m2x2_t value, _VL_T vl);
void vssseg3e_v_f16m2x3_m_vl (float16_t *base, ptrdiff_t bstride, vbool8_t mask, vfloat16m2x3_t value, _VL_T vl);
void vssseg4e_v_f16m2x4_m_vl (float16_t *base, ptrdiff_t bstride, vbool8_t mask, vfloat16m2x4_t value, _VL_T vl);
void vssseg2e_v_f16m4x2_m_vl (float16_t *base, ptrdiff_t bstride, vbool4_t mask, vfloat16m4x2_t value, _VL_T vl);
void vssseg2e_v_f32m1x2_m_vl (float32_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat32m1x2_t value, _VL_T vl);
void vssseg3e_v_f32m1x3_m_vl (float32_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat32m1x3_t value, _VL_T vl);
void vssseg4e_v_f32m1x4_m_vl (float32_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat32m1x4_t value, _VL_T vl);
void vssseg5e_v_f32m1x5_m_vl (float32_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat32m1x5_t value, _VL_T vl);
void vssseg6e_v_f32m1x6_m_vl (float32_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat32m1x6_t value, _VL_T vl);
void vssseg7e_v_f32m1x7_m_vl (float32_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat32m1x7_t value, _VL_T vl);
void vssseg8e_v_f32m1x8_m_vl (float32_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat32m1x8_t value, _VL_T vl);
void vssseg2e_v_f32m2x2_m_vl (float32_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat32m2x2_t value, _VL_T vl);
void vssseg3e_v_f32m2x3_m_vl (float32_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat32m2x3_t value, _VL_T vl);
void vssseg4e_v_f32m2x4_m_vl (float32_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat32m2x4_t value, _VL_T vl);
void vssseg2e_v_f32m4x2_m_vl (float32_t *base, ptrdiff_t bstride, vbool8_t mask, vfloat32m4x2_t value, _VL_T vl);
void vssseg2e_v_f64m1x2_m_vl (float64_t *base, ptrdiff_t bstride, vbool64_t mask, vfloat64m1x2_t value, _VL_T vl);
void vssseg3e_v_f64m1x3_m_vl (float64_t *base, ptrdiff_t bstride, vbool64_t mask, vfloat64m1x3_t value, _VL_T vl);
void vssseg4e_v_f64m1x4_m_vl (float64_t *base, ptrdiff_t bstride, vbool64_t mask, vfloat64m1x4_t value, _VL_T vl);
void vssseg5e_v_f64m1x5_m_vl (float64_t *base, ptrdiff_t bstride, vbool64_t mask, vfloat64m1x5_t value, _VL_T vl);
void vssseg6e_v_f64m1x6_m_vl (float64_t *base, ptrdiff_t bstride, vbool64_t mask, vfloat64m1x6_t value, _VL_T vl);
void vssseg7e_v_f64m1x7_m_vl (float64_t *base, ptrdiff_t bstride, vbool64_t mask, vfloat64m1x7_t value, _VL_T vl);
void vssseg8e_v_f64m1x8_m_vl (float64_t *base, ptrdiff_t bstride, vbool64_t mask, vfloat64m1x8_t value, _VL_T vl);
void vssseg2e_v_f64m2x2_m_vl (float64_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat64m2x2_t value, _VL_T vl);
void vssseg3e_v_f64m2x3_m_vl (float64_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat64m2x3_t value, _VL_T vl);
void vssseg4e_v_f64m2x4_m_vl (float64_t *base, ptrdiff_t bstride, vbool32_t mask, vfloat64m2x4_t value, _VL_T vl);
void vssseg2e_v_f64m4x2_m_vl (float64_t *base, ptrdiff_t bstride, vbool16_t mask, vfloat64m4x2_t value, _VL_T vl);
```
### [Vector Indexed Segment Load Functions]():

**Prototypes:**
``` C
vint8m1x2_t vlxseg2e_v_i8m1x2_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x3_t vlxseg3e_v_i8m1x3_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x4_t vlxseg4e_v_i8m1x4_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x5_t vlxseg5e_v_i8m1x5_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x6_t vlxseg6e_v_i8m1x6_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x7_t vlxseg7e_v_i8m1x7_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x8_t vlxseg8e_v_i8m1x8_vl (const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m2x2_t vlxseg2e_v_i8m2x2_vl (const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m2x3_t vlxseg3e_v_i8m2x3_vl (const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m2x4_t vlxseg4e_v_i8m2x4_vl (const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m4x2_t vlxseg2e_v_i8m4x2_vl (const int8_t *base, vuint8m4_t bindex, _VL_T vl);
vint16m1x2_t vlxseg2e_v_i16m1x2_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x3_t vlxseg3e_v_i16m1x3_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x4_t vlxseg4e_v_i16m1x4_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x5_t vlxseg5e_v_i16m1x5_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x6_t vlxseg6e_v_i16m1x6_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x7_t vlxseg7e_v_i16m1x7_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x8_t vlxseg8e_v_i16m1x8_vl (const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2x2_t vlxseg2e_v_i16m2x2_vl (const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m2x3_t vlxseg3e_v_i16m2x3_vl (const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m2x4_t vlxseg4e_v_i16m2x4_vl (const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4x2_t vlxseg2e_v_i16m4x2_vl (const int16_t *base, vuint16m4_t bindex, _VL_T vl);
vint32m1x2_t vlxseg2e_v_i32m1x2_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x3_t vlxseg3e_v_i32m1x3_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x4_t vlxseg4e_v_i32m1x4_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x5_t vlxseg5e_v_i32m1x5_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x6_t vlxseg6e_v_i32m1x6_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x7_t vlxseg7e_v_i32m1x7_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x8_t vlxseg8e_v_i32m1x8_vl (const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2x2_t vlxseg2e_v_i32m2x2_vl (const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m2x3_t vlxseg3e_v_i32m2x3_vl (const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m2x4_t vlxseg4e_v_i32m2x4_vl (const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4x2_t vlxseg2e_v_i32m4x2_vl (const int32_t *base, vuint32m4_t bindex, _VL_T vl);
vint64m1x2_t vlxseg2e_v_i64m1x2_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x3_t vlxseg3e_v_i64m1x3_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x4_t vlxseg4e_v_i64m1x4_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x5_t vlxseg5e_v_i64m1x5_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x6_t vlxseg6e_v_i64m1x6_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x7_t vlxseg7e_v_i64m1x7_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x8_t vlxseg8e_v_i64m1x8_vl (const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2x2_t vlxseg2e_v_i64m2x2_vl (const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m2x3_t vlxseg3e_v_i64m2x3_vl (const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m2x4_t vlxseg4e_v_i64m2x4_vl (const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4x2_t vlxseg2e_v_i64m4x2_vl (const int64_t *base, vuint64m4_t bindex, _VL_T vl);
vuint8m1x2_t vlxseg2e_v_u8m1x2_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x3_t vlxseg3e_v_u8m1x3_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x4_t vlxseg4e_v_u8m1x4_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x5_t vlxseg5e_v_u8m1x5_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x6_t vlxseg6e_v_u8m1x6_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x7_t vlxseg7e_v_u8m1x7_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x8_t vlxseg8e_v_u8m1x8_vl (const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m2x2_t vlxseg2e_v_u8m2x2_vl (const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m2x3_t vlxseg3e_v_u8m2x3_vl (const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m2x4_t vlxseg4e_v_u8m2x4_vl (const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m4x2_t vlxseg2e_v_u8m4x2_vl (const uint8_t *base, vuint8m4_t bindex, _VL_T vl);
vuint16m1x2_t vlxseg2e_v_u16m1x2_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x3_t vlxseg3e_v_u16m1x3_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x4_t vlxseg4e_v_u16m1x4_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x5_t vlxseg5e_v_u16m1x5_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x6_t vlxseg6e_v_u16m1x6_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x7_t vlxseg7e_v_u16m1x7_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x8_t vlxseg8e_v_u16m1x8_vl (const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2x2_t vlxseg2e_v_u16m2x2_vl (const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m2x3_t vlxseg3e_v_u16m2x3_vl (const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m2x4_t vlxseg4e_v_u16m2x4_vl (const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4x2_t vlxseg2e_v_u16m4x2_vl (const uint16_t *base, vuint16m4_t bindex, _VL_T vl);
vuint32m1x2_t vlxseg2e_v_u32m1x2_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x3_t vlxseg3e_v_u32m1x3_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x4_t vlxseg4e_v_u32m1x4_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x5_t vlxseg5e_v_u32m1x5_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x6_t vlxseg6e_v_u32m1x6_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x7_t vlxseg7e_v_u32m1x7_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x8_t vlxseg8e_v_u32m1x8_vl (const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2x2_t vlxseg2e_v_u32m2x2_vl (const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m2x3_t vlxseg3e_v_u32m2x3_vl (const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m2x4_t vlxseg4e_v_u32m2x4_vl (const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4x2_t vlxseg2e_v_u32m4x2_vl (const uint32_t *base, vuint32m4_t bindex, _VL_T vl);
vuint64m1x2_t vlxseg2e_v_u64m1x2_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x3_t vlxseg3e_v_u64m1x3_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x4_t vlxseg4e_v_u64m1x4_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x5_t vlxseg5e_v_u64m1x5_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x6_t vlxseg6e_v_u64m1x6_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x7_t vlxseg7e_v_u64m1x7_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x8_t vlxseg8e_v_u64m1x8_vl (const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2x2_t vlxseg2e_v_u64m2x2_vl (const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m2x3_t vlxseg3e_v_u64m2x3_vl (const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m2x4_t vlxseg4e_v_u64m2x4_vl (const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4x2_t vlxseg2e_v_u64m4x2_vl (const uint64_t *base, vuint64m4_t bindex, _VL_T vl);
vfloat16m1x2_t vlxseg2e_v_f16m1x2_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x3_t vlxseg3e_v_f16m1x3_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x4_t vlxseg4e_v_f16m1x4_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x5_t vlxseg5e_v_f16m1x5_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x6_t vlxseg6e_v_f16m1x6_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x7_t vlxseg7e_v_f16m1x7_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x8_t vlxseg8e_v_f16m1x8_vl (const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m2x2_t vlxseg2e_v_f16m2x2_vl (const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m2x3_t vlxseg3e_v_f16m2x3_vl (const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m2x4_t vlxseg4e_v_f16m2x4_vl (const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m4x2_t vlxseg2e_v_f16m4x2_vl (const float16_t *base, vuint16m4_t bindex, _VL_T vl);
vfloat32m1x2_t vlxseg2e_v_f32m1x2_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x3_t vlxseg3e_v_f32m1x3_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x4_t vlxseg4e_v_f32m1x4_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x5_t vlxseg5e_v_f32m1x5_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x6_t vlxseg6e_v_f32m1x6_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x7_t vlxseg7e_v_f32m1x7_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x8_t vlxseg8e_v_f32m1x8_vl (const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m2x2_t vlxseg2e_v_f32m2x2_vl (const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m2x3_t vlxseg3e_v_f32m2x3_vl (const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m2x4_t vlxseg4e_v_f32m2x4_vl (const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m4x2_t vlxseg2e_v_f32m4x2_vl (const float32_t *base, vuint32m4_t bindex, _VL_T vl);
vfloat64m1x2_t vlxseg2e_v_f64m1x2_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x3_t vlxseg3e_v_f64m1x3_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x4_t vlxseg4e_v_f64m1x4_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x5_t vlxseg5e_v_f64m1x5_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x6_t vlxseg6e_v_f64m1x6_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x7_t vlxseg7e_v_f64m1x7_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x8_t vlxseg8e_v_f64m1x8_vl (const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m2x2_t vlxseg2e_v_f64m2x2_vl (const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m2x3_t vlxseg3e_v_f64m2x3_vl (const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m2x4_t vlxseg4e_v_f64m2x4_vl (const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m4x2_t vlxseg2e_v_f64m4x2_vl (const float64_t *base, vuint64m4_t bindex, _VL_T vl);
// masked functions
vint8m1x2_t vlxseg2e_v_i8m1x2_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x3_t vlxseg3e_v_i8m1x3_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x4_t vlxseg4e_v_i8m1x4_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x5_t vlxseg5e_v_i8m1x5_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x6_t vlxseg6e_v_i8m1x6_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x7_t vlxseg7e_v_i8m1x7_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m1x8_t vlxseg8e_v_i8m1x8_m_vl (vbool8_t mask, vint8m1_t maskedoff, const int8_t *base, vuint8m1_t bindex, _VL_T vl);
vint8m2x2_t vlxseg2e_v_i8m2x2_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m2x3_t vlxseg3e_v_i8m2x3_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m2x4_t vlxseg4e_v_i8m2x4_m_vl (vbool4_t mask, vint8m2_t maskedoff, const int8_t *base, vuint8m2_t bindex, _VL_T vl);
vint8m4x2_t vlxseg2e_v_i8m4x2_m_vl (vbool2_t mask, vint8m4_t maskedoff, const int8_t *base, vuint8m4_t bindex, _VL_T vl);
vint16m1x2_t vlxseg2e_v_i16m1x2_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x3_t vlxseg3e_v_i16m1x3_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x4_t vlxseg4e_v_i16m1x4_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x5_t vlxseg5e_v_i16m1x5_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x6_t vlxseg6e_v_i16m1x6_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x7_t vlxseg7e_v_i16m1x7_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m1x8_t vlxseg8e_v_i16m1x8_m_vl (vbool16_t mask, vint16m1_t maskedoff, const int16_t *base, vuint16m1_t bindex, _VL_T vl);
vint16m2x2_t vlxseg2e_v_i16m2x2_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m2x3_t vlxseg3e_v_i16m2x3_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m2x4_t vlxseg4e_v_i16m2x4_m_vl (vbool8_t mask, vint16m2_t maskedoff, const int16_t *base, vuint16m2_t bindex, _VL_T vl);
vint16m4x2_t vlxseg2e_v_i16m4x2_m_vl (vbool4_t mask, vint16m4_t maskedoff, const int16_t *base, vuint16m4_t bindex, _VL_T vl);
vint32m1x2_t vlxseg2e_v_i32m1x2_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x3_t vlxseg3e_v_i32m1x3_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x4_t vlxseg4e_v_i32m1x4_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x5_t vlxseg5e_v_i32m1x5_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x6_t vlxseg6e_v_i32m1x6_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x7_t vlxseg7e_v_i32m1x7_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m1x8_t vlxseg8e_v_i32m1x8_m_vl (vbool32_t mask, vint32m1_t maskedoff, const int32_t *base, vuint32m1_t bindex, _VL_T vl);
vint32m2x2_t vlxseg2e_v_i32m2x2_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m2x3_t vlxseg3e_v_i32m2x3_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m2x4_t vlxseg4e_v_i32m2x4_m_vl (vbool16_t mask, vint32m2_t maskedoff, const int32_t *base, vuint32m2_t bindex, _VL_T vl);
vint32m4x2_t vlxseg2e_v_i32m4x2_m_vl (vbool8_t mask, vint32m4_t maskedoff, const int32_t *base, vuint32m4_t bindex, _VL_T vl);
vint64m1x2_t vlxseg2e_v_i64m1x2_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x3_t vlxseg3e_v_i64m1x3_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x4_t vlxseg4e_v_i64m1x4_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x5_t vlxseg5e_v_i64m1x5_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x6_t vlxseg6e_v_i64m1x6_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x7_t vlxseg7e_v_i64m1x7_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m1x8_t vlxseg8e_v_i64m1x8_m_vl (vbool64_t mask, vint64m1_t maskedoff, const int64_t *base, vuint64m1_t bindex, _VL_T vl);
vint64m2x2_t vlxseg2e_v_i64m2x2_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m2x3_t vlxseg3e_v_i64m2x3_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m2x4_t vlxseg4e_v_i64m2x4_m_vl (vbool32_t mask, vint64m2_t maskedoff, const int64_t *base, vuint64m2_t bindex, _VL_T vl);
vint64m4x2_t vlxseg2e_v_i64m4x2_m_vl (vbool16_t mask, vint64m4_t maskedoff, const int64_t *base, vuint64m4_t bindex, _VL_T vl);
vuint8m1x2_t vlxseg2e_v_u8m1x2_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x3_t vlxseg3e_v_u8m1x3_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x4_t vlxseg4e_v_u8m1x4_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x5_t vlxseg5e_v_u8m1x5_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x6_t vlxseg6e_v_u8m1x6_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x7_t vlxseg7e_v_u8m1x7_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m1x8_t vlxseg8e_v_u8m1x8_m_vl (vbool8_t mask, vuint8m1_t maskedoff, const uint8_t *base, vuint8m1_t bindex, _VL_T vl);
vuint8m2x2_t vlxseg2e_v_u8m2x2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m2x3_t vlxseg3e_v_u8m2x3_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m2x4_t vlxseg4e_v_u8m2x4_m_vl (vbool4_t mask, vuint8m2_t maskedoff, const uint8_t *base, vuint8m2_t bindex, _VL_T vl);
vuint8m4x2_t vlxseg2e_v_u8m4x2_m_vl (vbool2_t mask, vuint8m4_t maskedoff, const uint8_t *base, vuint8m4_t bindex, _VL_T vl);
vuint16m1x2_t vlxseg2e_v_u16m1x2_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x3_t vlxseg3e_v_u16m1x3_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x4_t vlxseg4e_v_u16m1x4_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x5_t vlxseg5e_v_u16m1x5_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x6_t vlxseg6e_v_u16m1x6_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x7_t vlxseg7e_v_u16m1x7_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m1x8_t vlxseg8e_v_u16m1x8_m_vl (vbool16_t mask, vuint16m1_t maskedoff, const uint16_t *base, vuint16m1_t bindex, _VL_T vl);
vuint16m2x2_t vlxseg2e_v_u16m2x2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m2x3_t vlxseg3e_v_u16m2x3_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m2x4_t vlxseg4e_v_u16m2x4_m_vl (vbool8_t mask, vuint16m2_t maskedoff, const uint16_t *base, vuint16m2_t bindex, _VL_T vl);
vuint16m4x2_t vlxseg2e_v_u16m4x2_m_vl (vbool4_t mask, vuint16m4_t maskedoff, const uint16_t *base, vuint16m4_t bindex, _VL_T vl);
vuint32m1x2_t vlxseg2e_v_u32m1x2_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x3_t vlxseg3e_v_u32m1x3_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x4_t vlxseg4e_v_u32m1x4_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x5_t vlxseg5e_v_u32m1x5_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x6_t vlxseg6e_v_u32m1x6_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x7_t vlxseg7e_v_u32m1x7_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m1x8_t vlxseg8e_v_u32m1x8_m_vl (vbool32_t mask, vuint32m1_t maskedoff, const uint32_t *base, vuint32m1_t bindex, _VL_T vl);
vuint32m2x2_t vlxseg2e_v_u32m2x2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m2x3_t vlxseg3e_v_u32m2x3_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m2x4_t vlxseg4e_v_u32m2x4_m_vl (vbool16_t mask, vuint32m2_t maskedoff, const uint32_t *base, vuint32m2_t bindex, _VL_T vl);
vuint32m4x2_t vlxseg2e_v_u32m4x2_m_vl (vbool8_t mask, vuint32m4_t maskedoff, const uint32_t *base, vuint32m4_t bindex, _VL_T vl);
vuint64m1x2_t vlxseg2e_v_u64m1x2_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x3_t vlxseg3e_v_u64m1x3_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x4_t vlxseg4e_v_u64m1x4_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x5_t vlxseg5e_v_u64m1x5_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x6_t vlxseg6e_v_u64m1x6_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x7_t vlxseg7e_v_u64m1x7_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m1x8_t vlxseg8e_v_u64m1x8_m_vl (vbool64_t mask, vuint64m1_t maskedoff, const uint64_t *base, vuint64m1_t bindex, _VL_T vl);
vuint64m2x2_t vlxseg2e_v_u64m2x2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m2x3_t vlxseg3e_v_u64m2x3_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m2x4_t vlxseg4e_v_u64m2x4_m_vl (vbool32_t mask, vuint64m2_t maskedoff, const uint64_t *base, vuint64m2_t bindex, _VL_T vl);
vuint64m4x2_t vlxseg2e_v_u64m4x2_m_vl (vbool16_t mask, vuint64m4_t maskedoff, const uint64_t *base, vuint64m4_t bindex, _VL_T vl);
vfloat16m1x2_t vlxseg2e_v_f16m1x2_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x3_t vlxseg3e_v_f16m1x3_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x4_t vlxseg4e_v_f16m1x4_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x5_t vlxseg5e_v_f16m1x5_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x6_t vlxseg6e_v_f16m1x6_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x7_t vlxseg7e_v_f16m1x7_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m1x8_t vlxseg8e_v_f16m1x8_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, const float16_t *base, vuint16m1_t bindex, _VL_T vl);
vfloat16m2x2_t vlxseg2e_v_f16m2x2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m2x3_t vlxseg3e_v_f16m2x3_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m2x4_t vlxseg4e_v_f16m2x4_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, const float16_t *base, vuint16m2_t bindex, _VL_T vl);
vfloat16m4x2_t vlxseg2e_v_f16m4x2_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, const float16_t *base, vuint16m4_t bindex, _VL_T vl);
vfloat32m1x2_t vlxseg2e_v_f32m1x2_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x3_t vlxseg3e_v_f32m1x3_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x4_t vlxseg4e_v_f32m1x4_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x5_t vlxseg5e_v_f32m1x5_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x6_t vlxseg6e_v_f32m1x6_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x7_t vlxseg7e_v_f32m1x7_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m1x8_t vlxseg8e_v_f32m1x8_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, const float32_t *base, vuint32m1_t bindex, _VL_T vl);
vfloat32m2x2_t vlxseg2e_v_f32m2x2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m2x3_t vlxseg3e_v_f32m2x3_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m2x4_t vlxseg4e_v_f32m2x4_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, const float32_t *base, vuint32m2_t bindex, _VL_T vl);
vfloat32m4x2_t vlxseg2e_v_f32m4x2_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, const float32_t *base, vuint32m4_t bindex, _VL_T vl);
vfloat64m1x2_t vlxseg2e_v_f64m1x2_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x3_t vlxseg3e_v_f64m1x3_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x4_t vlxseg4e_v_f64m1x4_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x5_t vlxseg5e_v_f64m1x5_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x6_t vlxseg6e_v_f64m1x6_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x7_t vlxseg7e_v_f64m1x7_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m1x8_t vlxseg8e_v_f64m1x8_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, const float64_t *base, vuint64m1_t bindex, _VL_T vl);
vfloat64m2x2_t vlxseg2e_v_f64m2x2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m2x3_t vlxseg3e_v_f64m2x3_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m2x4_t vlxseg4e_v_f64m2x4_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, const float64_t *base, vuint64m2_t bindex, _VL_T vl);
vfloat64m4x2_t vlxseg2e_v_f64m4x2_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, const float64_t *base, vuint64m4_t bindex, _VL_T vl);
```
### [Vector Indexed Segment Store Functions]():

**Prototypes:**
``` C
void vsxseg2e_v_i8m1x2_vl (int8_t *base, vuint8m1_t bindex, vint8m1x2_t value, _VL_T vl);
void vsxseg3e_v_i8m1x3_vl (int8_t *base, vuint8m1_t bindex, vint8m1x3_t value, _VL_T vl);
void vsxseg4e_v_i8m1x4_vl (int8_t *base, vuint8m1_t bindex, vint8m1x4_t value, _VL_T vl);
void vsxseg5e_v_i8m1x5_vl (int8_t *base, vuint8m1_t bindex, vint8m1x5_t value, _VL_T vl);
void vsxseg6e_v_i8m1x6_vl (int8_t *base, vuint8m1_t bindex, vint8m1x6_t value, _VL_T vl);
void vsxseg7e_v_i8m1x7_vl (int8_t *base, vuint8m1_t bindex, vint8m1x7_t value, _VL_T vl);
void vsxseg8e_v_i8m1x8_vl (int8_t *base, vuint8m1_t bindex, vint8m1x8_t value, _VL_T vl);
void vsxseg2e_v_i8m2x2_vl (int8_t *base, vuint8m2_t bindex, vint8m2x2_t value, _VL_T vl);
void vsxseg3e_v_i8m2x3_vl (int8_t *base, vuint8m2_t bindex, vint8m2x3_t value, _VL_T vl);
void vsxseg4e_v_i8m2x4_vl (int8_t *base, vuint8m2_t bindex, vint8m2x4_t value, _VL_T vl);
void vsxseg2e_v_i8m4x2_vl (int8_t *base, vuint8m4_t bindex, vint8m4x2_t value, _VL_T vl);
void vsxseg2e_v_i16m1x2_vl (int16_t *base, vuint16m1_t bindex, vint16m1x2_t value, _VL_T vl);
void vsxseg3e_v_i16m1x3_vl (int16_t *base, vuint16m1_t bindex, vint16m1x3_t value, _VL_T vl);
void vsxseg4e_v_i16m1x4_vl (int16_t *base, vuint16m1_t bindex, vint16m1x4_t value, _VL_T vl);
void vsxseg5e_v_i16m1x5_vl (int16_t *base, vuint16m1_t bindex, vint16m1x5_t value, _VL_T vl);
void vsxseg6e_v_i16m1x6_vl (int16_t *base, vuint16m1_t bindex, vint16m1x6_t value, _VL_T vl);
void vsxseg7e_v_i16m1x7_vl (int16_t *base, vuint16m1_t bindex, vint16m1x7_t value, _VL_T vl);
void vsxseg8e_v_i16m1x8_vl (int16_t *base, vuint16m1_t bindex, vint16m1x8_t value, _VL_T vl);
void vsxseg2e_v_i16m2x2_vl (int16_t *base, vuint16m2_t bindex, vint16m2x2_t value, _VL_T vl);
void vsxseg3e_v_i16m2x3_vl (int16_t *base, vuint16m2_t bindex, vint16m2x3_t value, _VL_T vl);
void vsxseg4e_v_i16m2x4_vl (int16_t *base, vuint16m2_t bindex, vint16m2x4_t value, _VL_T vl);
void vsxseg2e_v_i16m4x2_vl (int16_t *base, vuint16m4_t bindex, vint16m4x2_t value, _VL_T vl);
void vsxseg2e_v_i32m1x2_vl (int32_t *base, vuint32m1_t bindex, vint32m1x2_t value, _VL_T vl);
void vsxseg3e_v_i32m1x3_vl (int32_t *base, vuint32m1_t bindex, vint32m1x3_t value, _VL_T vl);
void vsxseg4e_v_i32m1x4_vl (int32_t *base, vuint32m1_t bindex, vint32m1x4_t value, _VL_T vl);
void vsxseg5e_v_i32m1x5_vl (int32_t *base, vuint32m1_t bindex, vint32m1x5_t value, _VL_T vl);
void vsxseg6e_v_i32m1x6_vl (int32_t *base, vuint32m1_t bindex, vint32m1x6_t value, _VL_T vl);
void vsxseg7e_v_i32m1x7_vl (int32_t *base, vuint32m1_t bindex, vint32m1x7_t value, _VL_T vl);
void vsxseg8e_v_i32m1x8_vl (int32_t *base, vuint32m1_t bindex, vint32m1x8_t value, _VL_T vl);
void vsxseg2e_v_i32m2x2_vl (int32_t *base, vuint32m2_t bindex, vint32m2x2_t value, _VL_T vl);
void vsxseg3e_v_i32m2x3_vl (int32_t *base, vuint32m2_t bindex, vint32m2x3_t value, _VL_T vl);
void vsxseg4e_v_i32m2x4_vl (int32_t *base, vuint32m2_t bindex, vint32m2x4_t value, _VL_T vl);
void vsxseg2e_v_i32m4x2_vl (int32_t *base, vuint32m4_t bindex, vint32m4x2_t value, _VL_T vl);
void vsxseg2e_v_i64m1x2_vl (int64_t *base, vuint64m1_t bindex, vint64m1x2_t value, _VL_T vl);
void vsxseg3e_v_i64m1x3_vl (int64_t *base, vuint64m1_t bindex, vint64m1x3_t value, _VL_T vl);
void vsxseg4e_v_i64m1x4_vl (int64_t *base, vuint64m1_t bindex, vint64m1x4_t value, _VL_T vl);
void vsxseg5e_v_i64m1x5_vl (int64_t *base, vuint64m1_t bindex, vint64m1x5_t value, _VL_T vl);
void vsxseg6e_v_i64m1x6_vl (int64_t *base, vuint64m1_t bindex, vint64m1x6_t value, _VL_T vl);
void vsxseg7e_v_i64m1x7_vl (int64_t *base, vuint64m1_t bindex, vint64m1x7_t value, _VL_T vl);
void vsxseg8e_v_i64m1x8_vl (int64_t *base, vuint64m1_t bindex, vint64m1x8_t value, _VL_T vl);
void vsxseg2e_v_i64m2x2_vl (int64_t *base, vuint64m2_t bindex, vint64m2x2_t value, _VL_T vl);
void vsxseg3e_v_i64m2x3_vl (int64_t *base, vuint64m2_t bindex, vint64m2x3_t value, _VL_T vl);
void vsxseg4e_v_i64m2x4_vl (int64_t *base, vuint64m2_t bindex, vint64m2x4_t value, _VL_T vl);
void vsxseg2e_v_i64m4x2_vl (int64_t *base, vuint64m4_t bindex, vint64m4x2_t value, _VL_T vl);
void vsxseg2e_v_u8m1x2_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1x2_t value, _VL_T vl);
void vsxseg3e_v_u8m1x3_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1x3_t value, _VL_T vl);
void vsxseg4e_v_u8m1x4_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1x4_t value, _VL_T vl);
void vsxseg5e_v_u8m1x5_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1x5_t value, _VL_T vl);
void vsxseg6e_v_u8m1x6_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1x6_t value, _VL_T vl);
void vsxseg7e_v_u8m1x7_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1x7_t value, _VL_T vl);
void vsxseg8e_v_u8m1x8_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1x8_t value, _VL_T vl);
void vsxseg2e_v_u8m2x2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2x2_t value, _VL_T vl);
void vsxseg3e_v_u8m2x3_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2x3_t value, _VL_T vl);
void vsxseg4e_v_u8m2x4_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2x4_t value, _VL_T vl);
void vsxseg2e_v_u8m4x2_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4x2_t value, _VL_T vl);
void vsxseg2e_v_u16m1x2_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1x2_t value, _VL_T vl);
void vsxseg3e_v_u16m1x3_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1x3_t value, _VL_T vl);
void vsxseg4e_v_u16m1x4_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1x4_t value, _VL_T vl);
void vsxseg5e_v_u16m1x5_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1x5_t value, _VL_T vl);
void vsxseg6e_v_u16m1x6_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1x6_t value, _VL_T vl);
void vsxseg7e_v_u16m1x7_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1x7_t value, _VL_T vl);
void vsxseg8e_v_u16m1x8_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1x8_t value, _VL_T vl);
void vsxseg2e_v_u16m2x2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2x2_t value, _VL_T vl);
void vsxseg3e_v_u16m2x3_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2x3_t value, _VL_T vl);
void vsxseg4e_v_u16m2x4_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2x4_t value, _VL_T vl);
void vsxseg2e_v_u16m4x2_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4x2_t value, _VL_T vl);
void vsxseg2e_v_u32m1x2_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1x2_t value, _VL_T vl);
void vsxseg3e_v_u32m1x3_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1x3_t value, _VL_T vl);
void vsxseg4e_v_u32m1x4_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1x4_t value, _VL_T vl);
void vsxseg5e_v_u32m1x5_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1x5_t value, _VL_T vl);
void vsxseg6e_v_u32m1x6_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1x6_t value, _VL_T vl);
void vsxseg7e_v_u32m1x7_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1x7_t value, _VL_T vl);
void vsxseg8e_v_u32m1x8_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1x8_t value, _VL_T vl);
void vsxseg2e_v_u32m2x2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2x2_t value, _VL_T vl);
void vsxseg3e_v_u32m2x3_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2x3_t value, _VL_T vl);
void vsxseg4e_v_u32m2x4_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2x4_t value, _VL_T vl);
void vsxseg2e_v_u32m4x2_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4x2_t value, _VL_T vl);
void vsxseg2e_v_u64m1x2_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1x2_t value, _VL_T vl);
void vsxseg3e_v_u64m1x3_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1x3_t value, _VL_T vl);
void vsxseg4e_v_u64m1x4_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1x4_t value, _VL_T vl);
void vsxseg5e_v_u64m1x5_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1x5_t value, _VL_T vl);
void vsxseg6e_v_u64m1x6_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1x6_t value, _VL_T vl);
void vsxseg7e_v_u64m1x7_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1x7_t value, _VL_T vl);
void vsxseg8e_v_u64m1x8_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1x8_t value, _VL_T vl);
void vsxseg2e_v_u64m2x2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2x2_t value, _VL_T vl);
void vsxseg3e_v_u64m2x3_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2x3_t value, _VL_T vl);
void vsxseg4e_v_u64m2x4_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2x4_t value, _VL_T vl);
void vsxseg2e_v_u64m4x2_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4x2_t value, _VL_T vl);
void vsxseg2e_v_f16m1x2_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1x2_t value, _VL_T vl);
void vsxseg3e_v_f16m1x3_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1x3_t value, _VL_T vl);
void vsxseg4e_v_f16m1x4_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1x4_t value, _VL_T vl);
void vsxseg5e_v_f16m1x5_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1x5_t value, _VL_T vl);
void vsxseg6e_v_f16m1x6_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1x6_t value, _VL_T vl);
void vsxseg7e_v_f16m1x7_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1x7_t value, _VL_T vl);
void vsxseg8e_v_f16m1x8_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1x8_t value, _VL_T vl);
void vsxseg2e_v_f16m2x2_vl (float16_t *base, vuint16m2_t bindex, vfloat16m2x2_t value, _VL_T vl);
void vsxseg3e_v_f16m2x3_vl (float16_t *base, vuint16m2_t bindex, vfloat16m2x3_t value, _VL_T vl);
void vsxseg4e_v_f16m2x4_vl (float16_t *base, vuint16m2_t bindex, vfloat16m2x4_t value, _VL_T vl);
void vsxseg2e_v_f16m4x2_vl (float16_t *base, vuint16m4_t bindex, vfloat16m4x2_t value, _VL_T vl);
void vsxseg2e_v_f32m1x2_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1x2_t value, _VL_T vl);
void vsxseg3e_v_f32m1x3_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1x3_t value, _VL_T vl);
void vsxseg4e_v_f32m1x4_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1x4_t value, _VL_T vl);
void vsxseg5e_v_f32m1x5_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1x5_t value, _VL_T vl);
void vsxseg6e_v_f32m1x6_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1x6_t value, _VL_T vl);
void vsxseg7e_v_f32m1x7_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1x7_t value, _VL_T vl);
void vsxseg8e_v_f32m1x8_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1x8_t value, _VL_T vl);
void vsxseg2e_v_f32m2x2_vl (float32_t *base, vuint32m2_t bindex, vfloat32m2x2_t value, _VL_T vl);
void vsxseg3e_v_f32m2x3_vl (float32_t *base, vuint32m2_t bindex, vfloat32m2x3_t value, _VL_T vl);
void vsxseg4e_v_f32m2x4_vl (float32_t *base, vuint32m2_t bindex, vfloat32m2x4_t value, _VL_T vl);
void vsxseg2e_v_f32m4x2_vl (float32_t *base, vuint32m4_t bindex, vfloat32m4x2_t value, _VL_T vl);
void vsxseg2e_v_f64m1x2_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1x2_t value, _VL_T vl);
void vsxseg3e_v_f64m1x3_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1x3_t value, _VL_T vl);
void vsxseg4e_v_f64m1x4_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1x4_t value, _VL_T vl);
void vsxseg5e_v_f64m1x5_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1x5_t value, _VL_T vl);
void vsxseg6e_v_f64m1x6_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1x6_t value, _VL_T vl);
void vsxseg7e_v_f64m1x7_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1x7_t value, _VL_T vl);
void vsxseg8e_v_f64m1x8_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1x8_t value, _VL_T vl);
void vsxseg2e_v_f64m2x2_vl (float64_t *base, vuint64m2_t bindex, vfloat64m2x2_t value, _VL_T vl);
void vsxseg3e_v_f64m2x3_vl (float64_t *base, vuint64m2_t bindex, vfloat64m2x3_t value, _VL_T vl);
void vsxseg4e_v_f64m2x4_vl (float64_t *base, vuint64m2_t bindex, vfloat64m2x4_t value, _VL_T vl);
void vsxseg2e_v_f64m4x2_vl (float64_t *base, vuint64m4_t bindex, vfloat64m4x2_t value, _VL_T vl);
// masked functions
void vsxseg2e_v_i8m1x2_m_vl (int8_t *base, vuint8m1_t bindex, vbool8_t mask, vint8m1x2_t value, _VL_T vl);
void vsxseg3e_v_i8m1x3_m_vl (int8_t *base, vuint8m1_t bindex, vbool8_t mask, vint8m1x3_t value, _VL_T vl);
void vsxseg4e_v_i8m1x4_m_vl (int8_t *base, vuint8m1_t bindex, vbool8_t mask, vint8m1x4_t value, _VL_T vl);
void vsxseg5e_v_i8m1x5_m_vl (int8_t *base, vuint8m1_t bindex, vbool8_t mask, vint8m1x5_t value, _VL_T vl);
void vsxseg6e_v_i8m1x6_m_vl (int8_t *base, vuint8m1_t bindex, vbool8_t mask, vint8m1x6_t value, _VL_T vl);
void vsxseg7e_v_i8m1x7_m_vl (int8_t *base, vuint8m1_t bindex, vbool8_t mask, vint8m1x7_t value, _VL_T vl);
void vsxseg8e_v_i8m1x8_m_vl (int8_t *base, vuint8m1_t bindex, vbool8_t mask, vint8m1x8_t value, _VL_T vl);
void vsxseg2e_v_i8m2x2_m_vl (int8_t *base, vuint8m2_t bindex, vbool4_t mask, vint8m2x2_t value, _VL_T vl);
void vsxseg3e_v_i8m2x3_m_vl (int8_t *base, vuint8m2_t bindex, vbool4_t mask, vint8m2x3_t value, _VL_T vl);
void vsxseg4e_v_i8m2x4_m_vl (int8_t *base, vuint8m2_t bindex, vbool4_t mask, vint8m2x4_t value, _VL_T vl);
void vsxseg2e_v_i8m4x2_m_vl (int8_t *base, vuint8m4_t bindex, vbool2_t mask, vint8m4x2_t value, _VL_T vl);
void vsxseg2e_v_i16m1x2_m_vl (int16_t *base, vuint16m1_t bindex, vbool16_t mask, vint16m1x2_t value, _VL_T vl);
void vsxseg3e_v_i16m1x3_m_vl (int16_t *base, vuint16m1_t bindex, vbool16_t mask, vint16m1x3_t value, _VL_T vl);
void vsxseg4e_v_i16m1x4_m_vl (int16_t *base, vuint16m1_t bindex, vbool16_t mask, vint16m1x4_t value, _VL_T vl);
void vsxseg5e_v_i16m1x5_m_vl (int16_t *base, vuint16m1_t bindex, vbool16_t mask, vint16m1x5_t value, _VL_T vl);
void vsxseg6e_v_i16m1x6_m_vl (int16_t *base, vuint16m1_t bindex, vbool16_t mask, vint16m1x6_t value, _VL_T vl);
void vsxseg7e_v_i16m1x7_m_vl (int16_t *base, vuint16m1_t bindex, vbool16_t mask, vint16m1x7_t value, _VL_T vl);
void vsxseg8e_v_i16m1x8_m_vl (int16_t *base, vuint16m1_t bindex, vbool16_t mask, vint16m1x8_t value, _VL_T vl);
void vsxseg2e_v_i16m2x2_m_vl (int16_t *base, vuint16m2_t bindex, vbool8_t mask, vint16m2x2_t value, _VL_T vl);
void vsxseg3e_v_i16m2x3_m_vl (int16_t *base, vuint16m2_t bindex, vbool8_t mask, vint16m2x3_t value, _VL_T vl);
void vsxseg4e_v_i16m2x4_m_vl (int16_t *base, vuint16m2_t bindex, vbool8_t mask, vint16m2x4_t value, _VL_T vl);
void vsxseg2e_v_i16m4x2_m_vl (int16_t *base, vuint16m4_t bindex, vbool4_t mask, vint16m4x2_t value, _VL_T vl);
void vsxseg2e_v_i32m1x2_m_vl (int32_t *base, vuint32m1_t bindex, vbool32_t mask, vint32m1x2_t value, _VL_T vl);
void vsxseg3e_v_i32m1x3_m_vl (int32_t *base, vuint32m1_t bindex, vbool32_t mask, vint32m1x3_t value, _VL_T vl);
void vsxseg4e_v_i32m1x4_m_vl (int32_t *base, vuint32m1_t bindex, vbool32_t mask, vint32m1x4_t value, _VL_T vl);
void vsxseg5e_v_i32m1x5_m_vl (int32_t *base, vuint32m1_t bindex, vbool32_t mask, vint32m1x5_t value, _VL_T vl);
void vsxseg6e_v_i32m1x6_m_vl (int32_t *base, vuint32m1_t bindex, vbool32_t mask, vint32m1x6_t value, _VL_T vl);
void vsxseg7e_v_i32m1x7_m_vl (int32_t *base, vuint32m1_t bindex, vbool32_t mask, vint32m1x7_t value, _VL_T vl);
void vsxseg8e_v_i32m1x8_m_vl (int32_t *base, vuint32m1_t bindex, vbool32_t mask, vint32m1x8_t value, _VL_T vl);
void vsxseg2e_v_i32m2x2_m_vl (int32_t *base, vuint32m2_t bindex, vbool16_t mask, vint32m2x2_t value, _VL_T vl);
void vsxseg3e_v_i32m2x3_m_vl (int32_t *base, vuint32m2_t bindex, vbool16_t mask, vint32m2x3_t value, _VL_T vl);
void vsxseg4e_v_i32m2x4_m_vl (int32_t *base, vuint32m2_t bindex, vbool16_t mask, vint32m2x4_t value, _VL_T vl);
void vsxseg2e_v_i32m4x2_m_vl (int32_t *base, vuint32m4_t bindex, vbool8_t mask, vint32m4x2_t value, _VL_T vl);
void vsxseg2e_v_i64m1x2_m_vl (int64_t *base, vuint64m1_t bindex, vbool64_t mask, vint64m1x2_t value, _VL_T vl);
void vsxseg3e_v_i64m1x3_m_vl (int64_t *base, vuint64m1_t bindex, vbool64_t mask, vint64m1x3_t value, _VL_T vl);
void vsxseg4e_v_i64m1x4_m_vl (int64_t *base, vuint64m1_t bindex, vbool64_t mask, vint64m1x4_t value, _VL_T vl);
void vsxseg5e_v_i64m1x5_m_vl (int64_t *base, vuint64m1_t bindex, vbool64_t mask, vint64m1x5_t value, _VL_T vl);
void vsxseg6e_v_i64m1x6_m_vl (int64_t *base, vuint64m1_t bindex, vbool64_t mask, vint64m1x6_t value, _VL_T vl);
void vsxseg7e_v_i64m1x7_m_vl (int64_t *base, vuint64m1_t bindex, vbool64_t mask, vint64m1x7_t value, _VL_T vl);
void vsxseg8e_v_i64m1x8_m_vl (int64_t *base, vuint64m1_t bindex, vbool64_t mask, vint64m1x8_t value, _VL_T vl);
void vsxseg2e_v_i64m2x2_m_vl (int64_t *base, vuint64m2_t bindex, vbool32_t mask, vint64m2x2_t value, _VL_T vl);
void vsxseg3e_v_i64m2x3_m_vl (int64_t *base, vuint64m2_t bindex, vbool32_t mask, vint64m2x3_t value, _VL_T vl);
void vsxseg4e_v_i64m2x4_m_vl (int64_t *base, vuint64m2_t bindex, vbool32_t mask, vint64m2x4_t value, _VL_T vl);
void vsxseg2e_v_i64m4x2_m_vl (int64_t *base, vuint64m4_t bindex, vbool16_t mask, vint64m4x2_t value, _VL_T vl);
void vsxseg2e_v_u8m1x2_m_vl (uint8_t *base, vuint8m1_t bindex, vbool8_t mask, vuint8m1x2_t value, _VL_T vl);
void vsxseg3e_v_u8m1x3_m_vl (uint8_t *base, vuint8m1_t bindex, vbool8_t mask, vuint8m1x3_t value, _VL_T vl);
void vsxseg4e_v_u8m1x4_m_vl (uint8_t *base, vuint8m1_t bindex, vbool8_t mask, vuint8m1x4_t value, _VL_T vl);
void vsxseg5e_v_u8m1x5_m_vl (uint8_t *base, vuint8m1_t bindex, vbool8_t mask, vuint8m1x5_t value, _VL_T vl);
void vsxseg6e_v_u8m1x6_m_vl (uint8_t *base, vuint8m1_t bindex, vbool8_t mask, vuint8m1x6_t value, _VL_T vl);
void vsxseg7e_v_u8m1x7_m_vl (uint8_t *base, vuint8m1_t bindex, vbool8_t mask, vuint8m1x7_t value, _VL_T vl);
void vsxseg8e_v_u8m1x8_m_vl (uint8_t *base, vuint8m1_t bindex, vbool8_t mask, vuint8m1x8_t value, _VL_T vl);
void vsxseg2e_v_u8m2x2_m_vl (uint8_t *base, vuint8m2_t bindex, vbool4_t mask, vuint8m2x2_t value, _VL_T vl);
void vsxseg3e_v_u8m2x3_m_vl (uint8_t *base, vuint8m2_t bindex, vbool4_t mask, vuint8m2x3_t value, _VL_T vl);
void vsxseg4e_v_u8m2x4_m_vl (uint8_t *base, vuint8m2_t bindex, vbool4_t mask, vuint8m2x4_t value, _VL_T vl);
void vsxseg2e_v_u8m4x2_m_vl (uint8_t *base, vuint8m4_t bindex, vbool2_t mask, vuint8m4x2_t value, _VL_T vl);
void vsxseg2e_v_u16m1x2_m_vl (uint16_t *base, vuint16m1_t bindex, vbool16_t mask, vuint16m1x2_t value, _VL_T vl);
void vsxseg3e_v_u16m1x3_m_vl (uint16_t *base, vuint16m1_t bindex, vbool16_t mask, vuint16m1x3_t value, _VL_T vl);
void vsxseg4e_v_u16m1x4_m_vl (uint16_t *base, vuint16m1_t bindex, vbool16_t mask, vuint16m1x4_t value, _VL_T vl);
void vsxseg5e_v_u16m1x5_m_vl (uint16_t *base, vuint16m1_t bindex, vbool16_t mask, vuint16m1x5_t value, _VL_T vl);
void vsxseg6e_v_u16m1x6_m_vl (uint16_t *base, vuint16m1_t bindex, vbool16_t mask, vuint16m1x6_t value, _VL_T vl);
void vsxseg7e_v_u16m1x7_m_vl (uint16_t *base, vuint16m1_t bindex, vbool16_t mask, vuint16m1x7_t value, _VL_T vl);
void vsxseg8e_v_u16m1x8_m_vl (uint16_t *base, vuint16m1_t bindex, vbool16_t mask, vuint16m1x8_t value, _VL_T vl);
void vsxseg2e_v_u16m2x2_m_vl (uint16_t *base, vuint16m2_t bindex, vbool8_t mask, vuint16m2x2_t value, _VL_T vl);
void vsxseg3e_v_u16m2x3_m_vl (uint16_t *base, vuint16m2_t bindex, vbool8_t mask, vuint16m2x3_t value, _VL_T vl);
void vsxseg4e_v_u16m2x4_m_vl (uint16_t *base, vuint16m2_t bindex, vbool8_t mask, vuint16m2x4_t value, _VL_T vl);
void vsxseg2e_v_u16m4x2_m_vl (uint16_t *base, vuint16m4_t bindex, vbool4_t mask, vuint16m4x2_t value, _VL_T vl);
void vsxseg2e_v_u32m1x2_m_vl (uint32_t *base, vuint32m1_t bindex, vbool32_t mask, vuint32m1x2_t value, _VL_T vl);
void vsxseg3e_v_u32m1x3_m_vl (uint32_t *base, vuint32m1_t bindex, vbool32_t mask, vuint32m1x3_t value, _VL_T vl);
void vsxseg4e_v_u32m1x4_m_vl (uint32_t *base, vuint32m1_t bindex, vbool32_t mask, vuint32m1x4_t value, _VL_T vl);
void vsxseg5e_v_u32m1x5_m_vl (uint32_t *base, vuint32m1_t bindex, vbool32_t mask, vuint32m1x5_t value, _VL_T vl);
void vsxseg6e_v_u32m1x6_m_vl (uint32_t *base, vuint32m1_t bindex, vbool32_t mask, vuint32m1x6_t value, _VL_T vl);
void vsxseg7e_v_u32m1x7_m_vl (uint32_t *base, vuint32m1_t bindex, vbool32_t mask, vuint32m1x7_t value, _VL_T vl);
void vsxseg8e_v_u32m1x8_m_vl (uint32_t *base, vuint32m1_t bindex, vbool32_t mask, vuint32m1x8_t value, _VL_T vl);
void vsxseg2e_v_u32m2x2_m_vl (uint32_t *base, vuint32m2_t bindex, vbool16_t mask, vuint32m2x2_t value, _VL_T vl);
void vsxseg3e_v_u32m2x3_m_vl (uint32_t *base, vuint32m2_t bindex, vbool16_t mask, vuint32m2x3_t value, _VL_T vl);
void vsxseg4e_v_u32m2x4_m_vl (uint32_t *base, vuint32m2_t bindex, vbool16_t mask, vuint32m2x4_t value, _VL_T vl);
void vsxseg2e_v_u32m4x2_m_vl (uint32_t *base, vuint32m4_t bindex, vbool8_t mask, vuint32m4x2_t value, _VL_T vl);
void vsxseg2e_v_u64m1x2_m_vl (uint64_t *base, vuint64m1_t bindex, vbool64_t mask, vuint64m1x2_t value, _VL_T vl);
void vsxseg3e_v_u64m1x3_m_vl (uint64_t *base, vuint64m1_t bindex, vbool64_t mask, vuint64m1x3_t value, _VL_T vl);
void vsxseg4e_v_u64m1x4_m_vl (uint64_t *base, vuint64m1_t bindex, vbool64_t mask, vuint64m1x4_t value, _VL_T vl);
void vsxseg5e_v_u64m1x5_m_vl (uint64_t *base, vuint64m1_t bindex, vbool64_t mask, vuint64m1x5_t value, _VL_T vl);
void vsxseg6e_v_u64m1x6_m_vl (uint64_t *base, vuint64m1_t bindex, vbool64_t mask, vuint64m1x6_t value, _VL_T vl);
void vsxseg7e_v_u64m1x7_m_vl (uint64_t *base, vuint64m1_t bindex, vbool64_t mask, vuint64m1x7_t value, _VL_T vl);
void vsxseg8e_v_u64m1x8_m_vl (uint64_t *base, vuint64m1_t bindex, vbool64_t mask, vuint64m1x8_t value, _VL_T vl);
void vsxseg2e_v_u64m2x2_m_vl (uint64_t *base, vuint64m2_t bindex, vbool32_t mask, vuint64m2x2_t value, _VL_T vl);
void vsxseg3e_v_u64m2x3_m_vl (uint64_t *base, vuint64m2_t bindex, vbool32_t mask, vuint64m2x3_t value, _VL_T vl);
void vsxseg4e_v_u64m2x4_m_vl (uint64_t *base, vuint64m2_t bindex, vbool32_t mask, vuint64m2x4_t value, _VL_T vl);
void vsxseg2e_v_u64m4x2_m_vl (uint64_t *base, vuint64m4_t bindex, vbool16_t mask, vuint64m4x2_t value, _VL_T vl);
void vsxseg2e_v_f16m1x2_m_vl (float16_t *base, vuint16m1_t bindex, vbool16_t mask, vfloat16m1x2_t value, _VL_T vl);
void vsxseg3e_v_f16m1x3_m_vl (float16_t *base, vuint16m1_t bindex, vbool16_t mask, vfloat16m1x3_t value, _VL_T vl);
void vsxseg4e_v_f16m1x4_m_vl (float16_t *base, vuint16m1_t bindex, vbool16_t mask, vfloat16m1x4_t value, _VL_T vl);
void vsxseg5e_v_f16m1x5_m_vl (float16_t *base, vuint16m1_t bindex, vbool16_t mask, vfloat16m1x5_t value, _VL_T vl);
void vsxseg6e_v_f16m1x6_m_vl (float16_t *base, vuint16m1_t bindex, vbool16_t mask, vfloat16m1x6_t value, _VL_T vl);
void vsxseg7e_v_f16m1x7_m_vl (float16_t *base, vuint16m1_t bindex, vbool16_t mask, vfloat16m1x7_t value, _VL_T vl);
void vsxseg8e_v_f16m1x8_m_vl (float16_t *base, vuint16m1_t bindex, vbool16_t mask, vfloat16m1x8_t value, _VL_T vl);
void vsxseg2e_v_f16m2x2_m_vl (float16_t *base, vuint16m2_t bindex, vbool8_t mask, vfloat16m2x2_t value, _VL_T vl);
void vsxseg3e_v_f16m2x3_m_vl (float16_t *base, vuint16m2_t bindex, vbool8_t mask, vfloat16m2x3_t value, _VL_T vl);
void vsxseg4e_v_f16m2x4_m_vl (float16_t *base, vuint16m2_t bindex, vbool8_t mask, vfloat16m2x4_t value, _VL_T vl);
void vsxseg2e_v_f16m4x2_m_vl (float16_t *base, vuint16m4_t bindex, vbool4_t mask, vfloat16m4x2_t value, _VL_T vl);
void vsxseg2e_v_f32m1x2_m_vl (float32_t *base, vuint32m1_t bindex, vbool32_t mask, vfloat32m1x2_t value, _VL_T vl);
void vsxseg3e_v_f32m1x3_m_vl (float32_t *base, vuint32m1_t bindex, vbool32_t mask, vfloat32m1x3_t value, _VL_T vl);
void vsxseg4e_v_f32m1x4_m_vl (float32_t *base, vuint32m1_t bindex, vbool32_t mask, vfloat32m1x4_t value, _VL_T vl);
void vsxseg5e_v_f32m1x5_m_vl (float32_t *base, vuint32m1_t bindex, vbool32_t mask, vfloat32m1x5_t value, _VL_T vl);
void vsxseg6e_v_f32m1x6_m_vl (float32_t *base, vuint32m1_t bindex, vbool32_t mask, vfloat32m1x6_t value, _VL_T vl);
void vsxseg7e_v_f32m1x7_m_vl (float32_t *base, vuint32m1_t bindex, vbool32_t mask, vfloat32m1x7_t value, _VL_T vl);
void vsxseg8e_v_f32m1x8_m_vl (float32_t *base, vuint32m1_t bindex, vbool32_t mask, vfloat32m1x8_t value, _VL_T vl);
void vsxseg2e_v_f32m2x2_m_vl (float32_t *base, vuint32m2_t bindex, vbool16_t mask, vfloat32m2x2_t value, _VL_T vl);
void vsxseg3e_v_f32m2x3_m_vl (float32_t *base, vuint32m2_t bindex, vbool16_t mask, vfloat32m2x3_t value, _VL_T vl);
void vsxseg4e_v_f32m2x4_m_vl (float32_t *base, vuint32m2_t bindex, vbool16_t mask, vfloat32m2x4_t value, _VL_T vl);
void vsxseg2e_v_f32m4x2_m_vl (float32_t *base, vuint32m4_t bindex, vbool8_t mask, vfloat32m4x2_t value, _VL_T vl);
void vsxseg2e_v_f64m1x2_m_vl (float64_t *base, vuint64m1_t bindex, vbool64_t mask, vfloat64m1x2_t value, _VL_T vl);
void vsxseg3e_v_f64m1x3_m_vl (float64_t *base, vuint64m1_t bindex, vbool64_t mask, vfloat64m1x3_t value, _VL_T vl);
void vsxseg4e_v_f64m1x4_m_vl (float64_t *base, vuint64m1_t bindex, vbool64_t mask, vfloat64m1x4_t value, _VL_T vl);
void vsxseg5e_v_f64m1x5_m_vl (float64_t *base, vuint64m1_t bindex, vbool64_t mask, vfloat64m1x5_t value, _VL_T vl);
void vsxseg6e_v_f64m1x6_m_vl (float64_t *base, vuint64m1_t bindex, vbool64_t mask, vfloat64m1x6_t value, _VL_T vl);
void vsxseg7e_v_f64m1x7_m_vl (float64_t *base, vuint64m1_t bindex, vbool64_t mask, vfloat64m1x7_t value, _VL_T vl);
void vsxseg8e_v_f64m1x8_m_vl (float64_t *base, vuint64m1_t bindex, vbool64_t mask, vfloat64m1x8_t value, _VL_T vl);
void vsxseg2e_v_f64m2x2_m_vl (float64_t *base, vuint64m2_t bindex, vbool32_t mask, vfloat64m2x2_t value, _VL_T vl);
void vsxseg3e_v_f64m2x3_m_vl (float64_t *base, vuint64m2_t bindex, vbool32_t mask, vfloat64m2x3_t value, _VL_T vl);
void vsxseg4e_v_f64m2x4_m_vl (float64_t *base, vuint64m2_t bindex, vbool32_t mask, vfloat64m2x4_t value, _VL_T vl);
void vsxseg2e_v_f64m4x2_m_vl (float64_t *base, vuint64m4_t bindex, vbool16_t mask, vfloat64m4x2_t value, _VL_T vl);
```
## Miscellaneous for Vector Tuple Types:

### [Vector Tuple Move Functions]():

**Prototypes:**
``` C
vint8m1x2_t vcopy_i8m1x2_vl (vint8m1x2_t src, _VL_T vl);
vint8m1x3_t vcopy_i8m1x3_vl (vint8m1x3_t src, _VL_T vl);
vint8m1x4_t vcopy_i8m1x4_vl (vint8m1x4_t src, _VL_T vl);
vint8m1x5_t vcopy_i8m1x5_vl (vint8m1x5_t src, _VL_T vl);
vint8m1x6_t vcopy_i8m1x6_vl (vint8m1x6_t src, _VL_T vl);
vint8m1x7_t vcopy_i8m1x7_vl (vint8m1x7_t src, _VL_T vl);
vint8m1x8_t vcopy_i8m1x8_vl (vint8m1x8_t src, _VL_T vl);
vint8m2x2_t vcopy_i8m2x2_vl (vint8m2x2_t src, _VL_T vl);
vint8m2x3_t vcopy_i8m2x3_vl (vint8m2x3_t src, _VL_T vl);
vint8m2x4_t vcopy_i8m2x4_vl (vint8m2x4_t src, _VL_T vl);
vint8m4x2_t vcopy_i8m4x2_vl (vint8m4x2_t src, _VL_T vl);
vint16m1x2_t vcopy_i16m1x2_vl (vint16m1x2_t src, _VL_T vl);
vint16m1x3_t vcopy_i16m1x3_vl (vint16m1x3_t src, _VL_T vl);
vint16m1x4_t vcopy_i16m1x4_vl (vint16m1x4_t src, _VL_T vl);
vint16m1x5_t vcopy_i16m1x5_vl (vint16m1x5_t src, _VL_T vl);
vint16m1x6_t vcopy_i16m1x6_vl (vint16m1x6_t src, _VL_T vl);
vint16m1x7_t vcopy_i16m1x7_vl (vint16m1x7_t src, _VL_T vl);
vint16m1x8_t vcopy_i16m1x8_vl (vint16m1x8_t src, _VL_T vl);
vint16m2x2_t vcopy_i16m2x2_vl (vint16m2x2_t src, _VL_T vl);
vint16m2x3_t vcopy_i16m2x3_vl (vint16m2x3_t src, _VL_T vl);
vint16m2x4_t vcopy_i16m2x4_vl (vint16m2x4_t src, _VL_T vl);
vint16m4x2_t vcopy_i16m4x2_vl (vint16m4x2_t src, _VL_T vl);
vint32m1x2_t vcopy_i32m1x2_vl (vint32m1x2_t src, _VL_T vl);
vint32m1x3_t vcopy_i32m1x3_vl (vint32m1x3_t src, _VL_T vl);
vint32m1x4_t vcopy_i32m1x4_vl (vint32m1x4_t src, _VL_T vl);
vint32m1x5_t vcopy_i32m1x5_vl (vint32m1x5_t src, _VL_T vl);
vint32m1x6_t vcopy_i32m1x6_vl (vint32m1x6_t src, _VL_T vl);
vint32m1x7_t vcopy_i32m1x7_vl (vint32m1x7_t src, _VL_T vl);
vint32m1x8_t vcopy_i32m1x8_vl (vint32m1x8_t src, _VL_T vl);
vint32m2x2_t vcopy_i32m2x2_vl (vint32m2x2_t src, _VL_T vl);
vint32m2x3_t vcopy_i32m2x3_vl (vint32m2x3_t src, _VL_T vl);
vint32m2x4_t vcopy_i32m2x4_vl (vint32m2x4_t src, _VL_T vl);
vint32m4x2_t vcopy_i32m4x2_vl (vint32m4x2_t src, _VL_T vl);
vint64m1x2_t vcopy_i64m1x2_vl (vint64m1x2_t src, _VL_T vl);
vint64m1x3_t vcopy_i64m1x3_vl (vint64m1x3_t src, _VL_T vl);
vint64m1x4_t vcopy_i64m1x4_vl (vint64m1x4_t src, _VL_T vl);
vint64m1x5_t vcopy_i64m1x5_vl (vint64m1x5_t src, _VL_T vl);
vint64m1x6_t vcopy_i64m1x6_vl (vint64m1x6_t src, _VL_T vl);
vint64m1x7_t vcopy_i64m1x7_vl (vint64m1x7_t src, _VL_T vl);
vint64m1x8_t vcopy_i64m1x8_vl (vint64m1x8_t src, _VL_T vl);
vint64m2x2_t vcopy_i64m2x2_vl (vint64m2x2_t src, _VL_T vl);
vint64m2x3_t vcopy_i64m2x3_vl (vint64m2x3_t src, _VL_T vl);
vint64m2x4_t vcopy_i64m2x4_vl (vint64m2x4_t src, _VL_T vl);
vint64m4x2_t vcopy_i64m4x2_vl (vint64m4x2_t src, _VL_T vl);
vuint8m1x2_t vcopy_u8m1x2_vl (vuint8m1x2_t src, _VL_T vl);
vuint8m1x3_t vcopy_u8m1x3_vl (vuint8m1x3_t src, _VL_T vl);
vuint8m1x4_t vcopy_u8m1x4_vl (vuint8m1x4_t src, _VL_T vl);
vuint8m1x5_t vcopy_u8m1x5_vl (vuint8m1x5_t src, _VL_T vl);
vuint8m1x6_t vcopy_u8m1x6_vl (vuint8m1x6_t src, _VL_T vl);
vuint8m1x7_t vcopy_u8m1x7_vl (vuint8m1x7_t src, _VL_T vl);
vuint8m1x8_t vcopy_u8m1x8_vl (vuint8m1x8_t src, _VL_T vl);
vuint8m2x2_t vcopy_u8m2x2_vl (vuint8m2x2_t src, _VL_T vl);
vuint8m2x3_t vcopy_u8m2x3_vl (vuint8m2x3_t src, _VL_T vl);
vuint8m2x4_t vcopy_u8m2x4_vl (vuint8m2x4_t src, _VL_T vl);
vuint8m4x2_t vcopy_u8m4x2_vl (vuint8m4x2_t src, _VL_T vl);
vuint16m1x2_t vcopy_u16m1x2_vl (vuint16m1x2_t src, _VL_T vl);
vuint16m1x3_t vcopy_u16m1x3_vl (vuint16m1x3_t src, _VL_T vl);
vuint16m1x4_t vcopy_u16m1x4_vl (vuint16m1x4_t src, _VL_T vl);
vuint16m1x5_t vcopy_u16m1x5_vl (vuint16m1x5_t src, _VL_T vl);
vuint16m1x6_t vcopy_u16m1x6_vl (vuint16m1x6_t src, _VL_T vl);
vuint16m1x7_t vcopy_u16m1x7_vl (vuint16m1x7_t src, _VL_T vl);
vuint16m1x8_t vcopy_u16m1x8_vl (vuint16m1x8_t src, _VL_T vl);
vuint16m2x2_t vcopy_u16m2x2_vl (vuint16m2x2_t src, _VL_T vl);
vuint16m2x3_t vcopy_u16m2x3_vl (vuint16m2x3_t src, _VL_T vl);
vuint16m2x4_t vcopy_u16m2x4_vl (vuint16m2x4_t src, _VL_T vl);
vuint16m4x2_t vcopy_u16m4x2_vl (vuint16m4x2_t src, _VL_T vl);
vuint32m1x2_t vcopy_u32m1x2_vl (vuint32m1x2_t src, _VL_T vl);
vuint32m1x3_t vcopy_u32m1x3_vl (vuint32m1x3_t src, _VL_T vl);
vuint32m1x4_t vcopy_u32m1x4_vl (vuint32m1x4_t src, _VL_T vl);
vuint32m1x5_t vcopy_u32m1x5_vl (vuint32m1x5_t src, _VL_T vl);
vuint32m1x6_t vcopy_u32m1x6_vl (vuint32m1x6_t src, _VL_T vl);
vuint32m1x7_t vcopy_u32m1x7_vl (vuint32m1x7_t src, _VL_T vl);
vuint32m1x8_t vcopy_u32m1x8_vl (vuint32m1x8_t src, _VL_T vl);
vuint32m2x2_t vcopy_u32m2x2_vl (vuint32m2x2_t src, _VL_T vl);
vuint32m2x3_t vcopy_u32m2x3_vl (vuint32m2x3_t src, _VL_T vl);
vuint32m2x4_t vcopy_u32m2x4_vl (vuint32m2x4_t src, _VL_T vl);
vuint32m4x2_t vcopy_u32m4x2_vl (vuint32m4x2_t src, _VL_T vl);
vuint64m1x2_t vcopy_u64m1x2_vl (vuint64m1x2_t src, _VL_T vl);
vuint64m1x3_t vcopy_u64m1x3_vl (vuint64m1x3_t src, _VL_T vl);
vuint64m1x4_t vcopy_u64m1x4_vl (vuint64m1x4_t src, _VL_T vl);
vuint64m1x5_t vcopy_u64m1x5_vl (vuint64m1x5_t src, _VL_T vl);
vuint64m1x6_t vcopy_u64m1x6_vl (vuint64m1x6_t src, _VL_T vl);
vuint64m1x7_t vcopy_u64m1x7_vl (vuint64m1x7_t src, _VL_T vl);
vuint64m1x8_t vcopy_u64m1x8_vl (vuint64m1x8_t src, _VL_T vl);
vuint64m2x2_t vcopy_u64m2x2_vl (vuint64m2x2_t src, _VL_T vl);
vuint64m2x3_t vcopy_u64m2x3_vl (vuint64m2x3_t src, _VL_T vl);
vuint64m2x4_t vcopy_u64m2x4_vl (vuint64m2x4_t src, _VL_T vl);
vuint64m4x2_t vcopy_u64m4x2_vl (vuint64m4x2_t src, _VL_T vl);
vfloat16m1x2_t vcopy_f16m1x2_vl (vfloat16m1x2_t src, _VL_T vl);
vfloat16m1x3_t vcopy_f16m1x3_vl (vfloat16m1x3_t src, _VL_T vl);
vfloat16m1x4_t vcopy_f16m1x4_vl (vfloat16m1x4_t src, _VL_T vl);
vfloat16m1x5_t vcopy_f16m1x5_vl (vfloat16m1x5_t src, _VL_T vl);
vfloat16m1x6_t vcopy_f16m1x6_vl (vfloat16m1x6_t src, _VL_T vl);
vfloat16m1x7_t vcopy_f16m1x7_vl (vfloat16m1x7_t src, _VL_T vl);
vfloat16m1x8_t vcopy_f16m1x8_vl (vfloat16m1x8_t src, _VL_T vl);
vfloat16m2x2_t vcopy_f16m2x2_vl (vfloat16m2x2_t src, _VL_T vl);
vfloat16m2x3_t vcopy_f16m2x3_vl (vfloat16m2x3_t src, _VL_T vl);
vfloat16m2x4_t vcopy_f16m2x4_vl (vfloat16m2x4_t src, _VL_T vl);
vfloat16m4x2_t vcopy_f16m4x2_vl (vfloat16m4x2_t src, _VL_T vl);
vfloat32m1x2_t vcopy_f32m1x2_vl (vfloat32m1x2_t src, _VL_T vl);
vfloat32m1x3_t vcopy_f32m1x3_vl (vfloat32m1x3_t src, _VL_T vl);
vfloat32m1x4_t vcopy_f32m1x4_vl (vfloat32m1x4_t src, _VL_T vl);
vfloat32m1x5_t vcopy_f32m1x5_vl (vfloat32m1x5_t src, _VL_T vl);
vfloat32m1x6_t vcopy_f32m1x6_vl (vfloat32m1x6_t src, _VL_T vl);
vfloat32m1x7_t vcopy_f32m1x7_vl (vfloat32m1x7_t src, _VL_T vl);
vfloat32m1x8_t vcopy_f32m1x8_vl (vfloat32m1x8_t src, _VL_T vl);
vfloat32m2x2_t vcopy_f32m2x2_vl (vfloat32m2x2_t src, _VL_T vl);
vfloat32m2x3_t vcopy_f32m2x3_vl (vfloat32m2x3_t src, _VL_T vl);
vfloat32m2x4_t vcopy_f32m2x4_vl (vfloat32m2x4_t src, _VL_T vl);
vfloat32m4x2_t vcopy_f32m4x2_vl (vfloat32m4x2_t src, _VL_T vl);
vfloat64m1x2_t vcopy_f64m1x2_vl (vfloat64m1x2_t src, _VL_T vl);
vfloat64m1x3_t vcopy_f64m1x3_vl (vfloat64m1x3_t src, _VL_T vl);
vfloat64m1x4_t vcopy_f64m1x4_vl (vfloat64m1x4_t src, _VL_T vl);
vfloat64m1x5_t vcopy_f64m1x5_vl (vfloat64m1x5_t src, _VL_T vl);
vfloat64m1x6_t vcopy_f64m1x6_vl (vfloat64m1x6_t src, _VL_T vl);
vfloat64m1x7_t vcopy_f64m1x7_vl (vfloat64m1x7_t src, _VL_T vl);
vfloat64m1x8_t vcopy_f64m1x8_vl (vfloat64m1x8_t src, _VL_T vl);
vfloat64m2x2_t vcopy_f64m2x2_vl (vfloat64m2x2_t src, _VL_T vl);
vfloat64m2x3_t vcopy_f64m2x3_vl (vfloat64m2x3_t src, _VL_T vl);
vfloat64m2x4_t vcopy_f64m2x4_vl (vfloat64m2x4_t src, _VL_T vl);
vfloat64m4x2_t vcopy_f64m4x2_vl (vfloat64m4x2_t src, _VL_T vl);
```
### [Vector Tuple Creation Functions]():

**Prototypes:**
``` C
vint8m1x2_t vcreate_i8m1x2_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1x3_t vcreate_i8m1x3_vl (vint8m1_t op1, vint8m1_t op2, vint8m1_t op3, _VL_T vl);
vint8m1x4_t vcreate_i8m1x4_vl (vint8m1_t op1, vint8m1_t op2, vint8m1_t op3, vint8m1_t op4, _VL_T vl);
vint8m1x5_t vcreate_i8m1x5_vl (vint8m1_t op1, vint8m1_t op2, vint8m1_t op3, vint8m1_t op4, vint8m1_t op5, _VL_T vl);
vint8m1x6_t vcreate_i8m1x6_vl (vint8m1_t op1, vint8m1_t op2, vint8m1_t op3, vint8m1_t op4, vint8m1_t op5, vint8m1_t op6, _VL_T vl);
vint8m1x7_t vcreate_i8m1x7_vl (vint8m1_t op1, vint8m1_t op2, vint8m1_t op3, vint8m1_t op4, vint8m1_t op5, vint8m1_t op6, vint8m1_t op7, _VL_T vl);
vint8m1x8_t vcreate_i8m1x8_vl (vint8m1_t op1, vint8m1_t op2, vint8m1_t op3, vint8m1_t op4, vint8m1_t op5, vint8m1_t op6, vint8m1_t op7, vint8m1_t op8, _VL_T vl);
vint8m2x2_t vcreate_i8m2x2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2x3_t vcreate_i8m2x3_vl (vint8m2_t op1, vint8m2_t op2, vint8m2_t op3, _VL_T vl);
vint8m2x4_t vcreate_i8m2x4_vl (vint8m2_t op1, vint8m2_t op2, vint8m2_t op3, vint8m2_t op4, _VL_T vl);
vint8m4x2_t vcreate_i8m4x2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m1x2_t vcreate_i16m1x2_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1x3_t vcreate_i16m1x3_vl (vint16m1_t op1, vint16m1_t op2, vint16m1_t op3, _VL_T vl);
vint16m1x4_t vcreate_i16m1x4_vl (vint16m1_t op1, vint16m1_t op2, vint16m1_t op3, vint16m1_t op4, _VL_T vl);
vint16m1x5_t vcreate_i16m1x5_vl (vint16m1_t op1, vint16m1_t op2, vint16m1_t op3, vint16m1_t op4, vint16m1_t op5, _VL_T vl);
vint16m1x6_t vcreate_i16m1x6_vl (vint16m1_t op1, vint16m1_t op2, vint16m1_t op3, vint16m1_t op4, vint16m1_t op5, vint16m1_t op6, _VL_T vl);
vint16m1x7_t vcreate_i16m1x7_vl (vint16m1_t op1, vint16m1_t op2, vint16m1_t op3, vint16m1_t op4, vint16m1_t op5, vint16m1_t op6, vint16m1_t op7, _VL_T vl);
vint16m1x8_t vcreate_i16m1x8_vl (vint16m1_t op1, vint16m1_t op2, vint16m1_t op3, vint16m1_t op4, vint16m1_t op5, vint16m1_t op6, vint16m1_t op7, vint16m1_t op8, _VL_T vl);
vint16m2x2_t vcreate_i16m2x2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2x3_t vcreate_i16m2x3_vl (vint16m2_t op1, vint16m2_t op2, vint16m2_t op3, _VL_T vl);
vint16m2x4_t vcreate_i16m2x4_vl (vint16m2_t op1, vint16m2_t op2, vint16m2_t op3, vint16m2_t op4, _VL_T vl);
vint16m4x2_t vcreate_i16m4x2_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m1x2_t vcreate_i32m1x2_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1x3_t vcreate_i32m1x3_vl (vint32m1_t op1, vint32m1_t op2, vint32m1_t op3, _VL_T vl);
vint32m1x4_t vcreate_i32m1x4_vl (vint32m1_t op1, vint32m1_t op2, vint32m1_t op3, vint32m1_t op4, _VL_T vl);
vint32m1x5_t vcreate_i32m1x5_vl (vint32m1_t op1, vint32m1_t op2, vint32m1_t op3, vint32m1_t op4, vint32m1_t op5, _VL_T vl);
vint32m1x6_t vcreate_i32m1x6_vl (vint32m1_t op1, vint32m1_t op2, vint32m1_t op3, vint32m1_t op4, vint32m1_t op5, vint32m1_t op6, _VL_T vl);
vint32m1x7_t vcreate_i32m1x7_vl (vint32m1_t op1, vint32m1_t op2, vint32m1_t op3, vint32m1_t op4, vint32m1_t op5, vint32m1_t op6, vint32m1_t op7, _VL_T vl);
vint32m1x8_t vcreate_i32m1x8_vl (vint32m1_t op1, vint32m1_t op2, vint32m1_t op3, vint32m1_t op4, vint32m1_t op5, vint32m1_t op6, vint32m1_t op7, vint32m1_t op8, _VL_T vl);
vint32m2x2_t vcreate_i32m2x2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2x3_t vcreate_i32m2x3_vl (vint32m2_t op1, vint32m2_t op2, vint32m2_t op3, _VL_T vl);
vint32m2x4_t vcreate_i32m2x4_vl (vint32m2_t op1, vint32m2_t op2, vint32m2_t op3, vint32m2_t op4, _VL_T vl);
vint32m4x2_t vcreate_i32m4x2_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m1x2_t vcreate_i64m1x2_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1x3_t vcreate_i64m1x3_vl (vint64m1_t op1, vint64m1_t op2, vint64m1_t op3, _VL_T vl);
vint64m1x4_t vcreate_i64m1x4_vl (vint64m1_t op1, vint64m1_t op2, vint64m1_t op3, vint64m1_t op4, _VL_T vl);
vint64m1x5_t vcreate_i64m1x5_vl (vint64m1_t op1, vint64m1_t op2, vint64m1_t op3, vint64m1_t op4, vint64m1_t op5, _VL_T vl);
vint64m1x6_t vcreate_i64m1x6_vl (vint64m1_t op1, vint64m1_t op2, vint64m1_t op3, vint64m1_t op4, vint64m1_t op5, vint64m1_t op6, _VL_T vl);
vint64m1x7_t vcreate_i64m1x7_vl (vint64m1_t op1, vint64m1_t op2, vint64m1_t op3, vint64m1_t op4, vint64m1_t op5, vint64m1_t op6, vint64m1_t op7, _VL_T vl);
vint64m1x8_t vcreate_i64m1x8_vl (vint64m1_t op1, vint64m1_t op2, vint64m1_t op3, vint64m1_t op4, vint64m1_t op5, vint64m1_t op6, vint64m1_t op7, vint64m1_t op8, _VL_T vl);
vint64m2x2_t vcreate_i64m2x2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2x3_t vcreate_i64m2x3_vl (vint64m2_t op1, vint64m2_t op2, vint64m2_t op3, _VL_T vl);
vint64m2x4_t vcreate_i64m2x4_vl (vint64m2_t op1, vint64m2_t op2, vint64m2_t op3, vint64m2_t op4, _VL_T vl);
vint64m4x2_t vcreate_i64m4x2_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vuint8m1x2_t vcreate_u8m1x2_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1x3_t vcreate_u8m1x3_vl (vuint8m1_t op1, vuint8m1_t op2, vuint8m1_t op3, _VL_T vl);
vuint8m1x4_t vcreate_u8m1x4_vl (vuint8m1_t op1, vuint8m1_t op2, vuint8m1_t op3, vuint8m1_t op4, _VL_T vl);
vuint8m1x5_t vcreate_u8m1x5_vl (vuint8m1_t op1, vuint8m1_t op2, vuint8m1_t op3, vuint8m1_t op4, vuint8m1_t op5, _VL_T vl);
vuint8m1x6_t vcreate_u8m1x6_vl (vuint8m1_t op1, vuint8m1_t op2, vuint8m1_t op3, vuint8m1_t op4, vuint8m1_t op5, vuint8m1_t op6, _VL_T vl);
vuint8m1x7_t vcreate_u8m1x7_vl (vuint8m1_t op1, vuint8m1_t op2, vuint8m1_t op3, vuint8m1_t op4, vuint8m1_t op5, vuint8m1_t op6, vuint8m1_t op7, _VL_T vl);
vuint8m1x8_t vcreate_u8m1x8_vl (vuint8m1_t op1, vuint8m1_t op2, vuint8m1_t op3, vuint8m1_t op4, vuint8m1_t op5, vuint8m1_t op6, vuint8m1_t op7, vuint8m1_t op8, _VL_T vl);
vuint8m2x2_t vcreate_u8m2x2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2x3_t vcreate_u8m2x3_vl (vuint8m2_t op1, vuint8m2_t op2, vuint8m2_t op3, _VL_T vl);
vuint8m2x4_t vcreate_u8m2x4_vl (vuint8m2_t op1, vuint8m2_t op2, vuint8m2_t op3, vuint8m2_t op4, _VL_T vl);
vuint8m4x2_t vcreate_u8m4x2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m1x2_t vcreate_u16m1x2_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1x3_t vcreate_u16m1x3_vl (vuint16m1_t op1, vuint16m1_t op2, vuint16m1_t op3, _VL_T vl);
vuint16m1x4_t vcreate_u16m1x4_vl (vuint16m1_t op1, vuint16m1_t op2, vuint16m1_t op3, vuint16m1_t op4, _VL_T vl);
vuint16m1x5_t vcreate_u16m1x5_vl (vuint16m1_t op1, vuint16m1_t op2, vuint16m1_t op3, vuint16m1_t op4, vuint16m1_t op5, _VL_T vl);
vuint16m1x6_t vcreate_u16m1x6_vl (vuint16m1_t op1, vuint16m1_t op2, vuint16m1_t op3, vuint16m1_t op4, vuint16m1_t op5, vuint16m1_t op6, _VL_T vl);
vuint16m1x7_t vcreate_u16m1x7_vl (vuint16m1_t op1, vuint16m1_t op2, vuint16m1_t op3, vuint16m1_t op4, vuint16m1_t op5, vuint16m1_t op6, vuint16m1_t op7, _VL_T vl);
vuint16m1x8_t vcreate_u16m1x8_vl (vuint16m1_t op1, vuint16m1_t op2, vuint16m1_t op3, vuint16m1_t op4, vuint16m1_t op5, vuint16m1_t op6, vuint16m1_t op7, vuint16m1_t op8, _VL_T vl);
vuint16m2x2_t vcreate_u16m2x2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2x3_t vcreate_u16m2x3_vl (vuint16m2_t op1, vuint16m2_t op2, vuint16m2_t op3, _VL_T vl);
vuint16m2x4_t vcreate_u16m2x4_vl (vuint16m2_t op1, vuint16m2_t op2, vuint16m2_t op3, vuint16m2_t op4, _VL_T vl);
vuint16m4x2_t vcreate_u16m4x2_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m1x2_t vcreate_u32m1x2_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1x3_t vcreate_u32m1x3_vl (vuint32m1_t op1, vuint32m1_t op2, vuint32m1_t op3, _VL_T vl);
vuint32m1x4_t vcreate_u32m1x4_vl (vuint32m1_t op1, vuint32m1_t op2, vuint32m1_t op3, vuint32m1_t op4, _VL_T vl);
vuint32m1x5_t vcreate_u32m1x5_vl (vuint32m1_t op1, vuint32m1_t op2, vuint32m1_t op3, vuint32m1_t op4, vuint32m1_t op5, _VL_T vl);
vuint32m1x6_t vcreate_u32m1x6_vl (vuint32m1_t op1, vuint32m1_t op2, vuint32m1_t op3, vuint32m1_t op4, vuint32m1_t op5, vuint32m1_t op6, _VL_T vl);
vuint32m1x7_t vcreate_u32m1x7_vl (vuint32m1_t op1, vuint32m1_t op2, vuint32m1_t op3, vuint32m1_t op4, vuint32m1_t op5, vuint32m1_t op6, vuint32m1_t op7, _VL_T vl);
vuint32m1x8_t vcreate_u32m1x8_vl (vuint32m1_t op1, vuint32m1_t op2, vuint32m1_t op3, vuint32m1_t op4, vuint32m1_t op5, vuint32m1_t op6, vuint32m1_t op7, vuint32m1_t op8, _VL_T vl);
vuint32m2x2_t vcreate_u32m2x2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2x3_t vcreate_u32m2x3_vl (vuint32m2_t op1, vuint32m2_t op2, vuint32m2_t op3, _VL_T vl);
vuint32m2x4_t vcreate_u32m2x4_vl (vuint32m2_t op1, vuint32m2_t op2, vuint32m2_t op3, vuint32m2_t op4, _VL_T vl);
vuint32m4x2_t vcreate_u32m4x2_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m1x2_t vcreate_u64m1x2_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1x3_t vcreate_u64m1x3_vl (vuint64m1_t op1, vuint64m1_t op2, vuint64m1_t op3, _VL_T vl);
vuint64m1x4_t vcreate_u64m1x4_vl (vuint64m1_t op1, vuint64m1_t op2, vuint64m1_t op3, vuint64m1_t op4, _VL_T vl);
vuint64m1x5_t vcreate_u64m1x5_vl (vuint64m1_t op1, vuint64m1_t op2, vuint64m1_t op3, vuint64m1_t op4, vuint64m1_t op5, _VL_T vl);
vuint64m1x6_t vcreate_u64m1x6_vl (vuint64m1_t op1, vuint64m1_t op2, vuint64m1_t op3, vuint64m1_t op4, vuint64m1_t op5, vuint64m1_t op6, _VL_T vl);
vuint64m1x7_t vcreate_u64m1x7_vl (vuint64m1_t op1, vuint64m1_t op2, vuint64m1_t op3, vuint64m1_t op4, vuint64m1_t op5, vuint64m1_t op6, vuint64m1_t op7, _VL_T vl);
vuint64m1x8_t vcreate_u64m1x8_vl (vuint64m1_t op1, vuint64m1_t op2, vuint64m1_t op3, vuint64m1_t op4, vuint64m1_t op5, vuint64m1_t op6, vuint64m1_t op7, vuint64m1_t op8, _VL_T vl);
vuint64m2x2_t vcreate_u64m2x2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2x3_t vcreate_u64m2x3_vl (vuint64m2_t op1, vuint64m2_t op2, vuint64m2_t op3, _VL_T vl);
vuint64m2x4_t vcreate_u64m2x4_vl (vuint64m2_t op1, vuint64m2_t op2, vuint64m2_t op3, vuint64m2_t op4, _VL_T vl);
vuint64m4x2_t vcreate_u64m4x2_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vfloat16m1x2_t vcreate_f16m1x2_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1x3_t vcreate_f16m1x3_vl (vfloat16m1_t op1, vfloat16m1_t op2, vfloat16m1_t op3, _VL_T vl);
vfloat16m1x4_t vcreate_f16m1x4_vl (vfloat16m1_t op1, vfloat16m1_t op2, vfloat16m1_t op3, vfloat16m1_t op4, _VL_T vl);
vfloat16m1x5_t vcreate_f16m1x5_vl (vfloat16m1_t op1, vfloat16m1_t op2, vfloat16m1_t op3, vfloat16m1_t op4, vfloat16m1_t op5, _VL_T vl);
vfloat16m1x6_t vcreate_f16m1x6_vl (vfloat16m1_t op1, vfloat16m1_t op2, vfloat16m1_t op3, vfloat16m1_t op4, vfloat16m1_t op5, vfloat16m1_t op6, _VL_T vl);
vfloat16m1x7_t vcreate_f16m1x7_vl (vfloat16m1_t op1, vfloat16m1_t op2, vfloat16m1_t op3, vfloat16m1_t op4, vfloat16m1_t op5, vfloat16m1_t op6, vfloat16m1_t op7, _VL_T vl);
vfloat16m1x8_t vcreate_f16m1x8_vl (vfloat16m1_t op1, vfloat16m1_t op2, vfloat16m1_t op3, vfloat16m1_t op4, vfloat16m1_t op5, vfloat16m1_t op6, vfloat16m1_t op7, vfloat16m1_t op8, _VL_T vl);
vfloat16m2x2_t vcreate_f16m2x2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2x3_t vcreate_f16m2x3_vl (vfloat16m2_t op1, vfloat16m2_t op2, vfloat16m2_t op3, _VL_T vl);
vfloat16m2x4_t vcreate_f16m2x4_vl (vfloat16m2_t op1, vfloat16m2_t op2, vfloat16m2_t op3, vfloat16m2_t op4, _VL_T vl);
vfloat16m4x2_t vcreate_f16m4x2_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m1x2_t vcreate_f32m1x2_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1x3_t vcreate_f32m1x3_vl (vfloat32m1_t op1, vfloat32m1_t op2, vfloat32m1_t op3, _VL_T vl);
vfloat32m1x4_t vcreate_f32m1x4_vl (vfloat32m1_t op1, vfloat32m1_t op2, vfloat32m1_t op3, vfloat32m1_t op4, _VL_T vl);
vfloat32m1x5_t vcreate_f32m1x5_vl (vfloat32m1_t op1, vfloat32m1_t op2, vfloat32m1_t op3, vfloat32m1_t op4, vfloat32m1_t op5, _VL_T vl);
vfloat32m1x6_t vcreate_f32m1x6_vl (vfloat32m1_t op1, vfloat32m1_t op2, vfloat32m1_t op3, vfloat32m1_t op4, vfloat32m1_t op5, vfloat32m1_t op6, _VL_T vl);
vfloat32m1x7_t vcreate_f32m1x7_vl (vfloat32m1_t op1, vfloat32m1_t op2, vfloat32m1_t op3, vfloat32m1_t op4, vfloat32m1_t op5, vfloat32m1_t op6, vfloat32m1_t op7, _VL_T vl);
vfloat32m1x8_t vcreate_f32m1x8_vl (vfloat32m1_t op1, vfloat32m1_t op2, vfloat32m1_t op3, vfloat32m1_t op4, vfloat32m1_t op5, vfloat32m1_t op6, vfloat32m1_t op7, vfloat32m1_t op8, _VL_T vl);
vfloat32m2x2_t vcreate_f32m2x2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2x3_t vcreate_f32m2x3_vl (vfloat32m2_t op1, vfloat32m2_t op2, vfloat32m2_t op3, _VL_T vl);
vfloat32m2x4_t vcreate_f32m2x4_vl (vfloat32m2_t op1, vfloat32m2_t op2, vfloat32m2_t op3, vfloat32m2_t op4, _VL_T vl);
vfloat32m4x2_t vcreate_f32m4x2_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m1x2_t vcreate_f64m1x2_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1x3_t vcreate_f64m1x3_vl (vfloat64m1_t op1, vfloat64m1_t op2, vfloat64m1_t op3, _VL_T vl);
vfloat64m1x4_t vcreate_f64m1x4_vl (vfloat64m1_t op1, vfloat64m1_t op2, vfloat64m1_t op3, vfloat64m1_t op4, _VL_T vl);
vfloat64m1x5_t vcreate_f64m1x5_vl (vfloat64m1_t op1, vfloat64m1_t op2, vfloat64m1_t op3, vfloat64m1_t op4, vfloat64m1_t op5, _VL_T vl);
vfloat64m1x6_t vcreate_f64m1x6_vl (vfloat64m1_t op1, vfloat64m1_t op2, vfloat64m1_t op3, vfloat64m1_t op4, vfloat64m1_t op5, vfloat64m1_t op6, _VL_T vl);
vfloat64m1x7_t vcreate_f64m1x7_vl (vfloat64m1_t op1, vfloat64m1_t op2, vfloat64m1_t op3, vfloat64m1_t op4, vfloat64m1_t op5, vfloat64m1_t op6, vfloat64m1_t op7, _VL_T vl);
vfloat64m1x8_t vcreate_f64m1x8_vl (vfloat64m1_t op1, vfloat64m1_t op2, vfloat64m1_t op3, vfloat64m1_t op4, vfloat64m1_t op5, vfloat64m1_t op6, vfloat64m1_t op7, vfloat64m1_t op8, _VL_T vl);
vfloat64m2x2_t vcreate_f64m2x2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2x3_t vcreate_f64m2x3_vl (vfloat64m2_t op1, vfloat64m2_t op2, vfloat64m2_t op3, _VL_T vl);
vfloat64m2x4_t vcreate_f64m2x4_vl (vfloat64m2_t op1, vfloat64m2_t op2, vfloat64m2_t op3, vfloat64m2_t op4, _VL_T vl);
vfloat64m4x2_t vcreate_f64m4x2_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
```
### [Vector Tuple Insertion Functions]():

**Prototypes:**
``` C
vint8m1x2_t vset_i8m1x2_vl (vint8m1x2_t tuple, size_t index, vint8m1_t val, _VL_T vl);
vint8m1x3_t vset_i8m1x3_vl (vint8m1x3_t tuple, size_t index, vint8m1_t val, _VL_T vl);
vint8m1x4_t vset_i8m1x4_vl (vint8m1x4_t tuple, size_t index, vint8m1_t val, _VL_T vl);
vint8m1x5_t vset_i8m1x5_vl (vint8m1x5_t tuple, size_t index, vint8m1_t val, _VL_T vl);
vint8m1x6_t vset_i8m1x6_vl (vint8m1x6_t tuple, size_t index, vint8m1_t val, _VL_T vl);
vint8m1x7_t vset_i8m1x7_vl (vint8m1x7_t tuple, size_t index, vint8m1_t val, _VL_T vl);
vint8m1x8_t vset_i8m1x8_vl (vint8m1x8_t tuple, size_t index, vint8m1_t val, _VL_T vl);
vint8m2x2_t vset_i8m2x2_vl (vint8m2x2_t tuple, size_t index, vint8m2_t val, _VL_T vl);
vint8m2x3_t vset_i8m2x3_vl (vint8m2x3_t tuple, size_t index, vint8m2_t val, _VL_T vl);
vint8m2x4_t vset_i8m2x4_vl (vint8m2x4_t tuple, size_t index, vint8m2_t val, _VL_T vl);
vint8m4x2_t vset_i8m4x2_vl (vint8m4x2_t tuple, size_t index, vint8m4_t val, _VL_T vl);
vint16m1x2_t vset_i16m1x2_vl (vint16m1x2_t tuple, size_t index, vint16m1_t val, _VL_T vl);
vint16m1x3_t vset_i16m1x3_vl (vint16m1x3_t tuple, size_t index, vint16m1_t val, _VL_T vl);
vint16m1x4_t vset_i16m1x4_vl (vint16m1x4_t tuple, size_t index, vint16m1_t val, _VL_T vl);
vint16m1x5_t vset_i16m1x5_vl (vint16m1x5_t tuple, size_t index, vint16m1_t val, _VL_T vl);
vint16m1x6_t vset_i16m1x6_vl (vint16m1x6_t tuple, size_t index, vint16m1_t val, _VL_T vl);
vint16m1x7_t vset_i16m1x7_vl (vint16m1x7_t tuple, size_t index, vint16m1_t val, _VL_T vl);
vint16m1x8_t vset_i16m1x8_vl (vint16m1x8_t tuple, size_t index, vint16m1_t val, _VL_T vl);
vint16m2x2_t vset_i16m2x2_vl (vint16m2x2_t tuple, size_t index, vint16m2_t val, _VL_T vl);
vint16m2x3_t vset_i16m2x3_vl (vint16m2x3_t tuple, size_t index, vint16m2_t val, _VL_T vl);
vint16m2x4_t vset_i16m2x4_vl (vint16m2x4_t tuple, size_t index, vint16m2_t val, _VL_T vl);
vint16m4x2_t vset_i16m4x2_vl (vint16m4x2_t tuple, size_t index, vint16m4_t val, _VL_T vl);
vint32m1x2_t vset_i32m1x2_vl (vint32m1x2_t tuple, size_t index, vint32m1_t val, _VL_T vl);
vint32m1x3_t vset_i32m1x3_vl (vint32m1x3_t tuple, size_t index, vint32m1_t val, _VL_T vl);
vint32m1x4_t vset_i32m1x4_vl (vint32m1x4_t tuple, size_t index, vint32m1_t val, _VL_T vl);
vint32m1x5_t vset_i32m1x5_vl (vint32m1x5_t tuple, size_t index, vint32m1_t val, _VL_T vl);
vint32m1x6_t vset_i32m1x6_vl (vint32m1x6_t tuple, size_t index, vint32m1_t val, _VL_T vl);
vint32m1x7_t vset_i32m1x7_vl (vint32m1x7_t tuple, size_t index, vint32m1_t val, _VL_T vl);
vint32m1x8_t vset_i32m1x8_vl (vint32m1x8_t tuple, size_t index, vint32m1_t val, _VL_T vl);
vint32m2x2_t vset_i32m2x2_vl (vint32m2x2_t tuple, size_t index, vint32m2_t val, _VL_T vl);
vint32m2x3_t vset_i32m2x3_vl (vint32m2x3_t tuple, size_t index, vint32m2_t val, _VL_T vl);
vint32m2x4_t vset_i32m2x4_vl (vint32m2x4_t tuple, size_t index, vint32m2_t val, _VL_T vl);
vint32m4x2_t vset_i32m4x2_vl (vint32m4x2_t tuple, size_t index, vint32m4_t val, _VL_T vl);
vint64m1x2_t vset_i64m1x2_vl (vint64m1x2_t tuple, size_t index, vint64m1_t val, _VL_T vl);
vint64m1x3_t vset_i64m1x3_vl (vint64m1x3_t tuple, size_t index, vint64m1_t val, _VL_T vl);
vint64m1x4_t vset_i64m1x4_vl (vint64m1x4_t tuple, size_t index, vint64m1_t val, _VL_T vl);
vint64m1x5_t vset_i64m1x5_vl (vint64m1x5_t tuple, size_t index, vint64m1_t val, _VL_T vl);
vint64m1x6_t vset_i64m1x6_vl (vint64m1x6_t tuple, size_t index, vint64m1_t val, _VL_T vl);
vint64m1x7_t vset_i64m1x7_vl (vint64m1x7_t tuple, size_t index, vint64m1_t val, _VL_T vl);
vint64m1x8_t vset_i64m1x8_vl (vint64m1x8_t tuple, size_t index, vint64m1_t val, _VL_T vl);
vint64m2x2_t vset_i64m2x2_vl (vint64m2x2_t tuple, size_t index, vint64m2_t val, _VL_T vl);
vint64m2x3_t vset_i64m2x3_vl (vint64m2x3_t tuple, size_t index, vint64m2_t val, _VL_T vl);
vint64m2x4_t vset_i64m2x4_vl (vint64m2x4_t tuple, size_t index, vint64m2_t val, _VL_T vl);
vint64m4x2_t vset_i64m4x2_vl (vint64m4x2_t tuple, size_t index, vint64m4_t val, _VL_T vl);
vuint8m1x2_t vset_u8m1x2_vl (vuint8m1x2_t tuple, size_t index, vuint8m1_t val, _VL_T vl);
vuint8m1x3_t vset_u8m1x3_vl (vuint8m1x3_t tuple, size_t index, vuint8m1_t val, _VL_T vl);
vuint8m1x4_t vset_u8m1x4_vl (vuint8m1x4_t tuple, size_t index, vuint8m1_t val, _VL_T vl);
vuint8m1x5_t vset_u8m1x5_vl (vuint8m1x5_t tuple, size_t index, vuint8m1_t val, _VL_T vl);
vuint8m1x6_t vset_u8m1x6_vl (vuint8m1x6_t tuple, size_t index, vuint8m1_t val, _VL_T vl);
vuint8m1x7_t vset_u8m1x7_vl (vuint8m1x7_t tuple, size_t index, vuint8m1_t val, _VL_T vl);
vuint8m1x8_t vset_u8m1x8_vl (vuint8m1x8_t tuple, size_t index, vuint8m1_t val, _VL_T vl);
vuint8m2x2_t vset_u8m2x2_vl (vuint8m2x2_t tuple, size_t index, vuint8m2_t val, _VL_T vl);
vuint8m2x3_t vset_u8m2x3_vl (vuint8m2x3_t tuple, size_t index, vuint8m2_t val, _VL_T vl);
vuint8m2x4_t vset_u8m2x4_vl (vuint8m2x4_t tuple, size_t index, vuint8m2_t val, _VL_T vl);
vuint8m4x2_t vset_u8m4x2_vl (vuint8m4x2_t tuple, size_t index, vuint8m4_t val, _VL_T vl);
vuint16m1x2_t vset_u16m1x2_vl (vuint16m1x2_t tuple, size_t index, vuint16m1_t val, _VL_T vl);
vuint16m1x3_t vset_u16m1x3_vl (vuint16m1x3_t tuple, size_t index, vuint16m1_t val, _VL_T vl);
vuint16m1x4_t vset_u16m1x4_vl (vuint16m1x4_t tuple, size_t index, vuint16m1_t val, _VL_T vl);
vuint16m1x5_t vset_u16m1x5_vl (vuint16m1x5_t tuple, size_t index, vuint16m1_t val, _VL_T vl);
vuint16m1x6_t vset_u16m1x6_vl (vuint16m1x6_t tuple, size_t index, vuint16m1_t val, _VL_T vl);
vuint16m1x7_t vset_u16m1x7_vl (vuint16m1x7_t tuple, size_t index, vuint16m1_t val, _VL_T vl);
vuint16m1x8_t vset_u16m1x8_vl (vuint16m1x8_t tuple, size_t index, vuint16m1_t val, _VL_T vl);
vuint16m2x2_t vset_u16m2x2_vl (vuint16m2x2_t tuple, size_t index, vuint16m2_t val, _VL_T vl);
vuint16m2x3_t vset_u16m2x3_vl (vuint16m2x3_t tuple, size_t index, vuint16m2_t val, _VL_T vl);
vuint16m2x4_t vset_u16m2x4_vl (vuint16m2x4_t tuple, size_t index, vuint16m2_t val, _VL_T vl);
vuint16m4x2_t vset_u16m4x2_vl (vuint16m4x2_t tuple, size_t index, vuint16m4_t val, _VL_T vl);
vuint32m1x2_t vset_u32m1x2_vl (vuint32m1x2_t tuple, size_t index, vuint32m1_t val, _VL_T vl);
vuint32m1x3_t vset_u32m1x3_vl (vuint32m1x3_t tuple, size_t index, vuint32m1_t val, _VL_T vl);
vuint32m1x4_t vset_u32m1x4_vl (vuint32m1x4_t tuple, size_t index, vuint32m1_t val, _VL_T vl);
vuint32m1x5_t vset_u32m1x5_vl (vuint32m1x5_t tuple, size_t index, vuint32m1_t val, _VL_T vl);
vuint32m1x6_t vset_u32m1x6_vl (vuint32m1x6_t tuple, size_t index, vuint32m1_t val, _VL_T vl);
vuint32m1x7_t vset_u32m1x7_vl (vuint32m1x7_t tuple, size_t index, vuint32m1_t val, _VL_T vl);
vuint32m1x8_t vset_u32m1x8_vl (vuint32m1x8_t tuple, size_t index, vuint32m1_t val, _VL_T vl);
vuint32m2x2_t vset_u32m2x2_vl (vuint32m2x2_t tuple, size_t index, vuint32m2_t val, _VL_T vl);
vuint32m2x3_t vset_u32m2x3_vl (vuint32m2x3_t tuple, size_t index, vuint32m2_t val, _VL_T vl);
vuint32m2x4_t vset_u32m2x4_vl (vuint32m2x4_t tuple, size_t index, vuint32m2_t val, _VL_T vl);
vuint32m4x2_t vset_u32m4x2_vl (vuint32m4x2_t tuple, size_t index, vuint32m4_t val, _VL_T vl);
vuint64m1x2_t vset_u64m1x2_vl (vuint64m1x2_t tuple, size_t index, vuint64m1_t val, _VL_T vl);
vuint64m1x3_t vset_u64m1x3_vl (vuint64m1x3_t tuple, size_t index, vuint64m1_t val, _VL_T vl);
vuint64m1x4_t vset_u64m1x4_vl (vuint64m1x4_t tuple, size_t index, vuint64m1_t val, _VL_T vl);
vuint64m1x5_t vset_u64m1x5_vl (vuint64m1x5_t tuple, size_t index, vuint64m1_t val, _VL_T vl);
vuint64m1x6_t vset_u64m1x6_vl (vuint64m1x6_t tuple, size_t index, vuint64m1_t val, _VL_T vl);
vuint64m1x7_t vset_u64m1x7_vl (vuint64m1x7_t tuple, size_t index, vuint64m1_t val, _VL_T vl);
vuint64m1x8_t vset_u64m1x8_vl (vuint64m1x8_t tuple, size_t index, vuint64m1_t val, _VL_T vl);
vuint64m2x2_t vset_u64m2x2_vl (vuint64m2x2_t tuple, size_t index, vuint64m2_t val, _VL_T vl);
vuint64m2x3_t vset_u64m2x3_vl (vuint64m2x3_t tuple, size_t index, vuint64m2_t val, _VL_T vl);
vuint64m2x4_t vset_u64m2x4_vl (vuint64m2x4_t tuple, size_t index, vuint64m2_t val, _VL_T vl);
vuint64m4x2_t vset_u64m4x2_vl (vuint64m4x2_t tuple, size_t index, vuint64m4_t val, _VL_T vl);
vfloat16m1x2_t vset_f16m1x2_vl (vfloat16m1x2_t tuple, size_t index, vfloat16m1_t val, _VL_T vl);
vfloat16m1x3_t vset_f16m1x3_vl (vfloat16m1x3_t tuple, size_t index, vfloat16m1_t val, _VL_T vl);
vfloat16m1x4_t vset_f16m1x4_vl (vfloat16m1x4_t tuple, size_t index, vfloat16m1_t val, _VL_T vl);
vfloat16m1x5_t vset_f16m1x5_vl (vfloat16m1x5_t tuple, size_t index, vfloat16m1_t val, _VL_T vl);
vfloat16m1x6_t vset_f16m1x6_vl (vfloat16m1x6_t tuple, size_t index, vfloat16m1_t val, _VL_T vl);
vfloat16m1x7_t vset_f16m1x7_vl (vfloat16m1x7_t tuple, size_t index, vfloat16m1_t val, _VL_T vl);
vfloat16m1x8_t vset_f16m1x8_vl (vfloat16m1x8_t tuple, size_t index, vfloat16m1_t val, _VL_T vl);
vfloat16m2x2_t vset_f16m2x2_vl (vfloat16m2x2_t tuple, size_t index, vfloat16m2_t val, _VL_T vl);
vfloat16m2x3_t vset_f16m2x3_vl (vfloat16m2x3_t tuple, size_t index, vfloat16m2_t val, _VL_T vl);
vfloat16m2x4_t vset_f16m2x4_vl (vfloat16m2x4_t tuple, size_t index, vfloat16m2_t val, _VL_T vl);
vfloat16m4x2_t vset_f16m4x2_vl (vfloat16m4x2_t tuple, size_t index, vfloat16m4_t val, _VL_T vl);
vfloat32m1x2_t vset_f32m1x2_vl (vfloat32m1x2_t tuple, size_t index, vfloat32m1_t val, _VL_T vl);
vfloat32m1x3_t vset_f32m1x3_vl (vfloat32m1x3_t tuple, size_t index, vfloat32m1_t val, _VL_T vl);
vfloat32m1x4_t vset_f32m1x4_vl (vfloat32m1x4_t tuple, size_t index, vfloat32m1_t val, _VL_T vl);
vfloat32m1x5_t vset_f32m1x5_vl (vfloat32m1x5_t tuple, size_t index, vfloat32m1_t val, _VL_T vl);
vfloat32m1x6_t vset_f32m1x6_vl (vfloat32m1x6_t tuple, size_t index, vfloat32m1_t val, _VL_T vl);
vfloat32m1x7_t vset_f32m1x7_vl (vfloat32m1x7_t tuple, size_t index, vfloat32m1_t val, _VL_T vl);
vfloat32m1x8_t vset_f32m1x8_vl (vfloat32m1x8_t tuple, size_t index, vfloat32m1_t val, _VL_T vl);
vfloat32m2x2_t vset_f32m2x2_vl (vfloat32m2x2_t tuple, size_t index, vfloat32m2_t val, _VL_T vl);
vfloat32m2x3_t vset_f32m2x3_vl (vfloat32m2x3_t tuple, size_t index, vfloat32m2_t val, _VL_T vl);
vfloat32m2x4_t vset_f32m2x4_vl (vfloat32m2x4_t tuple, size_t index, vfloat32m2_t val, _VL_T vl);
vfloat32m4x2_t vset_f32m4x2_vl (vfloat32m4x2_t tuple, size_t index, vfloat32m4_t val, _VL_T vl);
vfloat64m1x2_t vset_f64m1x2_vl (vfloat64m1x2_t tuple, size_t index, vfloat64m1_t val, _VL_T vl);
vfloat64m1x3_t vset_f64m1x3_vl (vfloat64m1x3_t tuple, size_t index, vfloat64m1_t val, _VL_T vl);
vfloat64m1x4_t vset_f64m1x4_vl (vfloat64m1x4_t tuple, size_t index, vfloat64m1_t val, _VL_T vl);
vfloat64m1x5_t vset_f64m1x5_vl (vfloat64m1x5_t tuple, size_t index, vfloat64m1_t val, _VL_T vl);
vfloat64m1x6_t vset_f64m1x6_vl (vfloat64m1x6_t tuple, size_t index, vfloat64m1_t val, _VL_T vl);
vfloat64m1x7_t vset_f64m1x7_vl (vfloat64m1x7_t tuple, size_t index, vfloat64m1_t val, _VL_T vl);
vfloat64m1x8_t vset_f64m1x8_vl (vfloat64m1x8_t tuple, size_t index, vfloat64m1_t val, _VL_T vl);
vfloat64m2x2_t vset_f64m2x2_vl (vfloat64m2x2_t tuple, size_t index, vfloat64m2_t val, _VL_T vl);
vfloat64m2x3_t vset_f64m2x3_vl (vfloat64m2x3_t tuple, size_t index, vfloat64m2_t val, _VL_T vl);
vfloat64m2x4_t vset_f64m2x4_vl (vfloat64m2x4_t tuple, size_t index, vfloat64m2_t val, _VL_T vl);
vfloat64m4x2_t vset_f64m4x2_vl (vfloat64m4x2_t tuple, size_t index, vfloat64m4_t val, _VL_T vl);
```
### [Vector Tuple Extraction Functions]():

**Prototypes:**
``` C
vint8m1_t vget_i8m1x2_vl (vint8m1x2_t tuple, size_t index, _VL_T vl);
vint8m1_t vget_i8m1x3_vl (vint8m1x3_t tuple, size_t index, _VL_T vl);
vint8m1_t vget_i8m1x4_vl (vint8m1x4_t tuple, size_t index, _VL_T vl);
vint8m1_t vget_i8m1x5_vl (vint8m1x5_t tuple, size_t index, _VL_T vl);
vint8m1_t vget_i8m1x6_vl (vint8m1x6_t tuple, size_t index, _VL_T vl);
vint8m1_t vget_i8m1x7_vl (vint8m1x7_t tuple, size_t index, _VL_T vl);
vint8m1_t vget_i8m1x8_vl (vint8m1x8_t tuple, size_t index, _VL_T vl);
vint8m2_t vget_i8m2x2_vl (vint8m2x2_t tuple, size_t index, _VL_T vl);
vint8m2_t vget_i8m2x3_vl (vint8m2x3_t tuple, size_t index, _VL_T vl);
vint8m2_t vget_i8m2x4_vl (vint8m2x4_t tuple, size_t index, _VL_T vl);
vint8m4_t vget_i8m4x2_vl (vint8m4x2_t tuple, size_t index, _VL_T vl);
vint16m1_t vget_i16m1x2_vl (vint16m1x2_t tuple, size_t index, _VL_T vl);
vint16m1_t vget_i16m1x3_vl (vint16m1x3_t tuple, size_t index, _VL_T vl);
vint16m1_t vget_i16m1x4_vl (vint16m1x4_t tuple, size_t index, _VL_T vl);
vint16m1_t vget_i16m1x5_vl (vint16m1x5_t tuple, size_t index, _VL_T vl);
vint16m1_t vget_i16m1x6_vl (vint16m1x6_t tuple, size_t index, _VL_T vl);
vint16m1_t vget_i16m1x7_vl (vint16m1x7_t tuple, size_t index, _VL_T vl);
vint16m1_t vget_i16m1x8_vl (vint16m1x8_t tuple, size_t index, _VL_T vl);
vint16m2_t vget_i16m2x2_vl (vint16m2x2_t tuple, size_t index, _VL_T vl);
vint16m2_t vget_i16m2x3_vl (vint16m2x3_t tuple, size_t index, _VL_T vl);
vint16m2_t vget_i16m2x4_vl (vint16m2x4_t tuple, size_t index, _VL_T vl);
vint16m4_t vget_i16m4x2_vl (vint16m4x2_t tuple, size_t index, _VL_T vl);
vint32m1_t vget_i32m1x2_vl (vint32m1x2_t tuple, size_t index, _VL_T vl);
vint32m1_t vget_i32m1x3_vl (vint32m1x3_t tuple, size_t index, _VL_T vl);
vint32m1_t vget_i32m1x4_vl (vint32m1x4_t tuple, size_t index, _VL_T vl);
vint32m1_t vget_i32m1x5_vl (vint32m1x5_t tuple, size_t index, _VL_T vl);
vint32m1_t vget_i32m1x6_vl (vint32m1x6_t tuple, size_t index, _VL_T vl);
vint32m1_t vget_i32m1x7_vl (vint32m1x7_t tuple, size_t index, _VL_T vl);
vint32m1_t vget_i32m1x8_vl (vint32m1x8_t tuple, size_t index, _VL_T vl);
vint32m2_t vget_i32m2x2_vl (vint32m2x2_t tuple, size_t index, _VL_T vl);
vint32m2_t vget_i32m2x3_vl (vint32m2x3_t tuple, size_t index, _VL_T vl);
vint32m2_t vget_i32m2x4_vl (vint32m2x4_t tuple, size_t index, _VL_T vl);
vint32m4_t vget_i32m4x2_vl (vint32m4x2_t tuple, size_t index, _VL_T vl);
vint64m1_t vget_i64m1x2_vl (vint64m1x2_t tuple, size_t index, _VL_T vl);
vint64m1_t vget_i64m1x3_vl (vint64m1x3_t tuple, size_t index, _VL_T vl);
vint64m1_t vget_i64m1x4_vl (vint64m1x4_t tuple, size_t index, _VL_T vl);
vint64m1_t vget_i64m1x5_vl (vint64m1x5_t tuple, size_t index, _VL_T vl);
vint64m1_t vget_i64m1x6_vl (vint64m1x6_t tuple, size_t index, _VL_T vl);
vint64m1_t vget_i64m1x7_vl (vint64m1x7_t tuple, size_t index, _VL_T vl);
vint64m1_t vget_i64m1x8_vl (vint64m1x8_t tuple, size_t index, _VL_T vl);
vint64m2_t vget_i64m2x2_vl (vint64m2x2_t tuple, size_t index, _VL_T vl);
vint64m2_t vget_i64m2x3_vl (vint64m2x3_t tuple, size_t index, _VL_T vl);
vint64m2_t vget_i64m2x4_vl (vint64m2x4_t tuple, size_t index, _VL_T vl);
vint64m4_t vget_i64m4x2_vl (vint64m4x2_t tuple, size_t index, _VL_T vl);
vuint8m1_t vget_u8m1x2_vl (vuint8m1x2_t tuple, size_t index, _VL_T vl);
vuint8m1_t vget_u8m1x3_vl (vuint8m1x3_t tuple, size_t index, _VL_T vl);
vuint8m1_t vget_u8m1x4_vl (vuint8m1x4_t tuple, size_t index, _VL_T vl);
vuint8m1_t vget_u8m1x5_vl (vuint8m1x5_t tuple, size_t index, _VL_T vl);
vuint8m1_t vget_u8m1x6_vl (vuint8m1x6_t tuple, size_t index, _VL_T vl);
vuint8m1_t vget_u8m1x7_vl (vuint8m1x7_t tuple, size_t index, _VL_T vl);
vuint8m1_t vget_u8m1x8_vl (vuint8m1x8_t tuple, size_t index, _VL_T vl);
vuint8m2_t vget_u8m2x2_vl (vuint8m2x2_t tuple, size_t index, _VL_T vl);
vuint8m2_t vget_u8m2x3_vl (vuint8m2x3_t tuple, size_t index, _VL_T vl);
vuint8m2_t vget_u8m2x4_vl (vuint8m2x4_t tuple, size_t index, _VL_T vl);
vuint8m4_t vget_u8m4x2_vl (vuint8m4x2_t tuple, size_t index, _VL_T vl);
vuint16m1_t vget_u16m1x2_vl (vuint16m1x2_t tuple, size_t index, _VL_T vl);
vuint16m1_t vget_u16m1x3_vl (vuint16m1x3_t tuple, size_t index, _VL_T vl);
vuint16m1_t vget_u16m1x4_vl (vuint16m1x4_t tuple, size_t index, _VL_T vl);
vuint16m1_t vget_u16m1x5_vl (vuint16m1x5_t tuple, size_t index, _VL_T vl);
vuint16m1_t vget_u16m1x6_vl (vuint16m1x6_t tuple, size_t index, _VL_T vl);
vuint16m1_t vget_u16m1x7_vl (vuint16m1x7_t tuple, size_t index, _VL_T vl);
vuint16m1_t vget_u16m1x8_vl (vuint16m1x8_t tuple, size_t index, _VL_T vl);
vuint16m2_t vget_u16m2x2_vl (vuint16m2x2_t tuple, size_t index, _VL_T vl);
vuint16m2_t vget_u16m2x3_vl (vuint16m2x3_t tuple, size_t index, _VL_T vl);
vuint16m2_t vget_u16m2x4_vl (vuint16m2x4_t tuple, size_t index, _VL_T vl);
vuint16m4_t vget_u16m4x2_vl (vuint16m4x2_t tuple, size_t index, _VL_T vl);
vuint32m1_t vget_u32m1x2_vl (vuint32m1x2_t tuple, size_t index, _VL_T vl);
vuint32m1_t vget_u32m1x3_vl (vuint32m1x3_t tuple, size_t index, _VL_T vl);
vuint32m1_t vget_u32m1x4_vl (vuint32m1x4_t tuple, size_t index, _VL_T vl);
vuint32m1_t vget_u32m1x5_vl (vuint32m1x5_t tuple, size_t index, _VL_T vl);
vuint32m1_t vget_u32m1x6_vl (vuint32m1x6_t tuple, size_t index, _VL_T vl);
vuint32m1_t vget_u32m1x7_vl (vuint32m1x7_t tuple, size_t index, _VL_T vl);
vuint32m1_t vget_u32m1x8_vl (vuint32m1x8_t tuple, size_t index, _VL_T vl);
vuint32m2_t vget_u32m2x2_vl (vuint32m2x2_t tuple, size_t index, _VL_T vl);
vuint32m2_t vget_u32m2x3_vl (vuint32m2x3_t tuple, size_t index, _VL_T vl);
vuint32m2_t vget_u32m2x4_vl (vuint32m2x4_t tuple, size_t index, _VL_T vl);
vuint32m4_t vget_u32m4x2_vl (vuint32m4x2_t tuple, size_t index, _VL_T vl);
vuint64m1_t vget_u64m1x2_vl (vuint64m1x2_t tuple, size_t index, _VL_T vl);
vuint64m1_t vget_u64m1x3_vl (vuint64m1x3_t tuple, size_t index, _VL_T vl);
vuint64m1_t vget_u64m1x4_vl (vuint64m1x4_t tuple, size_t index, _VL_T vl);
vuint64m1_t vget_u64m1x5_vl (vuint64m1x5_t tuple, size_t index, _VL_T vl);
vuint64m1_t vget_u64m1x6_vl (vuint64m1x6_t tuple, size_t index, _VL_T vl);
vuint64m1_t vget_u64m1x7_vl (vuint64m1x7_t tuple, size_t index, _VL_T vl);
vuint64m1_t vget_u64m1x8_vl (vuint64m1x8_t tuple, size_t index, _VL_T vl);
vuint64m2_t vget_u64m2x2_vl (vuint64m2x2_t tuple, size_t index, _VL_T vl);
vuint64m2_t vget_u64m2x3_vl (vuint64m2x3_t tuple, size_t index, _VL_T vl);
vuint64m2_t vget_u64m2x4_vl (vuint64m2x4_t tuple, size_t index, _VL_T vl);
vuint64m4_t vget_u64m4x2_vl (vuint64m4x2_t tuple, size_t index, _VL_T vl);
vfloat16m1_t vget_f16m1x2_vl (vfloat16m1x2_t tuple, size_t index, _VL_T vl);
vfloat16m1_t vget_f16m1x3_vl (vfloat16m1x3_t tuple, size_t index, _VL_T vl);
vfloat16m1_t vget_f16m1x4_vl (vfloat16m1x4_t tuple, size_t index, _VL_T vl);
vfloat16m1_t vget_f16m1x5_vl (vfloat16m1x5_t tuple, size_t index, _VL_T vl);
vfloat16m1_t vget_f16m1x6_vl (vfloat16m1x6_t tuple, size_t index, _VL_T vl);
vfloat16m1_t vget_f16m1x7_vl (vfloat16m1x7_t tuple, size_t index, _VL_T vl);
vfloat16m1_t vget_f16m1x8_vl (vfloat16m1x8_t tuple, size_t index, _VL_T vl);
vfloat16m2_t vget_f16m2x2_vl (vfloat16m2x2_t tuple, size_t index, _VL_T vl);
vfloat16m2_t vget_f16m2x3_vl (vfloat16m2x3_t tuple, size_t index, _VL_T vl);
vfloat16m2_t vget_f16m2x4_vl (vfloat16m2x4_t tuple, size_t index, _VL_T vl);
vfloat16m4_t vget_f16m4x2_vl (vfloat16m4x2_t tuple, size_t index, _VL_T vl);
vfloat32m1_t vget_f32m1x2_vl (vfloat32m1x2_t tuple, size_t index, _VL_T vl);
vfloat32m1_t vget_f32m1x3_vl (vfloat32m1x3_t tuple, size_t index, _VL_T vl);
vfloat32m1_t vget_f32m1x4_vl (vfloat32m1x4_t tuple, size_t index, _VL_T vl);
vfloat32m1_t vget_f32m1x5_vl (vfloat32m1x5_t tuple, size_t index, _VL_T vl);
vfloat32m1_t vget_f32m1x6_vl (vfloat32m1x6_t tuple, size_t index, _VL_T vl);
vfloat32m1_t vget_f32m1x7_vl (vfloat32m1x7_t tuple, size_t index, _VL_T vl);
vfloat32m1_t vget_f32m1x8_vl (vfloat32m1x8_t tuple, size_t index, _VL_T vl);
vfloat32m2_t vget_f32m2x2_vl (vfloat32m2x2_t tuple, size_t index, _VL_T vl);
vfloat32m2_t vget_f32m2x3_vl (vfloat32m2x3_t tuple, size_t index, _VL_T vl);
vfloat32m2_t vget_f32m2x4_vl (vfloat32m2x4_t tuple, size_t index, _VL_T vl);
vfloat32m4_t vget_f32m4x2_vl (vfloat32m4x2_t tuple, size_t index, _VL_T vl);
vfloat64m1_t vget_f64m1x2_vl (vfloat64m1x2_t tuple, size_t index, _VL_T vl);
vfloat64m1_t vget_f64m1x3_vl (vfloat64m1x3_t tuple, size_t index, _VL_T vl);
vfloat64m1_t vget_f64m1x4_vl (vfloat64m1x4_t tuple, size_t index, _VL_T vl);
vfloat64m1_t vget_f64m1x5_vl (vfloat64m1x5_t tuple, size_t index, _VL_T vl);
vfloat64m1_t vget_f64m1x6_vl (vfloat64m1x6_t tuple, size_t index, _VL_T vl);
vfloat64m1_t vget_f64m1x7_vl (vfloat64m1x7_t tuple, size_t index, _VL_T vl);
vfloat64m1_t vget_f64m1x8_vl (vfloat64m1x8_t tuple, size_t index, _VL_T vl);
vfloat64m2_t vget_f64m2x2_vl (vfloat64m2x2_t tuple, size_t index, _VL_T vl);
vfloat64m2_t vget_f64m2x3_vl (vfloat64m2x3_t tuple, size_t index, _VL_T vl);
vfloat64m2_t vget_f64m2x4_vl (vfloat64m2x4_t tuple, size_t index, _VL_T vl);
vfloat64m4_t vget_f64m4x2_vl (vfloat64m4x2_t tuple, size_t index, _VL_T vl);
```
## Vector AMO Operations Functions (Zvamo):

### [Vector AMO Operations Functions](rvv-intrinsic-api.md#8-vector-amo-operations-zvamo):

**Prototypes:**
``` C
vint8m1_t vamoswape_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoswape_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoswape_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoswape_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoswape_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoswape_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoswape_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoswape_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoswape_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoswapw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoswape_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoswapw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoswape_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoswapw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoswape_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoswapw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoswape_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoswapw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoswape_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoswapw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoswape_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoswapw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoswape_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoswapw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoswape_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoswape_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoswape_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoswape_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoswape_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoswape_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoswape_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoswape_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoswape_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoswapw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoswape_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoswapw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoswape_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoswapw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoswape_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoswapw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoswape_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoswapw_v_u64m1_vl (int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoswape_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoswapw_v_u64m2_vl (int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoswape_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoswapw_v_u64m4_vl (int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoswape_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoswapw_v_u64m8_vl (int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vfloat16m1_t vamoswape_v_f16m1_vl (float16_t *base, vuint16m1_t bindex, vfloat16m1_t value, _VL_T vl);
vfloat16m2_t vamoswape_v_f16m2_vl (float16_t *base, vuint16m2_t bindex, vfloat16m2_t value, _VL_T vl);
vfloat16m4_t vamoswape_v_f16m4_vl (float16_t *base, vuint16m4_t bindex, vfloat16m4_t value, _VL_T vl);
vfloat16m8_t vamoswape_v_f16m8_vl (float16_t *base, vuint16m8_t bindex, vfloat16m8_t value, _VL_T vl);
vfloat32m1_t vamoswape_v_f32m1_vl (float32_t *base, vuint32m1_t bindex, vfloat32m1_t value, _VL_T vl);
vfloat32m2_t vamoswape_v_f32m2_vl (float32_t *base, vuint32m2_t bindex, vfloat32m2_t value, _VL_T vl);
vfloat32m4_t vamoswape_v_f32m4_vl (float32_t *base, vuint32m4_t bindex, vfloat32m4_t value, _VL_T vl);
vfloat32m8_t vamoswape_v_f32m8_vl (float32_t *base, vuint32m8_t bindex, vfloat32m8_t value, _VL_T vl);
vfloat64m1_t vamoswape_v_f64m1_vl (float64_t *base, vuint64m1_t bindex, vfloat64m1_t value, _VL_T vl);
vfloat64m2_t vamoswape_v_f64m2_vl (float64_t *base, vuint64m2_t bindex, vfloat64m2_t value, _VL_T vl);
vfloat64m4_t vamoswape_v_f64m4_vl (float64_t *base, vuint64m4_t bindex, vfloat64m4_t value, _VL_T vl);
vfloat64m8_t vamoswape_v_f64m8_vl (float64_t *base, vuint64m8_t bindex, vfloat64m8_t value, _VL_T vl);
vint8m1_t vamoadde_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoadde_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoadde_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoadde_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoadde_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoadde_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoadde_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoadde_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoadde_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoaddw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoadde_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoaddw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoadde_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoaddw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoadde_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoaddw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoadde_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoaddw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoadde_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoaddw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoadde_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoaddw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoadde_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoaddw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoadde_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoadde_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoadde_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoadde_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoadde_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoadde_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoadde_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoadde_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoadde_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoaddw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoadde_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoaddw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoadde_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoaddw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoadde_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoaddw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoadde_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoaddw_v_u64m1_vl (int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoadde_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoaddw_v_u64m2_vl (int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoadde_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoaddw_v_u64m4_vl (int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoadde_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoaddw_v_u64m8_vl (int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamoxore_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoxore_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoxore_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoxore_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoxore_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoxore_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoxore_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoxore_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoxore_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoxorw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoxore_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoxorw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoxore_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoxorw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoxore_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoxorw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoxore_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoxorw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoxore_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoxorw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoxore_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoxorw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoxore_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoxorw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoxore_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoxore_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoxore_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoxore_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoxore_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoxore_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoxore_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoxore_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoxore_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoxorw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoxore_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoxorw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoxore_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoxorw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoxore_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoxorw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoxore_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoxorw_v_u64m1_vl (int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoxore_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoxorw_v_u64m2_vl (int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoxore_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoxorw_v_u64m4_vl (int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoxore_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoxorw_v_u64m8_vl (int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamoande_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoande_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoande_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoande_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoande_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoande_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoande_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoande_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoande_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoandw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoande_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoandw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoande_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoandw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoande_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoandw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoande_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoandw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoande_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoandw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoande_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoandw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoande_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoandw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoande_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoande_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoande_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoande_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoande_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoande_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoande_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoande_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoande_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoandw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoande_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoandw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoande_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoandw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoande_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoandw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoande_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoandw_v_u64m1_vl (int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoande_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoandw_v_u64m2_vl (int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoande_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoandw_v_u64m4_vl (int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoande_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoandw_v_u64m8_vl (int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamoore_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoore_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoore_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoore_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoore_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoore_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoore_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoore_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoore_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoorw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoore_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoorw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoore_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoorw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoore_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoorw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoore_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoorw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoore_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoorw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoore_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoorw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoore_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoorw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoore_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoore_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoore_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoore_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoore_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoore_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoore_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoore_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoore_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoorw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoore_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoorw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoore_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoorw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoore_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoorw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoore_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoorw_v_u64m1_vl (int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoore_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoorw_v_u64m2_vl (int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoore_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoorw_v_u64m4_vl (int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoore_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoorw_v_u64m8_vl (int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamomine_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamomine_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamomine_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamomine_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamomine_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamomine_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamomine_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamomine_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamomine_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamominw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamomine_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamominw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamomine_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamominw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamomine_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamominw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamomine_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamominw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamomine_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamominw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamomine_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamominw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamomine_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamominw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamominue_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamominue_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamominue_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamominue_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamominue_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamominue_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamominue_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamominue_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamominue_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamominuw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamominue_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamominuw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamominue_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamominuw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamominue_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamominuw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamominue_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamominuw_v_u64m1_vl (int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamominue_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamominuw_v_u64m2_vl (int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamominue_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamominuw_v_u64m4_vl (int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamominue_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamominuw_v_u64m8_vl (int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamomaxe_v_i8m1_vl (int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamomaxe_v_i8m2_vl (int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamomaxe_v_i8m4_vl (int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamomaxe_v_i8m8_vl (int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamomaxe_v_i16m1_vl (int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamomaxe_v_i16m2_vl (int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamomaxe_v_i16m4_vl (int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamomaxe_v_i16m8_vl (int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamomaxe_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamomaxw_v_i32m1_vl (int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamomaxe_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamomaxw_v_i32m2_vl (int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamomaxe_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamomaxw_v_i32m4_vl (int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamomaxe_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamomaxw_v_i32m8_vl (int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamomaxe_v_i64m1_vl (int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamomaxw_v_i64m1_vl (int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamomaxe_v_i64m2_vl (int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamomaxw_v_i64m2_vl (int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamomaxe_v_i64m4_vl (int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamomaxw_v_i64m4_vl (int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamomaxe_v_i64m8_vl (int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamomaxw_v_i64m8_vl (int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamomaxue_v_u8m1_vl (uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamomaxue_v_u8m2_vl (uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamomaxue_v_u8m4_vl (uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamomaxue_v_u8m8_vl (uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamomaxue_v_u16m1_vl (uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamomaxue_v_u16m2_vl (uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamomaxue_v_u16m4_vl (uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamomaxue_v_u16m8_vl (uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamomaxue_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamomaxuw_v_u32m1_vl (uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamomaxue_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamomaxuw_v_u32m2_vl (uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamomaxue_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamomaxuw_v_u32m4_vl (uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamomaxue_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamomaxuw_v_u32m8_vl (uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamomaxue_v_u64m1_vl (uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamomaxuw_v_u64m1_vl (int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamomaxue_v_u64m2_vl (uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamomaxuw_v_u64m2_vl (int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamomaxue_v_u64m4_vl (uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamomaxuw_v_u64m4_vl (int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamomaxue_v_u64m8_vl (uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamomaxuw_v_u64m8_vl (int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
// masked functions
vint8m1_t vamoswape_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoswape_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoswape_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoswape_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoswape_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoswape_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoswape_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoswape_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoswape_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoswapw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoswape_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoswapw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoswape_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoswapw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoswape_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoswapw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoswape_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoswapw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoswape_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoswapw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoswape_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoswapw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoswape_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoswapw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoswape_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoswape_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoswape_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoswape_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoswape_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoswape_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoswape_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoswape_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoswape_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoswapw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoswape_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoswapw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoswape_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoswapw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoswape_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoswapw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoswape_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoswapw_v_u64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoswape_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoswapw_v_u64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoswape_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoswapw_v_u64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoswape_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoswapw_v_u64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vfloat16m1_t vamoswape_v_f16m1_m_vl (vbool16_t mask, float16_t *base, vuint16m1_t bindex, vfloat16m1_t value, _VL_T vl);
vfloat16m2_t vamoswape_v_f16m2_m_vl (vbool8_t mask, float16_t *base, vuint16m2_t bindex, vfloat16m2_t value, _VL_T vl);
vfloat16m4_t vamoswape_v_f16m4_m_vl (vbool4_t mask, float16_t *base, vuint16m4_t bindex, vfloat16m4_t value, _VL_T vl);
vfloat16m8_t vamoswape_v_f16m8_m_vl (vbool2_t mask, float16_t *base, vuint16m8_t bindex, vfloat16m8_t value, _VL_T vl);
vfloat32m1_t vamoswape_v_f32m1_m_vl (vbool32_t mask, float32_t *base, vuint32m1_t bindex, vfloat32m1_t value, _VL_T vl);
vfloat32m2_t vamoswape_v_f32m2_m_vl (vbool16_t mask, float32_t *base, vuint32m2_t bindex, vfloat32m2_t value, _VL_T vl);
vfloat32m4_t vamoswape_v_f32m4_m_vl (vbool8_t mask, float32_t *base, vuint32m4_t bindex, vfloat32m4_t value, _VL_T vl);
vfloat32m8_t vamoswape_v_f32m8_m_vl (vbool4_t mask, float32_t *base, vuint32m8_t bindex, vfloat32m8_t value, _VL_T vl);
vfloat64m1_t vamoswape_v_f64m1_m_vl (vbool64_t mask, float64_t *base, vuint64m1_t bindex, vfloat64m1_t value, _VL_T vl);
vfloat64m2_t vamoswape_v_f64m2_m_vl (vbool32_t mask, float64_t *base, vuint64m2_t bindex, vfloat64m2_t value, _VL_T vl);
vfloat64m4_t vamoswape_v_f64m4_m_vl (vbool16_t mask, float64_t *base, vuint64m4_t bindex, vfloat64m4_t value, _VL_T vl);
vfloat64m8_t vamoswape_v_f64m8_m_vl (vbool8_t mask, float64_t *base, vuint64m8_t bindex, vfloat64m8_t value, _VL_T vl);
vint8m1_t vamoadde_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoadde_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoadde_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoadde_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoadde_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoadde_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoadde_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoadde_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoadde_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoaddw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoadde_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoaddw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoadde_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoaddw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoadde_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoaddw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoadde_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoaddw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoadde_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoaddw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoadde_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoaddw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoadde_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoaddw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoadde_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoadde_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoadde_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoadde_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoadde_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoadde_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoadde_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoadde_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoadde_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoaddw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoadde_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoaddw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoadde_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoaddw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoadde_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoaddw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoadde_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoaddw_v_u64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoadde_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoaddw_v_u64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoadde_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoaddw_v_u64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoadde_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoaddw_v_u64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamoxore_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoxore_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoxore_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoxore_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoxore_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoxore_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoxore_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoxore_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoxore_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoxorw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoxore_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoxorw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoxore_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoxorw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoxore_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoxorw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoxore_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoxorw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoxore_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoxorw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoxore_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoxorw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoxore_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoxorw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoxore_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoxore_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoxore_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoxore_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoxore_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoxore_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoxore_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoxore_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoxore_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoxorw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoxore_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoxorw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoxore_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoxorw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoxore_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoxorw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoxore_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoxorw_v_u64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoxore_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoxorw_v_u64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoxore_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoxorw_v_u64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoxore_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoxorw_v_u64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamoande_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoande_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoande_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoande_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoande_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoande_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoande_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoande_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoande_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoandw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoande_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoandw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoande_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoandw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoande_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoandw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoande_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoandw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoande_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoandw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoande_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoandw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoande_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoandw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoande_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoande_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoande_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoande_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoande_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoande_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoande_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoande_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoande_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoandw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoande_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoandw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoande_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoandw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoande_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoandw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoande_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoandw_v_u64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoande_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoandw_v_u64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoande_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoandw_v_u64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoande_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoandw_v_u64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamoore_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamoore_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamoore_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamoore_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamoore_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamoore_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamoore_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamoore_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamoore_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamoorw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamoore_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamoorw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamoore_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamoorw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamoore_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamoorw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamoore_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamoorw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamoore_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamoorw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamoore_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamoorw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamoore_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamoorw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamoore_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamoore_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamoore_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamoore_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamoore_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamoore_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamoore_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamoore_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamoore_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamoorw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamoore_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamoorw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamoore_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamoorw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamoore_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamoorw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamoore_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamoorw_v_u64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamoore_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamoorw_v_u64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamoore_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamoorw_v_u64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamoore_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamoorw_v_u64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamomine_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamomine_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamomine_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamomine_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamomine_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamomine_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamomine_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamomine_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamomine_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamominw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamomine_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamominw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamomine_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamominw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamomine_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamominw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamomine_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamominw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamomine_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamominw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamomine_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamominw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamomine_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamominw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamominue_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamominue_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamominue_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamominue_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamominue_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamominue_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamominue_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamominue_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamominue_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamominuw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamominue_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamominuw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamominue_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamominuw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamominue_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamominuw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamominue_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamominuw_v_u64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamominue_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamominuw_v_u64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamominue_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamominuw_v_u64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamominue_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamominuw_v_u64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vint8m1_t vamomaxe_v_i8m1_m_vl (vbool8_t mask, int8_t *base, vuint8m1_t bindex, vint8m1_t value, _VL_T vl);
vint8m2_t vamomaxe_v_i8m2_m_vl (vbool4_t mask, int8_t *base, vuint8m2_t bindex, vint8m2_t value, _VL_T vl);
vint8m4_t vamomaxe_v_i8m4_m_vl (vbool2_t mask, int8_t *base, vuint8m4_t bindex, vint8m4_t value, _VL_T vl);
vint8m8_t vamomaxe_v_i8m8_m_vl (vbool1_t mask, int8_t *base, vuint8m8_t bindex, vint8m8_t value, _VL_T vl);
vint16m1_t vamomaxe_v_i16m1_m_vl (vbool16_t mask, int16_t *base, vuint16m1_t bindex, vint16m1_t value, _VL_T vl);
vint16m2_t vamomaxe_v_i16m2_m_vl (vbool8_t mask, int16_t *base, vuint16m2_t bindex, vint16m2_t value, _VL_T vl);
vint16m4_t vamomaxe_v_i16m4_m_vl (vbool4_t mask, int16_t *base, vuint16m4_t bindex, vint16m4_t value, _VL_T vl);
vint16m8_t vamomaxe_v_i16m8_m_vl (vbool2_t mask, int16_t *base, vuint16m8_t bindex, vint16m8_t value, _VL_T vl);
vint32m1_t vamomaxe_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m1_t vamomaxw_v_i32m1_m_vl (vbool32_t mask, int32_t *base, vuint32m1_t bindex, vint32m1_t value, _VL_T vl);
vint32m2_t vamomaxe_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m2_t vamomaxw_v_i32m2_m_vl (vbool16_t mask, int32_t *base, vuint32m2_t bindex, vint32m2_t value, _VL_T vl);
vint32m4_t vamomaxe_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m4_t vamomaxw_v_i32m4_m_vl (vbool8_t mask, int32_t *base, vuint32m4_t bindex, vint32m4_t value, _VL_T vl);
vint32m8_t vamomaxe_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint32m8_t vamomaxw_v_i32m8_m_vl (vbool4_t mask, int32_t *base, vuint32m8_t bindex, vint32m8_t value, _VL_T vl);
vint64m1_t vamomaxe_v_i64m1_m_vl (vbool64_t mask, int64_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m1_t vamomaxw_v_i64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vint64m1_t value, _VL_T vl);
vint64m2_t vamomaxe_v_i64m2_m_vl (vbool32_t mask, int64_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m2_t vamomaxw_v_i64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vint64m2_t value, _VL_T vl);
vint64m4_t vamomaxe_v_i64m4_m_vl (vbool16_t mask, int64_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m4_t vamomaxw_v_i64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vint64m4_t value, _VL_T vl);
vint64m8_t vamomaxe_v_i64m8_m_vl (vbool8_t mask, int64_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vint64m8_t vamomaxw_v_i64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vint64m8_t value, _VL_T vl);
vuint8m1_t vamomaxue_v_u8m1_m_vl (vbool8_t mask, uint8_t *base, vuint8m1_t bindex, vuint8m1_t value, _VL_T vl);
vuint8m2_t vamomaxue_v_u8m2_m_vl (vbool4_t mask, uint8_t *base, vuint8m2_t bindex, vuint8m2_t value, _VL_T vl);
vuint8m4_t vamomaxue_v_u8m4_m_vl (vbool2_t mask, uint8_t *base, vuint8m4_t bindex, vuint8m4_t value, _VL_T vl);
vuint8m8_t vamomaxue_v_u8m8_m_vl (vbool1_t mask, uint8_t *base, vuint8m8_t bindex, vuint8m8_t value, _VL_T vl);
vuint16m1_t vamomaxue_v_u16m1_m_vl (vbool16_t mask, uint16_t *base, vuint16m1_t bindex, vuint16m1_t value, _VL_T vl);
vuint16m2_t vamomaxue_v_u16m2_m_vl (vbool8_t mask, uint16_t *base, vuint16m2_t bindex, vuint16m2_t value, _VL_T vl);
vuint16m4_t vamomaxue_v_u16m4_m_vl (vbool4_t mask, uint16_t *base, vuint16m4_t bindex, vuint16m4_t value, _VL_T vl);
vuint16m8_t vamomaxue_v_u16m8_m_vl (vbool2_t mask, uint16_t *base, vuint16m8_t bindex, vuint16m8_t value, _VL_T vl);
vuint32m1_t vamomaxue_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m1_t vamomaxuw_v_u32m1_m_vl (vbool32_t mask, uint32_t *base, vuint32m1_t bindex, vuint32m1_t value, _VL_T vl);
vuint32m2_t vamomaxue_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m2_t vamomaxuw_v_u32m2_m_vl (vbool16_t mask, uint32_t *base, vuint32m2_t bindex, vuint32m2_t value, _VL_T vl);
vuint32m4_t vamomaxue_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m4_t vamomaxuw_v_u32m4_m_vl (vbool8_t mask, uint32_t *base, vuint32m4_t bindex, vuint32m4_t value, _VL_T vl);
vuint32m8_t vamomaxue_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint32m8_t vamomaxuw_v_u32m8_m_vl (vbool4_t mask, uint32_t *base, vuint32m8_t bindex, vuint32m8_t value, _VL_T vl);
vuint64m1_t vamomaxue_v_u64m1_m_vl (vbool64_t mask, uint64_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m1_t vamomaxuw_v_u64m1_m_vl (vbool64_t mask, int32_t *base, vuint64m1_t bindex, vuint64m1_t value, _VL_T vl);
vuint64m2_t vamomaxue_v_u64m2_m_vl (vbool32_t mask, uint64_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m2_t vamomaxuw_v_u64m2_m_vl (vbool32_t mask, int32_t *base, vuint64m2_t bindex, vuint64m2_t value, _VL_T vl);
vuint64m4_t vamomaxue_v_u64m4_m_vl (vbool16_t mask, uint64_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m4_t vamomaxuw_v_u64m4_m_vl (vbool16_t mask, int32_t *base, vuint64m4_t bindex, vuint64m4_t value, _VL_T vl);
vuint64m8_t vamomaxue_v_u64m8_m_vl (vbool8_t mask, uint64_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
vuint64m8_t vamomaxuw_v_u64m8_m_vl (vbool8_t mask, int32_t *base, vuint64m8_t bindex, vuint64m8_t value, _VL_T vl);
```
## Vector Integer Arithmetic Functions:

### [Vector Single-Width Integer Add and Subtract Functions](rvv-intrinsic-api.md#121-vector-single-width-integer-add-and-subtract):

**Prototypes:**
``` C
vint8m1_t vadd_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vadd_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vadd_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vadd_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vadd_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vadd_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vadd_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vadd_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vadd_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vadd_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vadd_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vadd_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vadd_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vadd_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vadd_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vadd_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vadd_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vadd_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vadd_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vadd_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vadd_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vadd_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vadd_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vadd_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vadd_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vadd_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vadd_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vadd_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vadd_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vadd_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vadd_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vadd_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vadd_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vadd_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vadd_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vadd_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vadd_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vadd_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vadd_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vadd_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vadd_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vadd_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vadd_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vadd_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vadd_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vadd_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vadd_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vadd_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vadd_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vadd_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vadd_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vadd_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vadd_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vadd_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vadd_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vadd_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vadd_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vadd_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vadd_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vadd_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vadd_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vadd_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vadd_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vadd_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vsub_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vsub_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vsub_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vsub_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vsub_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vsub_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vsub_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vsub_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vsub_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vsub_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vsub_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vsub_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vsub_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vsub_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vsub_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vsub_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vsub_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vsub_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vsub_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vsub_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vsub_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vsub_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vsub_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vsub_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vsub_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vsub_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vsub_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vsub_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vsub_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vsub_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vsub_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vsub_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vsub_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsub_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsub_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsub_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsub_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsub_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsub_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsub_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsub_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsub_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vsub_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsub_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vsub_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsub_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vsub_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsub_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vsub_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsub_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vsub_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsub_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vsub_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsub_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vsub_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsub_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vsub_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsub_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vsub_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsub_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vsub_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsub_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vsub_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsub_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vrsub_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vrsub_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vrsub_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vrsub_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vrsub_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vrsub_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vrsub_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vrsub_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vrsub_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vrsub_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vrsub_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vrsub_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vrsub_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vrsub_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vrsub_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vrsub_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vrsub_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vrsub_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vrsub_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vrsub_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vrsub_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vrsub_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vrsub_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vrsub_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vrsub_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vrsub_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vrsub_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vrsub_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vrsub_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vrsub_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vrsub_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vrsub_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vint8m1_t vadd_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vadd_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vadd_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vadd_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vadd_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vadd_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vadd_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vadd_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vadd_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vadd_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vadd_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vadd_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vadd_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vadd_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vadd_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vadd_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vadd_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vadd_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vadd_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vadd_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vadd_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vadd_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vadd_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vadd_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vadd_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vadd_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vadd_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vadd_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vadd_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vadd_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vadd_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vadd_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vadd_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vadd_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vadd_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vadd_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vadd_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vadd_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vadd_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vadd_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vadd_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vadd_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vadd_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vadd_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vadd_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vadd_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vadd_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vadd_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vadd_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vadd_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vadd_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vadd_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vadd_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vadd_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vadd_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vadd_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vadd_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vadd_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vadd_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vadd_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vadd_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vadd_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vadd_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vadd_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vsub_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vsub_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vsub_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vsub_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vsub_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vsub_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vsub_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vsub_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vsub_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vsub_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vsub_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vsub_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vsub_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vsub_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vsub_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vsub_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vsub_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vsub_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vsub_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vsub_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vsub_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vsub_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vsub_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vsub_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vsub_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vsub_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vsub_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vsub_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vsub_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vsub_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vsub_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vsub_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vsub_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsub_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsub_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsub_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsub_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsub_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsub_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsub_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsub_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsub_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vsub_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsub_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vsub_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsub_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vsub_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsub_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vsub_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsub_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vsub_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsub_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vsub_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsub_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vsub_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsub_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vsub_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsub_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vsub_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsub_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vsub_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsub_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vsub_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsub_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vrsub_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vrsub_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vrsub_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vrsub_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vrsub_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vrsub_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vrsub_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vrsub_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vrsub_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vrsub_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vrsub_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vrsub_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vrsub_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vrsub_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vrsub_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vrsub_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vrsub_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vrsub_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vrsub_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vrsub_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vrsub_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vrsub_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vrsub_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vrsub_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vrsub_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vrsub_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vrsub_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vrsub_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vrsub_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vrsub_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vrsub_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vrsub_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Widening Integer Add/Subtract Functions](rvv-intrinsic-api.md#122-vector-widening-integer-addsubtract-operations):

**Prototypes:**
``` C
vint16m2_t vwadd_vv_i16m2_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwadd_vx_i16m2_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint16m2_t vwadd_wv_i16m2_vl (vint16m2_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwadd_wx_i16m2_vl (vint16m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwadd_vv_i16m4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwadd_vx_i16m4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwadd_wv_i16m4_vl (vint16m4_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwadd_wx_i16m4_vl (vint16m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwadd_vv_i16m8_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwadd_vx_i16m8_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwadd_wv_i16m8_vl (vint16m8_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwadd_wx_i16m8_vl (vint16m8_t op1, int8_t op2, _VL_T vl);
vint32m2_t vwadd_vv_i32m2_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwadd_vx_i32m2_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint32m2_t vwadd_wv_i32m2_vl (vint32m2_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwadd_wx_i32m2_vl (vint32m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwadd_vv_i32m4_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwadd_vx_i32m4_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwadd_wv_i32m4_vl (vint32m4_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwadd_wx_i32m4_vl (vint32m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwadd_vv_i32m8_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwadd_vx_i32m8_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwadd_wv_i32m8_vl (vint32m8_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwadd_wx_i32m8_vl (vint32m8_t op1, int16_t op2, _VL_T vl);
vint64m2_t vwadd_vv_i64m2_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwadd_vx_i64m2_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint64m2_t vwadd_wv_i64m2_vl (vint64m2_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwadd_wx_i64m2_vl (vint64m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwadd_vv_i64m4_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwadd_vx_i64m4_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwadd_wv_i64m4_vl (vint64m4_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwadd_wx_i64m4_vl (vint64m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwadd_vv_i64m8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwadd_vx_i64m8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwadd_wv_i64m8_vl (vint64m8_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwadd_wx_i64m8_vl (vint64m8_t op1, int32_t op2, _VL_T vl);
vuint16m2_t vwaddu_vv_u16m2_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwaddu_vx_u16m2_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vwaddu_wv_u16m2_vl (vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwaddu_wx_u16m2_vl (vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwaddu_vv_u16m4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwaddu_vx_u16m4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwaddu_wv_u16m4_vl (vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwaddu_wx_u16m4_vl (vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwaddu_vv_u16m8_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwaddu_vx_u16m8_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwaddu_wv_u16m8_vl (vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwaddu_wx_u16m8_vl (vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vwaddu_vv_u32m2_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwaddu_vx_u32m2_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint32m2_t vwaddu_wv_u32m2_vl (vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwaddu_wx_u32m2_vl (vuint32m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwaddu_vv_u32m4_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwaddu_vx_u32m4_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwaddu_wv_u32m4_vl (vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwaddu_wx_u32m4_vl (vuint32m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwaddu_vv_u32m8_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwaddu_vx_u32m8_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwaddu_wv_u32m8_vl (vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwaddu_wx_u32m8_vl (vuint32m8_t op1, uint16_t op2, _VL_T vl);
vuint64m2_t vwaddu_vv_u64m2_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwaddu_vx_u64m2_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint64m2_t vwaddu_wv_u64m2_vl (vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwaddu_wx_u64m2_vl (vuint64m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwaddu_vv_u64m4_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwaddu_vx_u64m4_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwaddu_wv_u64m4_vl (vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwaddu_wx_u64m4_vl (vuint64m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwaddu_vv_u64m8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwaddu_vx_u64m8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwaddu_wv_u64m8_vl (vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwaddu_wx_u64m8_vl (vuint64m8_t op1, uint32_t op2, _VL_T vl);
vint16m2_t vwsub_vv_i16m2_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwsub_vx_i16m2_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint16m2_t vwsub_wv_i16m2_vl (vint16m2_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwsub_wx_i16m2_vl (vint16m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwsub_vv_i16m4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwsub_vx_i16m4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwsub_wv_i16m4_vl (vint16m4_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwsub_wx_i16m4_vl (vint16m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwsub_vv_i16m8_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwsub_vx_i16m8_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwsub_wv_i16m8_vl (vint16m8_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwsub_wx_i16m8_vl (vint16m8_t op1, int8_t op2, _VL_T vl);
vint32m2_t vwsub_vv_i32m2_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwsub_vx_i32m2_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint32m2_t vwsub_wv_i32m2_vl (vint32m2_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwsub_wx_i32m2_vl (vint32m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwsub_vv_i32m4_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwsub_vx_i32m4_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwsub_wv_i32m4_vl (vint32m4_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwsub_wx_i32m4_vl (vint32m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwsub_vv_i32m8_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwsub_vx_i32m8_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwsub_wv_i32m8_vl (vint32m8_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwsub_wx_i32m8_vl (vint32m8_t op1, int16_t op2, _VL_T vl);
vint64m2_t vwsub_vv_i64m2_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwsub_vx_i64m2_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint64m2_t vwsub_wv_i64m2_vl (vint64m2_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwsub_wx_i64m2_vl (vint64m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwsub_vv_i64m4_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwsub_vx_i64m4_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwsub_wv_i64m4_vl (vint64m4_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwsub_wx_i64m4_vl (vint64m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwsub_vv_i64m8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwsub_vx_i64m8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwsub_wv_i64m8_vl (vint64m8_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwsub_wx_i64m8_vl (vint64m8_t op1, int32_t op2, _VL_T vl);
vuint16m2_t vwsubu_vv_u16m2_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwsubu_vx_u16m2_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vwsubu_wv_u16m2_vl (vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwsubu_wx_u16m2_vl (vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwsubu_vv_u16m4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwsubu_vx_u16m4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwsubu_wv_u16m4_vl (vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwsubu_wx_u16m4_vl (vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwsubu_vv_u16m8_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwsubu_vx_u16m8_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwsubu_wv_u16m8_vl (vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwsubu_wx_u16m8_vl (vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vwsubu_vv_u32m2_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwsubu_vx_u32m2_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint32m2_t vwsubu_wv_u32m2_vl (vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwsubu_wx_u32m2_vl (vuint32m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwsubu_vv_u32m4_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwsubu_vx_u32m4_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwsubu_wv_u32m4_vl (vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwsubu_wx_u32m4_vl (vuint32m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwsubu_vv_u32m8_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwsubu_vx_u32m8_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwsubu_wv_u32m8_vl (vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwsubu_wx_u32m8_vl (vuint32m8_t op1, uint16_t op2, _VL_T vl);
vuint64m2_t vwsubu_vv_u64m2_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwsubu_vx_u64m2_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint64m2_t vwsubu_wv_u64m2_vl (vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwsubu_wx_u64m2_vl (vuint64m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwsubu_vv_u64m4_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwsubu_vx_u64m4_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwsubu_wv_u64m4_vl (vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwsubu_wx_u64m4_vl (vuint64m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwsubu_vv_u64m8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwsubu_vx_u64m8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwsubu_wv_u64m8_vl (vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwsubu_wx_u64m8_vl (vuint64m8_t op1, uint32_t op2, _VL_T vl);
// masked functions
vint16m2_t vwadd_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwadd_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint16m2_t vwadd_wv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwadd_wx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwadd_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwadd_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwadd_wv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwadd_wx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwadd_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwadd_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwadd_wv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwadd_wx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int8_t op2, _VL_T vl);
vint32m2_t vwadd_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwadd_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint32m2_t vwadd_wv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwadd_wx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwadd_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwadd_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwadd_wv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwadd_wx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwadd_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwadd_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwadd_wv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwadd_wx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int16_t op2, _VL_T vl);
vint64m2_t vwadd_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwadd_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint64m2_t vwadd_wv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwadd_wx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwadd_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwadd_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwadd_wv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwadd_wx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwadd_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwadd_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwadd_wv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwadd_wx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int32_t op2, _VL_T vl);
vuint16m2_t vwaddu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwaddu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vwaddu_wv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwaddu_wx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwaddu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwaddu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwaddu_wv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwaddu_wx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwaddu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwaddu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwaddu_wv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwaddu_wx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vwaddu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwaddu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint32m2_t vwaddu_wv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwaddu_wx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwaddu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwaddu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwaddu_wv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwaddu_wx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwaddu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwaddu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwaddu_wv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwaddu_wx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint16_t op2, _VL_T vl);
vuint64m2_t vwaddu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwaddu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint64m2_t vwaddu_wv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwaddu_wx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwaddu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwaddu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwaddu_wv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwaddu_wx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwaddu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwaddu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwaddu_wv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwaddu_wx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint32_t op2, _VL_T vl);
vint16m2_t vwsub_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwsub_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint16m2_t vwsub_wv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwsub_wx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwsub_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwsub_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwsub_wv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwsub_wx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwsub_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwsub_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwsub_wv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwsub_wx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int8_t op2, _VL_T vl);
vint32m2_t vwsub_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwsub_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint32m2_t vwsub_wv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwsub_wx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwsub_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwsub_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwsub_wv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwsub_wx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwsub_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwsub_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwsub_wv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwsub_wx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int16_t op2, _VL_T vl);
vint64m2_t vwsub_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwsub_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint64m2_t vwsub_wv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwsub_wx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwsub_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwsub_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwsub_wv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwsub_wx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwsub_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwsub_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwsub_wv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwsub_wx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int32_t op2, _VL_T vl);
vuint16m2_t vwsubu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwsubu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vwsubu_wv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwsubu_wx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwsubu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwsubu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwsubu_wv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwsubu_wx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwsubu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwsubu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwsubu_wv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwsubu_wx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vwsubu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwsubu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint32m2_t vwsubu_wv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwsubu_wx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwsubu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwsubu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwsubu_wv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwsubu_wx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwsubu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwsubu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwsubu_wv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwsubu_wx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint16_t op2, _VL_T vl);
vuint64m2_t vwsubu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwsubu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint64m2_t vwsubu_wv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwsubu_wx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwsubu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwsubu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwsubu_wv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwsubu_wx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwsubu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwsubu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwsubu_wv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwsubu_wx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint32_t op2, _VL_T vl);
```
### [Vector Integer Add-with-Carry / Subtract-with-Borrow Functions](rvv-intrinsic-api.md#123-vector-integer-add-with-carry--subtract-with-borrow-operations):

**Prototypes:**
``` C
vint8m1_t vadc_vvm_i8m1_vl (vint8m1_t op1, vint8m1_t op2, vbool8_t carryin, _VL_T vl);
vint8m1_t vadc_vxm_i8m1_vl (vint8m1_t op1, int8_t op2, vbool8_t carryin, _VL_T vl);
vint8m2_t vadc_vvm_i8m2_vl (vint8m2_t op1, vint8m2_t op2, vbool4_t carryin, _VL_T vl);
vint8m2_t vadc_vxm_i8m2_vl (vint8m2_t op1, int8_t op2, vbool4_t carryin, _VL_T vl);
vint8m4_t vadc_vvm_i8m4_vl (vint8m4_t op1, vint8m4_t op2, vbool2_t carryin, _VL_T vl);
vint8m4_t vadc_vxm_i8m4_vl (vint8m4_t op1, int8_t op2, vbool2_t carryin, _VL_T vl);
vint8m8_t vadc_vvm_i8m8_vl (vint8m8_t op1, vint8m8_t op2, vbool1_t carryin, _VL_T vl);
vint8m8_t vadc_vxm_i8m8_vl (vint8m8_t op1, int8_t op2, vbool1_t carryin, _VL_T vl);
vint16m1_t vadc_vvm_i16m1_vl (vint16m1_t op1, vint16m1_t op2, vbool16_t carryin, _VL_T vl);
vint16m1_t vadc_vxm_i16m1_vl (vint16m1_t op1, int16_t op2, vbool16_t carryin, _VL_T vl);
vint16m2_t vadc_vvm_i16m2_vl (vint16m2_t op1, vint16m2_t op2, vbool8_t carryin, _VL_T vl);
vint16m2_t vadc_vxm_i16m2_vl (vint16m2_t op1, int16_t op2, vbool8_t carryin, _VL_T vl);
vint16m4_t vadc_vvm_i16m4_vl (vint16m4_t op1, vint16m4_t op2, vbool4_t carryin, _VL_T vl);
vint16m4_t vadc_vxm_i16m4_vl (vint16m4_t op1, int16_t op2, vbool4_t carryin, _VL_T vl);
vint16m8_t vadc_vvm_i16m8_vl (vint16m8_t op1, vint16m8_t op2, vbool2_t carryin, _VL_T vl);
vint16m8_t vadc_vxm_i16m8_vl (vint16m8_t op1, int16_t op2, vbool2_t carryin, _VL_T vl);
vint32m1_t vadc_vvm_i32m1_vl (vint32m1_t op1, vint32m1_t op2, vbool32_t carryin, _VL_T vl);
vint32m1_t vadc_vxm_i32m1_vl (vint32m1_t op1, int32_t op2, vbool32_t carryin, _VL_T vl);
vint32m2_t vadc_vvm_i32m2_vl (vint32m2_t op1, vint32m2_t op2, vbool16_t carryin, _VL_T vl);
vint32m2_t vadc_vxm_i32m2_vl (vint32m2_t op1, int32_t op2, vbool16_t carryin, _VL_T vl);
vint32m4_t vadc_vvm_i32m4_vl (vint32m4_t op1, vint32m4_t op2, vbool8_t carryin, _VL_T vl);
vint32m4_t vadc_vxm_i32m4_vl (vint32m4_t op1, int32_t op2, vbool8_t carryin, _VL_T vl);
vint32m8_t vadc_vvm_i32m8_vl (vint32m8_t op1, vint32m8_t op2, vbool4_t carryin, _VL_T vl);
vint32m8_t vadc_vxm_i32m8_vl (vint32m8_t op1, int32_t op2, vbool4_t carryin, _VL_T vl);
vint64m1_t vadc_vvm_i64m1_vl (vint64m1_t op1, vint64m1_t op2, vbool64_t carryin, _VL_T vl);
vint64m1_t vadc_vxm_i64m1_vl (vint64m1_t op1, int64_t op2, vbool64_t carryin, _VL_T vl);
vint64m2_t vadc_vvm_i64m2_vl (vint64m2_t op1, vint64m2_t op2, vbool32_t carryin, _VL_T vl);
vint64m2_t vadc_vxm_i64m2_vl (vint64m2_t op1, int64_t op2, vbool32_t carryin, _VL_T vl);
vint64m4_t vadc_vvm_i64m4_vl (vint64m4_t op1, vint64m4_t op2, vbool16_t carryin, _VL_T vl);
vint64m4_t vadc_vxm_i64m4_vl (vint64m4_t op1, int64_t op2, vbool16_t carryin, _VL_T vl);
vint64m8_t vadc_vvm_i64m8_vl (vint64m8_t op1, vint64m8_t op2, vbool8_t carryin, _VL_T vl);
vint64m8_t vadc_vxm_i64m8_vl (vint64m8_t op1, int64_t op2, vbool8_t carryin, _VL_T vl);
vuint8m1_t vadc_vvm_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, vbool8_t carryin, _VL_T vl);
vuint8m1_t vadc_vxm_u8m1_vl (vuint8m1_t op1, uint8_t op2, vbool8_t carryin, _VL_T vl);
vuint8m2_t vadc_vvm_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, vbool4_t carryin, _VL_T vl);
vuint8m2_t vadc_vxm_u8m2_vl (vuint8m2_t op1, uint8_t op2, vbool4_t carryin, _VL_T vl);
vuint8m4_t vadc_vvm_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, vbool2_t carryin, _VL_T vl);
vuint8m4_t vadc_vxm_u8m4_vl (vuint8m4_t op1, uint8_t op2, vbool2_t carryin, _VL_T vl);
vuint8m8_t vadc_vvm_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, vbool1_t carryin, _VL_T vl);
vuint8m8_t vadc_vxm_u8m8_vl (vuint8m8_t op1, uint8_t op2, vbool1_t carryin, _VL_T vl);
vuint16m1_t vadc_vvm_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, vbool16_t carryin, _VL_T vl);
vuint16m1_t vadc_vxm_u16m1_vl (vuint16m1_t op1, uint16_t op2, vbool16_t carryin, _VL_T vl);
vuint16m2_t vadc_vvm_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, vbool8_t carryin, _VL_T vl);
vuint16m2_t vadc_vxm_u16m2_vl (vuint16m2_t op1, uint16_t op2, vbool8_t carryin, _VL_T vl);
vuint16m4_t vadc_vvm_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, vbool4_t carryin, _VL_T vl);
vuint16m4_t vadc_vxm_u16m4_vl (vuint16m4_t op1, uint16_t op2, vbool4_t carryin, _VL_T vl);
vuint16m8_t vadc_vvm_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, vbool2_t carryin, _VL_T vl);
vuint16m8_t vadc_vxm_u16m8_vl (vuint16m8_t op1, uint16_t op2, vbool2_t carryin, _VL_T vl);
vuint32m1_t vadc_vvm_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, vbool32_t carryin, _VL_T vl);
vuint32m1_t vadc_vxm_u32m1_vl (vuint32m1_t op1, uint32_t op2, vbool32_t carryin, _VL_T vl);
vuint32m2_t vadc_vvm_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, vbool16_t carryin, _VL_T vl);
vuint32m2_t vadc_vxm_u32m2_vl (vuint32m2_t op1, uint32_t op2, vbool16_t carryin, _VL_T vl);
vuint32m4_t vadc_vvm_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, vbool8_t carryin, _VL_T vl);
vuint32m4_t vadc_vxm_u32m4_vl (vuint32m4_t op1, uint32_t op2, vbool8_t carryin, _VL_T vl);
vuint32m8_t vadc_vvm_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, vbool4_t carryin, _VL_T vl);
vuint32m8_t vadc_vxm_u32m8_vl (vuint32m8_t op1, uint32_t op2, vbool4_t carryin, _VL_T vl);
vuint64m1_t vadc_vvm_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, vbool64_t carryin, _VL_T vl);
vuint64m1_t vadc_vxm_u64m1_vl (vuint64m1_t op1, uint64_t op2, vbool64_t carryin, _VL_T vl);
vuint64m2_t vadc_vvm_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, vbool32_t carryin, _VL_T vl);
vuint64m2_t vadc_vxm_u64m2_vl (vuint64m2_t op1, uint64_t op2, vbool32_t carryin, _VL_T vl);
vuint64m4_t vadc_vvm_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, vbool16_t carryin, _VL_T vl);
vuint64m4_t vadc_vxm_u64m4_vl (vuint64m4_t op1, uint64_t op2, vbool16_t carryin, _VL_T vl);
vuint64m8_t vadc_vvm_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, vbool8_t carryin, _VL_T vl);
vuint64m8_t vadc_vxm_u64m8_vl (vuint64m8_t op1, uint64_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vvm_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_i8m1_b8_vl (vint8m1_t op1, int8_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmadc_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmadc_vvm_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vxm_i8m2_b4_vl (vint8m2_t op1, int8_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmadc_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmadc_vvm_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vxm_i8m4_b2_vl (vint8m4_t op1, int8_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmadc_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmadc_vvm_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, vbool1_t carryin, _VL_T vl);
vbool1_t vmadc_vxm_i8m8_b1_vl (vint8m8_t op1, int8_t op2, vbool1_t carryin, _VL_T vl);
vbool1_t vmadc_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmadc_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmadc_vvm_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vxm_i16m1_b16_vl (vint16m1_t op1, int16_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmadc_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmadc_vvm_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_i16m2_b8_vl (vint16m2_t op1, int16_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmadc_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmadc_vvm_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vxm_i16m4_b4_vl (vint16m4_t op1, int16_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmadc_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmadc_vvm_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vxm_i16m8_b2_vl (vint16m8_t op1, int16_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmadc_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmadc_vvm_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vxm_i32m1_b32_vl (vint32m1_t op1, int32_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmadc_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmadc_vvm_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vxm_i32m2_b16_vl (vint32m2_t op1, int32_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmadc_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmadc_vvm_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_i32m4_b8_vl (vint32m4_t op1, int32_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmadc_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmadc_vvm_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vxm_i32m8_b4_vl (vint32m8_t op1, int32_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmadc_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmadc_vvm_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, vbool64_t carryin, _VL_T vl);
vbool64_t vmadc_vxm_i64m1_b64_vl (vint64m1_t op1, int64_t op2, vbool64_t carryin, _VL_T vl);
vbool64_t vmadc_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmadc_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmadc_vvm_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vxm_i64m2_b32_vl (vint64m2_t op1, int64_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmadc_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmadc_vvm_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vxm_i64m4_b16_vl (vint64m4_t op1, int64_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmadc_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmadc_vvm_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_i64m8_b8_vl (vint64m8_t op1, int64_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmadc_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmadc_vvm_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmadc_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmadc_vvm_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vxm_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmadc_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmadc_vvm_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vxm_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmadc_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmadc_vvm_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, vbool1_t carryin, _VL_T vl);
vbool1_t vmadc_vxm_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, vbool1_t carryin, _VL_T vl);
vbool1_t vmadc_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmadc_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmadc_vvm_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vxm_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmadc_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmadc_vvm_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmadc_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmadc_vvm_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vxm_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmadc_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmadc_vvm_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vxm_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, vbool2_t carryin, _VL_T vl);
vbool2_t vmadc_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmadc_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmadc_vvm_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vxm_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmadc_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmadc_vvm_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vxm_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmadc_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmadc_vvm_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmadc_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmadc_vvm_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vxm_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, vbool4_t carryin, _VL_T vl);
vbool4_t vmadc_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmadc_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmadc_vvm_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, vbool64_t carryin, _VL_T vl);
vbool64_t vmadc_vxm_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, vbool64_t carryin, _VL_T vl);
vbool64_t vmadc_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmadc_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmadc_vvm_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vxm_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, vbool32_t carryin, _VL_T vl);
vbool32_t vmadc_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmadc_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmadc_vvm_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vxm_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, vbool16_t carryin, _VL_T vl);
vbool16_t vmadc_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmadc_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmadc_vvm_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vxm_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, vbool8_t carryin, _VL_T vl);
vbool8_t vmadc_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmadc_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vsbc_vvm_i8m1_vl (vint8m1_t op1, vint8m1_t op2, vbool8_t borrowin, _VL_T vl);
vint8m1_t vsbc_vxm_i8m1_vl (vint8m1_t op1, int8_t op2, vbool8_t borrowin, _VL_T vl);
vint8m2_t vsbc_vvm_i8m2_vl (vint8m2_t op1, vint8m2_t op2, vbool4_t borrowin, _VL_T vl);
vint8m2_t vsbc_vxm_i8m2_vl (vint8m2_t op1, int8_t op2, vbool4_t borrowin, _VL_T vl);
vint8m4_t vsbc_vvm_i8m4_vl (vint8m4_t op1, vint8m4_t op2, vbool2_t borrowin, _VL_T vl);
vint8m4_t vsbc_vxm_i8m4_vl (vint8m4_t op1, int8_t op2, vbool2_t borrowin, _VL_T vl);
vint8m8_t vsbc_vvm_i8m8_vl (vint8m8_t op1, vint8m8_t op2, vbool1_t borrowin, _VL_T vl);
vint8m8_t vsbc_vxm_i8m8_vl (vint8m8_t op1, int8_t op2, vbool1_t borrowin, _VL_T vl);
vint16m1_t vsbc_vvm_i16m1_vl (vint16m1_t op1, vint16m1_t op2, vbool16_t borrowin, _VL_T vl);
vint16m1_t vsbc_vxm_i16m1_vl (vint16m1_t op1, int16_t op2, vbool16_t borrowin, _VL_T vl);
vint16m2_t vsbc_vvm_i16m2_vl (vint16m2_t op1, vint16m2_t op2, vbool8_t borrowin, _VL_T vl);
vint16m2_t vsbc_vxm_i16m2_vl (vint16m2_t op1, int16_t op2, vbool8_t borrowin, _VL_T vl);
vint16m4_t vsbc_vvm_i16m4_vl (vint16m4_t op1, vint16m4_t op2, vbool4_t borrowin, _VL_T vl);
vint16m4_t vsbc_vxm_i16m4_vl (vint16m4_t op1, int16_t op2, vbool4_t borrowin, _VL_T vl);
vint16m8_t vsbc_vvm_i16m8_vl (vint16m8_t op1, vint16m8_t op2, vbool2_t borrowin, _VL_T vl);
vint16m8_t vsbc_vxm_i16m8_vl (vint16m8_t op1, int16_t op2, vbool2_t borrowin, _VL_T vl);
vint32m1_t vsbc_vvm_i32m1_vl (vint32m1_t op1, vint32m1_t op2, vbool32_t borrowin, _VL_T vl);
vint32m1_t vsbc_vxm_i32m1_vl (vint32m1_t op1, int32_t op2, vbool32_t borrowin, _VL_T vl);
vint32m2_t vsbc_vvm_i32m2_vl (vint32m2_t op1, vint32m2_t op2, vbool16_t borrowin, _VL_T vl);
vint32m2_t vsbc_vxm_i32m2_vl (vint32m2_t op1, int32_t op2, vbool16_t borrowin, _VL_T vl);
vint32m4_t vsbc_vvm_i32m4_vl (vint32m4_t op1, vint32m4_t op2, vbool8_t borrowin, _VL_T vl);
vint32m4_t vsbc_vxm_i32m4_vl (vint32m4_t op1, int32_t op2, vbool8_t borrowin, _VL_T vl);
vint32m8_t vsbc_vvm_i32m8_vl (vint32m8_t op1, vint32m8_t op2, vbool4_t borrowin, _VL_T vl);
vint32m8_t vsbc_vxm_i32m8_vl (vint32m8_t op1, int32_t op2, vbool4_t borrowin, _VL_T vl);
vint64m1_t vsbc_vvm_i64m1_vl (vint64m1_t op1, vint64m1_t op2, vbool64_t borrowin, _VL_T vl);
vint64m1_t vsbc_vxm_i64m1_vl (vint64m1_t op1, int64_t op2, vbool64_t borrowin, _VL_T vl);
vint64m2_t vsbc_vvm_i64m2_vl (vint64m2_t op1, vint64m2_t op2, vbool32_t borrowin, _VL_T vl);
vint64m2_t vsbc_vxm_i64m2_vl (vint64m2_t op1, int64_t op2, vbool32_t borrowin, _VL_T vl);
vint64m4_t vsbc_vvm_i64m4_vl (vint64m4_t op1, vint64m4_t op2, vbool16_t borrowin, _VL_T vl);
vint64m4_t vsbc_vxm_i64m4_vl (vint64m4_t op1, int64_t op2, vbool16_t borrowin, _VL_T vl);
vint64m8_t vsbc_vvm_i64m8_vl (vint64m8_t op1, vint64m8_t op2, vbool8_t borrowin, _VL_T vl);
vint64m8_t vsbc_vxm_i64m8_vl (vint64m8_t op1, int64_t op2, vbool8_t borrowin, _VL_T vl);
vuint8m1_t vsbc_vvm_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, vbool8_t borrowin, _VL_T vl);
vuint8m1_t vsbc_vxm_u8m1_vl (vuint8m1_t op1, uint8_t op2, vbool8_t borrowin, _VL_T vl);
vuint8m2_t vsbc_vvm_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, vbool4_t borrowin, _VL_T vl);
vuint8m2_t vsbc_vxm_u8m2_vl (vuint8m2_t op1, uint8_t op2, vbool4_t borrowin, _VL_T vl);
vuint8m4_t vsbc_vvm_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, vbool2_t borrowin, _VL_T vl);
vuint8m4_t vsbc_vxm_u8m4_vl (vuint8m4_t op1, uint8_t op2, vbool2_t borrowin, _VL_T vl);
vuint8m8_t vsbc_vvm_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, vbool1_t borrowin, _VL_T vl);
vuint8m8_t vsbc_vxm_u8m8_vl (vuint8m8_t op1, uint8_t op2, vbool1_t borrowin, _VL_T vl);
vuint16m1_t vsbc_vvm_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, vbool16_t borrowin, _VL_T vl);
vuint16m1_t vsbc_vxm_u16m1_vl (vuint16m1_t op1, uint16_t op2, vbool16_t borrowin, _VL_T vl);
vuint16m2_t vsbc_vvm_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, vbool8_t borrowin, _VL_T vl);
vuint16m2_t vsbc_vxm_u16m2_vl (vuint16m2_t op1, uint16_t op2, vbool8_t borrowin, _VL_T vl);
vuint16m4_t vsbc_vvm_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, vbool4_t borrowin, _VL_T vl);
vuint16m4_t vsbc_vxm_u16m4_vl (vuint16m4_t op1, uint16_t op2, vbool4_t borrowin, _VL_T vl);
vuint16m8_t vsbc_vvm_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, vbool2_t borrowin, _VL_T vl);
vuint16m8_t vsbc_vxm_u16m8_vl (vuint16m8_t op1, uint16_t op2, vbool2_t borrowin, _VL_T vl);
vuint32m1_t vsbc_vvm_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, vbool32_t borrowin, _VL_T vl);
vuint32m1_t vsbc_vxm_u32m1_vl (vuint32m1_t op1, uint32_t op2, vbool32_t borrowin, _VL_T vl);
vuint32m2_t vsbc_vvm_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, vbool16_t borrowin, _VL_T vl);
vuint32m2_t vsbc_vxm_u32m2_vl (vuint32m2_t op1, uint32_t op2, vbool16_t borrowin, _VL_T vl);
vuint32m4_t vsbc_vvm_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, vbool8_t borrowin, _VL_T vl);
vuint32m4_t vsbc_vxm_u32m4_vl (vuint32m4_t op1, uint32_t op2, vbool8_t borrowin, _VL_T vl);
vuint32m8_t vsbc_vvm_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, vbool4_t borrowin, _VL_T vl);
vuint32m8_t vsbc_vxm_u32m8_vl (vuint32m8_t op1, uint32_t op2, vbool4_t borrowin, _VL_T vl);
vuint64m1_t vsbc_vvm_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, vbool64_t borrowin, _VL_T vl);
vuint64m1_t vsbc_vxm_u64m1_vl (vuint64m1_t op1, uint64_t op2, vbool64_t borrowin, _VL_T vl);
vuint64m2_t vsbc_vvm_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, vbool32_t borrowin, _VL_T vl);
vuint64m2_t vsbc_vxm_u64m2_vl (vuint64m2_t op1, uint64_t op2, vbool32_t borrowin, _VL_T vl);
vuint64m4_t vsbc_vvm_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, vbool16_t borrowin, _VL_T vl);
vuint64m4_t vsbc_vxm_u64m4_vl (vuint64m4_t op1, uint64_t op2, vbool16_t borrowin, _VL_T vl);
vuint64m8_t vsbc_vvm_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, vbool8_t borrowin, _VL_T vl);
vuint64m8_t vsbc_vxm_u64m8_vl (vuint64m8_t op1, uint64_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vvm_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_i8m1_b8_vl (vint8m1_t op1, int8_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsbc_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsbc_vvm_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vxm_i8m2_b4_vl (vint8m2_t op1, int8_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsbc_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsbc_vvm_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vxm_i8m4_b2_vl (vint8m4_t op1, int8_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsbc_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsbc_vvm_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, vbool1_t borrowin, _VL_T vl);
vbool1_t vmsbc_vxm_i8m8_b1_vl (vint8m8_t op1, int8_t op2, vbool1_t borrowin, _VL_T vl);
vbool1_t vmsbc_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsbc_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsbc_vvm_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vxm_i16m1_b16_vl (vint16m1_t op1, int16_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsbc_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsbc_vvm_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_i16m2_b8_vl (vint16m2_t op1, int16_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsbc_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsbc_vvm_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vxm_i16m4_b4_vl (vint16m4_t op1, int16_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsbc_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsbc_vvm_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vxm_i16m8_b2_vl (vint16m8_t op1, int16_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsbc_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsbc_vvm_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vxm_i32m1_b32_vl (vint32m1_t op1, int32_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsbc_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsbc_vvm_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vxm_i32m2_b16_vl (vint32m2_t op1, int32_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsbc_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsbc_vvm_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_i32m4_b8_vl (vint32m4_t op1, int32_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsbc_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsbc_vvm_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vxm_i32m8_b4_vl (vint32m8_t op1, int32_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsbc_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsbc_vvm_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, vbool64_t borrowin, _VL_T vl);
vbool64_t vmsbc_vxm_i64m1_b64_vl (vint64m1_t op1, int64_t op2, vbool64_t borrowin, _VL_T vl);
vbool64_t vmsbc_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsbc_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsbc_vvm_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vxm_i64m2_b32_vl (vint64m2_t op1, int64_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsbc_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsbc_vvm_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vxm_i64m4_b16_vl (vint64m4_t op1, int64_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsbc_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsbc_vvm_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_i64m8_b8_vl (vint64m8_t op1, int64_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsbc_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsbc_vvm_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsbc_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsbc_vvm_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vxm_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsbc_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsbc_vvm_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vxm_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsbc_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsbc_vvm_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, vbool1_t borrowin, _VL_T vl);
vbool1_t vmsbc_vxm_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, vbool1_t borrowin, _VL_T vl);
vbool1_t vmsbc_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsbc_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsbc_vvm_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vxm_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsbc_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsbc_vvm_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsbc_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsbc_vvm_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vxm_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsbc_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsbc_vvm_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vxm_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, vbool2_t borrowin, _VL_T vl);
vbool2_t vmsbc_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsbc_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsbc_vvm_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vxm_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsbc_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsbc_vvm_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vxm_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsbc_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsbc_vvm_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsbc_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsbc_vvm_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vxm_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, vbool4_t borrowin, _VL_T vl);
vbool4_t vmsbc_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsbc_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsbc_vvm_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, vbool64_t borrowin, _VL_T vl);
vbool64_t vmsbc_vxm_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, vbool64_t borrowin, _VL_T vl);
vbool64_t vmsbc_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsbc_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsbc_vvm_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vxm_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, vbool32_t borrowin, _VL_T vl);
vbool32_t vmsbc_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsbc_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsbc_vvm_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vxm_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, vbool16_t borrowin, _VL_T vl);
vbool16_t vmsbc_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsbc_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsbc_vvm_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vxm_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, vbool8_t borrowin, _VL_T vl);
vbool8_t vmsbc_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsbc_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Bitwise Logical Functions](rvv-intrinsic-api.md#124-vector-bitwise-logical-operations):

**Prototypes:**
``` C
vint8m1_t vand_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vand_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vand_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vand_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vand_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vand_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vand_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vand_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vand_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vand_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vand_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vand_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vand_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vand_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vand_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vand_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vand_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vand_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vand_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vand_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vand_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vand_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vand_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vand_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vand_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vand_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vand_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vand_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vand_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vand_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vand_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vand_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vand_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vand_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vand_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vand_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vand_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vand_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vand_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vand_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vand_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vand_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vand_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vand_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vand_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vand_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vand_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vand_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vand_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vand_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vand_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vand_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vand_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vand_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vand_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vand_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vand_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vand_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vand_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vand_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vand_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vand_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vand_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vand_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vor_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vor_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vor_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vor_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vor_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vor_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vor_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vor_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vor_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vor_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vor_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vor_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vor_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vor_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vor_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vor_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vor_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vor_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vor_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vor_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vor_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vor_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vor_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vor_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vor_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vor_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vor_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vor_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vor_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vor_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vor_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vor_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vor_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vor_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vor_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vor_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vor_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vor_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vor_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vor_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vor_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vor_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vor_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vor_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vor_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vor_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vor_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vor_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vor_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vor_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vor_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vor_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vor_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vor_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vor_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vor_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vor_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vor_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vor_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vor_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vor_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vor_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vor_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vor_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vxor_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vxor_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vxor_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vxor_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vxor_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vxor_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vxor_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vxor_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vxor_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vxor_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vxor_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vxor_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vxor_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vxor_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vxor_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vxor_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vxor_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vxor_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vxor_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vxor_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vxor_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vxor_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vxor_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vxor_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vxor_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vxor_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vxor_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vxor_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vxor_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vxor_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vxor_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vxor_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vxor_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vxor_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vxor_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vxor_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vxor_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vxor_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vxor_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vxor_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vxor_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vxor_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vxor_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vxor_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vxor_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vxor_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vxor_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vxor_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vxor_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vxor_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vxor_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vxor_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vxor_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vxor_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vxor_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vxor_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vxor_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vxor_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vxor_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vxor_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vxor_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vxor_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vxor_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vxor_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vint8m1_t vand_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vand_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vand_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vand_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vand_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vand_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vand_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vand_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vand_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vand_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vand_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vand_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vand_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vand_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vand_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vand_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vand_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vand_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vand_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vand_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vand_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vand_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vand_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vand_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vand_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vand_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vand_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vand_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vand_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vand_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vand_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vand_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vand_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vand_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vand_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vand_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vand_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vand_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vand_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vand_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vand_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vand_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vand_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vand_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vand_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vand_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vand_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vand_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vand_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vand_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vand_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vand_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vand_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vand_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vand_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vand_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vand_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vand_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vand_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vand_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vand_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vand_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vand_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vand_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vor_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vor_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vor_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vor_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vor_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vor_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vor_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vor_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vor_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vor_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vor_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vor_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vor_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vor_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vor_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vor_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vor_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vor_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vor_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vor_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vor_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vor_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vor_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vor_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vor_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vor_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vor_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vor_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vor_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vor_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vor_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vor_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vor_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vor_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vor_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vor_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vor_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vor_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vor_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vor_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vor_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vor_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vor_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vor_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vor_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vor_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vor_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vor_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vor_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vor_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vor_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vor_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vor_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vor_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vor_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vor_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vor_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vor_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vor_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vor_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vor_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vor_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vor_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vor_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vxor_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vxor_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vxor_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vxor_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vxor_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vxor_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vxor_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vxor_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vxor_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vxor_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vxor_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vxor_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vxor_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vxor_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vxor_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vxor_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vxor_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vxor_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vxor_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vxor_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vxor_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vxor_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vxor_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vxor_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vxor_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vxor_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vxor_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vxor_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vxor_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vxor_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vxor_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vxor_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vxor_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vxor_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vxor_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vxor_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vxor_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vxor_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vxor_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vxor_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vxor_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vxor_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vxor_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vxor_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vxor_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vxor_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vxor_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vxor_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vxor_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vxor_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vxor_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vxor_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vxor_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vxor_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vxor_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vxor_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vxor_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vxor_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vxor_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vxor_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vxor_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vxor_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vxor_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vxor_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Bitwise Logical Functions](rvv-intrinsic-api.md#124-vector-bitwise-logical-operations):

**Prototypes:**
``` C
vint8m1_t vnot_v_i8m1_vl (vint8m1_t op1, _VL_T vl);
vint8m2_t vnot_v_i8m2_vl (vint8m2_t op1, _VL_T vl);
vint8m4_t vnot_v_i8m4_vl (vint8m4_t op1, _VL_T vl);
vint8m8_t vnot_v_i8m8_vl (vint8m8_t op1, _VL_T vl);
vint16m1_t vnot_v_i16m1_vl (vint16m1_t op1, _VL_T vl);
vint16m2_t vnot_v_i16m2_vl (vint16m2_t op1, _VL_T vl);
vint16m4_t vnot_v_i16m4_vl (vint16m4_t op1, _VL_T vl);
vint16m8_t vnot_v_i16m8_vl (vint16m8_t op1, _VL_T vl);
vint32m1_t vnot_v_i32m1_vl (vint32m1_t op1, _VL_T vl);
vint32m2_t vnot_v_i32m2_vl (vint32m2_t op1, _VL_T vl);
vint32m4_t vnot_v_i32m4_vl (vint32m4_t op1, _VL_T vl);
vint32m8_t vnot_v_i32m8_vl (vint32m8_t op1, _VL_T vl);
vint64m1_t vnot_v_i64m1_vl (vint64m1_t op1, _VL_T vl);
vint64m2_t vnot_v_i64m2_vl (vint64m2_t op1, _VL_T vl);
vint64m4_t vnot_v_i64m4_vl (vint64m4_t op1, _VL_T vl);
vint64m8_t vnot_v_i64m8_vl (vint64m8_t op1, _VL_T vl);
vuint8m1_t vnot_v_u8m1_vl (vuint8m1_t op1, _VL_T vl);
vuint8m2_t vnot_v_u8m2_vl (vuint8m2_t op1, _VL_T vl);
vuint8m4_t vnot_v_u8m4_vl (vuint8m4_t op1, _VL_T vl);
vuint8m8_t vnot_v_u8m8_vl (vuint8m8_t op1, _VL_T vl);
vuint16m1_t vnot_v_u16m1_vl (vuint16m1_t op1, _VL_T vl);
vuint16m2_t vnot_v_u16m2_vl (vuint16m2_t op1, _VL_T vl);
vuint16m4_t vnot_v_u16m4_vl (vuint16m4_t op1, _VL_T vl);
vuint16m8_t vnot_v_u16m8_vl (vuint16m8_t op1, _VL_T vl);
vuint32m1_t vnot_v_u32m1_vl (vuint32m1_t op1, _VL_T vl);
vuint32m2_t vnot_v_u32m2_vl (vuint32m2_t op1, _VL_T vl);
vuint32m4_t vnot_v_u32m4_vl (vuint32m4_t op1, _VL_T vl);
vuint32m8_t vnot_v_u32m8_vl (vuint32m8_t op1, _VL_T vl);
vuint64m1_t vnot_v_u64m1_vl (vuint64m1_t op1, _VL_T vl);
vuint64m2_t vnot_v_u64m2_vl (vuint64m2_t op1, _VL_T vl);
vuint64m4_t vnot_v_u64m4_vl (vuint64m4_t op1, _VL_T vl);
vuint64m8_t vnot_v_u64m8_vl (vuint64m8_t op1, _VL_T vl);
// masked functions
vint8m1_t vnot_v_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, _VL_T vl);
vint8m2_t vnot_v_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, _VL_T vl);
vint8m4_t vnot_v_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, _VL_T vl);
vint8m8_t vnot_v_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, _VL_T vl);
vint16m1_t vnot_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, _VL_T vl);
vint16m2_t vnot_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, _VL_T vl);
vint16m4_t vnot_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, _VL_T vl);
vint16m8_t vnot_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, _VL_T vl);
vint32m1_t vnot_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, _VL_T vl);
vint32m2_t vnot_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, _VL_T vl);
vint32m4_t vnot_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, _VL_T vl);
vint32m8_t vnot_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, _VL_T vl);
vint64m1_t vnot_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, _VL_T vl);
vint64m2_t vnot_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, _VL_T vl);
vint64m4_t vnot_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, _VL_T vl);
vint64m8_t vnot_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, _VL_T vl);
vuint8m1_t vnot_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, _VL_T vl);
vuint8m2_t vnot_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, _VL_T vl);
vuint8m4_t vnot_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, _VL_T vl);
vuint8m8_t vnot_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, _VL_T vl);
vuint16m1_t vnot_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, _VL_T vl);
vuint16m2_t vnot_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, _VL_T vl);
vuint16m4_t vnot_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, _VL_T vl);
vuint16m8_t vnot_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, _VL_T vl);
vuint32m1_t vnot_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, _VL_T vl);
vuint32m2_t vnot_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, _VL_T vl);
vuint32m4_t vnot_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, _VL_T vl);
vuint32m8_t vnot_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, _VL_T vl);
vuint64m1_t vnot_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, _VL_T vl);
vuint64m2_t vnot_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, _VL_T vl);
vuint64m4_t vnot_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, _VL_T vl);
vuint64m8_t vnot_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, _VL_T vl);
```
### [Vector Single-Width Bit Shift Functioans](rvv-intrinsic-api.md#125-vector-single-width-bit-shift-operations):

**Prototypes:**
``` C
vint8m1_t vsll_vv_i8m1_vl (vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vsll_vx_i8m1_vl (vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vsll_vv_i8m2_vl (vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vsll_vx_i8m2_vl (vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vsll_vv_i8m4_vl (vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vsll_vx_i8m4_vl (vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vsll_vv_i8m8_vl (vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vsll_vx_i8m8_vl (vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vsll_vv_i16m1_vl (vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vsll_vx_i16m1_vl (vint16m1_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vsll_vv_i16m2_vl (vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vsll_vx_i16m2_vl (vint16m2_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vsll_vv_i16m4_vl (vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vsll_vx_i16m4_vl (vint16m4_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vsll_vv_i16m8_vl (vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vsll_vx_i16m8_vl (vint16m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vsll_vv_i32m1_vl (vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vsll_vx_i32m1_vl (vint32m1_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vsll_vv_i32m2_vl (vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vsll_vx_i32m2_vl (vint32m2_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vsll_vv_i32m4_vl (vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vsll_vx_i32m4_vl (vint32m4_t op1, uint8_t op2, _VL_T vl);
vint32m8_t vsll_vv_i32m8_vl (vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vsll_vx_i32m8_vl (vint32m8_t op1, uint8_t op2, _VL_T vl);
vint64m1_t vsll_vv_i64m1_vl (vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vsll_vx_i64m1_vl (vint64m1_t op1, uint8_t op2, _VL_T vl);
vint64m2_t vsll_vv_i64m2_vl (vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vsll_vx_i64m2_vl (vint64m2_t op1, uint8_t op2, _VL_T vl);
vint64m4_t vsll_vv_i64m4_vl (vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vsll_vx_i64m4_vl (vint64m4_t op1, uint8_t op2, _VL_T vl);
vint64m8_t vsll_vv_i64m8_vl (vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vsll_vx_i64m8_vl (vint64m8_t op1, uint8_t op2, _VL_T vl);
vuint8m1_t vsll_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsll_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsll_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsll_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsll_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsll_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsll_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsll_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsll_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsll_vx_u16m1_vl (vuint16m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vsll_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsll_vx_u16m2_vl (vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vsll_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsll_vx_u16m4_vl (vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vsll_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsll_vx_u16m8_vl (vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vsll_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsll_vx_u32m1_vl (vuint32m1_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vsll_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsll_vx_u32m2_vl (vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vsll_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsll_vx_u32m4_vl (vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint32m8_t vsll_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsll_vx_u32m8_vl (vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint64m1_t vsll_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsll_vx_u64m1_vl (vuint64m1_t op1, uint8_t op2, _VL_T vl);
vuint64m2_t vsll_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsll_vx_u64m2_vl (vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint64m4_t vsll_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsll_vx_u64m4_vl (vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint64m8_t vsll_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsll_vx_u64m8_vl (vuint64m8_t op1, uint8_t op2, _VL_T vl);
vuint8m1_t vsrl_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsrl_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsrl_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsrl_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsrl_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsrl_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsrl_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsrl_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsrl_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsrl_vx_u16m1_vl (vuint16m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vsrl_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsrl_vx_u16m2_vl (vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vsrl_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsrl_vx_u16m4_vl (vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vsrl_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsrl_vx_u16m8_vl (vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vsrl_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsrl_vx_u32m1_vl (vuint32m1_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vsrl_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsrl_vx_u32m2_vl (vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vsrl_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsrl_vx_u32m4_vl (vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint32m8_t vsrl_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsrl_vx_u32m8_vl (vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint64m1_t vsrl_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsrl_vx_u64m1_vl (vuint64m1_t op1, uint8_t op2, _VL_T vl);
vuint64m2_t vsrl_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsrl_vx_u64m2_vl (vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint64m4_t vsrl_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsrl_vx_u64m4_vl (vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint64m8_t vsrl_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsrl_vx_u64m8_vl (vuint64m8_t op1, uint8_t op2, _VL_T vl);
vint8m1_t vsra_vv_i8m1_vl (vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vsra_vx_i8m1_vl (vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vsra_vv_i8m2_vl (vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vsra_vx_i8m2_vl (vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vsra_vv_i8m4_vl (vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vsra_vx_i8m4_vl (vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vsra_vv_i8m8_vl (vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vsra_vx_i8m8_vl (vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vsra_vv_i16m1_vl (vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vsra_vx_i16m1_vl (vint16m1_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vsra_vv_i16m2_vl (vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vsra_vx_i16m2_vl (vint16m2_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vsra_vv_i16m4_vl (vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vsra_vx_i16m4_vl (vint16m4_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vsra_vv_i16m8_vl (vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vsra_vx_i16m8_vl (vint16m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vsra_vv_i32m1_vl (vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vsra_vx_i32m1_vl (vint32m1_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vsra_vv_i32m2_vl (vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vsra_vx_i32m2_vl (vint32m2_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vsra_vv_i32m4_vl (vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vsra_vx_i32m4_vl (vint32m4_t op1, uint8_t op2, _VL_T vl);
vint32m8_t vsra_vv_i32m8_vl (vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vsra_vx_i32m8_vl (vint32m8_t op1, uint8_t op2, _VL_T vl);
vint64m1_t vsra_vv_i64m1_vl (vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vsra_vx_i64m1_vl (vint64m1_t op1, uint8_t op2, _VL_T vl);
vint64m2_t vsra_vv_i64m2_vl (vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vsra_vx_i64m2_vl (vint64m2_t op1, uint8_t op2, _VL_T vl);
vint64m4_t vsra_vv_i64m4_vl (vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vsra_vx_i64m4_vl (vint64m4_t op1, uint8_t op2, _VL_T vl);
vint64m8_t vsra_vv_i64m8_vl (vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vsra_vx_i64m8_vl (vint64m8_t op1, uint8_t op2, _VL_T vl);
// masked functions
vint8m1_t vsll_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vsll_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vsll_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vsll_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vsll_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vsll_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vsll_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vsll_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vsll_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vsll_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vsll_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vsll_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vsll_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vsll_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vsll_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vsll_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vsll_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vsll_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vsll_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vsll_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vsll_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vsll_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, uint8_t op2, _VL_T vl);
vint32m8_t vsll_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vsll_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, uint8_t op2, _VL_T vl);
vint64m1_t vsll_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vsll_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, uint8_t op2, _VL_T vl);
vint64m2_t vsll_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vsll_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, uint8_t op2, _VL_T vl);
vint64m4_t vsll_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vsll_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, uint8_t op2, _VL_T vl);
vint64m8_t vsll_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vsll_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, uint8_t op2, _VL_T vl);
vuint8m1_t vsll_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsll_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsll_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsll_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsll_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsll_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsll_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsll_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsll_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsll_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vsll_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsll_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vsll_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsll_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vsll_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsll_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vsll_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsll_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vsll_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsll_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vsll_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsll_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint32m8_t vsll_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsll_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint64m1_t vsll_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsll_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint8_t op2, _VL_T vl);
vuint64m2_t vsll_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsll_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint64m4_t vsll_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsll_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint64m8_t vsll_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsll_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint8_t op2, _VL_T vl);
vuint8m1_t vsrl_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsrl_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsrl_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsrl_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsrl_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsrl_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsrl_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsrl_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsrl_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsrl_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vsrl_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsrl_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vsrl_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsrl_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vsrl_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsrl_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vsrl_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsrl_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vsrl_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsrl_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vsrl_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsrl_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint32m8_t vsrl_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsrl_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint64m1_t vsrl_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsrl_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint8_t op2, _VL_T vl);
vuint64m2_t vsrl_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsrl_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint64m4_t vsrl_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsrl_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint64m8_t vsrl_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsrl_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint8_t op2, _VL_T vl);
vint8m1_t vsra_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vsra_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vsra_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vsra_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vsra_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vsra_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vsra_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vsra_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vsra_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vsra_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vsra_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vsra_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vsra_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vsra_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vsra_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vsra_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vsra_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vsra_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vsra_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vsra_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vsra_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vsra_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, uint8_t op2, _VL_T vl);
vint32m8_t vsra_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vsra_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, uint8_t op2, _VL_T vl);
vint64m1_t vsra_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vsra_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, uint8_t op2, _VL_T vl);
vint64m2_t vsra_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vsra_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, uint8_t op2, _VL_T vl);
vint64m4_t vsra_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vsra_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, uint8_t op2, _VL_T vl);
vint64m8_t vsra_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vsra_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, uint8_t op2, _VL_T vl);
```
### [Vector Narrowing Integer Right Shift Functions](rvv-intrinsic-api.md#126-vector-narrowing-integer-right-shift-operations):

**Prototypes:**
``` C
vuint8m1_t vnsrl_wv_u8m1_vl (vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnsrl_wx_u8m1_vl (vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vnsrl_wv_u8m2_vl (vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnsrl_wx_u8m2_vl (vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vnsrl_wv_u8m4_vl (vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnsrl_wx_u8m4_vl (vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vnsrl_wv_u16m1_vl (vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnsrl_wx_u16m1_vl (vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vnsrl_wv_u16m2_vl (vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnsrl_wx_u16m2_vl (vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vnsrl_wv_u16m4_vl (vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnsrl_wx_u16m4_vl (vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vnsrl_wv_u32m1_vl (vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnsrl_wx_u32m1_vl (vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vnsrl_wv_u32m2_vl (vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnsrl_wx_u32m2_vl (vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vnsrl_wv_u32m4_vl (vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnsrl_wx_u32m4_vl (vuint64m8_t op1, uint8_t op2, _VL_T vl);
vint8m1_t vnsra_wv_i8m1_vl (vint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vnsra_wx_i8m1_vl (vint16m2_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vnsra_wv_i8m2_vl (vint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vnsra_wx_i8m2_vl (vint16m4_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vnsra_wv_i8m4_vl (vint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vnsra_wx_i8m4_vl (vint16m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vnsra_wv_i16m1_vl (vint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vnsra_wx_i16m1_vl (vint32m2_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vnsra_wv_i16m2_vl (vint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vnsra_wx_i16m2_vl (vint32m4_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vnsra_wv_i16m4_vl (vint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vnsra_wx_i16m4_vl (vint32m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vnsra_wv_i32m1_vl (vint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vnsra_wx_i32m1_vl (vint64m2_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vnsra_wv_i32m2_vl (vint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vnsra_wx_i32m2_vl (vint64m4_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vnsra_wv_i32m4_vl (vint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vnsra_wx_i32m4_vl (vint64m8_t op1, uint8_t op2, _VL_T vl);
// masked functions
vuint8m1_t vnsrl_wv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnsrl_wx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vnsrl_wv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnsrl_wx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vnsrl_wv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnsrl_wx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vnsrl_wv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnsrl_wx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vnsrl_wv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnsrl_wx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vnsrl_wv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnsrl_wx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vnsrl_wv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnsrl_wx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vnsrl_wv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnsrl_wx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vnsrl_wv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnsrl_wx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint64m8_t op1, uint8_t op2, _VL_T vl);
vint8m1_t vnsra_wv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vnsra_wx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint16m2_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vnsra_wv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vnsra_wx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint16m4_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vnsra_wv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vnsra_wx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint16m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vnsra_wv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vnsra_wx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint32m2_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vnsra_wv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vnsra_wx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint32m4_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vnsra_wv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vnsra_wx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint32m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vnsra_wv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vnsra_wx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint64m2_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vnsra_wv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vnsra_wx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint64m4_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vnsra_wv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vnsra_wx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint64m8_t op1, uint8_t op2, _VL_T vl);
```
### [Vector Integer Comparison Functions](rvv-intrinsic-api.md#127-vector-integer-comparison-operations):

**Prototypes:**
``` C
vbool8_t vmseq_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmseq_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmseq_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmseq_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmseq_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmseq_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmseq_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmseq_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmseq_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmseq_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmseq_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmseq_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmseq_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmseq_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmseq_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmseq_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmseq_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmseq_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmseq_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmseq_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmseq_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmseq_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmseq_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmseq_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmseq_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmseq_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmseq_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmseq_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmseq_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmseq_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmseq_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmseq_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmseq_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmseq_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmseq_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmseq_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmseq_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmseq_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmseq_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmseq_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmseq_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmseq_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmseq_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmseq_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmseq_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmseq_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmseq_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmseq_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmseq_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmseq_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmseq_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmseq_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmseq_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmseq_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmseq_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmseq_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmseq_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmseq_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmseq_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmseq_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmseq_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmseq_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmseq_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmseq_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsne_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsne_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsne_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsne_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsne_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsne_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsne_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsne_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsne_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsne_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsne_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsne_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsne_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsne_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsne_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsne_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsne_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsne_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsne_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsne_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsne_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsne_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsne_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsne_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsne_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsne_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsne_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsne_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsne_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsne_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsne_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsne_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsne_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsne_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsne_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsne_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsne_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsne_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsne_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsne_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsne_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsne_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsne_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsne_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsne_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsne_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsne_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsne_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsne_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsne_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsne_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsne_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsne_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsne_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsne_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsne_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsne_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsne_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsne_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsne_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsne_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsne_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsne_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsne_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmslt_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmslt_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmslt_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmslt_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmslt_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmslt_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmslt_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmslt_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmslt_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmslt_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmslt_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmslt_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmslt_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmslt_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmslt_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmslt_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmslt_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmslt_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmslt_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmslt_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmslt_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmslt_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmslt_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmslt_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmslt_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmslt_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmslt_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmslt_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmslt_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmslt_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmslt_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmslt_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsltu_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsltu_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsltu_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsltu_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsltu_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsltu_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsltu_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsltu_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsltu_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsltu_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsltu_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsltu_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsltu_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsltu_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsltu_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsltu_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsltu_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsltu_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsltu_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsltu_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsltu_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsltu_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsltu_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsltu_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsle_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsle_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsle_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsle_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsle_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsle_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsle_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsle_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsle_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsle_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsle_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsle_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsle_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsle_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsle_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsle_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsle_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsle_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsle_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsle_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsle_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsle_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsle_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsle_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsle_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsle_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsle_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsle_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsle_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsle_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsle_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsle_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsleu_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsleu_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsleu_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsleu_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsleu_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsleu_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsleu_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsleu_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsleu_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsleu_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsleu_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsleu_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsleu_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsleu_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsleu_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsleu_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsleu_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsleu_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsleu_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsleu_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsleu_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsleu_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsleu_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsleu_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsgt_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsgt_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsgt_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsgt_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsgt_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsgt_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsgt_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsgt_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsgt_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsgt_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsgt_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsgt_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsgt_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsgt_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsgt_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsgt_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsgt_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsgt_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsgt_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsgt_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsgt_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsgt_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsgt_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsgt_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsgtu_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsgtu_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsgtu_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsgtu_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsgtu_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsgtu_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsgtu_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsgtu_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsgtu_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsgtu_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsgtu_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsgtu_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsgtu_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsgtu_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsgtu_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsgtu_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsgtu_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsgtu_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsgtu_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsgtu_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsgtu_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsgtu_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsgtu_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsgtu_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsge_vv_i8m1_b8_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsge_vx_i8m1_b8_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsge_vv_i8m2_b4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsge_vx_i8m2_b4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsge_vv_i8m4_b2_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsge_vx_i8m4_b2_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsge_vv_i8m8_b1_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsge_vx_i8m8_b1_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsge_vv_i16m1_b16_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsge_vx_i16m1_b16_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsge_vv_i16m2_b8_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsge_vx_i16m2_b8_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsge_vv_i16m4_b4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsge_vx_i16m4_b4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsge_vv_i16m8_b2_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsge_vx_i16m8_b2_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsge_vv_i32m1_b32_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsge_vx_i32m1_b32_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsge_vv_i32m2_b16_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsge_vx_i32m2_b16_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsge_vv_i32m4_b8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsge_vx_i32m4_b8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsge_vv_i32m8_b4_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsge_vx_i32m8_b4_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsge_vv_i64m1_b64_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsge_vx_i64m1_b64_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsge_vv_i64m2_b32_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsge_vx_i64m2_b32_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsge_vv_i64m4_b16_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsge_vx_i64m4_b16_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsge_vv_i64m8_b8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsge_vx_i64m8_b8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u8m1_b8_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u8m1_b8_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsgeu_vv_u8m2_b4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsgeu_vx_u8m2_b4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsgeu_vv_u8m4_b2_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsgeu_vx_u8m4_b2_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsgeu_vv_u8m8_b1_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsgeu_vx_u8m8_b1_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsgeu_vv_u16m1_b16_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsgeu_vx_u16m1_b16_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u16m2_b8_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u16m2_b8_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsgeu_vv_u16m4_b4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsgeu_vx_u16m4_b4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsgeu_vv_u16m8_b2_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsgeu_vx_u16m8_b2_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsgeu_vv_u32m1_b32_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsgeu_vx_u32m1_b32_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsgeu_vv_u32m2_b16_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsgeu_vx_u32m2_b16_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u32m4_b8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u32m4_b8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsgeu_vv_u32m8_b4_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsgeu_vx_u32m8_b4_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsgeu_vv_u64m1_b64_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsgeu_vx_u64m1_b64_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsgeu_vv_u64m2_b32_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsgeu_vx_u64m2_b32_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsgeu_vv_u64m4_b16_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsgeu_vx_u64m4_b16_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u64m8_b8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u64m8_b8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vbool8_t vmseq_vv_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmseq_vx_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmseq_vv_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmseq_vx_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmseq_vv_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmseq_vx_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmseq_vv_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmseq_vx_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmseq_vv_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmseq_vx_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmseq_vv_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmseq_vx_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmseq_vv_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmseq_vx_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmseq_vv_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmseq_vx_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmseq_vv_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmseq_vx_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmseq_vv_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmseq_vx_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmseq_vv_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmseq_vx_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmseq_vv_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmseq_vx_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmseq_vv_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmseq_vx_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmseq_vv_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmseq_vx_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmseq_vv_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmseq_vx_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmseq_vv_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmseq_vx_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmseq_vv_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmseq_vx_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmseq_vv_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmseq_vx_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmseq_vv_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmseq_vx_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmseq_vv_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmseq_vx_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmseq_vv_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmseq_vx_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmseq_vv_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmseq_vx_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmseq_vv_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmseq_vx_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmseq_vv_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmseq_vx_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmseq_vv_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmseq_vx_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmseq_vv_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmseq_vx_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmseq_vv_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmseq_vx_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmseq_vv_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmseq_vx_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmseq_vv_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmseq_vx_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmseq_vv_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmseq_vx_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmseq_vv_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmseq_vx_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmseq_vv_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmseq_vx_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsne_vv_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsne_vx_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsne_vv_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsne_vx_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsne_vv_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsne_vx_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsne_vv_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsne_vx_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsne_vv_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsne_vx_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsne_vv_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsne_vx_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsne_vv_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsne_vx_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsne_vv_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsne_vx_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsne_vv_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsne_vx_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsne_vv_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsne_vx_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsne_vv_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsne_vx_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsne_vv_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsne_vx_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsne_vv_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsne_vx_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsne_vv_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsne_vx_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsne_vv_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsne_vx_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsne_vv_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsne_vx_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsne_vv_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsne_vx_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsne_vv_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsne_vx_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsne_vv_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsne_vx_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsne_vv_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsne_vx_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsne_vv_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsne_vx_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsne_vv_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsne_vx_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsne_vv_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsne_vx_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsne_vv_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsne_vx_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsne_vv_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsne_vx_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsne_vv_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsne_vx_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsne_vv_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsne_vx_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsne_vv_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsne_vx_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsne_vv_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsne_vx_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsne_vv_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsne_vx_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsne_vv_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsne_vx_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsne_vv_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsne_vx_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmslt_vv_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmslt_vx_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmslt_vv_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmslt_vx_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmslt_vv_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmslt_vx_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmslt_vv_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmslt_vx_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmslt_vv_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmslt_vx_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmslt_vv_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmslt_vx_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmslt_vv_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmslt_vx_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmslt_vv_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmslt_vx_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmslt_vv_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmslt_vx_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmslt_vv_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmslt_vx_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmslt_vv_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmslt_vx_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmslt_vv_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmslt_vx_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmslt_vv_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmslt_vx_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmslt_vv_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmslt_vx_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmslt_vv_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmslt_vx_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmslt_vv_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmslt_vx_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsltu_vv_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsltu_vx_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsltu_vv_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsltu_vx_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsltu_vv_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsltu_vx_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsltu_vv_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsltu_vx_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsltu_vv_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsltu_vx_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsltu_vv_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsltu_vx_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsltu_vv_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsltu_vx_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsltu_vv_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsltu_vx_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsltu_vv_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsltu_vx_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsltu_vv_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsltu_vx_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsltu_vv_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsltu_vx_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsltu_vv_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsltu_vx_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsltu_vv_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsltu_vx_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsle_vv_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsle_vx_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsle_vv_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsle_vx_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsle_vv_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsle_vx_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsle_vv_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsle_vx_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsle_vv_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsle_vx_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsle_vv_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsle_vx_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsle_vv_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsle_vx_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsle_vv_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsle_vx_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsle_vv_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsle_vx_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsle_vv_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsle_vx_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsle_vv_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsle_vx_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsle_vv_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsle_vx_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsle_vv_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsle_vx_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsle_vv_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsle_vx_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsle_vv_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsle_vx_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsle_vv_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsle_vx_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsleu_vv_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsleu_vx_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsleu_vv_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsleu_vx_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsleu_vv_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsleu_vx_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsleu_vv_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsleu_vx_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsleu_vv_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsleu_vx_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsleu_vv_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsleu_vx_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsleu_vv_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsleu_vx_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsleu_vv_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsleu_vx_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsleu_vv_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsleu_vx_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsleu_vv_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsleu_vx_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsleu_vv_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsleu_vx_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsleu_vv_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsleu_vx_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsleu_vv_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsleu_vx_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsgt_vv_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsgt_vx_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsgt_vv_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsgt_vx_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsgt_vv_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsgt_vx_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsgt_vv_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsgt_vx_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsgt_vv_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsgt_vx_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsgt_vv_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsgt_vx_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsgt_vv_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsgt_vx_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsgt_vv_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsgt_vx_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsgt_vv_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsgt_vx_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsgt_vv_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsgt_vx_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsgt_vv_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsgt_vx_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsgt_vv_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsgt_vx_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsgt_vv_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsgt_vx_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsgtu_vv_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsgtu_vx_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsgtu_vv_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsgtu_vx_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsgtu_vv_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsgtu_vx_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsgtu_vv_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsgtu_vx_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsgtu_vv_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsgtu_vx_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsgtu_vv_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsgtu_vx_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsgtu_vv_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsgtu_vx_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsgtu_vv_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsgtu_vx_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsgtu_vv_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsgtu_vx_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsgtu_vv_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsgtu_vx_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsgtu_vv_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsgtu_vx_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsgtu_vv_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsgtu_vx_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsgtu_vv_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsgtu_vx_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsge_vv_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vbool8_t vmsge_vx_i8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vbool4_t vmsge_vv_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vbool4_t vmsge_vx_i8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vbool2_t vmsge_vv_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vbool2_t vmsge_vx_i8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vbool1_t vmsge_vv_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vbool1_t vmsge_vx_i8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vbool16_t vmsge_vv_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vbool16_t vmsge_vx_i16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vbool8_t vmsge_vv_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vbool8_t vmsge_vx_i16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vbool4_t vmsge_vv_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vbool4_t vmsge_vx_i16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vbool2_t vmsge_vv_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vbool2_t vmsge_vx_i16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vbool32_t vmsge_vv_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vbool32_t vmsge_vx_i32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vbool16_t vmsge_vv_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vbool16_t vmsge_vx_i32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vbool8_t vmsge_vv_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vbool8_t vmsge_vx_i32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vbool4_t vmsge_vv_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vbool4_t vmsge_vx_i32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vbool64_t vmsge_vv_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vbool64_t vmsge_vx_i64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vbool32_t vmsge_vv_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vbool32_t vmsge_vx_i64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vbool16_t vmsge_vv_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vbool16_t vmsge_vx_i64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsge_vv_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vbool8_t vmsge_vx_i64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u8m1_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vbool4_t vmsgeu_vv_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vbool4_t vmsgeu_vx_u8m2_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vbool2_t vmsgeu_vv_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vbool2_t vmsgeu_vx_u8m4_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vbool1_t vmsgeu_vv_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vbool1_t vmsgeu_vx_u8m8_b1_m_vl (vbool1_t mask, vbool1_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vbool16_t vmsgeu_vv_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vbool16_t vmsgeu_vx_u16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vbool4_t vmsgeu_vv_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vbool4_t vmsgeu_vx_u16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vbool2_t vmsgeu_vv_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vbool2_t vmsgeu_vx_u16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vbool32_t vmsgeu_vv_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vbool32_t vmsgeu_vx_u32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vbool16_t vmsgeu_vv_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vbool16_t vmsgeu_vx_u32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vbool4_t vmsgeu_vv_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vbool4_t vmsgeu_vx_u32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vbool64_t vmsgeu_vv_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vbool64_t vmsgeu_vx_u64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vbool32_t vmsgeu_vv_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vbool32_t vmsgeu_vx_u64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vbool16_t vmsgeu_vv_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vbool16_t vmsgeu_vx_u64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vbool8_t vmsgeu_vv_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vbool8_t vmsgeu_vx_u64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Integer Min/Max Functions](rvv-intrinsic-api.md#128-vector-integer-minmax-operations):

**Prototypes:**
``` C
vint8m1_t vmin_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmin_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmin_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmin_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmin_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmin_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmin_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmin_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmin_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmin_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmin_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmin_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmin_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmin_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmin_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmin_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmin_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmin_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmin_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmin_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmin_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmin_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmin_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmin_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmin_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmin_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmin_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmin_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmin_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmin_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmin_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmin_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vminu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vminu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vminu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vminu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vminu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vminu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vminu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vminu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vminu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vminu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vminu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vminu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vminu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vminu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vminu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vminu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vminu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vminu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vminu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vminu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vminu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vminu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vminu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vminu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vminu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vminu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vminu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vminu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vminu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vminu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vminu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vminu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vmax_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmax_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmax_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmax_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmax_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmax_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmax_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmax_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmax_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmax_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmax_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmax_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmax_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmax_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmax_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmax_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmax_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmax_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmax_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmax_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmax_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmax_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmax_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmax_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmax_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmax_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmax_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmax_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmax_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmax_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmax_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmax_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vmaxu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmaxu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vmaxu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmaxu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vmaxu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmaxu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vmaxu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmaxu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vmaxu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmaxu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vmaxu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmaxu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vmaxu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmaxu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vmaxu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmaxu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vmaxu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmaxu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vmaxu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmaxu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vmaxu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmaxu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vmaxu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmaxu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vmaxu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmaxu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vmaxu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmaxu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vmaxu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmaxu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vmaxu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmaxu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vint8m1_t vmin_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmin_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmin_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmin_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmin_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmin_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmin_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmin_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmin_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmin_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmin_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmin_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmin_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmin_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmin_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmin_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmin_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmin_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmin_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmin_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmin_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmin_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmin_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmin_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmin_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmin_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmin_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmin_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmin_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmin_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmin_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmin_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vminu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vminu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vminu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vminu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vminu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vminu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vminu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vminu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vminu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vminu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vminu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vminu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vminu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vminu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vminu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vminu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vminu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vminu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vminu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vminu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vminu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vminu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vminu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vminu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vminu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vminu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vminu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vminu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vminu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vminu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vminu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vminu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vmax_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmax_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmax_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmax_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmax_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmax_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmax_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmax_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmax_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmax_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmax_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmax_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmax_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmax_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmax_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmax_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmax_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmax_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmax_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmax_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmax_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmax_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmax_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmax_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmax_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmax_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmax_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmax_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmax_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmax_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmax_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmax_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vmaxu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmaxu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vmaxu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmaxu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vmaxu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmaxu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vmaxu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmaxu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vmaxu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmaxu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vmaxu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmaxu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vmaxu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmaxu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vmaxu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmaxu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vmaxu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmaxu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vmaxu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmaxu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vmaxu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmaxu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vmaxu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmaxu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vmaxu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmaxu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vmaxu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmaxu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vmaxu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmaxu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vmaxu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmaxu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Single-Width Integer Multiply Functions](rvv-intrinsic-api.md#129-vector-single-width-integer-multiply-operations):

**Prototypes:**
``` C
vint8m1_t vmul_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmul_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmul_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmul_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmul_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmul_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmul_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmul_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmul_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmul_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmul_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmul_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmul_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmul_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmul_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmul_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmul_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmul_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmul_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmul_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmul_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmul_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmul_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmul_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmul_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmul_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmul_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmul_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmul_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmul_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmul_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmul_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vmul_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmul_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vmul_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmul_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vmul_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmul_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vmul_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmul_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vmul_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmul_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vmul_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmul_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vmul_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmul_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vmul_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmul_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vmul_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmul_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vmul_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmul_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vmul_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmul_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vmul_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmul_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vmul_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmul_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vmul_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmul_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vmul_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmul_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vmul_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmul_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vmulh_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmulh_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmulh_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmulh_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmulh_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmulh_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmulh_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmulh_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmulh_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmulh_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmulh_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmulh_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmulh_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmulh_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmulh_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmulh_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmulh_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmulh_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmulh_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmulh_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmulh_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmulh_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmulh_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmulh_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmulh_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmulh_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmulh_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmulh_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmulh_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmulh_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmulh_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmulh_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vmulhu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmulhu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vmulhu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmulhu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vmulhu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmulhu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vmulhu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmulhu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vmulhu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmulhu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vmulhu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmulhu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vmulhu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmulhu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vmulhu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmulhu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vmulhu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmulhu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vmulhu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmulhu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vmulhu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmulhu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vmulhu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmulhu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vmulhu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmulhu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vmulhu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmulhu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vmulhu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmulhu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vmulhu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmulhu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vmulhsu_vv_i8m1_vl (vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vmulhsu_vx_i8m1_vl (vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vmulhsu_vv_i8m2_vl (vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vmulhsu_vx_i8m2_vl (vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vmulhsu_vv_i8m4_vl (vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vmulhsu_vx_i8m4_vl (vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vmulhsu_vv_i8m8_vl (vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vmulhsu_vx_i8m8_vl (vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vmulhsu_vv_i16m1_vl (vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vmulhsu_vx_i16m1_vl (vint16m1_t op1, uint16_t op2, _VL_T vl);
vint16m2_t vmulhsu_vv_i16m2_vl (vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vmulhsu_vx_i16m2_vl (vint16m2_t op1, uint16_t op2, _VL_T vl);
vint16m4_t vmulhsu_vv_i16m4_vl (vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vmulhsu_vx_i16m4_vl (vint16m4_t op1, uint16_t op2, _VL_T vl);
vint16m8_t vmulhsu_vv_i16m8_vl (vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vmulhsu_vx_i16m8_vl (vint16m8_t op1, uint16_t op2, _VL_T vl);
vint32m1_t vmulhsu_vv_i32m1_vl (vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vmulhsu_vx_i32m1_vl (vint32m1_t op1, uint32_t op2, _VL_T vl);
vint32m2_t vmulhsu_vv_i32m2_vl (vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vmulhsu_vx_i32m2_vl (vint32m2_t op1, uint32_t op2, _VL_T vl);
vint32m4_t vmulhsu_vv_i32m4_vl (vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vmulhsu_vx_i32m4_vl (vint32m4_t op1, uint32_t op2, _VL_T vl);
vint32m8_t vmulhsu_vv_i32m8_vl (vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vmulhsu_vx_i32m8_vl (vint32m8_t op1, uint32_t op2, _VL_T vl);
vint64m1_t vmulhsu_vv_i64m1_vl (vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vmulhsu_vx_i64m1_vl (vint64m1_t op1, uint64_t op2, _VL_T vl);
vint64m2_t vmulhsu_vv_i64m2_vl (vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vmulhsu_vx_i64m2_vl (vint64m2_t op1, uint64_t op2, _VL_T vl);
vint64m4_t vmulhsu_vv_i64m4_vl (vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vmulhsu_vx_i64m4_vl (vint64m4_t op1, uint64_t op2, _VL_T vl);
vint64m8_t vmulhsu_vv_i64m8_vl (vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vmulhsu_vx_i64m8_vl (vint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vint8m1_t vmul_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmul_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmul_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmul_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmul_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmul_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmul_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmul_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmul_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmul_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmul_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmul_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmul_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmul_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmul_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmul_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmul_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmul_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmul_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmul_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmul_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmul_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmul_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmul_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmul_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmul_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmul_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmul_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmul_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmul_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmul_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmul_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vmul_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmul_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vmul_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmul_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vmul_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmul_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vmul_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmul_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vmul_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmul_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vmul_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmul_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vmul_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmul_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vmul_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmul_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vmul_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmul_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vmul_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmul_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vmul_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmul_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vmul_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmul_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vmul_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmul_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vmul_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmul_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vmul_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmul_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vmul_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmul_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vmulh_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmulh_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmulh_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmulh_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmulh_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmulh_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmulh_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmulh_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmulh_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmulh_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmulh_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmulh_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmulh_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmulh_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmulh_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmulh_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmulh_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmulh_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmulh_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmulh_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmulh_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmulh_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmulh_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmulh_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmulh_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmulh_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmulh_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmulh_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmulh_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmulh_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmulh_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmulh_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vmulhu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmulhu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vmulhu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmulhu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vmulhu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmulhu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vmulhu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmulhu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vmulhu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmulhu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vmulhu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmulhu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vmulhu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmulhu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vmulhu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmulhu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vmulhu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmulhu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vmulhu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmulhu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vmulhu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmulhu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vmulhu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmulhu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vmulhu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmulhu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vmulhu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmulhu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vmulhu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmulhu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vmulhu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmulhu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vmulhsu_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vmulhsu_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vmulhsu_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vmulhsu_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vmulhsu_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vmulhsu_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vmulhsu_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vmulhsu_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vmulhsu_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vmulhsu_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, uint16_t op2, _VL_T vl);
vint16m2_t vmulhsu_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vmulhsu_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, uint16_t op2, _VL_T vl);
vint16m4_t vmulhsu_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vmulhsu_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, uint16_t op2, _VL_T vl);
vint16m8_t vmulhsu_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vmulhsu_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, uint16_t op2, _VL_T vl);
vint32m1_t vmulhsu_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vmulhsu_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, uint32_t op2, _VL_T vl);
vint32m2_t vmulhsu_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vmulhsu_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, uint32_t op2, _VL_T vl);
vint32m4_t vmulhsu_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vmulhsu_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, uint32_t op2, _VL_T vl);
vint32m8_t vmulhsu_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vmulhsu_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, uint32_t op2, _VL_T vl);
vint64m1_t vmulhsu_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vmulhsu_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, uint64_t op2, _VL_T vl);
vint64m2_t vmulhsu_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vmulhsu_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, uint64_t op2, _VL_T vl);
vint64m4_t vmulhsu_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vmulhsu_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, uint64_t op2, _VL_T vl);
vint64m8_t vmulhsu_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vmulhsu_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Integer Divide Functions](rvv-intrinsic-api.md#1210-vector-integer-divide-operations):

**Prototypes:**
``` C
vint8m1_t vdiv_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vdiv_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vdiv_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vdiv_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vdiv_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vdiv_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vdiv_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vdiv_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vdiv_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vdiv_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vdiv_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vdiv_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vdiv_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vdiv_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vdiv_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vdiv_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vdiv_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vdiv_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vdiv_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vdiv_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vdiv_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vdiv_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vdiv_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vdiv_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vdiv_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vdiv_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vdiv_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vdiv_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vdiv_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vdiv_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vdiv_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vdiv_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vdivu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vdivu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vdivu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vdivu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vdivu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vdivu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vdivu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vdivu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vdivu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vdivu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vdivu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vdivu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vdivu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vdivu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vdivu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vdivu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vdivu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vdivu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vdivu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vdivu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vdivu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vdivu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vdivu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vdivu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vdivu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vdivu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vdivu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vdivu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vdivu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vdivu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vdivu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vdivu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vrem_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vrem_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vrem_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vrem_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vrem_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vrem_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vrem_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vrem_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vrem_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vrem_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vrem_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vrem_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vrem_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vrem_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vrem_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vrem_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vrem_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vrem_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vrem_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vrem_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vrem_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vrem_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vrem_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vrem_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vrem_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vrem_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vrem_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vrem_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vrem_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vrem_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vrem_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vrem_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vremu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vremu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vremu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vremu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vremu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vremu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vremu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vremu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vremu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vremu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vremu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vremu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vremu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vremu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vremu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vremu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vremu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vremu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vremu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vremu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vremu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vremu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vremu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vremu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vremu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vremu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vremu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vremu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vremu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vremu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vremu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vremu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vint8m1_t vdiv_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vdiv_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vdiv_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vdiv_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vdiv_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vdiv_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vdiv_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vdiv_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vdiv_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vdiv_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vdiv_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vdiv_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vdiv_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vdiv_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vdiv_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vdiv_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vdiv_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vdiv_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vdiv_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vdiv_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vdiv_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vdiv_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vdiv_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vdiv_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vdiv_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vdiv_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vdiv_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vdiv_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vdiv_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vdiv_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vdiv_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vdiv_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vdivu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vdivu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vdivu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vdivu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vdivu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vdivu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vdivu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vdivu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vdivu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vdivu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vdivu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vdivu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vdivu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vdivu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vdivu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vdivu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vdivu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vdivu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vdivu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vdivu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vdivu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vdivu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vdivu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vdivu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vdivu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vdivu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vdivu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vdivu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vdivu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vdivu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vdivu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vdivu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vrem_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vrem_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vrem_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vrem_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vrem_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vrem_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vrem_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vrem_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vrem_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vrem_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vrem_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vrem_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vrem_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vrem_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vrem_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vrem_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vrem_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vrem_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vrem_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vrem_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vrem_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vrem_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vrem_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vrem_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vrem_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vrem_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vrem_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vrem_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vrem_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vrem_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vrem_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vrem_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vremu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vremu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vremu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vremu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vremu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vremu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vremu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vremu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vremu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vremu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vremu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vremu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vremu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vremu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vremu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vremu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vremu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vremu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vremu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vremu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vremu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vremu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vremu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vremu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vremu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vremu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vremu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vremu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vremu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vremu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vremu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vremu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Widening Integer Multiply Functions](rvv-intrinsic-api.md#1211-vector-widening-integer-multiply-operations):

**Prototypes:**
``` C
vint16m2_t vwmul_vv_i16m2_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwmul_vx_i16m2_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwmul_vv_i16m4_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwmul_vx_i16m4_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwmul_vv_i16m8_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwmul_vx_i16m8_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint32m2_t vwmul_vv_i32m2_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwmul_vx_i32m2_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwmul_vv_i32m4_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwmul_vx_i32m4_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwmul_vv_i32m8_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwmul_vx_i32m8_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint64m2_t vwmul_vv_i64m2_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwmul_vx_i64m2_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwmul_vv_i64m4_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwmul_vx_i64m4_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwmul_vv_i64m8_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwmul_vx_i64m8_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vuint16m2_t vwmulu_vv_u16m2_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwmulu_vx_u16m2_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwmulu_vv_u16m4_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwmulu_vx_u16m4_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwmulu_vv_u16m8_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwmulu_vx_u16m8_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vwmulu_vv_u32m2_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwmulu_vx_u32m2_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwmulu_vv_u32m4_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwmulu_vx_u32m4_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwmulu_vv_u32m8_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwmulu_vx_u32m8_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint64m2_t vwmulu_vv_u64m2_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwmulu_vx_u64m2_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwmulu_vv_u64m4_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwmulu_vx_u64m4_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwmulu_vv_u64m8_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwmulu_vx_u64m8_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vint16m2_t vwmulsu_vv_i16m2_vl (vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint16m2_t vwmulsu_vx_i16m2_vl (vint8m1_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vwmulsu_vv_i16m4_vl (vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint16m4_t vwmulsu_vx_i16m4_vl (vint8m2_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vwmulsu_vv_i16m8_vl (vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint16m8_t vwmulsu_vx_i16m8_vl (vint8m4_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vwmulsu_vv_i32m2_vl (vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint32m2_t vwmulsu_vx_i32m2_vl (vint16m1_t op1, uint16_t op2, _VL_T vl);
vint32m4_t vwmulsu_vv_i32m4_vl (vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vwmulsu_vx_i32m4_vl (vint16m2_t op1, uint16_t op2, _VL_T vl);
vint32m8_t vwmulsu_vv_i32m8_vl (vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint32m8_t vwmulsu_vx_i32m8_vl (vint16m4_t op1, uint16_t op2, _VL_T vl);
vint64m2_t vwmulsu_vv_i64m2_vl (vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint64m2_t vwmulsu_vx_i64m2_vl (vint32m1_t op1, uint32_t op2, _VL_T vl);
vint64m4_t vwmulsu_vv_i64m4_vl (vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint64m4_t vwmulsu_vx_i64m4_vl (vint32m2_t op1, uint32_t op2, _VL_T vl);
vint64m8_t vwmulsu_vv_i64m8_vl (vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint64m8_t vwmulsu_vx_i64m8_vl (vint32m4_t op1, uint32_t op2, _VL_T vl);
// masked functions
vint16m2_t vwmul_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwmul_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint16m4_t vwmul_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwmul_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint16m8_t vwmul_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwmul_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint32m2_t vwmul_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwmul_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint32m4_t vwmul_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwmul_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint32m8_t vwmul_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwmul_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint64m2_t vwmul_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwmul_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint64m4_t vwmul_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwmul_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint64m8_t vwmul_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwmul_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vuint16m2_t vwmulu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwmulu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vwmulu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwmulu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vwmulu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwmulu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vwmulu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwmulu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint32m4_t vwmulu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwmulu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint32m8_t vwmulu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwmulu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint64m2_t vwmulu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwmulu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint64m4_t vwmulu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwmulu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint64m8_t vwmulu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwmulu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vint16m2_t vwmulsu_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint16m2_t vwmulsu_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vwmulsu_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint16m4_t vwmulsu_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vwmulsu_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint16m8_t vwmulsu_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vwmulsu_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint32m2_t vwmulsu_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t op1, uint16_t op2, _VL_T vl);
vint32m4_t vwmulsu_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vwmulsu_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t op1, uint16_t op2, _VL_T vl);
vint32m8_t vwmulsu_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint32m8_t vwmulsu_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t op1, uint16_t op2, _VL_T vl);
vint64m2_t vwmulsu_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint64m2_t vwmulsu_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t op1, uint32_t op2, _VL_T vl);
vint64m4_t vwmulsu_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint64m4_t vwmulsu_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t op1, uint32_t op2, _VL_T vl);
vint64m8_t vwmulsu_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint64m8_t vwmulsu_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t op1, uint32_t op2, _VL_T vl);
```
### [Vector Single-Width Integer Multiply-Add Functions](rvv-intrinsic-api.md#1212-vector-single-width-integer-multiply-add-operations):

**Prototypes:**
``` C
vint8m1_t vmacc_vv_i8m1_vl (vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmacc_vx_i8m1_vl (vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vmacc_vv_i8m2_vl (vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmacc_vx_i8m2_vl (vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vmacc_vv_i8m4_vl (vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmacc_vx_i8m4_vl (vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vmacc_vv_i8m8_vl (vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmacc_vx_i8m8_vl (vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vmacc_vv_i16m1_vl (vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmacc_vx_i16m1_vl (vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vmacc_vv_i16m2_vl (vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmacc_vx_i16m2_vl (vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vmacc_vv_i16m4_vl (vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmacc_vx_i16m4_vl (vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vmacc_vv_i16m8_vl (vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmacc_vx_i16m8_vl (vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vmacc_vv_i32m1_vl (vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmacc_vx_i32m1_vl (vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vmacc_vv_i32m2_vl (vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmacc_vx_i32m2_vl (vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vmacc_vv_i32m4_vl (vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmacc_vx_i32m4_vl (vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vmacc_vv_i32m8_vl (vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmacc_vx_i32m8_vl (vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vmacc_vv_i64m1_vl (vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmacc_vx_i64m1_vl (vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vmacc_vv_i64m2_vl (vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmacc_vx_i64m2_vl (vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vmacc_vv_i64m4_vl (vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmacc_vx_i64m4_vl (vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vmacc_vv_i64m8_vl (vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmacc_vx_i64m8_vl (vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vmacc_vv_u8m1_vl (vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmacc_vx_u8m1_vl (vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vmacc_vv_u8m2_vl (vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmacc_vx_u8m2_vl (vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vmacc_vv_u8m4_vl (vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmacc_vx_u8m4_vl (vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vmacc_vv_u8m8_vl (vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmacc_vx_u8m8_vl (vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vmacc_vv_u16m1_vl (vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmacc_vx_u16m1_vl (vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vmacc_vv_u16m2_vl (vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmacc_vx_u16m2_vl (vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vmacc_vv_u16m4_vl (vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmacc_vx_u16m4_vl (vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vmacc_vv_u16m8_vl (vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmacc_vx_u16m8_vl (vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vmacc_vv_u32m1_vl (vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmacc_vx_u32m1_vl (vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vmacc_vv_u32m2_vl (vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmacc_vx_u32m2_vl (vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vmacc_vv_u32m4_vl (vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmacc_vx_u32m4_vl (vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vmacc_vv_u32m8_vl (vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmacc_vx_u32m8_vl (vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vmacc_vv_u64m1_vl (vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmacc_vx_u64m1_vl (vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vmacc_vv_u64m2_vl (vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmacc_vx_u64m2_vl (vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vmacc_vv_u64m4_vl (vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmacc_vx_u64m4_vl (vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vmacc_vv_u64m8_vl (vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmacc_vx_u64m8_vl (vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
vint8m1_t vnmsac_vv_i8m1_vl (vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vnmsac_vx_i8m1_vl (vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vnmsac_vv_i8m2_vl (vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vnmsac_vx_i8m2_vl (vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vnmsac_vv_i8m4_vl (vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vnmsac_vx_i8m4_vl (vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vnmsac_vv_i8m8_vl (vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vnmsac_vx_i8m8_vl (vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vnmsac_vv_i16m1_vl (vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vnmsac_vx_i16m1_vl (vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vnmsac_vv_i16m2_vl (vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vnmsac_vx_i16m2_vl (vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vnmsac_vv_i16m4_vl (vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vnmsac_vx_i16m4_vl (vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vnmsac_vv_i16m8_vl (vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vnmsac_vx_i16m8_vl (vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vnmsac_vv_i32m1_vl (vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vnmsac_vx_i32m1_vl (vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vnmsac_vv_i32m2_vl (vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vnmsac_vx_i32m2_vl (vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vnmsac_vv_i32m4_vl (vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vnmsac_vx_i32m4_vl (vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vnmsac_vv_i32m8_vl (vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vnmsac_vx_i32m8_vl (vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vnmsac_vv_i64m1_vl (vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vnmsac_vx_i64m1_vl (vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vnmsac_vv_i64m2_vl (vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vnmsac_vx_i64m2_vl (vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vnmsac_vv_i64m4_vl (vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vnmsac_vx_i64m4_vl (vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vnmsac_vv_i64m8_vl (vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vnmsac_vx_i64m8_vl (vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vnmsac_vv_u8m1_vl (vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnmsac_vx_u8m1_vl (vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vnmsac_vv_u8m2_vl (vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnmsac_vx_u8m2_vl (vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vnmsac_vv_u8m4_vl (vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnmsac_vx_u8m4_vl (vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vnmsac_vv_u8m8_vl (vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vnmsac_vx_u8m8_vl (vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vnmsac_vv_u16m1_vl (vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnmsac_vx_u16m1_vl (vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vnmsac_vv_u16m2_vl (vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnmsac_vx_u16m2_vl (vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vnmsac_vv_u16m4_vl (vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnmsac_vx_u16m4_vl (vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vnmsac_vv_u16m8_vl (vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vnmsac_vx_u16m8_vl (vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vnmsac_vv_u32m1_vl (vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnmsac_vx_u32m1_vl (vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vnmsac_vv_u32m2_vl (vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnmsac_vx_u32m2_vl (vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vnmsac_vv_u32m4_vl (vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnmsac_vx_u32m4_vl (vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vnmsac_vv_u32m8_vl (vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vnmsac_vx_u32m8_vl (vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vnmsac_vv_u64m1_vl (vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vnmsac_vx_u64m1_vl (vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vnmsac_vv_u64m2_vl (vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vnmsac_vx_u64m2_vl (vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vnmsac_vv_u64m4_vl (vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vnmsac_vx_u64m4_vl (vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vnmsac_vv_u64m8_vl (vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vnmsac_vx_u64m8_vl (vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
vint8m1_t vmadd_vv_i8m1_vl (vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmadd_vx_i8m1_vl (vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vmadd_vv_i8m2_vl (vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmadd_vx_i8m2_vl (vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vmadd_vv_i8m4_vl (vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmadd_vx_i8m4_vl (vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vmadd_vv_i8m8_vl (vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmadd_vx_i8m8_vl (vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vmadd_vv_i16m1_vl (vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmadd_vx_i16m1_vl (vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vmadd_vv_i16m2_vl (vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmadd_vx_i16m2_vl (vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vmadd_vv_i16m4_vl (vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmadd_vx_i16m4_vl (vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vmadd_vv_i16m8_vl (vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmadd_vx_i16m8_vl (vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vmadd_vv_i32m1_vl (vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmadd_vx_i32m1_vl (vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vmadd_vv_i32m2_vl (vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmadd_vx_i32m2_vl (vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vmadd_vv_i32m4_vl (vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmadd_vx_i32m4_vl (vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vmadd_vv_i32m8_vl (vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmadd_vx_i32m8_vl (vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vmadd_vv_i64m1_vl (vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmadd_vx_i64m1_vl (vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vmadd_vv_i64m2_vl (vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmadd_vx_i64m2_vl (vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vmadd_vv_i64m4_vl (vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmadd_vx_i64m4_vl (vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vmadd_vv_i64m8_vl (vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmadd_vx_i64m8_vl (vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vmadd_vv_u8m1_vl (vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmadd_vx_u8m1_vl (vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vmadd_vv_u8m2_vl (vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmadd_vx_u8m2_vl (vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vmadd_vv_u8m4_vl (vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmadd_vx_u8m4_vl (vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vmadd_vv_u8m8_vl (vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmadd_vx_u8m8_vl (vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vmadd_vv_u16m1_vl (vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmadd_vx_u16m1_vl (vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vmadd_vv_u16m2_vl (vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmadd_vx_u16m2_vl (vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vmadd_vv_u16m4_vl (vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmadd_vx_u16m4_vl (vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vmadd_vv_u16m8_vl (vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmadd_vx_u16m8_vl (vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vmadd_vv_u32m1_vl (vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmadd_vx_u32m1_vl (vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vmadd_vv_u32m2_vl (vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmadd_vx_u32m2_vl (vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vmadd_vv_u32m4_vl (vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmadd_vx_u32m4_vl (vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vmadd_vv_u32m8_vl (vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmadd_vx_u32m8_vl (vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vmadd_vv_u64m1_vl (vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmadd_vx_u64m1_vl (vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vmadd_vv_u64m2_vl (vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmadd_vx_u64m2_vl (vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vmadd_vv_u64m4_vl (vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmadd_vx_u64m4_vl (vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vmadd_vv_u64m8_vl (vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmadd_vx_u64m8_vl (vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
vint8m1_t vnmsub_vv_i8m1_vl (vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vnmsub_vx_i8m1_vl (vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vnmsub_vv_i8m2_vl (vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vnmsub_vx_i8m2_vl (vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vnmsub_vv_i8m4_vl (vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vnmsub_vx_i8m4_vl (vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vnmsub_vv_i8m8_vl (vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vnmsub_vx_i8m8_vl (vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vnmsub_vv_i16m1_vl (vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vnmsub_vx_i16m1_vl (vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vnmsub_vv_i16m2_vl (vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vnmsub_vx_i16m2_vl (vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vnmsub_vv_i16m4_vl (vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vnmsub_vx_i16m4_vl (vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vnmsub_vv_i16m8_vl (vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vnmsub_vx_i16m8_vl (vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vnmsub_vv_i32m1_vl (vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vnmsub_vx_i32m1_vl (vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vnmsub_vv_i32m2_vl (vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vnmsub_vx_i32m2_vl (vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vnmsub_vv_i32m4_vl (vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vnmsub_vx_i32m4_vl (vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vnmsub_vv_i32m8_vl (vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vnmsub_vx_i32m8_vl (vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vnmsub_vv_i64m1_vl (vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vnmsub_vx_i64m1_vl (vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vnmsub_vv_i64m2_vl (vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vnmsub_vx_i64m2_vl (vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vnmsub_vv_i64m4_vl (vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vnmsub_vx_i64m4_vl (vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vnmsub_vv_i64m8_vl (vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vnmsub_vx_i64m8_vl (vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vnmsub_vv_u8m1_vl (vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnmsub_vx_u8m1_vl (vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vnmsub_vv_u8m2_vl (vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnmsub_vx_u8m2_vl (vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vnmsub_vv_u8m4_vl (vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnmsub_vx_u8m4_vl (vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vnmsub_vv_u8m8_vl (vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vnmsub_vx_u8m8_vl (vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vnmsub_vv_u16m1_vl (vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnmsub_vx_u16m1_vl (vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vnmsub_vv_u16m2_vl (vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnmsub_vx_u16m2_vl (vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vnmsub_vv_u16m4_vl (vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnmsub_vx_u16m4_vl (vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vnmsub_vv_u16m8_vl (vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vnmsub_vx_u16m8_vl (vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vnmsub_vv_u32m1_vl (vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnmsub_vx_u32m1_vl (vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vnmsub_vv_u32m2_vl (vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnmsub_vx_u32m2_vl (vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vnmsub_vv_u32m4_vl (vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnmsub_vx_u32m4_vl (vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vnmsub_vv_u32m8_vl (vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vnmsub_vx_u32m8_vl (vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vnmsub_vv_u64m1_vl (vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vnmsub_vx_u64m1_vl (vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vnmsub_vv_u64m2_vl (vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vnmsub_vx_u64m2_vl (vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vnmsub_vv_u64m4_vl (vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vnmsub_vx_u64m4_vl (vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vnmsub_vv_u64m8_vl (vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vnmsub_vx_u64m8_vl (vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
// masked functions
vint8m1_t vmacc_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmacc_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vmacc_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmacc_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vmacc_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmacc_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vmacc_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmacc_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vmacc_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmacc_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vmacc_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmacc_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vmacc_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmacc_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vmacc_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmacc_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vmacc_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmacc_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vmacc_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmacc_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vmacc_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmacc_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vmacc_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmacc_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vmacc_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmacc_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vmacc_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmacc_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vmacc_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmacc_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vmacc_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmacc_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vmacc_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmacc_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vmacc_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmacc_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vmacc_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmacc_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vmacc_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmacc_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vmacc_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmacc_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vmacc_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmacc_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vmacc_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmacc_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vmacc_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmacc_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vmacc_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmacc_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vmacc_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmacc_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vmacc_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmacc_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vmacc_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmacc_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vmacc_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmacc_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vmacc_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmacc_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vmacc_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmacc_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vmacc_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmacc_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
vint8m1_t vnmsac_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vnmsac_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vnmsac_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vnmsac_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vnmsac_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vnmsac_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vnmsac_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vnmsac_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vnmsac_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vnmsac_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vnmsac_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vnmsac_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vnmsac_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vnmsac_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vnmsac_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vnmsac_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vnmsac_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vnmsac_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vnmsac_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vnmsac_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vnmsac_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vnmsac_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vnmsac_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vnmsac_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vnmsac_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vnmsac_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vnmsac_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vnmsac_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vnmsac_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vnmsac_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vnmsac_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vnmsac_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vnmsac_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnmsac_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vnmsac_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnmsac_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vnmsac_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnmsac_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vnmsac_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vnmsac_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vnmsac_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnmsac_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vnmsac_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnmsac_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vnmsac_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnmsac_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vnmsac_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vnmsac_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vnmsac_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnmsac_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vnmsac_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnmsac_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vnmsac_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnmsac_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vnmsac_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vnmsac_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vnmsac_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vnmsac_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vnmsac_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vnmsac_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vnmsac_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vnmsac_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vnmsac_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vnmsac_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
vint8m1_t vmadd_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmadd_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vmadd_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmadd_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vmadd_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmadd_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vmadd_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmadd_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vmadd_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmadd_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vmadd_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmadd_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vmadd_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmadd_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vmadd_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmadd_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vmadd_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmadd_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vmadd_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmadd_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vmadd_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmadd_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vmadd_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmadd_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vmadd_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmadd_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vmadd_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmadd_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vmadd_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmadd_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vmadd_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmadd_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vmadd_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmadd_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vmadd_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmadd_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vmadd_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmadd_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vmadd_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmadd_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vmadd_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmadd_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vmadd_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmadd_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vmadd_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmadd_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vmadd_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmadd_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vmadd_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmadd_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vmadd_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmadd_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vmadd_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmadd_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vmadd_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmadd_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vmadd_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmadd_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vmadd_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmadd_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vmadd_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmadd_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vmadd_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmadd_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
vint8m1_t vnmsub_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vnmsub_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint8m2_t vnmsub_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vnmsub_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint8m4_t vnmsub_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vnmsub_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint8m8_t vnmsub_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vnmsub_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t acc, int8_t op1, vint8m8_t op2, _VL_T vl);
vint16m1_t vnmsub_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vnmsub_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint16m2_t vnmsub_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vnmsub_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint16m4_t vnmsub_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vnmsub_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint16m8_t vnmsub_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vnmsub_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, int16_t op1, vint16m8_t op2, _VL_T vl);
vint32m1_t vnmsub_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vnmsub_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint32m2_t vnmsub_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vnmsub_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint32m4_t vnmsub_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vnmsub_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vint32m8_t vnmsub_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vnmsub_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int32_t op1, vint32m8_t op2, _VL_T vl);
vint64m1_t vnmsub_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vnmsub_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t acc, int64_t op1, vint64m1_t op2, _VL_T vl);
vint64m2_t vnmsub_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vnmsub_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, int64_t op1, vint64m2_t op2, _VL_T vl);
vint64m4_t vnmsub_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vnmsub_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int64_t op1, vint64m4_t op2, _VL_T vl);
vint64m8_t vnmsub_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vnmsub_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int64_t op1, vint64m8_t op2, _VL_T vl);
vuint8m1_t vnmsub_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnmsub_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m2_t vnmsub_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnmsub_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m4_t vnmsub_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnmsub_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m8_t vnmsub_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vnmsub_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t acc, uint8_t op1, vuint8m8_t op2, _VL_T vl);
vuint16m1_t vnmsub_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnmsub_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m2_t vnmsub_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnmsub_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m4_t vnmsub_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnmsub_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m8_t vnmsub_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vnmsub_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, uint16_t op1, vuint16m8_t op2, _VL_T vl);
vuint32m1_t vnmsub_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnmsub_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m2_t vnmsub_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnmsub_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m4_t vnmsub_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnmsub_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m8_t vnmsub_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vnmsub_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, uint32_t op1, vuint32m8_t op2, _VL_T vl);
vuint64m1_t vnmsub_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vnmsub_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t acc, uint64_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m2_t vnmsub_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vnmsub_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, uint64_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m4_t vnmsub_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vnmsub_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, uint64_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m8_t vnmsub_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vnmsub_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, uint64_t op1, vuint64m8_t op2, _VL_T vl);
```
### [Vector Widening Integer Multiply-Add Functions](rvv-intrinsic-api.md#1213-vector-widening-integer-multiply-add-operations):

**Prototypes:**
``` C
vint16m2_t vwmacc_vv_i16m2_vl (vint16m2_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwmacc_vx_i16m2_vl (vint16m2_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint16m4_t vwmacc_vv_i16m4_vl (vint16m4_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwmacc_vx_i16m4_vl (vint16m4_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint16m8_t vwmacc_vv_i16m8_vl (vint16m8_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwmacc_vx_i16m8_vl (vint16m8_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint32m2_t vwmacc_vv_i32m2_vl (vint32m2_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwmacc_vx_i32m2_vl (vint32m2_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint32m4_t vwmacc_vv_i32m4_vl (vint32m4_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwmacc_vx_i32m4_vl (vint32m4_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint32m8_t vwmacc_vv_i32m8_vl (vint32m8_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwmacc_vx_i32m8_vl (vint32m8_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint64m2_t vwmacc_vv_i64m2_vl (vint64m2_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwmacc_vx_i64m2_vl (vint64m2_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint64m4_t vwmacc_vv_i64m4_vl (vint64m4_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwmacc_vx_i64m4_vl (vint64m4_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint64m8_t vwmacc_vv_i64m8_vl (vint64m8_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwmacc_vx_i64m8_vl (vint64m8_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vuint16m2_t vwmaccu_vv_u16m2_vl (vuint16m2_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwmaccu_vx_u16m2_vl (vuint16m2_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m4_t vwmaccu_vv_u16m4_vl (vuint16m4_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwmaccu_vx_u16m4_vl (vuint16m4_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m8_t vwmaccu_vv_u16m8_vl (vuint16m8_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwmaccu_vx_u16m8_vl (vuint16m8_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint32m2_t vwmaccu_vv_u32m2_vl (vuint32m2_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwmaccu_vx_u32m2_vl (vuint32m2_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m4_t vwmaccu_vv_u32m4_vl (vuint32m4_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwmaccu_vx_u32m4_vl (vuint32m4_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m8_t vwmaccu_vv_u32m8_vl (vuint32m8_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwmaccu_vx_u32m8_vl (vuint32m8_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint64m2_t vwmaccu_vv_u64m2_vl (vuint64m2_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwmaccu_vx_u64m2_vl (vuint64m2_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m4_t vwmaccu_vv_u64m4_vl (vuint64m4_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwmaccu_vx_u64m4_vl (vuint64m4_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m8_t vwmaccu_vv_u64m8_vl (vuint64m8_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwmaccu_vx_u64m8_vl (vuint64m8_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vint16m2_t vwmaccsu_vv_i16m2_vl (vint16m2_t acc, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint16m2_t vwmaccsu_vx_i16m2_vl (vint16m2_t acc, int8_t op1, vuint8m1_t op2, _VL_T vl);
vint16m4_t vwmaccsu_vv_i16m4_vl (vint16m4_t acc, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint16m4_t vwmaccsu_vx_i16m4_vl (vint16m4_t acc, int8_t op1, vuint8m2_t op2, _VL_T vl);
vint16m8_t vwmaccsu_vv_i16m8_vl (vint16m8_t acc, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint16m8_t vwmaccsu_vx_i16m8_vl (vint16m8_t acc, int8_t op1, vuint8m4_t op2, _VL_T vl);
vint32m2_t vwmaccsu_vv_i32m2_vl (vint32m2_t acc, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint32m2_t vwmaccsu_vx_i32m2_vl (vint32m2_t acc, int16_t op1, vuint16m1_t op2, _VL_T vl);
vint32m4_t vwmaccsu_vv_i32m4_vl (vint32m4_t acc, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vwmaccsu_vx_i32m4_vl (vint32m4_t acc, int16_t op1, vuint16m2_t op2, _VL_T vl);
vint32m8_t vwmaccsu_vv_i32m8_vl (vint32m8_t acc, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint32m8_t vwmaccsu_vx_i32m8_vl (vint32m8_t acc, int16_t op1, vuint16m4_t op2, _VL_T vl);
vint64m2_t vwmaccsu_vv_i64m2_vl (vint64m2_t acc, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint64m2_t vwmaccsu_vx_i64m2_vl (vint64m2_t acc, int32_t op1, vuint32m1_t op2, _VL_T vl);
vint64m4_t vwmaccsu_vv_i64m4_vl (vint64m4_t acc, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint64m4_t vwmaccsu_vx_i64m4_vl (vint64m4_t acc, int32_t op1, vuint32m2_t op2, _VL_T vl);
vint64m8_t vwmaccsu_vv_i64m8_vl (vint64m8_t acc, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint64m8_t vwmaccsu_vx_i64m8_vl (vint64m8_t acc, int32_t op1, vuint32m4_t op2, _VL_T vl);
vint16m2_t vwmaccus_vx_i16m2_vl (vint16m2_t acc, uint8_t op1, vint8m1_t op2, _VL_T vl);
vint16m4_t vwmaccus_vx_i16m4_vl (vint16m4_t acc, uint8_t op1, vint8m2_t op2, _VL_T vl);
vint16m8_t vwmaccus_vx_i16m8_vl (vint16m8_t acc, uint8_t op1, vint8m4_t op2, _VL_T vl);
vint32m2_t vwmaccus_vx_i32m2_vl (vint32m2_t acc, uint16_t op1, vint16m1_t op2, _VL_T vl);
vint32m4_t vwmaccus_vx_i32m4_vl (vint32m4_t acc, uint16_t op1, vint16m2_t op2, _VL_T vl);
vint32m8_t vwmaccus_vx_i32m8_vl (vint32m8_t acc, uint16_t op1, vint16m4_t op2, _VL_T vl);
vint64m2_t vwmaccus_vx_i64m2_vl (vint64m2_t acc, uint32_t op1, vint32m1_t op2, _VL_T vl);
vint64m4_t vwmaccus_vx_i64m4_vl (vint64m4_t acc, uint32_t op1, vint32m2_t op2, _VL_T vl);
vint64m8_t vwmaccus_vx_i64m8_vl (vint64m8_t acc, uint32_t op1, vint32m4_t op2, _VL_T vl);
// masked functions
vint16m2_t vwmacc_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint16m2_t vwmacc_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint16m4_t vwmacc_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint16m4_t vwmacc_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint16m8_t vwmacc_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint16m8_t vwmacc_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, int8_t op1, vint8m4_t op2, _VL_T vl);
vint32m2_t vwmacc_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint32m2_t vwmacc_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint32m4_t vwmacc_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint32m4_t vwmacc_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vint32m8_t vwmacc_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint32m8_t vwmacc_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int16_t op1, vint16m4_t op2, _VL_T vl);
vint64m2_t vwmacc_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint64m2_t vwmacc_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, int32_t op1, vint32m1_t op2, _VL_T vl);
vint64m4_t vwmacc_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint64m4_t vwmacc_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int32_t op1, vint32m2_t op2, _VL_T vl);
vint64m8_t vwmacc_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint64m8_t vwmacc_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int32_t op1, vint32m4_t op2, _VL_T vl);
vuint16m2_t vwmaccu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m2_t vwmaccu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint16m4_t vwmaccu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m4_t vwmaccu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint16m8_t vwmaccu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint16m8_t vwmaccu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t acc, uint8_t op1, vuint8m4_t op2, _VL_T vl);
vuint32m2_t vwmaccu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m2_t vwmaccu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint32m4_t vwmaccu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m4_t vwmaccu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vuint32m8_t vwmaccu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint32m8_t vwmaccu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, uint16_t op1, vuint16m4_t op2, _VL_T vl);
vuint64m2_t vwmaccu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m2_t vwmaccu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t acc, uint32_t op1, vuint32m1_t op2, _VL_T vl);
vuint64m4_t vwmaccu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m4_t vwmaccu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, uint32_t op1, vuint32m2_t op2, _VL_T vl);
vuint64m8_t vwmaccu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint64m8_t vwmaccu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, uint32_t op1, vuint32m4_t op2, _VL_T vl);
vint16m2_t vwmaccsu_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint16m2_t vwmaccsu_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, int8_t op1, vuint8m1_t op2, _VL_T vl);
vint16m4_t vwmaccsu_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint16m4_t vwmaccsu_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, int8_t op1, vuint8m2_t op2, _VL_T vl);
vint16m8_t vwmaccsu_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint16m8_t vwmaccsu_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, int8_t op1, vuint8m4_t op2, _VL_T vl);
vint32m2_t vwmaccsu_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint32m2_t vwmaccsu_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, int16_t op1, vuint16m1_t op2, _VL_T vl);
vint32m4_t vwmaccsu_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vwmaccsu_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int16_t op1, vuint16m2_t op2, _VL_T vl);
vint32m8_t vwmaccsu_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint32m8_t vwmaccsu_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int16_t op1, vuint16m4_t op2, _VL_T vl);
vint64m2_t vwmaccsu_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint64m2_t vwmaccsu_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, int32_t op1, vuint32m1_t op2, _VL_T vl);
vint64m4_t vwmaccsu_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint64m4_t vwmaccsu_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int32_t op1, vuint32m2_t op2, _VL_T vl);
vint64m8_t vwmaccsu_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint64m8_t vwmaccsu_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int32_t op1, vuint32m4_t op2, _VL_T vl);
vint16m2_t vwmaccus_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t acc, uint8_t op1, vint8m1_t op2, _VL_T vl);
vint16m4_t vwmaccus_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t acc, uint8_t op1, vint8m2_t op2, _VL_T vl);
vint16m8_t vwmaccus_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t acc, uint8_t op1, vint8m4_t op2, _VL_T vl);
vint32m2_t vwmaccus_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t acc, uint16_t op1, vint16m1_t op2, _VL_T vl);
vint32m4_t vwmaccus_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, uint16_t op1, vint16m2_t op2, _VL_T vl);
vint32m8_t vwmaccus_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, uint16_t op1, vint16m4_t op2, _VL_T vl);
vint64m2_t vwmaccus_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t acc, uint32_t op1, vint32m1_t op2, _VL_T vl);
vint64m4_t vwmaccus_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, uint32_t op1, vint32m2_t op2, _VL_T vl);
vint64m8_t vwmaccus_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, uint32_t op1, vint32m4_t op2, _VL_T vl);
```
### [Vector Quad-Widening Integer Multiply-Add Functions](rvv-intrinsic-api.md#1214-vector-quad-widening-integer-multiply-add-operations-extension-zvqmac):

**Prototypes:**
``` C
vint32m4_t vqmacc_vv_i32m4_vl (vint32m4_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint32m4_t vqmacc_vx_i32m4_vl (vint32m4_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint32m8_t vqmacc_vv_i32m8_vl (vint32m8_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint32m8_t vqmacc_vx_i32m8_vl (vint32m8_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint64m4_t vqmacc_vv_i64m4_vl (vint64m4_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint64m4_t vqmacc_vx_i64m4_vl (vint64m4_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint64m8_t vqmacc_vv_i64m8_vl (vint64m8_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint64m8_t vqmacc_vx_i64m8_vl (vint64m8_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vuint32m4_t vqmaccu_vv_u32m4_vl (vuint32m4_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint32m4_t vqmaccu_vx_u32m4_vl (vuint32m4_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint32m8_t vqmaccu_vv_u32m8_vl (vuint32m8_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint32m8_t vqmaccu_vx_u32m8_vl (vuint32m8_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint64m4_t vqmaccu_vv_u64m4_vl (vuint64m4_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint64m4_t vqmaccu_vx_u64m4_vl (vuint64m4_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint64m8_t vqmaccu_vv_u64m8_vl (vuint64m8_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint64m8_t vqmaccu_vx_u64m8_vl (vuint64m8_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vqmaccsu_vv_i32m4_vl (vint32m4_t acc, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint32m4_t vqmaccsu_vx_i32m4_vl (vint32m4_t acc, int8_t op1, vuint8m1_t op2, _VL_T vl);
vint32m8_t vqmaccsu_vv_i32m8_vl (vint32m8_t acc, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint32m8_t vqmaccsu_vx_i32m8_vl (vint32m8_t acc, int8_t op1, vuint8m2_t op2, _VL_T vl);
vint64m4_t vqmaccsu_vv_i64m4_vl (vint64m4_t acc, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint64m4_t vqmaccsu_vx_i64m4_vl (vint64m4_t acc, int16_t op1, vuint16m1_t op2, _VL_T vl);
vint64m8_t vqmaccsu_vv_i64m8_vl (vint64m8_t acc, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint64m8_t vqmaccsu_vx_i64m8_vl (vint64m8_t acc, int16_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vqmaccus_vx_i32m4_vl (vint32m4_t acc, uint8_t op1, vint8m1_t op2, _VL_T vl);
vint32m8_t vqmaccus_vx_i32m8_vl (vint32m8_t acc, uint8_t op1, vint8m2_t op2, _VL_T vl);
vint64m4_t vqmaccus_vx_i64m4_vl (vint64m4_t acc, uint16_t op1, vint16m1_t op2, _VL_T vl);
vint64m8_t vqmaccus_vx_i64m8_vl (vint64m8_t acc, uint16_t op1, vint16m2_t op2, _VL_T vl);
// masked functions
vint32m4_t vqmacc_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint32m4_t vqmacc_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int8_t op1, vint8m1_t op2, _VL_T vl);
vint32m8_t vqmacc_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint32m8_t vqmacc_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int8_t op1, vint8m2_t op2, _VL_T vl);
vint64m4_t vqmacc_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint64m4_t vqmacc_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int16_t op1, vint16m1_t op2, _VL_T vl);
vint64m8_t vqmacc_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint64m8_t vqmacc_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int16_t op1, vint16m2_t op2, _VL_T vl);
vuint32m4_t vqmaccu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint32m4_t vqmaccu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t acc, uint8_t op1, vuint8m1_t op2, _VL_T vl);
vuint32m8_t vqmaccu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint32m8_t vqmaccu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t acc, uint8_t op1, vuint8m2_t op2, _VL_T vl);
vuint64m4_t vqmaccu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint64m4_t vqmaccu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t acc, uint16_t op1, vuint16m1_t op2, _VL_T vl);
vuint64m8_t vqmaccu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint64m8_t vqmaccu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t acc, uint16_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vqmaccsu_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint32m4_t vqmaccsu_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, int8_t op1, vuint8m1_t op2, _VL_T vl);
vint32m8_t vqmaccsu_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint32m8_t vqmaccsu_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, int8_t op1, vuint8m2_t op2, _VL_T vl);
vint64m4_t vqmaccsu_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint64m4_t vqmaccsu_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, int16_t op1, vuint16m1_t op2, _VL_T vl);
vint64m8_t vqmaccsu_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint64m8_t vqmaccsu_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, int16_t op1, vuint16m2_t op2, _VL_T vl);
vint32m4_t vqmaccus_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t acc, uint8_t op1, vint8m1_t op2, _VL_T vl);
vint32m8_t vqmaccus_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t acc, uint8_t op1, vint8m2_t op2, _VL_T vl);
vint64m4_t vqmaccus_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t acc, uint16_t op1, vint16m1_t op2, _VL_T vl);
vint64m8_t vqmaccus_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t acc, uint16_t op1, vint16m2_t op2, _VL_T vl);
```
### [Vector Integer Merge Functions](rvv-intrinsic-api.md#1215-vector-integer-merge-operations):

**Prototypes:**
``` C
vint8m1_t vmerge_vvm_i8m1_m_vl (vbool8_t mask, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vmerge_vxm_i8m1_m_vl (vbool8_t mask, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vmerge_vvm_i8m2_m_vl (vbool4_t mask, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vmerge_vxm_i8m2_m_vl (vbool4_t mask, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vmerge_vvm_i8m4_m_vl (vbool2_t mask, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vmerge_vxm_i8m4_m_vl (vbool2_t mask, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vmerge_vvm_i8m8_m_vl (vbool1_t mask, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vmerge_vxm_i8m8_m_vl (vbool1_t mask, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vmerge_vvm_i16m1_m_vl (vbool16_t mask, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vmerge_vxm_i16m1_m_vl (vbool16_t mask, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vmerge_vvm_i16m2_m_vl (vbool8_t mask, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vmerge_vxm_i16m2_m_vl (vbool8_t mask, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vmerge_vvm_i16m4_m_vl (vbool4_t mask, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vmerge_vxm_i16m4_m_vl (vbool4_t mask, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vmerge_vvm_i16m8_m_vl (vbool2_t mask, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vmerge_vxm_i16m8_m_vl (vbool2_t mask, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vmerge_vvm_i32m1_m_vl (vbool32_t mask, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vmerge_vxm_i32m1_m_vl (vbool32_t mask, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vmerge_vvm_i32m2_m_vl (vbool16_t mask, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vmerge_vxm_i32m2_m_vl (vbool16_t mask, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vmerge_vvm_i32m4_m_vl (vbool8_t mask, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vmerge_vxm_i32m4_m_vl (vbool8_t mask, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vmerge_vvm_i32m8_m_vl (vbool4_t mask, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vmerge_vxm_i32m8_m_vl (vbool4_t mask, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vmerge_vvm_i64m1_m_vl (vbool64_t mask, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vmerge_vxm_i64m1_m_vl (vbool64_t mask, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vmerge_vvm_i64m2_m_vl (vbool32_t mask, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vmerge_vxm_i64m2_m_vl (vbool32_t mask, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vmerge_vvm_i64m4_m_vl (vbool16_t mask, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vmerge_vxm_i64m4_m_vl (vbool16_t mask, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vmerge_vvm_i64m8_m_vl (vbool8_t mask, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vmerge_vxm_i64m8_m_vl (vbool8_t mask, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vmerge_vvm_u8m1_m_vl (vbool8_t mask, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vmerge_vxm_u8m1_m_vl (vbool8_t mask, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vmerge_vvm_u8m2_m_vl (vbool4_t mask, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vmerge_vxm_u8m2_m_vl (vbool4_t mask, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vmerge_vvm_u8m4_m_vl (vbool2_t mask, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vmerge_vxm_u8m4_m_vl (vbool2_t mask, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vmerge_vvm_u8m8_m_vl (vbool1_t mask, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vmerge_vxm_u8m8_m_vl (vbool1_t mask, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vmerge_vvm_u16m1_m_vl (vbool16_t mask, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vmerge_vxm_u16m1_m_vl (vbool16_t mask, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vmerge_vvm_u16m2_m_vl (vbool8_t mask, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vmerge_vxm_u16m2_m_vl (vbool8_t mask, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vmerge_vvm_u16m4_m_vl (vbool4_t mask, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vmerge_vxm_u16m4_m_vl (vbool4_t mask, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vmerge_vvm_u16m8_m_vl (vbool2_t mask, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vmerge_vxm_u16m8_m_vl (vbool2_t mask, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vmerge_vvm_u32m1_m_vl (vbool32_t mask, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vmerge_vxm_u32m1_m_vl (vbool32_t mask, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vmerge_vvm_u32m2_m_vl (vbool16_t mask, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vmerge_vxm_u32m2_m_vl (vbool16_t mask, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vmerge_vvm_u32m4_m_vl (vbool8_t mask, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vmerge_vxm_u32m4_m_vl (vbool8_t mask, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vmerge_vvm_u32m8_m_vl (vbool4_t mask, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vmerge_vxm_u32m8_m_vl (vbool4_t mask, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vmerge_vvm_u64m1_m_vl (vbool64_t mask, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vmerge_vxm_u64m1_m_vl (vbool64_t mask, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vmerge_vvm_u64m2_m_vl (vbool32_t mask, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vmerge_vxm_u64m2_m_vl (vbool32_t mask, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vmerge_vvm_u64m4_m_vl (vbool16_t mask, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vmerge_vxm_u64m4_m_vl (vbool16_t mask, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vmerge_vvm_u64m8_m_vl (vbool8_t mask, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vmerge_vxm_u64m8_m_vl (vbool8_t mask, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Integer Move Functions](rvv-intrinsic-api.md#1216-vector-integer-move-operations):

**Prototypes:**
``` C
vint8m1_t vmv_v_v_i8m1_vl (vint8m1_t src, _VL_T vl);
vint8m1_t vmv_v_x_i8m1_vl (int8_t src, _VL_T vl);
vint8m2_t vmv_v_v_i8m2_vl (vint8m2_t src, _VL_T vl);
vint8m2_t vmv_v_x_i8m2_vl (int8_t src, _VL_T vl);
vint8m4_t vmv_v_v_i8m4_vl (vint8m4_t src, _VL_T vl);
vint8m4_t vmv_v_x_i8m4_vl (int8_t src, _VL_T vl);
vint8m8_t vmv_v_v_i8m8_vl (vint8m8_t src, _VL_T vl);
vint8m8_t vmv_v_x_i8m8_vl (int8_t src, _VL_T vl);
vint16m1_t vmv_v_v_i16m1_vl (vint16m1_t src, _VL_T vl);
vint16m1_t vmv_v_x_i16m1_vl (int16_t src, _VL_T vl);
vint16m2_t vmv_v_v_i16m2_vl (vint16m2_t src, _VL_T vl);
vint16m2_t vmv_v_x_i16m2_vl (int16_t src, _VL_T vl);
vint16m4_t vmv_v_v_i16m4_vl (vint16m4_t src, _VL_T vl);
vint16m4_t vmv_v_x_i16m4_vl (int16_t src, _VL_T vl);
vint16m8_t vmv_v_v_i16m8_vl (vint16m8_t src, _VL_T vl);
vint16m8_t vmv_v_x_i16m8_vl (int16_t src, _VL_T vl);
vint32m1_t vmv_v_v_i32m1_vl (vint32m1_t src, _VL_T vl);
vint32m1_t vmv_v_x_i32m1_vl (int32_t src, _VL_T vl);
vint32m2_t vmv_v_v_i32m2_vl (vint32m2_t src, _VL_T vl);
vint32m2_t vmv_v_x_i32m2_vl (int32_t src, _VL_T vl);
vint32m4_t vmv_v_v_i32m4_vl (vint32m4_t src, _VL_T vl);
vint32m4_t vmv_v_x_i32m4_vl (int32_t src, _VL_T vl);
vint32m8_t vmv_v_v_i32m8_vl (vint32m8_t src, _VL_T vl);
vint32m8_t vmv_v_x_i32m8_vl (int32_t src, _VL_T vl);
vint64m1_t vmv_v_v_i64m1_vl (vint64m1_t src, _VL_T vl);
vint64m1_t vmv_v_x_i64m1_vl (int64_t src, _VL_T vl);
vint64m2_t vmv_v_v_i64m2_vl (vint64m2_t src, _VL_T vl);
vint64m2_t vmv_v_x_i64m2_vl (int64_t src, _VL_T vl);
vint64m4_t vmv_v_v_i64m4_vl (vint64m4_t src, _VL_T vl);
vint64m4_t vmv_v_x_i64m4_vl (int64_t src, _VL_T vl);
vint64m8_t vmv_v_v_i64m8_vl (vint64m8_t src, _VL_T vl);
vint64m8_t vmv_v_x_i64m8_vl (int64_t src, _VL_T vl);
vuint8m1_t vmv_v_v_u8m1_vl (vuint8m1_t src, _VL_T vl);
vuint8m1_t vmv_v_x_u8m1_vl (uint8_t src, _VL_T vl);
vuint8m2_t vmv_v_v_u8m2_vl (vuint8m2_t src, _VL_T vl);
vuint8m2_t vmv_v_x_u8m2_vl (uint8_t src, _VL_T vl);
vuint8m4_t vmv_v_v_u8m4_vl (vuint8m4_t src, _VL_T vl);
vuint8m4_t vmv_v_x_u8m4_vl (uint8_t src, _VL_T vl);
vuint8m8_t vmv_v_v_u8m8_vl (vuint8m8_t src, _VL_T vl);
vuint8m8_t vmv_v_x_u8m8_vl (uint8_t src, _VL_T vl);
vuint16m1_t vmv_v_v_u16m1_vl (vuint16m1_t src, _VL_T vl);
vuint16m1_t vmv_v_x_u16m1_vl (uint16_t src, _VL_T vl);
vuint16m2_t vmv_v_v_u16m2_vl (vuint16m2_t src, _VL_T vl);
vuint16m2_t vmv_v_x_u16m2_vl (uint16_t src, _VL_T vl);
vuint16m4_t vmv_v_v_u16m4_vl (vuint16m4_t src, _VL_T vl);
vuint16m4_t vmv_v_x_u16m4_vl (uint16_t src, _VL_T vl);
vuint16m8_t vmv_v_v_u16m8_vl (vuint16m8_t src, _VL_T vl);
vuint16m8_t vmv_v_x_u16m8_vl (uint16_t src, _VL_T vl);
vuint32m1_t vmv_v_v_u32m1_vl (vuint32m1_t src, _VL_T vl);
vuint32m1_t vmv_v_x_u32m1_vl (uint32_t src, _VL_T vl);
vuint32m2_t vmv_v_v_u32m2_vl (vuint32m2_t src, _VL_T vl);
vuint32m2_t vmv_v_x_u32m2_vl (uint32_t src, _VL_T vl);
vuint32m4_t vmv_v_v_u32m4_vl (vuint32m4_t src, _VL_T vl);
vuint32m4_t vmv_v_x_u32m4_vl (uint32_t src, _VL_T vl);
vuint32m8_t vmv_v_v_u32m8_vl (vuint32m8_t src, _VL_T vl);
vuint32m8_t vmv_v_x_u32m8_vl (uint32_t src, _VL_T vl);
vuint64m1_t vmv_v_v_u64m1_vl (vuint64m1_t src, _VL_T vl);
vuint64m1_t vmv_v_x_u64m1_vl (uint64_t src, _VL_T vl);
vuint64m2_t vmv_v_v_u64m2_vl (vuint64m2_t src, _VL_T vl);
vuint64m2_t vmv_v_x_u64m2_vl (uint64_t src, _VL_T vl);
vuint64m4_t vmv_v_v_u64m4_vl (vuint64m4_t src, _VL_T vl);
vuint64m4_t vmv_v_x_u64m4_vl (uint64_t src, _VL_T vl);
vuint64m8_t vmv_v_v_u64m8_vl (vuint64m8_t src, _VL_T vl);
vuint64m8_t vmv_v_x_u64m8_vl (uint64_t src, _VL_T vl);
```
## Vector Fixed-Point Arithmetic Functions:

### [Vector Single-Width Saturating Add and Subtract Functions](rvv-intrinsic-api.md#131-vector-single-width-saturating-add-and-subtract):

**Prototypes:**
``` C
vint8m1_t vsadd_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vsadd_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vsadd_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vsadd_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vsadd_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vsadd_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vsadd_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vsadd_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vsadd_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vsadd_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vsadd_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vsadd_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vsadd_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vsadd_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vsadd_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vsadd_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vsadd_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vsadd_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vsadd_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vsadd_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vsadd_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vsadd_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vsadd_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vsadd_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vsadd_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vsadd_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vsadd_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vsadd_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vsadd_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vsadd_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vsadd_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vsadd_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vsaddu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsaddu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsaddu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsaddu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsaddu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsaddu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsaddu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsaddu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsaddu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsaddu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vsaddu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsaddu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vsaddu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsaddu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vsaddu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsaddu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vsaddu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsaddu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vsaddu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsaddu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vsaddu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsaddu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vsaddu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsaddu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vsaddu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsaddu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vsaddu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsaddu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vsaddu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsaddu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vsaddu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsaddu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vssub_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vssub_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vssub_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vssub_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vssub_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vssub_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vssub_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vssub_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vssub_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vssub_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vssub_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vssub_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vssub_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vssub_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vssub_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vssub_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vssub_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vssub_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vssub_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vssub_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vssub_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vssub_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vssub_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vssub_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vssub_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vssub_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vssub_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vssub_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vssub_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vssub_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vssub_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vssub_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vssubu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vssubu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vssubu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vssubu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vssubu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vssubu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vssubu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vssubu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vssubu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vssubu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vssubu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vssubu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vssubu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vssubu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vssubu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vssubu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vssubu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vssubu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vssubu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vssubu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vssubu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vssubu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vssubu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vssubu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vssubu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vssubu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vssubu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vssubu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vssubu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vssubu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vssubu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vssubu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vint8m1_t vsadd_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vsadd_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vsadd_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vsadd_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vsadd_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vsadd_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vsadd_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vsadd_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vsadd_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vsadd_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vsadd_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vsadd_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vsadd_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vsadd_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vsadd_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vsadd_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vsadd_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vsadd_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vsadd_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vsadd_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vsadd_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vsadd_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vsadd_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vsadd_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vsadd_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vsadd_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vsadd_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vsadd_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vsadd_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vsadd_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vsadd_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vsadd_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vsaddu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vsaddu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vsaddu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vsaddu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vsaddu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vsaddu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vsaddu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vsaddu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vsaddu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vsaddu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vsaddu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vsaddu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vsaddu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vsaddu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vsaddu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vsaddu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vsaddu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vsaddu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vsaddu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vsaddu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vsaddu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vsaddu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vsaddu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vsaddu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vsaddu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vsaddu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vsaddu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vsaddu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vsaddu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vsaddu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vsaddu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vsaddu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vssub_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vssub_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vssub_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vssub_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vssub_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vssub_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vssub_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vssub_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vssub_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vssub_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vssub_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vssub_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vssub_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vssub_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vssub_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vssub_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vssub_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vssub_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vssub_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vssub_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vssub_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vssub_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vssub_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vssub_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vssub_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vssub_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vssub_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vssub_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vssub_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vssub_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vssub_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vssub_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vssubu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vssubu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vssubu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vssubu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vssubu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vssubu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vssubu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vssubu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vssubu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vssubu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vssubu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vssubu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vssubu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vssubu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vssubu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vssubu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vssubu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vssubu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vssubu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vssubu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vssubu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vssubu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vssubu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vssubu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vssubu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vssubu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vssubu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vssubu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vssubu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vssubu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vssubu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vssubu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Single-Width Averaging Add and Subtract Functions](rvv-intrinsic-api.md#132-vector-single-width-averaging-add-and-subtract):

**Prototypes:**
``` C
vint8m1_t vaadd_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vaadd_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vaadd_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vaadd_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vaadd_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vaadd_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vaadd_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vaadd_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vaadd_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vaadd_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vaadd_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vaadd_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vaadd_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vaadd_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vaadd_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vaadd_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vaadd_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vaadd_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vaadd_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vaadd_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vaadd_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vaadd_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vaadd_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vaadd_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vaadd_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vaadd_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vaadd_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vaadd_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vaadd_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vaadd_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vaadd_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vaadd_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vaaddu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vaaddu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vaaddu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vaaddu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vaaddu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vaaddu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vaaddu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vaaddu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vaaddu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vaaddu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vaaddu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vaaddu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vaaddu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vaaddu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vaaddu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vaaddu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vaaddu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vaaddu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vaaddu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vaaddu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vaaddu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vaaddu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vaaddu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vaaddu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vaaddu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vaaddu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vaaddu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vaaddu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vaaddu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vaaddu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vaaddu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vaaddu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vasub_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vasub_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vasub_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vasub_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vasub_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vasub_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vasub_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vasub_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vasub_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vasub_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vasub_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vasub_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vasub_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vasub_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vasub_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vasub_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vasub_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vasub_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vasub_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vasub_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vasub_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vasub_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vasub_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vasub_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vasub_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vasub_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vasub_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vasub_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vasub_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vasub_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vasub_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vasub_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vasubu_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vasubu_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vasubu_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vasubu_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vasubu_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vasubu_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vasubu_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vasubu_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vasubu_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vasubu_vx_u16m1_vl (vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vasubu_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vasubu_vx_u16m2_vl (vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vasubu_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vasubu_vx_u16m4_vl (vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vasubu_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vasubu_vx_u16m8_vl (vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vasubu_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vasubu_vx_u32m1_vl (vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vasubu_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vasubu_vx_u32m2_vl (vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vasubu_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vasubu_vx_u32m4_vl (vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vasubu_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vasubu_vx_u32m8_vl (vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vasubu_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vasubu_vx_u64m1_vl (vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vasubu_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vasubu_vx_u64m2_vl (vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vasubu_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vasubu_vx_u64m4_vl (vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vasubu_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vasubu_vx_u64m8_vl (vuint64m8_t op1, uint64_t op2, _VL_T vl);
// masked functions
vint8m1_t vaadd_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vaadd_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vaadd_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vaadd_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vaadd_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vaadd_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vaadd_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vaadd_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vaadd_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vaadd_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vaadd_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vaadd_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vaadd_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vaadd_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vaadd_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vaadd_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vaadd_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vaadd_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vaadd_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vaadd_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vaadd_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vaadd_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vaadd_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vaadd_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vaadd_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vaadd_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vaadd_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vaadd_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vaadd_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vaadd_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vaadd_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vaadd_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vaaddu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vaaddu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vaaddu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vaaddu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vaaddu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vaaddu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vaaddu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vaaddu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vaaddu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vaaddu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vaaddu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vaaddu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vaaddu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vaaddu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vaaddu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vaaddu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vaaddu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vaaddu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vaaddu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vaaddu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vaaddu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vaaddu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vaaddu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vaaddu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vaaddu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vaaddu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vaaddu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vaaddu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vaaddu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vaaddu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vaaddu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vaaddu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
vint8m1_t vasub_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vasub_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vasub_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vasub_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vasub_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vasub_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vasub_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vasub_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vasub_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vasub_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vasub_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vasub_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vasub_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vasub_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vasub_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vasub_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vasub_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vasub_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vasub_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vasub_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vasub_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vasub_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vasub_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vasub_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vasub_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vasub_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vasub_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vasub_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vasub_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vasub_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vasub_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vasub_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
vuint8m1_t vasubu_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vasubu_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vasubu_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vasubu_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vasubu_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vasubu_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vasubu_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vasubu_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vasubu_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vasubu_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint16_t op2, _VL_T vl);
vuint16m2_t vasubu_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vasubu_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint16_t op2, _VL_T vl);
vuint16m4_t vasubu_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vasubu_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint16_t op2, _VL_T vl);
vuint16m8_t vasubu_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vasubu_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint16_t op2, _VL_T vl);
vuint32m1_t vasubu_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vasubu_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint32_t op2, _VL_T vl);
vuint32m2_t vasubu_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vasubu_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint32_t op2, _VL_T vl);
vuint32m4_t vasubu_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vasubu_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint32_t op2, _VL_T vl);
vuint32m8_t vasubu_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vasubu_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint32_t op2, _VL_T vl);
vuint64m1_t vasubu_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vasubu_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint64_t op2, _VL_T vl);
vuint64m2_t vasubu_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vasubu_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint64_t op2, _VL_T vl);
vuint64m4_t vasubu_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vasubu_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint64_t op2, _VL_T vl);
vuint64m8_t vasubu_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vasubu_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint64_t op2, _VL_T vl);
```
### [Vector Single-Width Fractional Multiply with Rounding and Saturation Functions](rvv-intrinsic-api.md#133-vector-single-width-fractional-multiply-with-rounding-and-saturation):

**Prototypes:**
``` C
vint8m1_t vsmul_vv_i8m1_vl (vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vsmul_vx_i8m1_vl (vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vsmul_vv_i8m2_vl (vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vsmul_vx_i8m2_vl (vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vsmul_vv_i8m4_vl (vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vsmul_vx_i8m4_vl (vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vsmul_vv_i8m8_vl (vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vsmul_vx_i8m8_vl (vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vsmul_vv_i16m1_vl (vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vsmul_vx_i16m1_vl (vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vsmul_vv_i16m2_vl (vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vsmul_vx_i16m2_vl (vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vsmul_vv_i16m4_vl (vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vsmul_vx_i16m4_vl (vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vsmul_vv_i16m8_vl (vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vsmul_vx_i16m8_vl (vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vsmul_vv_i32m1_vl (vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vsmul_vx_i32m1_vl (vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vsmul_vv_i32m2_vl (vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vsmul_vx_i32m2_vl (vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vsmul_vv_i32m4_vl (vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vsmul_vx_i32m4_vl (vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vsmul_vv_i32m8_vl (vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vsmul_vx_i32m8_vl (vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vsmul_vv_i64m1_vl (vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vsmul_vx_i64m1_vl (vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vsmul_vv_i64m2_vl (vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vsmul_vx_i64m2_vl (vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vsmul_vv_i64m4_vl (vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vsmul_vx_i64m4_vl (vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vsmul_vv_i64m8_vl (vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vsmul_vx_i64m8_vl (vint64m8_t op1, int64_t op2, _VL_T vl);
// masked functions
vint8m1_t vsmul_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vint8m1_t op2, _VL_T vl);
vint8m1_t vsmul_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, int8_t op2, _VL_T vl);
vint8m2_t vsmul_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vint8m2_t op2, _VL_T vl);
vint8m2_t vsmul_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, int8_t op2, _VL_T vl);
vint8m4_t vsmul_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vint8m4_t op2, _VL_T vl);
vint8m4_t vsmul_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, int8_t op2, _VL_T vl);
vint8m8_t vsmul_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vint8m8_t op2, _VL_T vl);
vint8m8_t vsmul_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, int8_t op2, _VL_T vl);
vint16m1_t vsmul_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vint16m1_t op2, _VL_T vl);
vint16m1_t vsmul_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, int16_t op2, _VL_T vl);
vint16m2_t vsmul_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vint16m2_t op2, _VL_T vl);
vint16m2_t vsmul_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, int16_t op2, _VL_T vl);
vint16m4_t vsmul_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vint16m4_t op2, _VL_T vl);
vint16m4_t vsmul_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, int16_t op2, _VL_T vl);
vint16m8_t vsmul_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vint16m8_t op2, _VL_T vl);
vint16m8_t vsmul_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, int16_t op2, _VL_T vl);
vint32m1_t vsmul_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vint32m1_t op2, _VL_T vl);
vint32m1_t vsmul_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, int32_t op2, _VL_T vl);
vint32m2_t vsmul_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vint32m2_t op2, _VL_T vl);
vint32m2_t vsmul_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, int32_t op2, _VL_T vl);
vint32m4_t vsmul_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vint32m4_t op2, _VL_T vl);
vint32m4_t vsmul_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, int32_t op2, _VL_T vl);
vint32m8_t vsmul_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vint32m8_t op2, _VL_T vl);
vint32m8_t vsmul_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, int32_t op2, _VL_T vl);
vint64m1_t vsmul_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vint64m1_t op2, _VL_T vl);
vint64m1_t vsmul_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, int64_t op2, _VL_T vl);
vint64m2_t vsmul_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vint64m2_t op2, _VL_T vl);
vint64m2_t vsmul_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, int64_t op2, _VL_T vl);
vint64m4_t vsmul_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vint64m4_t op2, _VL_T vl);
vint64m4_t vsmul_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, int64_t op2, _VL_T vl);
vint64m8_t vsmul_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vint64m8_t op2, _VL_T vl);
vint64m8_t vsmul_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, int64_t op2, _VL_T vl);
```
### [Vector Single-Width Scaling Shift Functions](rvv-intrinsic-api.md#134-vector-single-width-scaling-shift-operations):

**Prototypes:**
``` C
vuint8m1_t vssrl_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vssrl_vx_u8m1_vl (vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vssrl_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vssrl_vx_u8m2_vl (vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vssrl_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vssrl_vx_u8m4_vl (vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vssrl_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vssrl_vx_u8m8_vl (vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vssrl_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vssrl_vx_u16m1_vl (vuint16m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vssrl_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vssrl_vx_u16m2_vl (vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vssrl_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vssrl_vx_u16m4_vl (vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vssrl_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vssrl_vx_u16m8_vl (vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vssrl_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vssrl_vx_u32m1_vl (vuint32m1_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vssrl_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vssrl_vx_u32m2_vl (vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vssrl_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vssrl_vx_u32m4_vl (vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint32m8_t vssrl_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vssrl_vx_u32m8_vl (vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint64m1_t vssrl_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vssrl_vx_u64m1_vl (vuint64m1_t op1, uint8_t op2, _VL_T vl);
vuint64m2_t vssrl_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vssrl_vx_u64m2_vl (vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint64m4_t vssrl_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vssrl_vx_u64m4_vl (vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint64m8_t vssrl_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vssrl_vx_u64m8_vl (vuint64m8_t op1, uint8_t op2, _VL_T vl);
vint8m1_t vssra_vv_i8m1_vl (vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vssra_vx_i8m1_vl (vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vssra_vv_i8m2_vl (vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vssra_vx_i8m2_vl (vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vssra_vv_i8m4_vl (vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vssra_vx_i8m4_vl (vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vssra_vv_i8m8_vl (vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vssra_vx_i8m8_vl (vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vssra_vv_i16m1_vl (vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vssra_vx_i16m1_vl (vint16m1_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vssra_vv_i16m2_vl (vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vssra_vx_i16m2_vl (vint16m2_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vssra_vv_i16m4_vl (vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vssra_vx_i16m4_vl (vint16m4_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vssra_vv_i16m8_vl (vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vssra_vx_i16m8_vl (vint16m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vssra_vv_i32m1_vl (vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vssra_vx_i32m1_vl (vint32m1_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vssra_vv_i32m2_vl (vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vssra_vx_i32m2_vl (vint32m2_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vssra_vv_i32m4_vl (vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vssra_vx_i32m4_vl (vint32m4_t op1, uint8_t op2, _VL_T vl);
vint32m8_t vssra_vv_i32m8_vl (vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vssra_vx_i32m8_vl (vint32m8_t op1, uint8_t op2, _VL_T vl);
vint64m1_t vssra_vv_i64m1_vl (vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vssra_vx_i64m1_vl (vint64m1_t op1, uint8_t op2, _VL_T vl);
vint64m2_t vssra_vv_i64m2_vl (vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vssra_vx_i64m2_vl (vint64m2_t op1, uint8_t op2, _VL_T vl);
vint64m4_t vssra_vv_i64m4_vl (vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vssra_vx_i64m4_vl (vint64m4_t op1, uint8_t op2, _VL_T vl);
vint64m8_t vssra_vv_i64m8_vl (vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vssra_vx_i64m8_vl (vint64m8_t op1, uint8_t op2, _VL_T vl);
// masked functions
vuint8m1_t vssrl_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vssrl_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vssrl_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vssrl_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vssrl_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vssrl_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, uint8_t op2, _VL_T vl);
vuint8m8_t vssrl_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vssrl_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vssrl_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vssrl_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vssrl_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vssrl_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vssrl_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vssrl_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint16m8_t vssrl_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vssrl_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vssrl_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vssrl_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vssrl_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vssrl_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vssrl_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vssrl_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint32m8_t vssrl_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vssrl_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint64m1_t vssrl_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vssrl_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, uint8_t op2, _VL_T vl);
vuint64m2_t vssrl_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vssrl_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint64m4_t vssrl_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vssrl_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint64m8_t vssrl_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vssrl_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, uint8_t op2, _VL_T vl);
vint8m1_t vssra_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vssra_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vssra_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vssra_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vssra_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vssra_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, uint8_t op2, _VL_T vl);
vint8m8_t vssra_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vssra_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vssra_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vssra_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vssra_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vssra_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vssra_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vssra_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, uint8_t op2, _VL_T vl);
vint16m8_t vssra_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vssra_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vssra_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vssra_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vssra_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vssra_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vssra_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vssra_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, uint8_t op2, _VL_T vl);
vint32m8_t vssra_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vssra_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, uint8_t op2, _VL_T vl);
vint64m1_t vssra_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vssra_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, uint8_t op2, _VL_T vl);
vint64m2_t vssra_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vssra_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, uint8_t op2, _VL_T vl);
vint64m4_t vssra_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vssra_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, uint8_t op2, _VL_T vl);
vint64m8_t vssra_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vssra_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, uint8_t op2, _VL_T vl);
```
### [Vector Narrowing Fixed-Point Clip Functions](rvv-intrinsic-api.md#135-vector-narrowing-fixed-point-clip-operations):

**Prototypes:**
``` C
vint8m1_t vnclip_wv_i8m1_vl (vint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vnclip_wx_i8m1_vl (vint16m2_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vnclip_wv_i8m2_vl (vint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vnclip_wx_i8m2_vl (vint16m4_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vnclip_wv_i8m4_vl (vint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vnclip_wx_i8m4_vl (vint16m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vnclip_wv_i16m1_vl (vint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vnclip_wx_i16m1_vl (vint32m2_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vnclip_wv_i16m2_vl (vint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vnclip_wx_i16m2_vl (vint32m4_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vnclip_wv_i16m4_vl (vint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vnclip_wx_i16m4_vl (vint32m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vnclip_wv_i32m1_vl (vint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vnclip_wx_i32m1_vl (vint64m2_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vnclip_wv_i32m2_vl (vint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vnclip_wx_i32m2_vl (vint64m4_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vnclip_wv_i32m4_vl (vint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vnclip_wx_i32m4_vl (vint64m8_t op1, uint8_t op2, _VL_T vl);
vuint8m1_t vnclipu_wv_u8m1_vl (vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnclipu_wx_u8m1_vl (vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vnclipu_wv_u8m2_vl (vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnclipu_wx_u8m2_vl (vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vnclipu_wv_u8m4_vl (vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnclipu_wx_u8m4_vl (vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vnclipu_wv_u16m1_vl (vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnclipu_wx_u16m1_vl (vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vnclipu_wv_u16m2_vl (vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnclipu_wx_u16m2_vl (vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vnclipu_wv_u16m4_vl (vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnclipu_wx_u16m4_vl (vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vnclipu_wv_u32m1_vl (vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnclipu_wx_u32m1_vl (vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vnclipu_wv_u32m2_vl (vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnclipu_wx_u32m2_vl (vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vnclipu_wv_u32m4_vl (vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnclipu_wx_u32m4_vl (vuint64m8_t op1, uint8_t op2, _VL_T vl);
// masked functions
vint8m1_t vnclip_wv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vnclip_wx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint16m2_t op1, uint8_t op2, _VL_T vl);
vint8m2_t vnclip_wv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vnclip_wx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint16m4_t op1, uint8_t op2, _VL_T vl);
vint8m4_t vnclip_wv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vnclip_wx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint16m8_t op1, uint8_t op2, _VL_T vl);
vint16m1_t vnclip_wv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vnclip_wx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint32m2_t op1, uint8_t op2, _VL_T vl);
vint16m2_t vnclip_wv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vnclip_wx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint32m4_t op1, uint8_t op2, _VL_T vl);
vint16m4_t vnclip_wv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vnclip_wx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint32m8_t op1, uint8_t op2, _VL_T vl);
vint32m1_t vnclip_wv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vnclip_wx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint64m2_t op1, uint8_t op2, _VL_T vl);
vint32m2_t vnclip_wv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vnclip_wx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint64m4_t op1, uint8_t op2, _VL_T vl);
vint32m4_t vnclip_wv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vnclip_wx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint64m8_t op1, uint8_t op2, _VL_T vl);
vuint8m1_t vnclipu_wv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint16m2_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vnclipu_wx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint16m2_t op1, uint8_t op2, _VL_T vl);
vuint8m2_t vnclipu_wv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint16m4_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vnclipu_wx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint16m4_t op1, uint8_t op2, _VL_T vl);
vuint8m4_t vnclipu_wv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint16m8_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vnclipu_wx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint16m8_t op1, uint8_t op2, _VL_T vl);
vuint16m1_t vnclipu_wv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vnclipu_wx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint32m2_t op1, uint8_t op2, _VL_T vl);
vuint16m2_t vnclipu_wv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vnclipu_wx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint32m4_t op1, uint8_t op2, _VL_T vl);
vuint16m4_t vnclipu_wv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vnclipu_wx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint32m8_t op1, uint8_t op2, _VL_T vl);
vuint32m1_t vnclipu_wv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint64m2_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vnclipu_wx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint64m2_t op1, uint8_t op2, _VL_T vl);
vuint32m2_t vnclipu_wv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint64m4_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vnclipu_wx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint64m4_t op1, uint8_t op2, _VL_T vl);
vuint32m4_t vnclipu_wv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint64m8_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vnclipu_wx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint64m8_t op1, uint8_t op2, _VL_T vl);
```
## Vector Floating-Point Functions:

### [Vector Single-Width Floating-Point Add/Subtract Functions](rvv-intrinsic-api.md#142-vector-single-width-floating-point-addsubtract-operations):

**Prototypes:**
``` C
vfloat16m1_t vfadd_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfadd_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfadd_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfadd_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfadd_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfadd_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfadd_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfadd_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfadd_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfadd_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfadd_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfadd_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfadd_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfadd_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfadd_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfadd_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfadd_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfadd_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfadd_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfadd_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfadd_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfadd_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfadd_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfadd_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfsub_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsub_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsub_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsub_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsub_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsub_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsub_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsub_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsub_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsub_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsub_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsub_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsub_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsub_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsub_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsub_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsub_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsub_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsub_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsub_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsub_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsub_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsub_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsub_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfrsub_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfrsub_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfrsub_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfrsub_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfrsub_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfrsub_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfrsub_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfrsub_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfrsub_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfrsub_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfrsub_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfrsub_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
// masked functions
vfloat16m1_t vfadd_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfadd_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfadd_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfadd_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfadd_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfadd_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfadd_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfadd_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfadd_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfadd_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfadd_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfadd_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfadd_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfadd_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfadd_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfadd_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfadd_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfadd_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfadd_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfadd_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfadd_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfadd_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfadd_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfadd_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfsub_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsub_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsub_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsub_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsub_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsub_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsub_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsub_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsub_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsub_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsub_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsub_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsub_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsub_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsub_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsub_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsub_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsub_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsub_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsub_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsub_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsub_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsub_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsub_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfrsub_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfrsub_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfrsub_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfrsub_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfrsub_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfrsub_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfrsub_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfrsub_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfrsub_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfrsub_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfrsub_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfrsub_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
```
### [Vector Widening Floating-Point Add/Subtract Functions](rvv-intrinsic-api.md#143-vector-widening-floating-point-addsubtract-operations):

**Prototypes:**
``` C
vfloat32m2_t vfwadd_vv_f32m2_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwadd_vf_f32m2_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat32m2_t vfwadd_wv_f32m2_vl (vfloat32m2_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwadd_wf_f32m2_vl (vfloat32m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwadd_vv_f32m4_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwadd_vf_f32m4_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwadd_wv_f32m4_vl (vfloat32m4_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwadd_wf_f32m4_vl (vfloat32m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwadd_vv_f32m8_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwadd_vf_f32m8_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwadd_wv_f32m8_vl (vfloat32m8_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwadd_wf_f32m8_vl (vfloat32m8_t op1, float16_t op2, _VL_T vl);
vfloat64m2_t vfwadd_vv_f64m2_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwadd_vf_f64m2_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat64m2_t vfwadd_wv_f64m2_vl (vfloat64m2_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwadd_wf_f64m2_vl (vfloat64m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwadd_vv_f64m4_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwadd_vf_f64m4_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwadd_wv_f64m4_vl (vfloat64m4_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwadd_wf_f64m4_vl (vfloat64m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwadd_vv_f64m8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwadd_vf_f64m8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwadd_wv_f64m8_vl (vfloat64m8_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwadd_wf_f64m8_vl (vfloat64m8_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfwsub_vv_f32m2_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwsub_vf_f32m2_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat32m2_t vfwsub_wv_f32m2_vl (vfloat32m2_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwsub_wf_f32m2_vl (vfloat32m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwsub_vv_f32m4_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwsub_vf_f32m4_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwsub_wv_f32m4_vl (vfloat32m4_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwsub_wf_f32m4_vl (vfloat32m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwsub_vv_f32m8_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwsub_vf_f32m8_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwsub_wv_f32m8_vl (vfloat32m8_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwsub_wf_f32m8_vl (vfloat32m8_t op1, float16_t op2, _VL_T vl);
vfloat64m2_t vfwsub_vv_f64m2_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwsub_vf_f64m2_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat64m2_t vfwsub_wv_f64m2_vl (vfloat64m2_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwsub_wf_f64m2_vl (vfloat64m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwsub_vv_f64m4_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwsub_vf_f64m4_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwsub_wv_f64m4_vl (vfloat64m4_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwsub_wf_f64m4_vl (vfloat64m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwsub_vv_f64m8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwsub_vf_f64m8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwsub_wv_f64m8_vl (vfloat64m8_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwsub_wf_f64m8_vl (vfloat64m8_t op1, float32_t op2, _VL_T vl);
// masked functions
vfloat32m2_t vfwadd_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwadd_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat32m2_t vfwadd_wv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwadd_wf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwadd_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwadd_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwadd_wv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwadd_wf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwadd_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwadd_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwadd_wv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwadd_wf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float16_t op2, _VL_T vl);
vfloat64m2_t vfwadd_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwadd_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat64m2_t vfwadd_wv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwadd_wf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwadd_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwadd_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwadd_wv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwadd_wf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwadd_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwadd_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwadd_wv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwadd_wf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfwsub_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwsub_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat32m2_t vfwsub_wv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwsub_wf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwsub_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwsub_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwsub_wv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwsub_wf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwsub_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwsub_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwsub_wv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwsub_wf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float16_t op2, _VL_T vl);
vfloat64m2_t vfwsub_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwsub_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat64m2_t vfwsub_wv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwsub_wf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwsub_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwsub_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwsub_wv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwsub_wf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwsub_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwsub_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwsub_wv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwsub_wf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float32_t op2, _VL_T vl);
```
### [Vector Single-Width Floating-Point Multiply/Divide Functions](rvv-intrinsic-api.md#144-vector-single-width-floating-point-multiplydivide-operations):

**Prototypes:**
``` C
vfloat16m1_t vfmul_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmul_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfmul_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmul_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfmul_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmul_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfmul_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmul_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfmul_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmul_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfmul_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmul_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfmul_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmul_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfmul_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmul_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfmul_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmul_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfmul_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmul_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfmul_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmul_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfmul_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmul_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfdiv_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfdiv_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfdiv_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfdiv_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfdiv_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfdiv_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfdiv_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfdiv_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfdiv_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfdiv_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfdiv_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfdiv_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfdiv_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfdiv_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfdiv_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfdiv_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfdiv_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfdiv_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfdiv_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfdiv_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfdiv_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfdiv_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfdiv_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfdiv_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfrdiv_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfrdiv_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfrdiv_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfrdiv_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfrdiv_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfrdiv_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfrdiv_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfrdiv_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfrdiv_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfrdiv_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfrdiv_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfrdiv_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
// masked functions
vfloat16m1_t vfmul_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmul_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfmul_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmul_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfmul_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmul_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfmul_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmul_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfmul_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmul_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfmul_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmul_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfmul_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmul_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfmul_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmul_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfmul_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmul_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfmul_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmul_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfmul_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmul_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfmul_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmul_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfdiv_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfdiv_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfdiv_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfdiv_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfdiv_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfdiv_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfdiv_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfdiv_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfdiv_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfdiv_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfdiv_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfdiv_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfdiv_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfdiv_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfdiv_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfdiv_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfdiv_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfdiv_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfdiv_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfdiv_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfdiv_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfdiv_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfdiv_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfdiv_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfrdiv_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfrdiv_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfrdiv_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfrdiv_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfrdiv_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfrdiv_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfrdiv_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfrdiv_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfrdiv_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfrdiv_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfrdiv_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfrdiv_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
```
### [Vector Widening Floating-Point Multiply Functions](rvv-intrinsic-api.md#145-vector-widening-floating-point-multiply-operations):

**Prototypes:**
``` C
vfloat32m2_t vfwmul_vv_f32m2_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwmul_vf_f32m2_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwmul_vv_f32m4_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwmul_vf_f32m4_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwmul_vv_f32m8_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwmul_vf_f32m8_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat64m2_t vfwmul_vv_f64m2_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwmul_vf_f64m2_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwmul_vv_f64m4_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwmul_vf_f64m4_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwmul_vv_f64m8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwmul_vf_f64m8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
// masked functions
vfloat32m2_t vfwmul_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwmul_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat32m4_t vfwmul_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwmul_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat32m8_t vfwmul_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwmul_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat64m2_t vfwmul_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwmul_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat64m4_t vfwmul_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwmul_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat64m8_t vfwmul_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwmul_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
```
### [Vector Single-Width Floating-Point Fused Multiply-Add Functions](rvv-intrinsic-api.md#146-vector-single-width-floating-point-fused-multiply-add-operations):

**Prototypes:**
``` C
vfloat16m1_t vfmacc_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmacc_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmacc_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmacc_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmacc_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmacc_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmacc_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmacc_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmacc_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmacc_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmacc_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmacc_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmacc_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmacc_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmacc_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmacc_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmacc_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmacc_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmacc_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmacc_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmacc_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmacc_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmacc_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmacc_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmacc_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmacc_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmacc_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmacc_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmacc_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmacc_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmacc_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmacc_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmacc_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmacc_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmacc_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmacc_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmacc_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmacc_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmacc_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmacc_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmacc_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmacc_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmacc_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmacc_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmacc_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmacc_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmacc_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmacc_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfmsac_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmsac_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmsac_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmsac_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmsac_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmsac_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmsac_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmsac_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmsac_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmsac_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmsac_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmsac_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmsac_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmsac_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmsac_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmsac_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmsac_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmsac_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmsac_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmsac_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmsac_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmsac_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmsac_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmsac_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmsac_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmsac_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmsac_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmsac_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmsac_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmsac_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmsac_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmsac_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmsac_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmsac_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmsac_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmsac_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmsac_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmsac_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmsac_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmsac_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmsac_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmsac_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmsac_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmsac_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmsac_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmsac_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmsac_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmsac_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfmadd_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmadd_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmadd_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmadd_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmadd_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmadd_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmadd_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmadd_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmadd_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmadd_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmadd_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmadd_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmadd_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmadd_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmadd_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmadd_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmadd_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmadd_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmadd_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmadd_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmadd_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmadd_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmadd_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmadd_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmadd_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmadd_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmadd_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmadd_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmadd_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmadd_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmadd_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmadd_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmadd_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmadd_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmadd_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmadd_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmadd_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmadd_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmadd_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmadd_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmadd_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmadd_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmadd_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmadd_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmadd_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmadd_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmadd_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmadd_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfmsub_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmsub_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmsub_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmsub_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmsub_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmsub_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmsub_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmsub_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmsub_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmsub_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmsub_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmsub_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmsub_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmsub_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmsub_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmsub_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmsub_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmsub_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmsub_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmsub_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmsub_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmsub_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmsub_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmsub_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmsub_vv_f16m1_vl (vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmsub_vf_f16m1_vl (vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmsub_vv_f16m2_vl (vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmsub_vf_f16m2_vl (vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmsub_vv_f16m4_vl (vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmsub_vf_f16m4_vl (vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmsub_vv_f16m8_vl (vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmsub_vf_f16m8_vl (vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmsub_vv_f32m1_vl (vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmsub_vf_f32m1_vl (vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmsub_vv_f32m2_vl (vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmsub_vf_f32m2_vl (vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmsub_vv_f32m4_vl (vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmsub_vf_f32m4_vl (vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmsub_vv_f32m8_vl (vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmsub_vf_f32m8_vl (vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmsub_vv_f64m1_vl (vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmsub_vf_f64m1_vl (vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmsub_vv_f64m2_vl (vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmsub_vf_f64m2_vl (vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmsub_vv_f64m4_vl (vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmsub_vf_f64m4_vl (vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmsub_vv_f64m8_vl (vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmsub_vf_f64m8_vl (vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
// masked functions
vfloat16m1_t vfmacc_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmacc_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmacc_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmacc_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmacc_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmacc_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmacc_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmacc_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmacc_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmacc_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmacc_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmacc_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmacc_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmacc_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmacc_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmacc_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmacc_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmacc_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmacc_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmacc_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmacc_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmacc_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmacc_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmacc_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmacc_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmacc_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmacc_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmacc_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmacc_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmacc_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmacc_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmacc_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmacc_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmacc_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmacc_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmacc_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmacc_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmacc_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmacc_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmacc_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmacc_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmacc_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmacc_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmacc_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmacc_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmacc_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmacc_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmacc_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfmsac_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmsac_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmsac_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmsac_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmsac_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmsac_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmsac_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmsac_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmsac_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmsac_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmsac_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmsac_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmsac_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmsac_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmsac_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmsac_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmsac_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmsac_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmsac_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmsac_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmsac_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmsac_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmsac_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmsac_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmsac_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmsac_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmsac_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmsac_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmsac_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmsac_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmsac_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmsac_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmsac_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmsac_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmsac_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmsac_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmsac_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmsac_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmsac_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmsac_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmsac_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmsac_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmsac_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmsac_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmsac_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmsac_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmsac_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmsac_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfmadd_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmadd_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmadd_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmadd_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmadd_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmadd_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmadd_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmadd_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmadd_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmadd_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmadd_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmadd_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmadd_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmadd_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmadd_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmadd_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmadd_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmadd_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmadd_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmadd_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmadd_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmadd_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmadd_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmadd_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmadd_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmadd_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmadd_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmadd_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmadd_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmadd_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmadd_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmadd_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmadd_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmadd_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmadd_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmadd_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmadd_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmadd_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmadd_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmadd_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmadd_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmadd_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmadd_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmadd_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmadd_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmadd_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmadd_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmadd_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfmsub_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmsub_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfmsub_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmsub_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfmsub_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmsub_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfmsub_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmsub_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfmsub_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmsub_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfmsub_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmsub_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfmsub_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmsub_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfmsub_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmsub_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfmsub_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmsub_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfmsub_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmsub_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfmsub_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmsub_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfmsub_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmsub_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat16m1_t vfnmsub_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfnmsub_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m2_t vfnmsub_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfnmsub_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m4_t vfnmsub_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfnmsub_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m8_t vfnmsub_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfnmsub_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t acc, float16_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat32m1_t vfnmsub_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfnmsub_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m2_t vfnmsub_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfnmsub_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m4_t vfnmsub_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfnmsub_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m8_t vfnmsub_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfnmsub_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t acc, float32_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat64m1_t vfnmsub_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfnmsub_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t acc, float64_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m2_t vfnmsub_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfnmsub_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t acc, float64_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m4_t vfnmsub_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfnmsub_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t acc, float64_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m8_t vfnmsub_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfnmsub_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t acc, float64_t op1, vfloat64m8_t op2, _VL_T vl);
```
### [Vector Widening Floating-Point Fused Multiply-Add Functions](rvv-intrinsic-api.md#147-vector-widening-floating-point-fused-multiply-add-operations):

**Prototypes:**
``` C
vfloat32m2_t vfwmacc_vv_f16m1_vl (vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwmacc_vf_f16m1_vl (vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwmacc_vv_f16m2_vl (vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwmacc_vf_f16m2_vl (vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwmacc_vv_f16m4_vl (vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwmacc_vf_f16m4_vl (vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwmacc_vv_f32m1_vl (vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwmacc_vf_f32m1_vl (vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwmacc_vv_f32m2_vl (vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwmacc_vf_f32m2_vl (vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwmacc_vv_f32m4_vl (vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwmacc_vf_f32m4_vl (vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m2_t vfwnmacc_vv_f16m1_vl (vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwnmacc_vf_f16m1_vl (vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwnmacc_vv_f16m2_vl (vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwnmacc_vf_f16m2_vl (vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwnmacc_vv_f16m4_vl (vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwnmacc_vf_f16m4_vl (vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwnmacc_vv_f32m1_vl (vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwnmacc_vf_f32m1_vl (vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwnmacc_vv_f32m2_vl (vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwnmacc_vf_f32m2_vl (vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwnmacc_vv_f32m4_vl (vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwnmacc_vf_f32m4_vl (vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m2_t vfwmsac_vv_f16m1_vl (vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwmsac_vf_f16m1_vl (vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwmsac_vv_f16m2_vl (vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwmsac_vf_f16m2_vl (vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwmsac_vv_f16m4_vl (vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwmsac_vf_f16m4_vl (vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwmsac_vv_f32m1_vl (vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwmsac_vf_f32m1_vl (vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwmsac_vv_f32m2_vl (vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwmsac_vf_f32m2_vl (vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwmsac_vv_f32m4_vl (vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwmsac_vf_f32m4_vl (vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m2_t vfwnmsac_vv_f16m1_vl (vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwnmsac_vf_f16m1_vl (vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwnmsac_vv_f16m2_vl (vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwnmsac_vf_f16m2_vl (vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwnmsac_vv_f16m4_vl (vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwnmsac_vf_f16m4_vl (vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwnmsac_vv_f32m1_vl (vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwnmsac_vf_f32m1_vl (vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwnmsac_vv_f32m2_vl (vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwnmsac_vf_f32m2_vl (vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwnmsac_vv_f32m4_vl (vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwnmsac_vf_f32m4_vl (vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
// masked functions
vfloat32m2_t vfwmacc_vv_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwmacc_vf_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwmacc_vv_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwmacc_vf_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwmacc_vv_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwmacc_vf_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwmacc_vv_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwmacc_vf_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwmacc_vv_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwmacc_vf_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwmacc_vv_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwmacc_vf_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m2_t vfwnmacc_vv_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwnmacc_vf_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwnmacc_vv_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwnmacc_vf_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwnmacc_vv_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwnmacc_vf_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwnmacc_vv_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwnmacc_vf_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwnmacc_vv_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwnmacc_vf_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwnmacc_vv_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwnmacc_vf_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m2_t vfwmsac_vv_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwmsac_vf_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwmsac_vv_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwmsac_vf_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwmsac_vv_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwmsac_vf_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwmsac_vv_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwmsac_vf_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwmsac_vv_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwmsac_vf_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwmsac_vv_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwmsac_vf_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m2_t vfwnmsac_vv_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m2_t vfwnmsac_vf_f16m1_m_vl (vbool16_t mask, vfloat32m2_t acc, float16_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat32m4_t vfwnmsac_vv_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m4_t vfwnmsac_vf_f16m2_m_vl (vbool8_t mask, vfloat32m4_t acc, float16_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat32m8_t vfwnmsac_vv_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat32m8_t vfwnmsac_vf_f16m4_m_vl (vbool4_t mask, vfloat32m8_t acc, float16_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat64m2_t vfwnmsac_vv_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m2_t vfwnmsac_vf_f32m1_m_vl (vbool32_t mask, vfloat64m2_t acc, float32_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat64m4_t vfwnmsac_vv_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m4_t vfwnmsac_vf_f32m2_m_vl (vbool16_t mask, vfloat64m4_t acc, float32_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat64m8_t vfwnmsac_vv_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat64m8_t vfwnmsac_vf_f32m4_m_vl (vbool8_t mask, vfloat64m8_t acc, float32_t op1, vfloat32m4_t op2, _VL_T vl);
```
### [Vector Floating-Point Square-Root Functions](rvv-intrinsic-api.md#148-vector-floating-point-square-root-operations):

**Prototypes:**
``` C
vfloat16m1_t vfsqrt_v_f16m1_vl (vfloat16m1_t op1, _VL_T vl);
vfloat16m2_t vfsqrt_v_f16m2_vl (vfloat16m2_t op1, _VL_T vl);
vfloat16m4_t vfsqrt_v_f16m4_vl (vfloat16m4_t op1, _VL_T vl);
vfloat16m8_t vfsqrt_v_f16m8_vl (vfloat16m8_t op1, _VL_T vl);
vfloat32m1_t vfsqrt_v_f32m1_vl (vfloat32m1_t op1, _VL_T vl);
vfloat32m2_t vfsqrt_v_f32m2_vl (vfloat32m2_t op1, _VL_T vl);
vfloat32m4_t vfsqrt_v_f32m4_vl (vfloat32m4_t op1, _VL_T vl);
vfloat32m8_t vfsqrt_v_f32m8_vl (vfloat32m8_t op1, _VL_T vl);
vfloat64m1_t vfsqrt_v_f64m1_vl (vfloat64m1_t op1, _VL_T vl);
vfloat64m2_t vfsqrt_v_f64m2_vl (vfloat64m2_t op1, _VL_T vl);
vfloat64m4_t vfsqrt_v_f64m4_vl (vfloat64m4_t op1, _VL_T vl);
vfloat64m8_t vfsqrt_v_f64m8_vl (vfloat64m8_t op1, _VL_T vl);
// masked functions
vfloat16m1_t vfsqrt_v_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, _VL_T vl);
vfloat16m2_t vfsqrt_v_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, _VL_T vl);
vfloat16m4_t vfsqrt_v_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, _VL_T vl);
vfloat16m8_t vfsqrt_v_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, _VL_T vl);
vfloat32m1_t vfsqrt_v_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, _VL_T vl);
vfloat32m2_t vfsqrt_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, _VL_T vl);
vfloat32m4_t vfsqrt_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, _VL_T vl);
vfloat32m8_t vfsqrt_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, _VL_T vl);
vfloat64m1_t vfsqrt_v_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, _VL_T vl);
vfloat64m2_t vfsqrt_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, _VL_T vl);
vfloat64m4_t vfsqrt_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, _VL_T vl);
vfloat64m8_t vfsqrt_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, _VL_T vl);
```
### [Vector Floating-Point MIN/MAX Functions](rvv-intrinsic-api.md#149-vector-floating-point-minmax-operations):

**Prototypes:**
``` C
vfloat16m1_t vfmin_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmin_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfmin_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmin_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfmin_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmin_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfmin_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmin_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfmin_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmin_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfmin_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmin_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfmin_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmin_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfmin_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmin_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfmin_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmin_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfmin_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmin_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfmin_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmin_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfmin_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmin_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfmax_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmax_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfmax_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmax_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfmax_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmax_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfmax_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmax_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfmax_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmax_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfmax_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmax_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfmax_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmax_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfmax_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmax_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfmax_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmax_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfmax_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmax_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfmax_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmax_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfmax_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmax_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
// masked functions
vfloat16m1_t vfmin_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmin_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfmin_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmin_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfmin_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmin_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfmin_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmin_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfmin_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmin_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfmin_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmin_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfmin_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmin_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfmin_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmin_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfmin_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmin_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfmin_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmin_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfmin_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmin_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfmin_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmin_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfmax_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmax_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfmax_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmax_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfmax_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmax_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfmax_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmax_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfmax_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmax_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfmax_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmax_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfmax_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmax_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfmax_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmax_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfmax_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmax_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfmax_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmax_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfmax_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmax_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfmax_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmax_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
```
### [Vector Floating-Point Sign-Injection Functions](rvv-intrinsic-api.md#1410-vector-floating-point-sign-injection-operations):

**Prototypes:**
``` C
vfloat16m1_t vfsgnj_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsgnj_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsgnj_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsgnj_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsgnj_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsgnj_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsgnj_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsgnj_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsgnj_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsgnj_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsgnj_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsgnj_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsgnj_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsgnj_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsgnj_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsgnj_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsgnj_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsgnj_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsgnj_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsgnj_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsgnj_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsgnj_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsgnj_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsgnj_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfsgnjn_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsgnjn_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsgnjn_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsgnjn_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsgnjn_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsgnjn_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsgnjn_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsgnjn_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsgnjn_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsgnjn_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsgnjn_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsgnjn_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsgnjn_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsgnjn_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsgnjn_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsgnjn_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsgnjn_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsgnjn_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsgnjn_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsgnjn_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsgnjn_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsgnjn_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsgnjn_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsgnjn_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfsgnjx_vv_f16m1_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsgnjx_vf_f16m1_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsgnjx_vv_f16m2_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsgnjx_vf_f16m2_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsgnjx_vv_f16m4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsgnjx_vf_f16m4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsgnjx_vv_f16m8_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsgnjx_vf_f16m8_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsgnjx_vv_f32m1_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsgnjx_vf_f32m1_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsgnjx_vv_f32m2_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsgnjx_vf_f32m2_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsgnjx_vv_f32m4_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsgnjx_vf_f32m4_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsgnjx_vv_f32m8_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsgnjx_vf_f32m8_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsgnjx_vv_f64m1_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsgnjx_vf_f64m1_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsgnjx_vv_f64m2_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsgnjx_vf_f64m2_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsgnjx_vv_f64m4_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsgnjx_vf_f64m4_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsgnjx_vv_f64m8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsgnjx_vf_f64m8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
// masked functions
vfloat16m1_t vfsgnj_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsgnj_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsgnj_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsgnj_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsgnj_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsgnj_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsgnj_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsgnj_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsgnj_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsgnj_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsgnj_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsgnj_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsgnj_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsgnj_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsgnj_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsgnj_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsgnj_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsgnj_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsgnj_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsgnj_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsgnj_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsgnj_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsgnj_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsgnj_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfsgnjn_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsgnjn_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsgnjn_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsgnjn_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsgnjn_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsgnjn_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsgnjn_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsgnjn_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsgnjn_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsgnjn_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsgnjn_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsgnjn_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsgnjn_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsgnjn_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsgnjn_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsgnjn_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsgnjn_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsgnjn_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsgnjn_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsgnjn_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsgnjn_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsgnjn_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsgnjn_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsgnjn_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vfloat16m1_t vfsgnjx_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfsgnjx_vf_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vfsgnjx_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfsgnjx_vf_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vfsgnjx_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfsgnjx_vf_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vfsgnjx_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfsgnjx_vf_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vfsgnjx_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfsgnjx_vf_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vfsgnjx_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfsgnjx_vf_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vfsgnjx_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfsgnjx_vf_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vfsgnjx_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfsgnjx_vf_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vfsgnjx_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfsgnjx_vf_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vfsgnjx_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfsgnjx_vf_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vfsgnjx_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfsgnjx_vf_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vfsgnjx_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfsgnjx_vf_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
```
### [Vector Floating-Point Compare Functions](rvv-intrinsic-api.md#1411-vector-floating-point-compare-operations):

**Prototypes:**
``` C
vbool16_t vmfeq_vv_f16m1_b16_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfeq_vf_f16m1_b16_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfeq_vv_f16m2_b8_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfeq_vf_f16m2_b8_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfeq_vv_f16m4_b4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfeq_vf_f16m4_b4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfeq_vv_f16m8_b2_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfeq_vf_f16m8_b2_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfeq_vv_f32m1_b32_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfeq_vf_f32m1_b32_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfeq_vv_f32m2_b16_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfeq_vf_f32m2_b16_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfeq_vv_f32m4_b8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfeq_vf_f32m4_b8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfeq_vv_f32m8_b4_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfeq_vf_f32m8_b4_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfeq_vv_f64m1_b64_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfeq_vf_f64m1_b64_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfeq_vv_f64m2_b32_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfeq_vf_f64m2_b32_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfeq_vv_f64m4_b16_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfeq_vf_f64m4_b16_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfeq_vv_f64m8_b8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfeq_vf_f64m8_b8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfne_vv_f16m1_b16_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfne_vf_f16m1_b16_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfne_vv_f16m2_b8_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfne_vf_f16m2_b8_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfne_vv_f16m4_b4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfne_vf_f16m4_b4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfne_vv_f16m8_b2_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfne_vf_f16m8_b2_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfne_vv_f32m1_b32_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfne_vf_f32m1_b32_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfne_vv_f32m2_b16_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfne_vf_f32m2_b16_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfne_vv_f32m4_b8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfne_vf_f32m4_b8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfne_vv_f32m8_b4_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfne_vf_f32m8_b4_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfne_vv_f64m1_b64_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfne_vf_f64m1_b64_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfne_vv_f64m2_b32_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfne_vf_f64m2_b32_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfne_vv_f64m4_b16_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfne_vf_f64m4_b16_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfne_vv_f64m8_b8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfne_vf_f64m8_b8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmflt_vv_f16m1_b16_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmflt_vf_f16m1_b16_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmflt_vv_f16m2_b8_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmflt_vf_f16m2_b8_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmflt_vv_f16m4_b4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmflt_vf_f16m4_b4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmflt_vv_f16m8_b2_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmflt_vf_f16m8_b2_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmflt_vv_f32m1_b32_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmflt_vf_f32m1_b32_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmflt_vv_f32m2_b16_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmflt_vf_f32m2_b16_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmflt_vv_f32m4_b8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmflt_vf_f32m4_b8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmflt_vv_f32m8_b4_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmflt_vf_f32m8_b4_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmflt_vv_f64m1_b64_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmflt_vf_f64m1_b64_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmflt_vv_f64m2_b32_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmflt_vf_f64m2_b32_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmflt_vv_f64m4_b16_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmflt_vf_f64m4_b16_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmflt_vv_f64m8_b8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmflt_vf_f64m8_b8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfle_vv_f16m1_b16_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfle_vf_f16m1_b16_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfle_vv_f16m2_b8_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfle_vf_f16m2_b8_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfle_vv_f16m4_b4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfle_vf_f16m4_b4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfle_vv_f16m8_b2_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfle_vf_f16m8_b2_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfle_vv_f32m1_b32_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfle_vf_f32m1_b32_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfle_vv_f32m2_b16_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfle_vf_f32m2_b16_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfle_vv_f32m4_b8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfle_vf_f32m4_b8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfle_vv_f32m8_b4_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfle_vf_f32m8_b4_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfle_vv_f64m1_b64_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfle_vf_f64m1_b64_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfle_vv_f64m2_b32_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfle_vf_f64m2_b32_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfle_vv_f64m4_b16_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfle_vf_f64m4_b16_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfle_vv_f64m8_b8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfle_vf_f64m8_b8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfgt_vv_f16m1_b16_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfgt_vf_f16m1_b16_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfgt_vv_f16m2_b8_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfgt_vf_f16m2_b8_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfgt_vv_f16m4_b4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfgt_vf_f16m4_b4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfgt_vv_f16m8_b2_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfgt_vf_f16m8_b2_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfgt_vv_f32m1_b32_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfgt_vf_f32m1_b32_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfgt_vv_f32m2_b16_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfgt_vf_f32m2_b16_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfgt_vv_f32m4_b8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfgt_vf_f32m4_b8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfgt_vv_f32m8_b4_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfgt_vf_f32m8_b4_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfgt_vv_f64m1_b64_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfgt_vf_f64m1_b64_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfgt_vv_f64m2_b32_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfgt_vf_f64m2_b32_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfgt_vv_f64m4_b16_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfgt_vf_f64m4_b16_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfgt_vv_f64m8_b8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfgt_vf_f64m8_b8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfge_vv_f16m1_b16_vl (vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfge_vf_f16m1_b16_vl (vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfge_vv_f16m2_b8_vl (vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfge_vf_f16m2_b8_vl (vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfge_vv_f16m4_b4_vl (vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfge_vf_f16m4_b4_vl (vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfge_vv_f16m8_b2_vl (vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfge_vf_f16m8_b2_vl (vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfge_vv_f32m1_b32_vl (vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfge_vf_f32m1_b32_vl (vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfge_vv_f32m2_b16_vl (vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfge_vf_f32m2_b16_vl (vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfge_vv_f32m4_b8_vl (vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfge_vf_f32m4_b8_vl (vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfge_vv_f32m8_b4_vl (vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfge_vf_f32m8_b4_vl (vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfge_vv_f64m1_b64_vl (vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfge_vf_f64m1_b64_vl (vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfge_vv_f64m2_b32_vl (vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfge_vf_f64m2_b32_vl (vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfge_vv_f64m4_b16_vl (vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfge_vf_f64m4_b16_vl (vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfge_vv_f64m8_b8_vl (vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfge_vf_f64m8_b8_vl (vfloat64m8_t op1, float64_t op2, _VL_T vl);
// masked functions
vbool16_t vmfeq_vv_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfeq_vf_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfeq_vv_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfeq_vf_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfeq_vv_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfeq_vf_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfeq_vv_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfeq_vf_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfeq_vv_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfeq_vf_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfeq_vv_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfeq_vf_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfeq_vv_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfeq_vf_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfeq_vv_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfeq_vf_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfeq_vv_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfeq_vf_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfeq_vv_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfeq_vf_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfeq_vv_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfeq_vf_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfeq_vv_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfeq_vf_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfne_vv_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfne_vf_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfne_vv_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfne_vf_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfne_vv_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfne_vf_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfne_vv_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfne_vf_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfne_vv_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfne_vf_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfne_vv_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfne_vf_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfne_vv_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfne_vf_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfne_vv_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfne_vf_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfne_vv_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfne_vf_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfne_vv_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfne_vf_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfne_vv_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfne_vf_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfne_vv_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfne_vf_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmflt_vv_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmflt_vf_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmflt_vv_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmflt_vf_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmflt_vv_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmflt_vf_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmflt_vv_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmflt_vf_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmflt_vv_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmflt_vf_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmflt_vv_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmflt_vf_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmflt_vv_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmflt_vf_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmflt_vv_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmflt_vf_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmflt_vv_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmflt_vf_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmflt_vv_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmflt_vf_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmflt_vv_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmflt_vf_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmflt_vv_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmflt_vf_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfle_vv_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfle_vf_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfle_vv_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfle_vf_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfle_vv_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfle_vf_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfle_vv_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfle_vf_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfle_vv_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfle_vf_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfle_vv_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfle_vf_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfle_vv_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfle_vf_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfle_vv_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfle_vf_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfle_vv_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfle_vf_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfle_vv_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfle_vf_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfle_vv_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfle_vf_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfle_vv_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfle_vf_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfgt_vv_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfgt_vf_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfgt_vv_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfgt_vf_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfgt_vv_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfgt_vf_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfgt_vv_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfgt_vf_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfgt_vv_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfgt_vf_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfgt_vv_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfgt_vf_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfgt_vv_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfgt_vf_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfgt_vv_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfgt_vf_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfgt_vv_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfgt_vf_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfgt_vv_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfgt_vf_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfgt_vv_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfgt_vf_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfgt_vv_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfgt_vf_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfge_vv_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vbool16_t vmfge_vf_f16m1_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vbool8_t vmfge_vv_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vbool8_t vmfge_vf_f16m2_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vbool4_t vmfge_vv_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vbool4_t vmfge_vf_f16m4_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vbool2_t vmfge_vv_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vbool2_t vmfge_vf_f16m8_b2_m_vl (vbool2_t mask, vbool2_t maskedoff, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vbool32_t vmfge_vv_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vbool32_t vmfge_vf_f32m1_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vbool16_t vmfge_vv_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vbool16_t vmfge_vf_f32m2_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vbool8_t vmfge_vv_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vbool8_t vmfge_vf_f32m4_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vbool4_t vmfge_vv_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vbool4_t vmfge_vf_f32m8_b4_m_vl (vbool4_t mask, vbool4_t maskedoff, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vbool64_t vmfge_vv_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vbool64_t vmfge_vf_f64m1_b64_m_vl (vbool64_t mask, vbool64_t maskedoff, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vbool32_t vmfge_vv_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vbool32_t vmfge_vf_f64m2_b32_m_vl (vbool32_t mask, vbool32_t maskedoff, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vbool16_t vmfge_vv_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vbool16_t vmfge_vf_f64m4_b16_m_vl (vbool16_t mask, vbool16_t maskedoff, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vbool8_t vmfge_vv_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vbool8_t vmfge_vf_f64m8_b8_m_vl (vbool8_t mask, vbool8_t maskedoff, vfloat64m8_t op1, float64_t op2, _VL_T vl);
```
### [Vector Floating-Point Classify Functions](rvv-intrinsic-api.md#1412-vector-floating-point-classify-operations):

**Prototypes:**
``` C
vuint16m1_t vfclass_v_f16m1_vl (vfloat16m1_t op1, _VL_T vl);
vuint16m2_t vfclass_v_f16m2_vl (vfloat16m2_t op1, _VL_T vl);
vuint16m4_t vfclass_v_f16m4_vl (vfloat16m4_t op1, _VL_T vl);
vuint16m8_t vfclass_v_f16m8_vl (vfloat16m8_t op1, _VL_T vl);
vuint32m1_t vfclass_v_f32m1_vl (vfloat32m1_t op1, _VL_T vl);
vuint32m2_t vfclass_v_f32m2_vl (vfloat32m2_t op1, _VL_T vl);
vuint32m4_t vfclass_v_f32m4_vl (vfloat32m4_t op1, _VL_T vl);
vuint32m8_t vfclass_v_f32m8_vl (vfloat32m8_t op1, _VL_T vl);
vuint64m1_t vfclass_v_f64m1_vl (vfloat64m1_t op1, _VL_T vl);
vuint64m2_t vfclass_v_f64m2_vl (vfloat64m2_t op1, _VL_T vl);
vuint64m4_t vfclass_v_f64m4_vl (vfloat64m4_t op1, _VL_T vl);
vuint64m8_t vfclass_v_f64m8_vl (vfloat64m8_t op1, _VL_T vl);
// masked functions
vuint16m1_t vfclass_v_f16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vfloat16m1_t op1, _VL_T vl);
vuint16m2_t vfclass_v_f16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vfloat16m2_t op1, _VL_T vl);
vuint16m4_t vfclass_v_f16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vfloat16m4_t op1, _VL_T vl);
vuint16m8_t vfclass_v_f16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vfloat16m8_t op1, _VL_T vl);
vuint32m1_t vfclass_v_f32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vfloat32m1_t op1, _VL_T vl);
vuint32m2_t vfclass_v_f32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vfloat32m2_t op1, _VL_T vl);
vuint32m4_t vfclass_v_f32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vfloat32m4_t op1, _VL_T vl);
vuint32m8_t vfclass_v_f32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vfloat32m8_t op1, _VL_T vl);
vuint64m1_t vfclass_v_f64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vfloat64m1_t op1, _VL_T vl);
vuint64m2_t vfclass_v_f64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vfloat64m2_t op1, _VL_T vl);
vuint64m4_t vfclass_v_f64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vfloat64m4_t op1, _VL_T vl);
vuint64m8_t vfclass_v_f64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vfloat64m8_t op1, _VL_T vl);
```
### [Vector Floating-Point Merge Functions](rvv-intrinsic-api.md#1413-vector-floating-point-merge-operations):

**Prototypes:**
``` C
vfloat16m1_t vmerge_vvm_f16m1_m_vl (vbool16_t mask, vfloat16m1_t op1, vfloat16m1_t op2, _VL_T vl);
vfloat16m1_t vfmerge_vfm_f16m1_m_vl (vbool16_t mask, vfloat16m1_t op1, float16_t op2, _VL_T vl);
vfloat16m2_t vmerge_vvm_f16m2_m_vl (vbool8_t mask, vfloat16m2_t op1, vfloat16m2_t op2, _VL_T vl);
vfloat16m2_t vfmerge_vfm_f16m2_m_vl (vbool8_t mask, vfloat16m2_t op1, float16_t op2, _VL_T vl);
vfloat16m4_t vmerge_vvm_f16m4_m_vl (vbool4_t mask, vfloat16m4_t op1, vfloat16m4_t op2, _VL_T vl);
vfloat16m4_t vfmerge_vfm_f16m4_m_vl (vbool4_t mask, vfloat16m4_t op1, float16_t op2, _VL_T vl);
vfloat16m8_t vmerge_vvm_f16m8_m_vl (vbool2_t mask, vfloat16m8_t op1, vfloat16m8_t op2, _VL_T vl);
vfloat16m8_t vfmerge_vfm_f16m8_m_vl (vbool2_t mask, vfloat16m8_t op1, float16_t op2, _VL_T vl);
vfloat32m1_t vmerge_vvm_f32m1_m_vl (vbool32_t mask, vfloat32m1_t op1, vfloat32m1_t op2, _VL_T vl);
vfloat32m1_t vfmerge_vfm_f32m1_m_vl (vbool32_t mask, vfloat32m1_t op1, float32_t op2, _VL_T vl);
vfloat32m2_t vmerge_vvm_f32m2_m_vl (vbool16_t mask, vfloat32m2_t op1, vfloat32m2_t op2, _VL_T vl);
vfloat32m2_t vfmerge_vfm_f32m2_m_vl (vbool16_t mask, vfloat32m2_t op1, float32_t op2, _VL_T vl);
vfloat32m4_t vmerge_vvm_f32m4_m_vl (vbool8_t mask, vfloat32m4_t op1, vfloat32m4_t op2, _VL_T vl);
vfloat32m4_t vfmerge_vfm_f32m4_m_vl (vbool8_t mask, vfloat32m4_t op1, float32_t op2, _VL_T vl);
vfloat32m8_t vmerge_vvm_f32m8_m_vl (vbool4_t mask, vfloat32m8_t op1, vfloat32m8_t op2, _VL_T vl);
vfloat32m8_t vfmerge_vfm_f32m8_m_vl (vbool4_t mask, vfloat32m8_t op1, float32_t op2, _VL_T vl);
vfloat64m1_t vmerge_vvm_f64m1_m_vl (vbool64_t mask, vfloat64m1_t op1, vfloat64m1_t op2, _VL_T vl);
vfloat64m1_t vfmerge_vfm_f64m1_m_vl (vbool64_t mask, vfloat64m1_t op1, float64_t op2, _VL_T vl);
vfloat64m2_t vmerge_vvm_f64m2_m_vl (vbool32_t mask, vfloat64m2_t op1, vfloat64m2_t op2, _VL_T vl);
vfloat64m2_t vfmerge_vfm_f64m2_m_vl (vbool32_t mask, vfloat64m2_t op1, float64_t op2, _VL_T vl);
vfloat64m4_t vmerge_vvm_f64m4_m_vl (vbool16_t mask, vfloat64m4_t op1, vfloat64m4_t op2, _VL_T vl);
vfloat64m4_t vfmerge_vfm_f64m4_m_vl (vbool16_t mask, vfloat64m4_t op1, float64_t op2, _VL_T vl);
vfloat64m8_t vmerge_vvm_f64m8_m_vl (vbool8_t mask, vfloat64m8_t op1, vfloat64m8_t op2, _VL_T vl);
vfloat64m8_t vfmerge_vfm_f64m8_m_vl (vbool8_t mask, vfloat64m8_t op1, float64_t op2, _VL_T vl);
```
### [Vector Floating-Point Move Functions](rvv-intrinsic-api.md#1414-vector-floating-point-move-operations):

**Prototypes:**
``` C
vfloat16m1_t vmv_v_v_f16m1_vl (vfloat16m1_t src, _VL_T vl);
vfloat16m1_t vfmv_v_f_f16m1_vl (float16_t src, _VL_T vl);
vfloat16m2_t vmv_v_v_f16m2_vl (vfloat16m2_t src, _VL_T vl);
vfloat16m2_t vfmv_v_f_f16m2_vl (float16_t src, _VL_T vl);
vfloat16m4_t vmv_v_v_f16m4_vl (vfloat16m4_t src, _VL_T vl);
vfloat16m4_t vfmv_v_f_f16m4_vl (float16_t src, _VL_T vl);
vfloat16m8_t vmv_v_v_f16m8_vl (vfloat16m8_t src, _VL_T vl);
vfloat16m8_t vfmv_v_f_f16m8_vl (float16_t src, _VL_T vl);
vfloat32m1_t vmv_v_v_f32m1_vl (vfloat32m1_t src, _VL_T vl);
vfloat32m1_t vfmv_v_f_f32m1_vl (float32_t src, _VL_T vl);
vfloat32m2_t vmv_v_v_f32m2_vl (vfloat32m2_t src, _VL_T vl);
vfloat32m2_t vfmv_v_f_f32m2_vl (float32_t src, _VL_T vl);
vfloat32m4_t vmv_v_v_f32m4_vl (vfloat32m4_t src, _VL_T vl);
vfloat32m4_t vfmv_v_f_f32m4_vl (float32_t src, _VL_T vl);
vfloat32m8_t vmv_v_v_f32m8_vl (vfloat32m8_t src, _VL_T vl);
vfloat32m8_t vfmv_v_f_f32m8_vl (float32_t src, _VL_T vl);
vfloat64m1_t vmv_v_v_f64m1_vl (vfloat64m1_t src, _VL_T vl);
vfloat64m1_t vfmv_v_f_f64m1_vl (float64_t src, _VL_T vl);
vfloat64m2_t vmv_v_v_f64m2_vl (vfloat64m2_t src, _VL_T vl);
vfloat64m2_t vfmv_v_f_f64m2_vl (float64_t src, _VL_T vl);
vfloat64m4_t vmv_v_v_f64m4_vl (vfloat64m4_t src, _VL_T vl);
vfloat64m4_t vfmv_v_f_f64m4_vl (float64_t src, _VL_T vl);
vfloat64m8_t vmv_v_v_f64m8_vl (vfloat64m8_t src, _VL_T vl);
vfloat64m8_t vfmv_v_f_f64m8_vl (float64_t src, _VL_T vl);
```
### [Single-Width Floating-Point/Integer Type-Convert Functions](rvv-intrinsic-api.md#1415-single-width-floating-pointinteger-type-convert-operations):

**Prototypes:**
``` C
vint16m1_t vfcvt_x_f_v_i16m1_vl (vfloat16m1_t src, _VL_T vl);
vint16m2_t vfcvt_x_f_v_i16m2_vl (vfloat16m2_t src, _VL_T vl);
vint16m4_t vfcvt_x_f_v_i16m4_vl (vfloat16m4_t src, _VL_T vl);
vint16m8_t vfcvt_x_f_v_i16m8_vl (vfloat16m8_t src, _VL_T vl);
vuint16m1_t vfcvt_xu_f_v_u16m1_vl (vfloat16m1_t src, _VL_T vl);
vuint16m2_t vfcvt_xu_f_v_u16m2_vl (vfloat16m2_t src, _VL_T vl);
vuint16m4_t vfcvt_xu_f_v_u16m4_vl (vfloat16m4_t src, _VL_T vl);
vuint16m8_t vfcvt_xu_f_v_u16m8_vl (vfloat16m8_t src, _VL_T vl);
vfloat16m1_t vfcvt_f_x_v_f16m1_vl (vint16m1_t src, _VL_T vl);
vfloat16m2_t vfcvt_f_x_v_f16m2_vl (vint16m2_t src, _VL_T vl);
vfloat16m4_t vfcvt_f_x_v_f16m4_vl (vint16m4_t src, _VL_T vl);
vfloat16m8_t vfcvt_f_x_v_f16m8_vl (vint16m8_t src, _VL_T vl);
vfloat16m1_t vfcvt_f_xu_v_f16m1_vl (vuint16m1_t src, _VL_T vl);
vfloat16m2_t vfcvt_f_xu_v_f16m2_vl (vuint16m2_t src, _VL_T vl);
vfloat16m4_t vfcvt_f_xu_v_f16m4_vl (vuint16m4_t src, _VL_T vl);
vfloat16m8_t vfcvt_f_xu_v_f16m8_vl (vuint16m8_t src, _VL_T vl);
vint32m1_t vfcvt_x_f_v_i32m1_vl (vfloat32m1_t src, _VL_T vl);
vint32m2_t vfcvt_x_f_v_i32m2_vl (vfloat32m2_t src, _VL_T vl);
vint32m4_t vfcvt_x_f_v_i32m4_vl (vfloat32m4_t src, _VL_T vl);
vint32m8_t vfcvt_x_f_v_i32m8_vl (vfloat32m8_t src, _VL_T vl);
vuint32m1_t vfcvt_xu_f_v_u32m1_vl (vfloat32m1_t src, _VL_T vl);
vuint32m2_t vfcvt_xu_f_v_u32m2_vl (vfloat32m2_t src, _VL_T vl);
vuint32m4_t vfcvt_xu_f_v_u32m4_vl (vfloat32m4_t src, _VL_T vl);
vuint32m8_t vfcvt_xu_f_v_u32m8_vl (vfloat32m8_t src, _VL_T vl);
vfloat32m1_t vfcvt_f_x_v_f32m1_vl (vint32m1_t src, _VL_T vl);
vfloat32m2_t vfcvt_f_x_v_f32m2_vl (vint32m2_t src, _VL_T vl);
vfloat32m4_t vfcvt_f_x_v_f32m4_vl (vint32m4_t src, _VL_T vl);
vfloat32m8_t vfcvt_f_x_v_f32m8_vl (vint32m8_t src, _VL_T vl);
vfloat32m1_t vfcvt_f_xu_v_f32m1_vl (vuint32m1_t src, _VL_T vl);
vfloat32m2_t vfcvt_f_xu_v_f32m2_vl (vuint32m2_t src, _VL_T vl);
vfloat32m4_t vfcvt_f_xu_v_f32m4_vl (vuint32m4_t src, _VL_T vl);
vfloat32m8_t vfcvt_f_xu_v_f32m8_vl (vuint32m8_t src, _VL_T vl);
vint64m1_t vfcvt_x_f_v_i64m1_vl (vfloat64m1_t src, _VL_T vl);
vint64m2_t vfcvt_x_f_v_i64m2_vl (vfloat64m2_t src, _VL_T vl);
vint64m4_t vfcvt_x_f_v_i64m4_vl (vfloat64m4_t src, _VL_T vl);
vint64m8_t vfcvt_x_f_v_i64m8_vl (vfloat64m8_t src, _VL_T vl);
vuint64m1_t vfcvt_xu_f_v_u64m1_vl (vfloat64m1_t src, _VL_T vl);
vuint64m2_t vfcvt_xu_f_v_u64m2_vl (vfloat64m2_t src, _VL_T vl);
vuint64m4_t vfcvt_xu_f_v_u64m4_vl (vfloat64m4_t src, _VL_T vl);
vuint64m8_t vfcvt_xu_f_v_u64m8_vl (vfloat64m8_t src, _VL_T vl);
vfloat64m1_t vfcvt_f_x_v_f64m1_vl (vint64m1_t src, _VL_T vl);
vfloat64m2_t vfcvt_f_x_v_f64m2_vl (vint64m2_t src, _VL_T vl);
vfloat64m4_t vfcvt_f_x_v_f64m4_vl (vint64m4_t src, _VL_T vl);
vfloat64m8_t vfcvt_f_x_v_f64m8_vl (vint64m8_t src, _VL_T vl);
vfloat64m1_t vfcvt_f_xu_v_f64m1_vl (vuint64m1_t src, _VL_T vl);
vfloat64m2_t vfcvt_f_xu_v_f64m2_vl (vuint64m2_t src, _VL_T vl);
vfloat64m4_t vfcvt_f_xu_v_f64m4_vl (vuint64m4_t src, _VL_T vl);
vfloat64m8_t vfcvt_f_xu_v_f64m8_vl (vuint64m8_t src, _VL_T vl);
// masked functions
vint16m1_t vfcvt_x_f_v_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vfloat16m1_t src, _VL_T vl);
vint16m2_t vfcvt_x_f_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vfloat16m2_t src, _VL_T vl);
vint16m4_t vfcvt_x_f_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vfloat16m4_t src, _VL_T vl);
vint16m8_t vfcvt_x_f_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vfloat16m8_t src, _VL_T vl);
vuint16m1_t vfcvt_xu_f_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vfloat16m1_t src, _VL_T vl);
vuint16m2_t vfcvt_xu_f_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vfloat16m2_t src, _VL_T vl);
vuint16m4_t vfcvt_xu_f_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vfloat16m4_t src, _VL_T vl);
vuint16m8_t vfcvt_xu_f_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vfloat16m8_t src, _VL_T vl);
vfloat16m1_t vfcvt_f_x_v_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vint16m1_t src, _VL_T vl);
vfloat16m2_t vfcvt_f_x_v_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vint16m2_t src, _VL_T vl);
vfloat16m4_t vfcvt_f_x_v_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vint16m4_t src, _VL_T vl);
vfloat16m8_t vfcvt_f_x_v_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vint16m8_t src, _VL_T vl);
vfloat16m1_t vfcvt_f_xu_v_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vuint16m1_t src, _VL_T vl);
vfloat16m2_t vfcvt_f_xu_v_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vuint16m2_t src, _VL_T vl);
vfloat16m4_t vfcvt_f_xu_v_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vuint16m4_t src, _VL_T vl);
vfloat16m8_t vfcvt_f_xu_v_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vuint16m8_t src, _VL_T vl);
vint32m1_t vfcvt_x_f_v_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vfloat32m1_t src, _VL_T vl);
vint32m2_t vfcvt_x_f_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vfloat32m2_t src, _VL_T vl);
vint32m4_t vfcvt_x_f_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vfloat32m4_t src, _VL_T vl);
vint32m8_t vfcvt_x_f_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vfloat32m8_t src, _VL_T vl);
vuint32m1_t vfcvt_xu_f_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vfloat32m1_t src, _VL_T vl);
vuint32m2_t vfcvt_xu_f_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vfloat32m2_t src, _VL_T vl);
vuint32m4_t vfcvt_xu_f_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vfloat32m4_t src, _VL_T vl);
vuint32m8_t vfcvt_xu_f_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vfloat32m8_t src, _VL_T vl);
vfloat32m1_t vfcvt_f_x_v_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vint32m1_t src, _VL_T vl);
vfloat32m2_t vfcvt_f_x_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vint32m2_t src, _VL_T vl);
vfloat32m4_t vfcvt_f_x_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vint32m4_t src, _VL_T vl);
vfloat32m8_t vfcvt_f_x_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vint32m8_t src, _VL_T vl);
vfloat32m1_t vfcvt_f_xu_v_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vuint32m1_t src, _VL_T vl);
vfloat32m2_t vfcvt_f_xu_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vuint32m2_t src, _VL_T vl);
vfloat32m4_t vfcvt_f_xu_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vuint32m4_t src, _VL_T vl);
vfloat32m8_t vfcvt_f_xu_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vuint32m8_t src, _VL_T vl);
vint64m1_t vfcvt_x_f_v_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vfloat64m1_t src, _VL_T vl);
vint64m2_t vfcvt_x_f_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vfloat64m2_t src, _VL_T vl);
vint64m4_t vfcvt_x_f_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vfloat64m4_t src, _VL_T vl);
vint64m8_t vfcvt_x_f_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vfloat64m8_t src, _VL_T vl);
vuint64m1_t vfcvt_xu_f_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vfloat64m1_t src, _VL_T vl);
vuint64m2_t vfcvt_xu_f_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vfloat64m2_t src, _VL_T vl);
vuint64m4_t vfcvt_xu_f_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vfloat64m4_t src, _VL_T vl);
vuint64m8_t vfcvt_xu_f_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vfloat64m8_t src, _VL_T vl);
vfloat64m1_t vfcvt_f_x_v_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vint64m1_t src, _VL_T vl);
vfloat64m2_t vfcvt_f_x_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vint64m2_t src, _VL_T vl);
vfloat64m4_t vfcvt_f_x_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vint64m4_t src, _VL_T vl);
vfloat64m8_t vfcvt_f_x_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vint64m8_t src, _VL_T vl);
vfloat64m1_t vfcvt_f_xu_v_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vuint64m1_t src, _VL_T vl);
vfloat64m2_t vfcvt_f_xu_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vuint64m2_t src, _VL_T vl);
vfloat64m4_t vfcvt_f_xu_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vuint64m4_t src, _VL_T vl);
vfloat64m8_t vfcvt_f_xu_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vuint64m8_t src, _VL_T vl);
```
### [Widening Floating-Point/Integer Type-Convert Functions](rvv-intrinsic-api.md#1416-widening-floating-pointinteger-type-convert-operations):

**Prototypes:**
``` C
vint16m2_t vwcvt_x_x_v_i16m2_vl (vint8m1_t src, _VL_T vl);
vint16m4_t vwcvt_x_x_v_i16m4_vl (vint8m2_t src, _VL_T vl);
vint16m8_t vwcvt_x_x_v_i16m8_vl (vint8m4_t src, _VL_T vl);
vuint16m2_t vwcvtu_x_x_v_u16m2_vl (vuint8m1_t src, _VL_T vl);
vuint16m4_t vwcvtu_x_x_v_u16m4_vl (vuint8m2_t src, _VL_T vl);
vuint16m8_t vwcvtu_x_x_v_u16m8_vl (vuint8m4_t src, _VL_T vl);
vint32m2_t vfwcvt_x_f_v_i32m2_vl (vfloat16m1_t src, _VL_T vl);
vint32m4_t vfwcvt_x_f_v_i32m4_vl (vfloat16m2_t src, _VL_T vl);
vint32m8_t vfwcvt_x_f_v_i32m8_vl (vfloat16m4_t src, _VL_T vl);
vint32m2_t vwcvt_x_x_v_i32m2_vl (vint16m1_t src, _VL_T vl);
vint32m4_t vwcvt_x_x_v_i32m4_vl (vint16m2_t src, _VL_T vl);
vint32m8_t vwcvt_x_x_v_i32m8_vl (vint16m4_t src, _VL_T vl);
vuint32m2_t vwcvtu_x_x_v_u32m2_vl (vuint16m1_t src, _VL_T vl);
vuint32m4_t vwcvtu_x_x_v_u32m4_vl (vuint16m2_t src, _VL_T vl);
vuint32m8_t vwcvtu_x_x_v_u32m8_vl (vuint16m4_t src, _VL_T vl);
vuint32m2_t vfwcvt_xu_f_v_u32m2_vl (vfloat16m1_t src, _VL_T vl);
vuint32m4_t vfwcvt_xu_f_v_u32m4_vl (vfloat16m2_t src, _VL_T vl);
vuint32m8_t vfwcvt_xu_f_v_u32m8_vl (vfloat16m4_t src, _VL_T vl);
vfloat32m2_t vfwcvt_f_x_v_f32m2_vl (vint16m1_t src, _VL_T vl);
vfloat32m4_t vfwcvt_f_x_v_f32m4_vl (vint16m2_t src, _VL_T vl);
vfloat32m8_t vfwcvt_f_x_v_f32m8_vl (vint16m4_t src, _VL_T vl);
vfloat32m2_t vfwcvt_f_xu_v_f32m2_vl (vuint16m1_t src, _VL_T vl);
vfloat32m4_t vfwcvt_f_xu_v_f32m4_vl (vuint16m2_t src, _VL_T vl);
vfloat32m8_t vfwcvt_f_xu_v_f32m8_vl (vuint16m4_t src, _VL_T vl);
vfloat32m2_t vfwcvt_f_f_v_f32m2_vl (vfloat16m1_t src, _VL_T vl);
vfloat32m4_t vfwcvt_f_f_v_f32m4_vl (vfloat16m2_t src, _VL_T vl);
vfloat32m8_t vfwcvt_f_f_v_f32m8_vl (vfloat16m4_t src, _VL_T vl);
vint64m2_t vfwcvt_x_f_v_i64m2_vl (vfloat32m1_t src, _VL_T vl);
vint64m4_t vfwcvt_x_f_v_i64m4_vl (vfloat32m2_t src, _VL_T vl);
vint64m8_t vfwcvt_x_f_v_i64m8_vl (vfloat32m4_t src, _VL_T vl);
vint64m2_t vwcvt_x_x_v_i64m2_vl (vint32m1_t src, _VL_T vl);
vint64m4_t vwcvt_x_x_v_i64m4_vl (vint32m2_t src, _VL_T vl);
vint64m8_t vwcvt_x_x_v_i64m8_vl (vint32m4_t src, _VL_T vl);
vuint64m2_t vwcvtu_x_x_v_u64m2_vl (vuint32m1_t src, _VL_T vl);
vuint64m4_t vwcvtu_x_x_v_u64m4_vl (vuint32m2_t src, _VL_T vl);
vuint64m8_t vwcvtu_x_x_v_u64m8_vl (vuint32m4_t src, _VL_T vl);
vuint64m2_t vfwcvt_xu_f_v_u64m2_vl (vfloat32m1_t src, _VL_T vl);
vuint64m4_t vfwcvt_xu_f_v_u64m4_vl (vfloat32m2_t src, _VL_T vl);
vuint64m8_t vfwcvt_xu_f_v_u64m8_vl (vfloat32m4_t src, _VL_T vl);
vfloat64m2_t vfwcvt_f_x_v_f64m2_vl (vint32m1_t src, _VL_T vl);
vfloat64m4_t vfwcvt_f_x_v_f64m4_vl (vint32m2_t src, _VL_T vl);
vfloat64m8_t vfwcvt_f_x_v_f64m8_vl (vint32m4_t src, _VL_T vl);
vfloat64m2_t vfwcvt_f_xu_v_f64m2_vl (vuint32m1_t src, _VL_T vl);
vfloat64m4_t vfwcvt_f_xu_v_f64m4_vl (vuint32m2_t src, _VL_T vl);
vfloat64m8_t vfwcvt_f_xu_v_f64m8_vl (vuint32m4_t src, _VL_T vl);
vfloat64m2_t vfwcvt_f_f_v_f64m2_vl (vfloat32m1_t src, _VL_T vl);
vfloat64m4_t vfwcvt_f_f_v_f64m4_vl (vfloat32m2_t src, _VL_T vl);
vfloat64m8_t vfwcvt_f_f_v_f64m8_vl (vfloat32m4_t src, _VL_T vl);
// masked functions
vint16m2_t vwcvt_x_x_v_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint8m1_t src, _VL_T vl);
vint16m4_t vwcvt_x_x_v_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint8m2_t src, _VL_T vl);
vint16m8_t vwcvt_x_x_v_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint8m4_t src, _VL_T vl);
vuint16m2_t vwcvtu_x_x_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint8m1_t src, _VL_T vl);
vuint16m4_t vwcvtu_x_x_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint8m2_t src, _VL_T vl);
vuint16m8_t vwcvtu_x_x_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint8m4_t src, _VL_T vl);
vint32m2_t vfwcvt_x_f_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vfloat16m1_t src, _VL_T vl);
vint32m4_t vfwcvt_x_f_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vfloat16m2_t src, _VL_T vl);
vint32m8_t vfwcvt_x_f_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vfloat16m4_t src, _VL_T vl);
vint32m2_t vwcvt_x_x_v_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint16m1_t src, _VL_T vl);
vint32m4_t vwcvt_x_x_v_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint16m2_t src, _VL_T vl);
vint32m8_t vwcvt_x_x_v_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint16m4_t src, _VL_T vl);
vuint32m2_t vwcvtu_x_x_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint16m1_t src, _VL_T vl);
vuint32m4_t vwcvtu_x_x_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint16m2_t src, _VL_T vl);
vuint32m8_t vwcvtu_x_x_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint16m4_t src, _VL_T vl);
vuint32m2_t vfwcvt_xu_f_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vfloat16m1_t src, _VL_T vl);
vuint32m4_t vfwcvt_xu_f_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vfloat16m2_t src, _VL_T vl);
vuint32m8_t vfwcvt_xu_f_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vfloat16m4_t src, _VL_T vl);
vfloat32m2_t vfwcvt_f_x_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vint16m1_t src, _VL_T vl);
vfloat32m4_t vfwcvt_f_x_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vint16m2_t src, _VL_T vl);
vfloat32m8_t vfwcvt_f_x_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vint16m4_t src, _VL_T vl);
vfloat32m2_t vfwcvt_f_xu_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vuint16m1_t src, _VL_T vl);
vfloat32m4_t vfwcvt_f_xu_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vuint16m2_t src, _VL_T vl);
vfloat32m8_t vfwcvt_f_xu_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vuint16m4_t src, _VL_T vl);
vfloat32m2_t vfwcvt_f_f_v_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat16m1_t src, _VL_T vl);
vfloat32m4_t vfwcvt_f_f_v_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat16m2_t src, _VL_T vl);
vfloat32m8_t vfwcvt_f_f_v_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat16m4_t src, _VL_T vl);
vint64m2_t vfwcvt_x_f_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vfloat32m1_t src, _VL_T vl);
vint64m4_t vfwcvt_x_f_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vfloat32m2_t src, _VL_T vl);
vint64m8_t vfwcvt_x_f_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vfloat32m4_t src, _VL_T vl);
vint64m2_t vwcvt_x_x_v_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint32m1_t src, _VL_T vl);
vint64m4_t vwcvt_x_x_v_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint32m2_t src, _VL_T vl);
vint64m8_t vwcvt_x_x_v_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint32m4_t src, _VL_T vl);
vuint64m2_t vwcvtu_x_x_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint32m1_t src, _VL_T vl);
vuint64m4_t vwcvtu_x_x_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint32m2_t src, _VL_T vl);
vuint64m8_t vwcvtu_x_x_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint32m4_t src, _VL_T vl);
vuint64m2_t vfwcvt_xu_f_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vfloat32m1_t src, _VL_T vl);
vuint64m4_t vfwcvt_xu_f_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vfloat32m2_t src, _VL_T vl);
vuint64m8_t vfwcvt_xu_f_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vfloat32m4_t src, _VL_T vl);
vfloat64m2_t vfwcvt_f_x_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vint32m1_t src, _VL_T vl);
vfloat64m4_t vfwcvt_f_x_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vint32m2_t src, _VL_T vl);
vfloat64m8_t vfwcvt_f_x_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vint32m4_t src, _VL_T vl);
vfloat64m2_t vfwcvt_f_xu_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vuint32m1_t src, _VL_T vl);
vfloat64m4_t vfwcvt_f_xu_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vuint32m2_t src, _VL_T vl);
vfloat64m8_t vfwcvt_f_xu_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vuint32m4_t src, _VL_T vl);
vfloat64m2_t vfwcvt_f_f_v_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat32m1_t src, _VL_T vl);
vfloat64m4_t vfwcvt_f_f_v_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat32m2_t src, _VL_T vl);
vfloat64m8_t vfwcvt_f_f_v_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat32m4_t src, _VL_T vl);
```
### [Narrowing Floating-Point/Integer Type-Convert Functions](rvv-intrinsic-api.md#1417-narrowing-floating-pointinteger-type-convert-operations):

**Prototypes:**
``` C
vint16m1_t vfncvt_x_f_w_i16m1_vl (vfloat32m2_t src, _VL_T vl);
vint16m2_t vfncvt_x_f_w_i16m2_vl (vfloat32m4_t src, _VL_T vl);
vint16m4_t vfncvt_x_f_w_i16m4_vl (vfloat32m8_t src, _VL_T vl);
vuint16m1_t vfncvt_xu_f_w_u16m1_vl (vfloat32m2_t src, _VL_T vl);
vuint16m2_t vfncvt_xu_f_w_u16m2_vl (vfloat32m4_t src, _VL_T vl);
vuint16m4_t vfncvt_xu_f_w_u16m4_vl (vfloat32m8_t src, _VL_T vl);
vfloat16m1_t vfncvt_f_x_w_f16m1_vl (vint32m2_t src, _VL_T vl);
vfloat16m2_t vfncvt_f_x_w_f16m2_vl (vint32m4_t src, _VL_T vl);
vfloat16m4_t vfncvt_f_x_w_f16m4_vl (vint32m8_t src, _VL_T vl);
vfloat16m1_t vfncvt_f_xu_w_f16m1_vl (vuint32m2_t src, _VL_T vl);
vfloat16m2_t vfncvt_f_xu_w_f16m2_vl (vuint32m4_t src, _VL_T vl);
vfloat16m4_t vfncvt_f_xu_w_f16m4_vl (vuint32m8_t src, _VL_T vl);
vfloat16m1_t vfncvt_f_f_w_f16m1_vl (vfloat32m2_t src, _VL_T vl);
vfloat16m1_t vfncvt_rod_f_f_w_f16m1_vl (vfloat32m2_t src, _VL_T vl);
vfloat16m2_t vfncvt_f_f_w_f16m2_vl (vfloat32m4_t src, _VL_T vl);
vfloat16m2_t vfncvt_rod_f_f_w_f16m2_vl (vfloat32m4_t src, _VL_T vl);
vfloat16m4_t vfncvt_f_f_w_f16m4_vl (vfloat32m8_t src, _VL_T vl);
vfloat16m4_t vfncvt_rod_f_f_w_f16m4_vl (vfloat32m8_t src, _VL_T vl);
vint32m1_t vfncvt_x_f_w_i32m1_vl (vfloat64m2_t src, _VL_T vl);
vint32m2_t vfncvt_x_f_w_i32m2_vl (vfloat64m4_t src, _VL_T vl);
vint32m4_t vfncvt_x_f_w_i32m4_vl (vfloat64m8_t src, _VL_T vl);
vuint32m1_t vfncvt_xu_f_w_u32m1_vl (vfloat64m2_t src, _VL_T vl);
vuint32m2_t vfncvt_xu_f_w_u32m2_vl (vfloat64m4_t src, _VL_T vl);
vuint32m4_t vfncvt_xu_f_w_u32m4_vl (vfloat64m8_t src, _VL_T vl);
vfloat32m1_t vfncvt_f_x_w_f32m1_vl (vint64m2_t src, _VL_T vl);
vfloat32m2_t vfncvt_f_x_w_f32m2_vl (vint64m4_t src, _VL_T vl);
vfloat32m4_t vfncvt_f_x_w_f32m4_vl (vint64m8_t src, _VL_T vl);
vfloat32m1_t vfncvt_f_xu_w_f32m1_vl (vuint64m2_t src, _VL_T vl);
vfloat32m2_t vfncvt_f_xu_w_f32m2_vl (vuint64m4_t src, _VL_T vl);
vfloat32m4_t vfncvt_f_xu_w_f32m4_vl (vuint64m8_t src, _VL_T vl);
vfloat32m1_t vfncvt_f_f_w_f32m1_vl (vfloat64m2_t src, _VL_T vl);
vfloat32m1_t vfncvt_rod_f_f_w_f32m1_vl (vfloat64m2_t src, _VL_T vl);
vfloat32m2_t vfncvt_f_f_w_f32m2_vl (vfloat64m4_t src, _VL_T vl);
vfloat32m2_t vfncvt_rod_f_f_w_f32m2_vl (vfloat64m4_t src, _VL_T vl);
vfloat32m4_t vfncvt_f_f_w_f32m4_vl (vfloat64m8_t src, _VL_T vl);
vfloat32m4_t vfncvt_rod_f_f_w_f32m4_vl (vfloat64m8_t src, _VL_T vl);
// masked functions
vint16m1_t vfncvt_x_f_w_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vfloat32m2_t src, _VL_T vl);
vint16m2_t vfncvt_x_f_w_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vfloat32m4_t src, _VL_T vl);
vint16m4_t vfncvt_x_f_w_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vfloat32m8_t src, _VL_T vl);
vuint16m1_t vfncvt_xu_f_w_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vfloat32m2_t src, _VL_T vl);
vuint16m2_t vfncvt_xu_f_w_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vfloat32m4_t src, _VL_T vl);
vuint16m4_t vfncvt_xu_f_w_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vfloat32m8_t src, _VL_T vl);
vfloat16m1_t vfncvt_f_x_w_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vint32m2_t src, _VL_T vl);
vfloat16m2_t vfncvt_f_x_w_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vint32m4_t src, _VL_T vl);
vfloat16m4_t vfncvt_f_x_w_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vint32m8_t src, _VL_T vl);
vfloat16m1_t vfncvt_f_xu_w_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vuint32m2_t src, _VL_T vl);
vfloat16m2_t vfncvt_f_xu_w_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vuint32m4_t src, _VL_T vl);
vfloat16m4_t vfncvt_f_xu_w_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vuint32m8_t src, _VL_T vl);
vfloat16m1_t vfncvt_f_f_w_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat32m2_t src, _VL_T vl);
vfloat16m1_t vfncvt_rod_f_f_w_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat32m2_t src, _VL_T vl);
vfloat16m2_t vfncvt_f_f_w_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat32m4_t src, _VL_T vl);
vfloat16m2_t vfncvt_rod_f_f_w_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat32m4_t src, _VL_T vl);
vfloat16m4_t vfncvt_f_f_w_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat32m8_t src, _VL_T vl);
vfloat16m4_t vfncvt_rod_f_f_w_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat32m8_t src, _VL_T vl);
vint32m1_t vfncvt_x_f_w_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vfloat64m2_t src, _VL_T vl);
vint32m2_t vfncvt_x_f_w_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vfloat64m4_t src, _VL_T vl);
vint32m4_t vfncvt_x_f_w_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vfloat64m8_t src, _VL_T vl);
vuint32m1_t vfncvt_xu_f_w_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vfloat64m2_t src, _VL_T vl);
vuint32m2_t vfncvt_xu_f_w_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vfloat64m4_t src, _VL_T vl);
vuint32m4_t vfncvt_xu_f_w_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vfloat64m8_t src, _VL_T vl);
vfloat32m1_t vfncvt_f_x_w_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vint64m2_t src, _VL_T vl);
vfloat32m2_t vfncvt_f_x_w_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vint64m4_t src, _VL_T vl);
vfloat32m4_t vfncvt_f_x_w_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vint64m8_t src, _VL_T vl);
vfloat32m1_t vfncvt_f_xu_w_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vuint64m2_t src, _VL_T vl);
vfloat32m2_t vfncvt_f_xu_w_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vuint64m4_t src, _VL_T vl);
vfloat32m4_t vfncvt_f_xu_w_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vuint64m8_t src, _VL_T vl);
vfloat32m1_t vfncvt_f_f_w_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat64m2_t src, _VL_T vl);
vfloat32m1_t vfncvt_rod_f_f_w_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat64m2_t src, _VL_T vl);
vfloat32m2_t vfncvt_f_f_w_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat64m4_t src, _VL_T vl);
vfloat32m2_t vfncvt_rod_f_f_w_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat64m4_t src, _VL_T vl);
vfloat32m4_t vfncvt_f_f_w_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat64m8_t src, _VL_T vl);
vfloat32m4_t vfncvt_rod_f_f_w_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat64m8_t src, _VL_T vl);
```
## Vector Reduction Functions:

### [Vector Single-Width Integer Reduction Functions](rvv-intrinsic-api.md#151-vector-single-width-integer-reduction-operations):

**Prototypes:**
``` C
vint8m1_t vredsum_vs_i8m1_i8m1_vl (vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredsum_vs_i8m2_i8m1_vl (vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredsum_vs_i8m4_i8m1_vl (vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredsum_vs_i8m8_i8m1_vl (vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m1_i16m1_vl (vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m2_i16m1_vl (vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m4_i16m1_vl (vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m8_i16m1_vl (vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m1_i32m1_vl (vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m2_i32m1_vl (vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m4_i32m1_vl (vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m8_i32m1_vl (vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m1_i64m1_vl (vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m2_i64m1_vl (vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m4_i64m1_vl (vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m8_i64m1_vl (vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m1_u8m1_vl (vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m2_u8m1_vl (vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m4_u8m1_vl (vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m8_u8m1_vl (vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m1_u16m1_vl (vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m2_u16m1_vl (vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m4_u16m1_vl (vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m8_u16m1_vl (vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m1_u32m1_vl (vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m2_u32m1_vl (vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m4_u32m1_vl (vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m8_u32m1_vl (vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m1_u64m1_vl (vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m2_u64m1_vl (vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m4_u64m1_vl (vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m8_u64m1_vl (vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m1_i8m1_vl (vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m2_i8m1_vl (vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m4_i8m1_vl (vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m8_i8m1_vl (vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m1_i16m1_vl (vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m2_i16m1_vl (vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m4_i16m1_vl (vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m8_i16m1_vl (vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m1_i32m1_vl (vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m2_i32m1_vl (vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m4_i32m1_vl (vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m8_i32m1_vl (vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m1_i64m1_vl (vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m2_i64m1_vl (vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m4_i64m1_vl (vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m8_i64m1_vl (vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m1_u8m1_vl (vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m2_u8m1_vl (vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m4_u8m1_vl (vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m8_u8m1_vl (vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m1_u16m1_vl (vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m2_u16m1_vl (vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m4_u16m1_vl (vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m8_u16m1_vl (vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m1_u32m1_vl (vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m2_u32m1_vl (vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m4_u32m1_vl (vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m8_u32m1_vl (vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m1_u64m1_vl (vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m2_u64m1_vl (vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m4_u64m1_vl (vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m8_u64m1_vl (vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m1_i8m1_vl (vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m2_i8m1_vl (vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m4_i8m1_vl (vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m8_i8m1_vl (vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m1_i16m1_vl (vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m2_i16m1_vl (vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m4_i16m1_vl (vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m8_i16m1_vl (vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m1_i32m1_vl (vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m2_i32m1_vl (vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m4_i32m1_vl (vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m8_i32m1_vl (vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m1_i64m1_vl (vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m2_i64m1_vl (vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m4_i64m1_vl (vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m8_i64m1_vl (vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m1_u8m1_vl (vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m2_u8m1_vl (vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m4_u8m1_vl (vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m8_u8m1_vl (vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m1_u16m1_vl (vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m2_u16m1_vl (vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m4_u16m1_vl (vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m8_u16m1_vl (vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m1_u32m1_vl (vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m2_u32m1_vl (vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m4_u32m1_vl (vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m8_u32m1_vl (vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m1_u64m1_vl (vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m2_u64m1_vl (vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m4_u64m1_vl (vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m8_u64m1_vl (vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m1_i8m1_vl (vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m2_i8m1_vl (vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m4_i8m1_vl (vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m8_i8m1_vl (vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m1_i16m1_vl (vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m2_i16m1_vl (vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m4_i16m1_vl (vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m8_i16m1_vl (vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m1_i32m1_vl (vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m2_i32m1_vl (vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m4_i32m1_vl (vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m8_i32m1_vl (vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m1_i64m1_vl (vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m2_i64m1_vl (vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m4_i64m1_vl (vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m8_i64m1_vl (vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m1_u8m1_vl (vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m2_u8m1_vl (vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m4_u8m1_vl (vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m8_u8m1_vl (vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m1_u16m1_vl (vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m2_u16m1_vl (vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m4_u16m1_vl (vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m8_u16m1_vl (vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m1_u32m1_vl (vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m2_u32m1_vl (vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m4_u32m1_vl (vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m8_u32m1_vl (vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m1_u64m1_vl (vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m2_u64m1_vl (vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m4_u64m1_vl (vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m8_u64m1_vl (vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m1_i8m1_vl (vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m2_i8m1_vl (vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m4_i8m1_vl (vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m8_i8m1_vl (vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m1_i16m1_vl (vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m2_i16m1_vl (vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m4_i16m1_vl (vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m8_i16m1_vl (vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m1_i32m1_vl (vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m2_i32m1_vl (vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m4_i32m1_vl (vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m8_i32m1_vl (vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m1_i64m1_vl (vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m2_i64m1_vl (vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m4_i64m1_vl (vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m8_i64m1_vl (vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m1_u8m1_vl (vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m2_u8m1_vl (vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m4_u8m1_vl (vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m8_u8m1_vl (vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m1_u16m1_vl (vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m2_u16m1_vl (vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m4_u16m1_vl (vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m8_u16m1_vl (vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m1_u32m1_vl (vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m2_u32m1_vl (vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m4_u32m1_vl (vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m8_u32m1_vl (vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m1_u64m1_vl (vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m2_u64m1_vl (vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m4_u64m1_vl (vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m8_u64m1_vl (vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m1_i8m1_vl (vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m2_i8m1_vl (vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m4_i8m1_vl (vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m8_i8m1_vl (vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m1_i16m1_vl (vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m2_i16m1_vl (vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m4_i16m1_vl (vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m8_i16m1_vl (vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m1_i32m1_vl (vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m2_i32m1_vl (vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m4_i32m1_vl (vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m8_i32m1_vl (vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m1_i64m1_vl (vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m2_i64m1_vl (vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m4_i64m1_vl (vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m8_i64m1_vl (vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m1_u8m1_vl (vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m2_u8m1_vl (vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m4_u8m1_vl (vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m8_u8m1_vl (vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m1_u16m1_vl (vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m2_u16m1_vl (vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m4_u16m1_vl (vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m8_u16m1_vl (vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m1_u32m1_vl (vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m2_u32m1_vl (vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m4_u32m1_vl (vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m8_u32m1_vl (vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m1_u64m1_vl (vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m2_u64m1_vl (vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m4_u64m1_vl (vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m8_u64m1_vl (vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
// masked functions
vint8m1_t vredsum_vs_i8m1_i8m1_m_vl (vbool8_t mask, vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredsum_vs_i8m2_i8m1_m_vl (vbool4_t mask, vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredsum_vs_i8m4_i8m1_m_vl (vbool2_t mask, vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredsum_vs_i8m8_i8m1_m_vl (vbool1_t mask, vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m1_i16m1_m_vl (vbool16_t mask, vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m2_i16m1_m_vl (vbool8_t mask, vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m4_i16m1_m_vl (vbool4_t mask, vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredsum_vs_i16m8_i16m1_m_vl (vbool2_t mask, vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m1_i32m1_m_vl (vbool32_t mask, vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m2_i32m1_m_vl (vbool16_t mask, vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m4_i32m1_m_vl (vbool8_t mask, vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredsum_vs_i32m8_i32m1_m_vl (vbool4_t mask, vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m1_i64m1_m_vl (vbool64_t mask, vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m2_i64m1_m_vl (vbool32_t mask, vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m4_i64m1_m_vl (vbool16_t mask, vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredsum_vs_i64m8_i64m1_m_vl (vbool8_t mask, vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m1_u8m1_m_vl (vbool8_t mask, vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m2_u8m1_m_vl (vbool4_t mask, vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m4_u8m1_m_vl (vbool2_t mask, vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredsum_vs_u8m8_u8m1_m_vl (vbool1_t mask, vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m1_u16m1_m_vl (vbool16_t mask, vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m2_u16m1_m_vl (vbool8_t mask, vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m4_u16m1_m_vl (vbool4_t mask, vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredsum_vs_u16m8_u16m1_m_vl (vbool2_t mask, vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m1_u32m1_m_vl (vbool32_t mask, vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m2_u32m1_m_vl (vbool16_t mask, vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m4_u32m1_m_vl (vbool8_t mask, vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredsum_vs_u32m8_u32m1_m_vl (vbool4_t mask, vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m1_u64m1_m_vl (vbool64_t mask, vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m2_u64m1_m_vl (vbool32_t mask, vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m4_u64m1_m_vl (vbool16_t mask, vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredsum_vs_u64m8_u64m1_m_vl (vbool8_t mask, vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m1_i8m1_m_vl (vbool8_t mask, vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m2_i8m1_m_vl (vbool4_t mask, vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m4_i8m1_m_vl (vbool2_t mask, vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmax_vs_i8m8_i8m1_m_vl (vbool1_t mask, vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m1_i16m1_m_vl (vbool16_t mask, vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m2_i16m1_m_vl (vbool8_t mask, vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m4_i16m1_m_vl (vbool4_t mask, vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmax_vs_i16m8_i16m1_m_vl (vbool2_t mask, vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m1_i32m1_m_vl (vbool32_t mask, vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m2_i32m1_m_vl (vbool16_t mask, vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m4_i32m1_m_vl (vbool8_t mask, vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmax_vs_i32m8_i32m1_m_vl (vbool4_t mask, vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m1_i64m1_m_vl (vbool64_t mask, vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m2_i64m1_m_vl (vbool32_t mask, vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m4_i64m1_m_vl (vbool16_t mask, vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmax_vs_i64m8_i64m1_m_vl (vbool8_t mask, vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m1_u8m1_m_vl (vbool8_t mask, vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m2_u8m1_m_vl (vbool4_t mask, vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m4_u8m1_m_vl (vbool2_t mask, vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredmaxu_vs_u8m8_u8m1_m_vl (vbool1_t mask, vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m1_u16m1_m_vl (vbool16_t mask, vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m2_u16m1_m_vl (vbool8_t mask, vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m4_u16m1_m_vl (vbool4_t mask, vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredmaxu_vs_u16m8_u16m1_m_vl (vbool2_t mask, vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m1_u32m1_m_vl (vbool32_t mask, vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m2_u32m1_m_vl (vbool16_t mask, vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m4_u32m1_m_vl (vbool8_t mask, vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredmaxu_vs_u32m8_u32m1_m_vl (vbool4_t mask, vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m1_u64m1_m_vl (vbool64_t mask, vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m2_u64m1_m_vl (vbool32_t mask, vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m4_u64m1_m_vl (vbool16_t mask, vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredmaxu_vs_u64m8_u64m1_m_vl (vbool8_t mask, vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m1_i8m1_m_vl (vbool8_t mask, vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m2_i8m1_m_vl (vbool4_t mask, vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m4_i8m1_m_vl (vbool2_t mask, vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredmin_vs_i8m8_i8m1_m_vl (vbool1_t mask, vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m1_i16m1_m_vl (vbool16_t mask, vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m2_i16m1_m_vl (vbool8_t mask, vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m4_i16m1_m_vl (vbool4_t mask, vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredmin_vs_i16m8_i16m1_m_vl (vbool2_t mask, vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m1_i32m1_m_vl (vbool32_t mask, vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m2_i32m1_m_vl (vbool16_t mask, vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m4_i32m1_m_vl (vbool8_t mask, vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredmin_vs_i32m8_i32m1_m_vl (vbool4_t mask, vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m1_i64m1_m_vl (vbool64_t mask, vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m2_i64m1_m_vl (vbool32_t mask, vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m4_i64m1_m_vl (vbool16_t mask, vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredmin_vs_i64m8_i64m1_m_vl (vbool8_t mask, vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m1_u8m1_m_vl (vbool8_t mask, vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m2_u8m1_m_vl (vbool4_t mask, vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m4_u8m1_m_vl (vbool2_t mask, vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredminu_vs_u8m8_u8m1_m_vl (vbool1_t mask, vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m1_u16m1_m_vl (vbool16_t mask, vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m2_u16m1_m_vl (vbool8_t mask, vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m4_u16m1_m_vl (vbool4_t mask, vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredminu_vs_u16m8_u16m1_m_vl (vbool2_t mask, vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m1_u32m1_m_vl (vbool32_t mask, vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m2_u32m1_m_vl (vbool16_t mask, vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m4_u32m1_m_vl (vbool8_t mask, vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredminu_vs_u32m8_u32m1_m_vl (vbool4_t mask, vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m1_u64m1_m_vl (vbool64_t mask, vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m2_u64m1_m_vl (vbool32_t mask, vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m4_u64m1_m_vl (vbool16_t mask, vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredminu_vs_u64m8_u64m1_m_vl (vbool8_t mask, vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m1_i8m1_m_vl (vbool8_t mask, vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m2_i8m1_m_vl (vbool4_t mask, vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m4_i8m1_m_vl (vbool2_t mask, vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredand_vs_i8m8_i8m1_m_vl (vbool1_t mask, vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m1_i16m1_m_vl (vbool16_t mask, vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m2_i16m1_m_vl (vbool8_t mask, vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m4_i16m1_m_vl (vbool4_t mask, vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredand_vs_i16m8_i16m1_m_vl (vbool2_t mask, vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m1_i32m1_m_vl (vbool32_t mask, vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m2_i32m1_m_vl (vbool16_t mask, vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m4_i32m1_m_vl (vbool8_t mask, vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredand_vs_i32m8_i32m1_m_vl (vbool4_t mask, vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m1_i64m1_m_vl (vbool64_t mask, vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m2_i64m1_m_vl (vbool32_t mask, vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m4_i64m1_m_vl (vbool16_t mask, vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredand_vs_i64m8_i64m1_m_vl (vbool8_t mask, vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m1_u8m1_m_vl (vbool8_t mask, vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m2_u8m1_m_vl (vbool4_t mask, vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m4_u8m1_m_vl (vbool2_t mask, vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredand_vs_u8m8_u8m1_m_vl (vbool1_t mask, vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m1_u16m1_m_vl (vbool16_t mask, vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m2_u16m1_m_vl (vbool8_t mask, vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m4_u16m1_m_vl (vbool4_t mask, vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredand_vs_u16m8_u16m1_m_vl (vbool2_t mask, vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m1_u32m1_m_vl (vbool32_t mask, vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m2_u32m1_m_vl (vbool16_t mask, vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m4_u32m1_m_vl (vbool8_t mask, vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredand_vs_u32m8_u32m1_m_vl (vbool4_t mask, vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m1_u64m1_m_vl (vbool64_t mask, vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m2_u64m1_m_vl (vbool32_t mask, vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m4_u64m1_m_vl (vbool16_t mask, vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredand_vs_u64m8_u64m1_m_vl (vbool8_t mask, vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m1_i8m1_m_vl (vbool8_t mask, vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m2_i8m1_m_vl (vbool4_t mask, vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m4_i8m1_m_vl (vbool2_t mask, vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredor_vs_i8m8_i8m1_m_vl (vbool1_t mask, vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m1_i16m1_m_vl (vbool16_t mask, vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m2_i16m1_m_vl (vbool8_t mask, vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m4_i16m1_m_vl (vbool4_t mask, vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredor_vs_i16m8_i16m1_m_vl (vbool2_t mask, vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m1_i32m1_m_vl (vbool32_t mask, vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m2_i32m1_m_vl (vbool16_t mask, vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m4_i32m1_m_vl (vbool8_t mask, vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredor_vs_i32m8_i32m1_m_vl (vbool4_t mask, vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m1_i64m1_m_vl (vbool64_t mask, vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m2_i64m1_m_vl (vbool32_t mask, vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m4_i64m1_m_vl (vbool16_t mask, vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredor_vs_i64m8_i64m1_m_vl (vbool8_t mask, vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m1_u8m1_m_vl (vbool8_t mask, vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m2_u8m1_m_vl (vbool4_t mask, vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m4_u8m1_m_vl (vbool2_t mask, vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredor_vs_u8m8_u8m1_m_vl (vbool1_t mask, vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m1_u16m1_m_vl (vbool16_t mask, vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m2_u16m1_m_vl (vbool8_t mask, vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m4_u16m1_m_vl (vbool4_t mask, vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredor_vs_u16m8_u16m1_m_vl (vbool2_t mask, vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m1_u32m1_m_vl (vbool32_t mask, vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m2_u32m1_m_vl (vbool16_t mask, vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m4_u32m1_m_vl (vbool8_t mask, vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredor_vs_u32m8_u32m1_m_vl (vbool4_t mask, vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m1_u64m1_m_vl (vbool64_t mask, vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m2_u64m1_m_vl (vbool32_t mask, vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m4_u64m1_m_vl (vbool16_t mask, vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredor_vs_u64m8_u64m1_m_vl (vbool8_t mask, vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m1_i8m1_m_vl (vbool8_t mask, vint8m1_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m2_i8m1_m_vl (vbool4_t mask, vint8m2_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m4_i8m1_m_vl (vbool2_t mask, vint8m4_t vector, vint8m1_t scalar, _VL_T vl);
vint8m1_t vredxor_vs_i8m8_i8m1_m_vl (vbool1_t mask, vint8m8_t vector, vint8m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m1_i16m1_m_vl (vbool16_t mask, vint16m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m2_i16m1_m_vl (vbool8_t mask, vint16m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m4_i16m1_m_vl (vbool4_t mask, vint16m4_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vredxor_vs_i16m8_i16m1_m_vl (vbool2_t mask, vint16m8_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m1_i32m1_m_vl (vbool32_t mask, vint32m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m2_i32m1_m_vl (vbool16_t mask, vint32m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m4_i32m1_m_vl (vbool8_t mask, vint32m4_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vredxor_vs_i32m8_i32m1_m_vl (vbool4_t mask, vint32m8_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m1_i64m1_m_vl (vbool64_t mask, vint64m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m2_i64m1_m_vl (vbool32_t mask, vint64m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m4_i64m1_m_vl (vbool16_t mask, vint64m4_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vredxor_vs_i64m8_i64m1_m_vl (vbool8_t mask, vint64m8_t vector, vint64m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m1_u8m1_m_vl (vbool8_t mask, vuint8m1_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m2_u8m1_m_vl (vbool4_t mask, vuint8m2_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m4_u8m1_m_vl (vbool2_t mask, vuint8m4_t vector, vuint8m1_t scalar, _VL_T vl);
vuint8m1_t vredxor_vs_u8m8_u8m1_m_vl (vbool1_t mask, vuint8m8_t vector, vuint8m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m1_u16m1_m_vl (vbool16_t mask, vuint16m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m2_u16m1_m_vl (vbool8_t mask, vuint16m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m4_u16m1_m_vl (vbool4_t mask, vuint16m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vredxor_vs_u16m8_u16m1_m_vl (vbool2_t mask, vuint16m8_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m1_u32m1_m_vl (vbool32_t mask, vuint32m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m2_u32m1_m_vl (vbool16_t mask, vuint32m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m4_u32m1_m_vl (vbool8_t mask, vuint32m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vredxor_vs_u32m8_u32m1_m_vl (vbool4_t mask, vuint32m8_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m1_u64m1_m_vl (vbool64_t mask, vuint64m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m2_u64m1_m_vl (vbool32_t mask, vuint64m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m4_u64m1_m_vl (vbool16_t mask, vuint64m4_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vredxor_vs_u64m8_u64m1_m_vl (vbool8_t mask, vuint64m8_t vector, vuint64m1_t scalar, _VL_T vl);
```
### [Vector Widening Integer Reduction Functions](rvv-intrinsic-api.md##152-vector-widening-integer-reduction-operations):

**Prototypes:**
``` C
vint16m1_t vwredsum_vs_i8m1_i16m1_vl (vint8m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vwredsum_vs_i8m2_i16m1_vl (vint8m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vwredsum_vs_i8m4_i16m1_vl (vint8m4_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vwredsum_vs_i16m1_i32m1_vl (vint16m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vwredsum_vs_i16m2_i32m1_vl (vint16m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vwredsum_vs_i16m4_i32m1_vl (vint16m4_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vwredsum_vs_i32m1_i64m1_vl (vint32m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vwredsum_vs_i32m2_i64m1_vl (vint32m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vwredsum_vs_i32m4_i64m1_vl (vint32m4_t vector, vint64m1_t scalar, _VL_T vl);
vuint16m1_t vwredsumu_vs_u8m1_u16m1_vl (vuint8m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vwredsumu_vs_u8m2_u16m1_vl (vuint8m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vwredsumu_vs_u8m4_u16m1_vl (vuint8m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vwredsumu_vs_u16m1_u32m1_vl (vuint16m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vwredsumu_vs_u16m2_u32m1_vl (vuint16m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vwredsumu_vs_u16m4_u32m1_vl (vuint16m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vwredsumu_vs_u32m1_u64m1_vl (vuint32m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vwredsumu_vs_u32m2_u64m1_vl (vuint32m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vwredsumu_vs_u32m4_u64m1_vl (vuint32m4_t vector, vuint64m1_t scalar, _VL_T vl);
// masked functions
vint16m1_t vwredsum_vs_i8m1_i16m1_m_vl (vbool8_t mask, vint8m1_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vwredsum_vs_i8m2_i16m1_m_vl (vbool4_t mask, vint8m2_t vector, vint16m1_t scalar, _VL_T vl);
vint16m1_t vwredsum_vs_i8m4_i16m1_m_vl (vbool2_t mask, vint8m4_t vector, vint16m1_t scalar, _VL_T vl);
vint32m1_t vwredsum_vs_i16m1_i32m1_m_vl (vbool16_t mask, vint16m1_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vwredsum_vs_i16m2_i32m1_m_vl (vbool8_t mask, vint16m2_t vector, vint32m1_t scalar, _VL_T vl);
vint32m1_t vwredsum_vs_i16m4_i32m1_m_vl (vbool4_t mask, vint16m4_t vector, vint32m1_t scalar, _VL_T vl);
vint64m1_t vwredsum_vs_i32m1_i64m1_m_vl (vbool32_t mask, vint32m1_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vwredsum_vs_i32m2_i64m1_m_vl (vbool16_t mask, vint32m2_t vector, vint64m1_t scalar, _VL_T vl);
vint64m1_t vwredsum_vs_i32m4_i64m1_m_vl (vbool8_t mask, vint32m4_t vector, vint64m1_t scalar, _VL_T vl);
vuint16m1_t vwredsumu_vs_u8m1_u16m1_m_vl (vbool8_t mask, vuint8m1_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vwredsumu_vs_u8m2_u16m1_m_vl (vbool4_t mask, vuint8m2_t vector, vuint16m1_t scalar, _VL_T vl);
vuint16m1_t vwredsumu_vs_u8m4_u16m1_m_vl (vbool2_t mask, vuint8m4_t vector, vuint16m1_t scalar, _VL_T vl);
vuint32m1_t vwredsumu_vs_u16m1_u32m1_m_vl (vbool16_t mask, vuint16m1_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vwredsumu_vs_u16m2_u32m1_m_vl (vbool8_t mask, vuint16m2_t vector, vuint32m1_t scalar, _VL_T vl);
vuint32m1_t vwredsumu_vs_u16m4_u32m1_m_vl (vbool4_t mask, vuint16m4_t vector, vuint32m1_t scalar, _VL_T vl);
vuint64m1_t vwredsumu_vs_u32m1_u64m1_m_vl (vbool32_t mask, vuint32m1_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vwredsumu_vs_u32m2_u64m1_m_vl (vbool16_t mask, vuint32m2_t vector, vuint64m1_t scalar, _VL_T vl);
vuint64m1_t vwredsumu_vs_u32m4_u64m1_m_vl (vbool8_t mask, vuint32m4_t vector, vuint64m1_t scalar, _VL_T vl);
```
### [Vector Single-Width Floating-Point Reduction Functions](rvv-intrinsic-api.md#153-vector-single-width-floating-point-reduction-operations):

**Prototypes:**
``` C
vfloat16m1_t vfredosum_vs_f16m1_f16m1_vl (vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredosum_vs_f16m2_f16m1_vl (vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredosum_vs_f16m4_f16m1_vl (vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredosum_vs_f16m8_f16m1_vl (vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m1_f32m1_vl (vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m2_f32m1_vl (vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m4_f32m1_vl (vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m8_f32m1_vl (vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m1_f64m1_vl (vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m2_f64m1_vl (vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m4_f64m1_vl (vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m8_f64m1_vl (vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m1_f16m1_vl (vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m2_f16m1_vl (vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m4_f16m1_vl (vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m8_f16m1_vl (vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m1_f32m1_vl (vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m2_f32m1_vl (vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m4_f32m1_vl (vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m8_f32m1_vl (vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m1_f64m1_vl (vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m2_f64m1_vl (vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m4_f64m1_vl (vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m8_f64m1_vl (vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m1_f16m1_vl (vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m2_f16m1_vl (vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m4_f16m1_vl (vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m8_f16m1_vl (vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m1_f32m1_vl (vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m2_f32m1_vl (vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m4_f32m1_vl (vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m8_f32m1_vl (vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m1_f64m1_vl (vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m2_f64m1_vl (vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m4_f64m1_vl (vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m8_f64m1_vl (vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m1_f16m1_vl (vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m2_f16m1_vl (vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m4_f16m1_vl (vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m8_f16m1_vl (vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m1_f32m1_vl (vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m2_f32m1_vl (vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m4_f32m1_vl (vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m8_f32m1_vl (vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m1_f64m1_vl (vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m2_f64m1_vl (vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m4_f64m1_vl (vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m8_f64m1_vl (vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
// masked functions
vfloat16m1_t vfredosum_vs_f16m1_f16m1_m_vl (vbool16_t mask, vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredosum_vs_f16m2_f16m1_m_vl (vbool8_t mask, vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredosum_vs_f16m4_f16m1_m_vl (vbool4_t mask, vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredosum_vs_f16m8_f16m1_m_vl (vbool2_t mask, vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m1_f32m1_m_vl (vbool32_t mask, vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m2_f32m1_m_vl (vbool16_t mask, vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m4_f32m1_m_vl (vbool8_t mask, vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredosum_vs_f32m8_f32m1_m_vl (vbool4_t mask, vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m1_f64m1_m_vl (vbool64_t mask, vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m2_f64m1_m_vl (vbool32_t mask, vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m4_f64m1_m_vl (vbool16_t mask, vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredosum_vs_f64m8_f64m1_m_vl (vbool8_t mask, vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m1_f16m1_m_vl (vbool16_t mask, vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m2_f16m1_m_vl (vbool8_t mask, vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m4_f16m1_m_vl (vbool4_t mask, vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredsum_vs_f16m8_f16m1_m_vl (vbool2_t mask, vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m1_f32m1_m_vl (vbool32_t mask, vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m2_f32m1_m_vl (vbool16_t mask, vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m4_f32m1_m_vl (vbool8_t mask, vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredsum_vs_f32m8_f32m1_m_vl (vbool4_t mask, vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m1_f64m1_m_vl (vbool64_t mask, vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m2_f64m1_m_vl (vbool32_t mask, vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m4_f64m1_m_vl (vbool16_t mask, vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredsum_vs_f64m8_f64m1_m_vl (vbool8_t mask, vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m1_f16m1_m_vl (vbool16_t mask, vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m2_f16m1_m_vl (vbool8_t mask, vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m4_f16m1_m_vl (vbool4_t mask, vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmax_vs_f16m8_f16m1_m_vl (vbool2_t mask, vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m1_f32m1_m_vl (vbool32_t mask, vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m2_f32m1_m_vl (vbool16_t mask, vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m4_f32m1_m_vl (vbool8_t mask, vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmax_vs_f32m8_f32m1_m_vl (vbool4_t mask, vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m1_f64m1_m_vl (vbool64_t mask, vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m2_f64m1_m_vl (vbool32_t mask, vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m4_f64m1_m_vl (vbool16_t mask, vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmax_vs_f64m8_f64m1_m_vl (vbool8_t mask, vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m1_f16m1_m_vl (vbool16_t mask, vfloat16m1_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m2_f16m1_m_vl (vbool8_t mask, vfloat16m2_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m4_f16m1_m_vl (vbool4_t mask, vfloat16m4_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat16m1_t vfredmin_vs_f16m8_f16m1_m_vl (vbool2_t mask, vfloat16m8_t vector, vfloat16m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m1_f32m1_m_vl (vbool32_t mask, vfloat32m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m2_f32m1_m_vl (vbool16_t mask, vfloat32m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m4_f32m1_m_vl (vbool8_t mask, vfloat32m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfredmin_vs_f32m8_f32m1_m_vl (vbool4_t mask, vfloat32m8_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m1_f64m1_m_vl (vbool64_t mask, vfloat64m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m2_f64m1_m_vl (vbool32_t mask, vfloat64m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m4_f64m1_m_vl (vbool16_t mask, vfloat64m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfredmin_vs_f64m8_f64m1_m_vl (vbool8_t mask, vfloat64m8_t vector, vfloat64m1_t scalar, _VL_T vl);
```
### [Vector Widening Floating-Point Reduction Functions](rvv-intrinsic-api.md#154-vector-widening-floating-point-reduction-operations):

**Prototypes:**
``` C
vfloat32m1_t vfwredosum_vs_f16m1_f32m1_vl (vfloat16m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredosum_vs_f16m2_f32m1_vl (vfloat16m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredosum_vs_f16m4_f32m1_vl (vfloat16m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredosum_vs_f32m1_f64m1_vl (vfloat32m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredosum_vs_f32m2_f64m1_vl (vfloat32m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredosum_vs_f32m4_f64m1_vl (vfloat32m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredsum_vs_f16m1_f32m1_vl (vfloat16m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredsum_vs_f16m2_f32m1_vl (vfloat16m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredsum_vs_f16m4_f32m1_vl (vfloat16m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredsum_vs_f32m1_f64m1_vl (vfloat32m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredsum_vs_f32m2_f64m1_vl (vfloat32m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredsum_vs_f32m4_f64m1_vl (vfloat32m4_t vector, vfloat64m1_t scalar, _VL_T vl);
// masked functions
vfloat32m1_t vfwredosum_vs_f16m1_f32m1_m_vl (vbool16_t mask, vfloat16m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredosum_vs_f16m2_f32m1_m_vl (vbool8_t mask, vfloat16m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredosum_vs_f16m4_f32m1_m_vl (vbool4_t mask, vfloat16m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredosum_vs_f32m1_f64m1_m_vl (vbool32_t mask, vfloat32m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredosum_vs_f32m2_f64m1_m_vl (vbool16_t mask, vfloat32m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredosum_vs_f32m4_f64m1_m_vl (vbool8_t mask, vfloat32m4_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredsum_vs_f16m1_f32m1_m_vl (vbool16_t mask, vfloat16m1_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredsum_vs_f16m2_f32m1_m_vl (vbool8_t mask, vfloat16m2_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat32m1_t vfwredsum_vs_f16m4_f32m1_m_vl (vbool4_t mask, vfloat16m4_t vector, vfloat32m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredsum_vs_f32m1_f64m1_m_vl (vbool32_t mask, vfloat32m1_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredsum_vs_f32m2_f64m1_m_vl (vbool16_t mask, vfloat32m2_t vector, vfloat64m1_t scalar, _VL_T vl);
vfloat64m1_t vfwredsum_vs_f32m4_f64m1_m_vl (vbool8_t mask, vfloat32m4_t vector, vfloat64m1_t scalar, _VL_T vl);
```
## Vector Mask Functions:

### [Vector Mask-Register Logical Functions](rvv-intrinsic-api.md#161-vector-mask-register-logical-operations):

**Prototypes:**
``` C
vbool1_t vmand_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmand_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmand_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmand_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmand_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmand_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmand_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmnand_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmnand_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmnand_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmnand_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmnand_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmnand_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmnand_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmandnot_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmandnot_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmandnot_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmandnot_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmandnot_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmandnot_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmandnot_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmxor_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmxor_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmxor_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmxor_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmxor_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmxor_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmxor_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmor_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmor_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmor_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmor_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmor_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmor_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmor_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmnor_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmnor_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmnor_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmnor_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmnor_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmnor_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmnor_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmornot_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmornot_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmornot_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmornot_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmornot_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmornot_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmornot_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmxnor_mm_b1 (vbool1_t op1, vbool1_t op2);
vbool2_t vmxnor_mm_b2 (vbool2_t op1, vbool2_t op2);
vbool4_t vmxnor_mm_b4 (vbool4_t op1, vbool4_t op2);
vbool8_t vmxnor_mm_b8 (vbool8_t op1, vbool8_t op2);
vbool16_t vmxnor_mm_b16 (vbool16_t op1, vbool16_t op2);
vbool32_t vmxnor_mm_b32 (vbool32_t op1, vbool32_t op2);
vbool64_t vmxnor_mm_b64 (vbool64_t op1, vbool64_t op2);
vbool1_t vmcpy_m_b1 (vbool1_t op1);
vbool2_t vmcpy_m_b2 (vbool2_t op1);
vbool4_t vmcpy_m_b4 (vbool4_t op1);
vbool8_t vmcpy_m_b8 (vbool8_t op1);
vbool16_t vmcpy_m_b16 (vbool16_t op1);
vbool32_t vmcpy_m_b32 (vbool32_t op1);
vbool64_t vmcpy_m_b64 (vbool64_t op1);
vbool1_t vmclr_b1 ();
vbool2_t vmclr_b2 ();
vbool4_t vmclr_b4 ();
vbool8_t vmclr_b8 ();
vbool16_t vmclr_b16 ();
vbool32_t vmclr_b32 ();
vbool64_t vmclr_b64 ();
vbool1_t vmset_b1 ();
vbool2_t vmset_b2 ();
vbool4_t vmset_b4 ();
vbool8_t vmset_b8 ();
vbool16_t vmset_b16 ();
vbool32_t vmset_b32 ();
vbool64_t vmset_b64 ();
vbool1_t vmnot_m_b1 (vbool1_t op1);
vbool2_t vmnot_m_b2 (vbool2_t op1);
vbool4_t vmnot_m_b4 (vbool4_t op1);
vbool8_t vmnot_m_b8 (vbool8_t op1);
vbool16_t vmnot_m_b16 (vbool16_t op1);
vbool32_t vmnot_m_b32 (vbool32_t op1);
vbool64_t vmnot_m_b64 (vbool64_t op1);
```
### [Vector mask population count Functions](rvv-intrinsic-api.md#162-vector-mask-population-count-vpopc):

**Prototypes:**
``` C
unsigned long vpopc_m_b1 (vbool1_t op1);
unsigned long vpopc_m_b2 (vbool2_t op1);
unsigned long vpopc_m_b4 (vbool4_t op1);
unsigned long vpopc_m_b8 (vbool8_t op1);
unsigned long vpopc_m_b16 (vbool16_t op1);
unsigned long vpopc_m_b32 (vbool32_t op1);
unsigned long vpopc_m_b64 (vbool64_t op1);
// masked functions
unsigned long vpopc_m_b1_m (vbool1_t mask, vbool1_t op1);
unsigned long vpopc_m_b2_m (vbool2_t mask, vbool2_t op1);
unsigned long vpopc_m_b4_m (vbool4_t mask, vbool4_t op1);
unsigned long vpopc_m_b8_m (vbool8_t mask, vbool8_t op1);
unsigned long vpopc_m_b16_m (vbool16_t mask, vbool16_t op1);
unsigned long vpopc_m_b32_m (vbool32_t mask, vbool32_t op1);
unsigned long vpopc_m_b64_m (vbool64_t mask, vbool64_t op1);
```
### [Find-first-set mask bit Functions](rvv-intrinsic-api.md#163-vfirst-find-first-set-mask-bit):

**Prototypes:**
``` C
long vfirst_m_b1 (vbool1_t op1);
long vfirst_m_b2 (vbool2_t op1);
long vfirst_m_b4 (vbool4_t op1);
long vfirst_m_b8 (vbool8_t op1);
long vfirst_m_b16 (vbool16_t op1);
long vfirst_m_b32 (vbool32_t op1);
long vfirst_m_b64 (vbool64_t op1);
// masked functions
long vfirst_m_b1_m (vbool1_t mask, vbool1_t op1);
long vfirst_m_b2_m (vbool2_t mask, vbool2_t op1);
long vfirst_m_b4_m (vbool4_t mask, vbool4_t op1);
long vfirst_m_b8_m (vbool8_t mask, vbool8_t op1);
long vfirst_m_b16_m (vbool16_t mask, vbool16_t op1);
long vfirst_m_b32_m (vbool32_t mask, vbool32_t op1);
long vfirst_m_b64_m (vbool64_t mask, vbool64_t op1);
```
### [Set-before-first mask bit Functions](rvv-intrinsic-api.md#164-vmsbfm-set-before-first-mask-bit):

**Prototypes:**
``` C
vbool1_t vmsbf_m_b1 (vbool1_t op1);
vbool2_t vmsbf_m_b2 (vbool2_t op1);
vbool4_t vmsbf_m_b4 (vbool4_t op1);
vbool8_t vmsbf_m_b8 (vbool8_t op1);
vbool16_t vmsbf_m_b16 (vbool16_t op1);
vbool32_t vmsbf_m_b32 (vbool32_t op1);
vbool64_t vmsbf_m_b64 (vbool64_t op1);
// masked functions
vbool1_t vmsbf_m_b1_m (vbool1_t mask, vbool1_t maskedoff, vbool1_t op1);
vbool2_t vmsbf_m_b2_m (vbool2_t mask, vbool2_t maskedoff, vbool2_t op1);
vbool4_t vmsbf_m_b4_m (vbool4_t mask, vbool4_t maskedoff, vbool4_t op1);
vbool8_t vmsbf_m_b8_m (vbool8_t mask, vbool8_t maskedoff, vbool8_t op1);
vbool16_t vmsbf_m_b16_m (vbool16_t mask, vbool16_t maskedoff, vbool16_t op1);
vbool32_t vmsbf_m_b32_m (vbool32_t mask, vbool32_t maskedoff, vbool32_t op1);
vbool64_t vmsbf_m_b64_m (vbool64_t mask, vbool64_t maskedoff, vbool64_t op1);
```
### [Set-including-first mask bit Functions](rvv-intrinsic-api.md#165-vmsifm-set-including-first-mask-bit):

**Prototypes:**
``` C
vbool1_t vmsif_m_b1 (vbool1_t op1);
vbool2_t vmsif_m_b2 (vbool2_t op1);
vbool4_t vmsif_m_b4 (vbool4_t op1);
vbool8_t vmsif_m_b8 (vbool8_t op1);
vbool16_t vmsif_m_b16 (vbool16_t op1);
vbool32_t vmsif_m_b32 (vbool32_t op1);
vbool64_t vmsif_m_b64 (vbool64_t op1);
// masked functions
vbool1_t vmsif_m_b1_m (vbool1_t mask, vbool1_t maskedoff, vbool1_t op1);
vbool2_t vmsif_m_b2_m (vbool2_t mask, vbool2_t maskedoff, vbool2_t op1);
vbool4_t vmsif_m_b4_m (vbool4_t mask, vbool4_t maskedoff, vbool4_t op1);
vbool8_t vmsif_m_b8_m (vbool8_t mask, vbool8_t maskedoff, vbool8_t op1);
vbool16_t vmsif_m_b16_m (vbool16_t mask, vbool16_t maskedoff, vbool16_t op1);
vbool32_t vmsif_m_b32_m (vbool32_t mask, vbool32_t maskedoff, vbool32_t op1);
vbool64_t vmsif_m_b64_m (vbool64_t mask, vbool64_t maskedoff, vbool64_t op1);
```
### [Set-only-first mask bit Functions](rvv-intrinsic-api.md#166-vmsofm-set-only-first-mask-bit):

**Prototypes:**
``` C
vbool1_t vmsof_m_b1 (vbool1_t op1);
vbool2_t vmsof_m_b2 (vbool2_t op1);
vbool4_t vmsof_m_b4 (vbool4_t op1);
vbool8_t vmsof_m_b8 (vbool8_t op1);
vbool16_t vmsof_m_b16 (vbool16_t op1);
vbool32_t vmsof_m_b32 (vbool32_t op1);
vbool64_t vmsof_m_b64 (vbool64_t op1);
// masked functions
vbool1_t vmsof_m_b1_m (vbool1_t mask, vbool1_t maskedoff, vbool1_t op1);
vbool2_t vmsof_m_b2_m (vbool2_t mask, vbool2_t maskedoff, vbool2_t op1);
vbool4_t vmsof_m_b4_m (vbool4_t mask, vbool4_t maskedoff, vbool4_t op1);
vbool8_t vmsof_m_b8_m (vbool8_t mask, vbool8_t maskedoff, vbool8_t op1);
vbool16_t vmsof_m_b16_m (vbool16_t mask, vbool16_t maskedoff, vbool16_t op1);
vbool32_t vmsof_m_b32_m (vbool32_t mask, vbool32_t maskedoff, vbool32_t op1);
vbool64_t vmsof_m_b64_m (vbool64_t mask, vbool64_t maskedoff, vbool64_t op1);
```
### [Vector Iota Functions](rvv-intrinsic-api.md#168-vector-iota-operations):

**Prototypes:**
``` C
vuint8m1_t viota_m_u8m1_vl (vbool8_t op1, _VL_T vl);
vuint8m2_t viota_m_u8m2_vl (vbool4_t op1, _VL_T vl);
vuint8m4_t viota_m_u8m4_vl (vbool2_t op1, _VL_T vl);
vuint8m8_t viota_m_u8m8_vl (vbool1_t op1, _VL_T vl);
vuint16m1_t viota_m_u16m1_vl (vbool16_t op1, _VL_T vl);
vuint16m2_t viota_m_u16m2_vl (vbool8_t op1, _VL_T vl);
vuint16m4_t viota_m_u16m4_vl (vbool4_t op1, _VL_T vl);
vuint16m8_t viota_m_u16m8_vl (vbool2_t op1, _VL_T vl);
vuint32m1_t viota_m_u32m1_vl (vbool32_t op1, _VL_T vl);
vuint32m2_t viota_m_u32m2_vl (vbool16_t op1, _VL_T vl);
vuint32m4_t viota_m_u32m4_vl (vbool8_t op1, _VL_T vl);
vuint32m8_t viota_m_u32m8_vl (vbool4_t op1, _VL_T vl);
vuint64m1_t viota_m_u64m1_vl (vbool64_t op1, _VL_T vl);
vuint64m2_t viota_m_u64m2_vl (vbool32_t op1, _VL_T vl);
vuint64m4_t viota_m_u64m4_vl (vbool16_t op1, _VL_T vl);
vuint64m8_t viota_m_u64m8_vl (vbool8_t op1, _VL_T vl);
// masked functions
vuint8m1_t viota_m_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vbool8_t op1, _VL_T vl);
vuint8m2_t viota_m_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vbool4_t op1, _VL_T vl);
vuint8m4_t viota_m_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vbool2_t op1, _VL_T vl);
vuint8m8_t viota_m_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vbool1_t op1, _VL_T vl);
vuint16m1_t viota_m_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vbool16_t op1, _VL_T vl);
vuint16m2_t viota_m_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vbool8_t op1, _VL_T vl);
vuint16m4_t viota_m_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vbool4_t op1, _VL_T vl);
vuint16m8_t viota_m_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vbool2_t op1, _VL_T vl);
vuint32m1_t viota_m_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vbool32_t op1, _VL_T vl);
vuint32m2_t viota_m_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vbool16_t op1, _VL_T vl);
vuint32m4_t viota_m_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vbool8_t op1, _VL_T vl);
vuint32m8_t viota_m_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vbool4_t op1, _VL_T vl);
vuint64m1_t viota_m_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vbool64_t op1, _VL_T vl);
vuint64m2_t viota_m_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vbool32_t op1, _VL_T vl);
vuint64m4_t viota_m_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vbool16_t op1, _VL_T vl);
vuint64m8_t viota_m_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vbool8_t op1, _VL_T vl);
```
### [Vector Element Index Functions](rvv-intrinsic-api.md#169-vector-element-index-operations):

**Prototypes:**
``` C
vuint8m1_t vid_v_u8m1_vl (_VL_T vl);
vuint8m2_t vid_v_u8m2_vl (_VL_T vl);
vuint8m4_t vid_v_u8m4_vl (_VL_T vl);
vuint8m8_t vid_v_u8m8_vl (_VL_T vl);
vuint16m1_t vid_v_u16m1_vl (_VL_T vl);
vuint16m2_t vid_v_u16m2_vl (_VL_T vl);
vuint16m4_t vid_v_u16m4_vl (_VL_T vl);
vuint16m8_t vid_v_u16m8_vl (_VL_T vl);
vuint32m1_t vid_v_u32m1_vl (_VL_T vl);
vuint32m2_t vid_v_u32m2_vl (_VL_T vl);
vuint32m4_t vid_v_u32m4_vl (_VL_T vl);
vuint32m8_t vid_v_u32m8_vl (_VL_T vl);
vuint64m1_t vid_v_u64m1_vl (_VL_T vl);
vuint64m2_t vid_v_u64m2_vl (_VL_T vl);
vuint64m4_t vid_v_u64m4_vl (_VL_T vl);
vuint64m8_t vid_v_u64m8_vl (_VL_T vl);
// masked functions
vuint8m1_t vid_v_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, _VL_T vl);
vuint8m2_t vid_v_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, _VL_T vl);
vuint8m4_t vid_v_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, _VL_T vl);
vuint8m8_t vid_v_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, _VL_T vl);
vuint16m1_t vid_v_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, _VL_T vl);
vuint16m2_t vid_v_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, _VL_T vl);
vuint16m4_t vid_v_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, _VL_T vl);
vuint16m8_t vid_v_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, _VL_T vl);
vuint32m1_t vid_v_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, _VL_T vl);
vuint32m2_t vid_v_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, _VL_T vl);
vuint32m4_t vid_v_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, _VL_T vl);
vuint32m8_t vid_v_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, _VL_T vl);
vuint64m1_t vid_v_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, _VL_T vl);
vuint64m2_t vid_v_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, _VL_T vl);
vuint64m4_t vid_v_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, _VL_T vl);
vuint64m8_t vid_v_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, _VL_T vl);
```
## Vector Permutation Functions:

### [Integer and Floating-Point Scalar Move Functions](rvv-intrinsic-api.md#171-integer-scalar-move-operations):

**Prototypes:**
``` C
int8_t vmv_x_s_i8m1_i8_vl (vint8m1_t src, _VL_T vl);
vint8m1_t vmv_s_x_i8m1_vl (vint8m1_t dst, int8_t src, _VL_T vl);
int8_t vmv_x_s_i8m2_i8_vl (vint8m2_t src, _VL_T vl);
vint8m2_t vmv_s_x_i8m2_vl (vint8m2_t dst, int8_t src, _VL_T vl);
int8_t vmv_x_s_i8m4_i8_vl (vint8m4_t src, _VL_T vl);
vint8m4_t vmv_s_x_i8m4_vl (vint8m4_t dst, int8_t src, _VL_T vl);
int8_t vmv_x_s_i8m8_i8_vl (vint8m8_t src, _VL_T vl);
vint8m8_t vmv_s_x_i8m8_vl (vint8m8_t dst, int8_t src, _VL_T vl);
int16_t vmv_x_s_i16m1_i16_vl (vint16m1_t src, _VL_T vl);
vint16m1_t vmv_s_x_i16m1_vl (vint16m1_t dst, int16_t src, _VL_T vl);
int16_t vmv_x_s_i16m2_i16_vl (vint16m2_t src, _VL_T vl);
vint16m2_t vmv_s_x_i16m2_vl (vint16m2_t dst, int16_t src, _VL_T vl);
int16_t vmv_x_s_i16m4_i16_vl (vint16m4_t src, _VL_T vl);
vint16m4_t vmv_s_x_i16m4_vl (vint16m4_t dst, int16_t src, _VL_T vl);
int16_t vmv_x_s_i16m8_i16_vl (vint16m8_t src, _VL_T vl);
vint16m8_t vmv_s_x_i16m8_vl (vint16m8_t dst, int16_t src, _VL_T vl);
int32_t vmv_x_s_i32m1_i32_vl (vint32m1_t src, _VL_T vl);
vint32m1_t vmv_s_x_i32m1_vl (vint32m1_t dst, int32_t src, _VL_T vl);
int32_t vmv_x_s_i32m2_i32_vl (vint32m2_t src, _VL_T vl);
vint32m2_t vmv_s_x_i32m2_vl (vint32m2_t dst, int32_t src, _VL_T vl);
int32_t vmv_x_s_i32m4_i32_vl (vint32m4_t src, _VL_T vl);
vint32m4_t vmv_s_x_i32m4_vl (vint32m4_t dst, int32_t src, _VL_T vl);
int32_t vmv_x_s_i32m8_i32_vl (vint32m8_t src, _VL_T vl);
vint32m8_t vmv_s_x_i32m8_vl (vint32m8_t dst, int32_t src, _VL_T vl);
int64_t vmv_x_s_i64m1_i64_vl (vint64m1_t src, _VL_T vl);
vint64m1_t vmv_s_x_i64m1_vl (vint64m1_t dst, int64_t src, _VL_T vl);
int64_t vmv_x_s_i64m2_i64_vl (vint64m2_t src, _VL_T vl);
vint64m2_t vmv_s_x_i64m2_vl (vint64m2_t dst, int64_t src, _VL_T vl);
int64_t vmv_x_s_i64m4_i64_vl (vint64m4_t src, _VL_T vl);
vint64m4_t vmv_s_x_i64m4_vl (vint64m4_t dst, int64_t src, _VL_T vl);
int64_t vmv_x_s_i64m8_i64_vl (vint64m8_t src, _VL_T vl);
vint64m8_t vmv_s_x_i64m8_vl (vint64m8_t dst, int64_t src, _VL_T vl);
uint8_t vmv_x_s_u8m1_u8_vl (vuint8m1_t src, _VL_T vl);
vuint8m1_t vmv_s_x_u8m1_vl (vuint8m1_t dst, uint8_t src, _VL_T vl);
uint8_t vmv_x_s_u8m2_u8_vl (vuint8m2_t src, _VL_T vl);
vuint8m2_t vmv_s_x_u8m2_vl (vuint8m2_t dst, uint8_t src, _VL_T vl);
uint8_t vmv_x_s_u8m4_u8_vl (vuint8m4_t src, _VL_T vl);
vuint8m4_t vmv_s_x_u8m4_vl (vuint8m4_t dst, uint8_t src, _VL_T vl);
uint8_t vmv_x_s_u8m8_u8_vl (vuint8m8_t src, _VL_T vl);
vuint8m8_t vmv_s_x_u8m8_vl (vuint8m8_t dst, uint8_t src, _VL_T vl);
uint16_t vmv_x_s_u16m1_u16_vl (vuint16m1_t src, _VL_T vl);
vuint16m1_t vmv_s_x_u16m1_vl (vuint16m1_t dst, uint16_t src, _VL_T vl);
uint16_t vmv_x_s_u16m2_u16_vl (vuint16m2_t src, _VL_T vl);
vuint16m2_t vmv_s_x_u16m2_vl (vuint16m2_t dst, uint16_t src, _VL_T vl);
uint16_t vmv_x_s_u16m4_u16_vl (vuint16m4_t src, _VL_T vl);
vuint16m4_t vmv_s_x_u16m4_vl (vuint16m4_t dst, uint16_t src, _VL_T vl);
uint16_t vmv_x_s_u16m8_u16_vl (vuint16m8_t src, _VL_T vl);
vuint16m8_t vmv_s_x_u16m8_vl (vuint16m8_t dst, uint16_t src, _VL_T vl);
uint32_t vmv_x_s_u32m1_u32_vl (vuint32m1_t src, _VL_T vl);
vuint32m1_t vmv_s_x_u32m1_vl (vuint32m1_t dst, uint32_t src, _VL_T vl);
uint32_t vmv_x_s_u32m2_u32_vl (vuint32m2_t src, _VL_T vl);
vuint32m2_t vmv_s_x_u32m2_vl (vuint32m2_t dst, uint32_t src, _VL_T vl);
uint32_t vmv_x_s_u32m4_u32_vl (vuint32m4_t src, _VL_T vl);
vuint32m4_t vmv_s_x_u32m4_vl (vuint32m4_t dst, uint32_t src, _VL_T vl);
uint32_t vmv_x_s_u32m8_u32_vl (vuint32m8_t src, _VL_T vl);
vuint32m8_t vmv_s_x_u32m8_vl (vuint32m8_t dst, uint32_t src, _VL_T vl);
uint64_t vmv_x_s_u64m1_u64_vl (vuint64m1_t src, _VL_T vl);
vuint64m1_t vmv_s_x_u64m1_vl (vuint64m1_t dst, uint64_t src, _VL_T vl);
uint64_t vmv_x_s_u64m2_u64_vl (vuint64m2_t src, _VL_T vl);
vuint64m2_t vmv_s_x_u64m2_vl (vuint64m2_t dst, uint64_t src, _VL_T vl);
uint64_t vmv_x_s_u64m4_u64_vl (vuint64m4_t src, _VL_T vl);
vuint64m4_t vmv_s_x_u64m4_vl (vuint64m4_t dst, uint64_t src, _VL_T vl);
uint64_t vmv_x_s_u64m8_u64_vl (vuint64m8_t src, _VL_T vl);
vuint64m8_t vmv_s_x_u64m8_vl (vuint64m8_t dst, uint64_t src, _VL_T vl);
float16_t vfmv_f_s_f16m1_f16_vl (vfloat16m1_t src, _VL_T vl);
vfloat16m1_t vfmv_s_f_f16m1_vl (vfloat16m1_t dst, float16_t src, _VL_T vl);
float16_t vfmv_f_s_f16m2_f16_vl (vfloat16m2_t src, _VL_T vl);
vfloat16m2_t vfmv_s_f_f16m2_vl (vfloat16m2_t dst, float16_t src, _VL_T vl);
float16_t vfmv_f_s_f16m4_f16_vl (vfloat16m4_t src, _VL_T vl);
vfloat16m4_t vfmv_s_f_f16m4_vl (vfloat16m4_t dst, float16_t src, _VL_T vl);
float16_t vfmv_f_s_f16m8_f16_vl (vfloat16m8_t src, _VL_T vl);
vfloat16m8_t vfmv_s_f_f16m8_vl (vfloat16m8_t dst, float16_t src, _VL_T vl);
float32_t vfmv_f_s_f32m1_f32_vl (vfloat32m1_t src, _VL_T vl);
vfloat32m1_t vfmv_s_f_f32m1_vl (vfloat32m1_t dst, float32_t src, _VL_T vl);
float32_t vfmv_f_s_f32m2_f32_vl (vfloat32m2_t src, _VL_T vl);
vfloat32m2_t vfmv_s_f_f32m2_vl (vfloat32m2_t dst, float32_t src, _VL_T vl);
float32_t vfmv_f_s_f32m4_f32_vl (vfloat32m4_t src, _VL_T vl);
vfloat32m4_t vfmv_s_f_f32m4_vl (vfloat32m4_t dst, float32_t src, _VL_T vl);
float32_t vfmv_f_s_f32m8_f32_vl (vfloat32m8_t src, _VL_T vl);
vfloat32m8_t vfmv_s_f_f32m8_vl (vfloat32m8_t dst, float32_t src, _VL_T vl);
float64_t vfmv_f_s_f64m1_f64_vl (vfloat64m1_t src, _VL_T vl);
vfloat64m1_t vfmv_s_f_f64m1_vl (vfloat64m1_t dst, float64_t src, _VL_T vl);
float64_t vfmv_f_s_f64m2_f64_vl (vfloat64m2_t src, _VL_T vl);
vfloat64m2_t vfmv_s_f_f64m2_vl (vfloat64m2_t dst, float64_t src, _VL_T vl);
float64_t vfmv_f_s_f64m4_f64_vl (vfloat64m4_t src, _VL_T vl);
vfloat64m4_t vfmv_s_f_f64m4_vl (vfloat64m4_t dst, float64_t src, _VL_T vl);
float64_t vfmv_f_s_f64m8_f64_vl (vfloat64m8_t src, _VL_T vl);
vfloat64m8_t vfmv_s_f_f64m8_vl (vfloat64m8_t dst, float64_t src, _VL_T vl);
```
### [Vector Slideup and Slidedown Functions](rvv-intrinsic-api.md#173-vector-slide-operations):

**Prototypes:**
``` C
vint8m1_t vslideup_vx_i8m1_vl (vint8m1_t src, size_t offset, _VL_T vl);
vint8m2_t vslideup_vx_i8m2_vl (vint8m2_t src, size_t offset, _VL_T vl);
vint8m4_t vslideup_vx_i8m4_vl (vint8m4_t src, size_t offset, _VL_T vl);
vint8m8_t vslideup_vx_i8m8_vl (vint8m8_t src, size_t offset, _VL_T vl);
vint16m1_t vslideup_vx_i16m1_vl (vint16m1_t src, size_t offset, _VL_T vl);
vint16m2_t vslideup_vx_i16m2_vl (vint16m2_t src, size_t offset, _VL_T vl);
vint16m4_t vslideup_vx_i16m4_vl (vint16m4_t src, size_t offset, _VL_T vl);
vint16m8_t vslideup_vx_i16m8_vl (vint16m8_t src, size_t offset, _VL_T vl);
vint32m1_t vslideup_vx_i32m1_vl (vint32m1_t src, size_t offset, _VL_T vl);
vint32m2_t vslideup_vx_i32m2_vl (vint32m2_t src, size_t offset, _VL_T vl);
vint32m4_t vslideup_vx_i32m4_vl (vint32m4_t src, size_t offset, _VL_T vl);
vint32m8_t vslideup_vx_i32m8_vl (vint32m8_t src, size_t offset, _VL_T vl);
vint64m1_t vslideup_vx_i64m1_vl (vint64m1_t src, size_t offset, _VL_T vl);
vint64m2_t vslideup_vx_i64m2_vl (vint64m2_t src, size_t offset, _VL_T vl);
vint64m4_t vslideup_vx_i64m4_vl (vint64m4_t src, size_t offset, _VL_T vl);
vint64m8_t vslideup_vx_i64m8_vl (vint64m8_t src, size_t offset, _VL_T vl);
vuint8m1_t vslideup_vx_u8m1_vl (vuint8m1_t src, size_t offset, _VL_T vl);
vuint8m2_t vslideup_vx_u8m2_vl (vuint8m2_t src, size_t offset, _VL_T vl);
vuint8m4_t vslideup_vx_u8m4_vl (vuint8m4_t src, size_t offset, _VL_T vl);
vuint8m8_t vslideup_vx_u8m8_vl (vuint8m8_t src, size_t offset, _VL_T vl);
vuint16m1_t vslideup_vx_u16m1_vl (vuint16m1_t src, size_t offset, _VL_T vl);
vuint16m2_t vslideup_vx_u16m2_vl (vuint16m2_t src, size_t offset, _VL_T vl);
vuint16m4_t vslideup_vx_u16m4_vl (vuint16m4_t src, size_t offset, _VL_T vl);
vuint16m8_t vslideup_vx_u16m8_vl (vuint16m8_t src, size_t offset, _VL_T vl);
vuint32m1_t vslideup_vx_u32m1_vl (vuint32m1_t src, size_t offset, _VL_T vl);
vuint32m2_t vslideup_vx_u32m2_vl (vuint32m2_t src, size_t offset, _VL_T vl);
vuint32m4_t vslideup_vx_u32m4_vl (vuint32m4_t src, size_t offset, _VL_T vl);
vuint32m8_t vslideup_vx_u32m8_vl (vuint32m8_t src, size_t offset, _VL_T vl);
vuint64m1_t vslideup_vx_u64m1_vl (vuint64m1_t src, size_t offset, _VL_T vl);
vuint64m2_t vslideup_vx_u64m2_vl (vuint64m2_t src, size_t offset, _VL_T vl);
vuint64m4_t vslideup_vx_u64m4_vl (vuint64m4_t src, size_t offset, _VL_T vl);
vuint64m8_t vslideup_vx_u64m8_vl (vuint64m8_t src, size_t offset, _VL_T vl);
vfloat16m1_t vslideup_vx_f16m1_vl (vfloat16m1_t src, size_t offset, _VL_T vl);
vfloat16m2_t vslideup_vx_f16m2_vl (vfloat16m2_t src, size_t offset, _VL_T vl);
vfloat16m4_t vslideup_vx_f16m4_vl (vfloat16m4_t src, size_t offset, _VL_T vl);
vfloat16m8_t vslideup_vx_f16m8_vl (vfloat16m8_t src, size_t offset, _VL_T vl);
vfloat32m1_t vslideup_vx_f32m1_vl (vfloat32m1_t src, size_t offset, _VL_T vl);
vfloat32m2_t vslideup_vx_f32m2_vl (vfloat32m2_t src, size_t offset, _VL_T vl);
vfloat32m4_t vslideup_vx_f32m4_vl (vfloat32m4_t src, size_t offset, _VL_T vl);
vfloat32m8_t vslideup_vx_f32m8_vl (vfloat32m8_t src, size_t offset, _VL_T vl);
vfloat64m1_t vslideup_vx_f64m1_vl (vfloat64m1_t src, size_t offset, _VL_T vl);
vfloat64m2_t vslideup_vx_f64m2_vl (vfloat64m2_t src, size_t offset, _VL_T vl);
vfloat64m4_t vslideup_vx_f64m4_vl (vfloat64m4_t src, size_t offset, _VL_T vl);
vfloat64m8_t vslideup_vx_f64m8_vl (vfloat64m8_t src, size_t offset, _VL_T vl);
vint8m1_t vslidedown_vx_i8m1_vl (vint8m1_t src, size_t offset, _VL_T vl);
vint8m2_t vslidedown_vx_i8m2_vl (vint8m2_t src, size_t offset, _VL_T vl);
vint8m4_t vslidedown_vx_i8m4_vl (vint8m4_t src, size_t offset, _VL_T vl);
vint8m8_t vslidedown_vx_i8m8_vl (vint8m8_t src, size_t offset, _VL_T vl);
vint16m1_t vslidedown_vx_i16m1_vl (vint16m1_t src, size_t offset, _VL_T vl);
vint16m2_t vslidedown_vx_i16m2_vl (vint16m2_t src, size_t offset, _VL_T vl);
vint16m4_t vslidedown_vx_i16m4_vl (vint16m4_t src, size_t offset, _VL_T vl);
vint16m8_t vslidedown_vx_i16m8_vl (vint16m8_t src, size_t offset, _VL_T vl);
vint32m1_t vslidedown_vx_i32m1_vl (vint32m1_t src, size_t offset, _VL_T vl);
vint32m2_t vslidedown_vx_i32m2_vl (vint32m2_t src, size_t offset, _VL_T vl);
vint32m4_t vslidedown_vx_i32m4_vl (vint32m4_t src, size_t offset, _VL_T vl);
vint32m8_t vslidedown_vx_i32m8_vl (vint32m8_t src, size_t offset, _VL_T vl);
vint64m1_t vslidedown_vx_i64m1_vl (vint64m1_t src, size_t offset, _VL_T vl);
vint64m2_t vslidedown_vx_i64m2_vl (vint64m2_t src, size_t offset, _VL_T vl);
vint64m4_t vslidedown_vx_i64m4_vl (vint64m4_t src, size_t offset, _VL_T vl);
vint64m8_t vslidedown_vx_i64m8_vl (vint64m8_t src, size_t offset, _VL_T vl);
vuint8m1_t vslidedown_vx_u8m1_vl (vuint8m1_t src, size_t offset, _VL_T vl);
vuint8m2_t vslidedown_vx_u8m2_vl (vuint8m2_t src, size_t offset, _VL_T vl);
vuint8m4_t vslidedown_vx_u8m4_vl (vuint8m4_t src, size_t offset, _VL_T vl);
vuint8m8_t vslidedown_vx_u8m8_vl (vuint8m8_t src, size_t offset, _VL_T vl);
vuint16m1_t vslidedown_vx_u16m1_vl (vuint16m1_t src, size_t offset, _VL_T vl);
vuint16m2_t vslidedown_vx_u16m2_vl (vuint16m2_t src, size_t offset, _VL_T vl);
vuint16m4_t vslidedown_vx_u16m4_vl (vuint16m4_t src, size_t offset, _VL_T vl);
vuint16m8_t vslidedown_vx_u16m8_vl (vuint16m8_t src, size_t offset, _VL_T vl);
vuint32m1_t vslidedown_vx_u32m1_vl (vuint32m1_t src, size_t offset, _VL_T vl);
vuint32m2_t vslidedown_vx_u32m2_vl (vuint32m2_t src, size_t offset, _VL_T vl);
vuint32m4_t vslidedown_vx_u32m4_vl (vuint32m4_t src, size_t offset, _VL_T vl);
vuint32m8_t vslidedown_vx_u32m8_vl (vuint32m8_t src, size_t offset, _VL_T vl);
vuint64m1_t vslidedown_vx_u64m1_vl (vuint64m1_t src, size_t offset, _VL_T vl);
vuint64m2_t vslidedown_vx_u64m2_vl (vuint64m2_t src, size_t offset, _VL_T vl);
vuint64m4_t vslidedown_vx_u64m4_vl (vuint64m4_t src, size_t offset, _VL_T vl);
vuint64m8_t vslidedown_vx_u64m8_vl (vuint64m8_t src, size_t offset, _VL_T vl);
vfloat16m1_t vslidedown_vx_f16m1_vl (vfloat16m1_t src, size_t offset, _VL_T vl);
vfloat16m2_t vslidedown_vx_f16m2_vl (vfloat16m2_t src, size_t offset, _VL_T vl);
vfloat16m4_t vslidedown_vx_f16m4_vl (vfloat16m4_t src, size_t offset, _VL_T vl);
vfloat16m8_t vslidedown_vx_f16m8_vl (vfloat16m8_t src, size_t offset, _VL_T vl);
vfloat32m1_t vslidedown_vx_f32m1_vl (vfloat32m1_t src, size_t offset, _VL_T vl);
vfloat32m2_t vslidedown_vx_f32m2_vl (vfloat32m2_t src, size_t offset, _VL_T vl);
vfloat32m4_t vslidedown_vx_f32m4_vl (vfloat32m4_t src, size_t offset, _VL_T vl);
vfloat32m8_t vslidedown_vx_f32m8_vl (vfloat32m8_t src, size_t offset, _VL_T vl);
vfloat64m1_t vslidedown_vx_f64m1_vl (vfloat64m1_t src, size_t offset, _VL_T vl);
vfloat64m2_t vslidedown_vx_f64m2_vl (vfloat64m2_t src, size_t offset, _VL_T vl);
vfloat64m4_t vslidedown_vx_f64m4_vl (vfloat64m4_t src, size_t offset, _VL_T vl);
vfloat64m8_t vslidedown_vx_f64m8_vl (vfloat64m8_t src, size_t offset, _VL_T vl);
// masked functions
vint8m1_t vslideup_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, size_t offset, _VL_T vl);
vint8m2_t vslideup_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, size_t offset, _VL_T vl);
vint8m4_t vslideup_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, size_t offset, _VL_T vl);
vint8m8_t vslideup_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, size_t offset, _VL_T vl);
vint16m1_t vslideup_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, size_t offset, _VL_T vl);
vint16m2_t vslideup_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, size_t offset, _VL_T vl);
vint16m4_t vslideup_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, size_t offset, _VL_T vl);
vint16m8_t vslideup_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, size_t offset, _VL_T vl);
vint32m1_t vslideup_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, size_t offset, _VL_T vl);
vint32m2_t vslideup_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, size_t offset, _VL_T vl);
vint32m4_t vslideup_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, size_t offset, _VL_T vl);
vint32m8_t vslideup_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, size_t offset, _VL_T vl);
vint64m1_t vslideup_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, size_t offset, _VL_T vl);
vint64m2_t vslideup_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, size_t offset, _VL_T vl);
vint64m4_t vslideup_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, size_t offset, _VL_T vl);
vint64m8_t vslideup_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, size_t offset, _VL_T vl);
vuint8m1_t vslideup_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, size_t offset, _VL_T vl);
vuint8m2_t vslideup_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, size_t offset, _VL_T vl);
vuint8m4_t vslideup_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, size_t offset, _VL_T vl);
vuint8m8_t vslideup_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, size_t offset, _VL_T vl);
vuint16m1_t vslideup_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, size_t offset, _VL_T vl);
vuint16m2_t vslideup_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, size_t offset, _VL_T vl);
vuint16m4_t vslideup_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, size_t offset, _VL_T vl);
vuint16m8_t vslideup_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, size_t offset, _VL_T vl);
vuint32m1_t vslideup_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, size_t offset, _VL_T vl);
vuint32m2_t vslideup_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, size_t offset, _VL_T vl);
vuint32m4_t vslideup_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, size_t offset, _VL_T vl);
vuint32m8_t vslideup_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, size_t offset, _VL_T vl);
vuint64m1_t vslideup_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, size_t offset, _VL_T vl);
vuint64m2_t vslideup_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, size_t offset, _VL_T vl);
vuint64m4_t vslideup_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, size_t offset, _VL_T vl);
vuint64m8_t vslideup_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, size_t offset, _VL_T vl);
vfloat16m1_t vslideup_vx_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, size_t offset, _VL_T vl);
vfloat16m2_t vslideup_vx_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, size_t offset, _VL_T vl);
vfloat16m4_t vslideup_vx_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, size_t offset, _VL_T vl);
vfloat16m8_t vslideup_vx_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, size_t offset, _VL_T vl);
vfloat32m1_t vslideup_vx_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, size_t offset, _VL_T vl);
vfloat32m2_t vslideup_vx_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, size_t offset, _VL_T vl);
vfloat32m4_t vslideup_vx_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, size_t offset, _VL_T vl);
vfloat32m8_t vslideup_vx_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, size_t offset, _VL_T vl);
vfloat64m1_t vslideup_vx_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, size_t offset, _VL_T vl);
vfloat64m2_t vslideup_vx_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, size_t offset, _VL_T vl);
vfloat64m4_t vslideup_vx_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, size_t offset, _VL_T vl);
vfloat64m8_t vslideup_vx_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, size_t offset, _VL_T vl);
vint8m1_t vslidedown_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, size_t offset, _VL_T vl);
vint8m2_t vslidedown_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, size_t offset, _VL_T vl);
vint8m4_t vslidedown_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, size_t offset, _VL_T vl);
vint8m8_t vslidedown_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, size_t offset, _VL_T vl);
vint16m1_t vslidedown_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, size_t offset, _VL_T vl);
vint16m2_t vslidedown_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, size_t offset, _VL_T vl);
vint16m4_t vslidedown_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, size_t offset, _VL_T vl);
vint16m8_t vslidedown_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, size_t offset, _VL_T vl);
vint32m1_t vslidedown_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, size_t offset, _VL_T vl);
vint32m2_t vslidedown_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, size_t offset, _VL_T vl);
vint32m4_t vslidedown_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, size_t offset, _VL_T vl);
vint32m8_t vslidedown_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, size_t offset, _VL_T vl);
vint64m1_t vslidedown_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, size_t offset, _VL_T vl);
vint64m2_t vslidedown_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, size_t offset, _VL_T vl);
vint64m4_t vslidedown_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, size_t offset, _VL_T vl);
vint64m8_t vslidedown_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, size_t offset, _VL_T vl);
vuint8m1_t vslidedown_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, size_t offset, _VL_T vl);
vuint8m2_t vslidedown_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, size_t offset, _VL_T vl);
vuint8m4_t vslidedown_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, size_t offset, _VL_T vl);
vuint8m8_t vslidedown_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, size_t offset, _VL_T vl);
vuint16m1_t vslidedown_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, size_t offset, _VL_T vl);
vuint16m2_t vslidedown_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, size_t offset, _VL_T vl);
vuint16m4_t vslidedown_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, size_t offset, _VL_T vl);
vuint16m8_t vslidedown_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, size_t offset, _VL_T vl);
vuint32m1_t vslidedown_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, size_t offset, _VL_T vl);
vuint32m2_t vslidedown_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, size_t offset, _VL_T vl);
vuint32m4_t vslidedown_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, size_t offset, _VL_T vl);
vuint32m8_t vslidedown_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, size_t offset, _VL_T vl);
vuint64m1_t vslidedown_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, size_t offset, _VL_T vl);
vuint64m2_t vslidedown_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, size_t offset, _VL_T vl);
vuint64m4_t vslidedown_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, size_t offset, _VL_T vl);
vuint64m8_t vslidedown_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, size_t offset, _VL_T vl);
vfloat16m1_t vslidedown_vx_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, size_t offset, _VL_T vl);
vfloat16m2_t vslidedown_vx_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, size_t offset, _VL_T vl);
vfloat16m4_t vslidedown_vx_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, size_t offset, _VL_T vl);
vfloat16m8_t vslidedown_vx_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, size_t offset, _VL_T vl);
vfloat32m1_t vslidedown_vx_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, size_t offset, _VL_T vl);
vfloat32m2_t vslidedown_vx_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, size_t offset, _VL_T vl);
vfloat32m4_t vslidedown_vx_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, size_t offset, _VL_T vl);
vfloat32m8_t vslidedown_vx_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, size_t offset, _VL_T vl);
vfloat64m1_t vslidedown_vx_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, size_t offset, _VL_T vl);
vfloat64m2_t vslidedown_vx_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, size_t offset, _VL_T vl);
vfloat64m4_t vslidedown_vx_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, size_t offset, _VL_T vl);
vfloat64m8_t vslidedown_vx_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, size_t offset, _VL_T vl);
```
### [Vector Slide1up and Slide1down Functions](rvv-intrinsic-api.md#173-vector-slide1up-and-slide1down-functions):

**Prototypes:**
``` C
vint8m1_t vslide1up_vx_i8m1_vl (vint8m1_t src, int8_t value, _VL_T vl);
vint8m2_t vslide1up_vx_i8m2_vl (vint8m2_t src, int8_t value, _VL_T vl);
vint8m4_t vslide1up_vx_i8m4_vl (vint8m4_t src, int8_t value, _VL_T vl);
vint8m8_t vslide1up_vx_i8m8_vl (vint8m8_t src, int8_t value, _VL_T vl);
vint16m1_t vslide1up_vx_i16m1_vl (vint16m1_t src, int16_t value, _VL_T vl);
vint16m2_t vslide1up_vx_i16m2_vl (vint16m2_t src, int16_t value, _VL_T vl);
vint16m4_t vslide1up_vx_i16m4_vl (vint16m4_t src, int16_t value, _VL_T vl);
vint16m8_t vslide1up_vx_i16m8_vl (vint16m8_t src, int16_t value, _VL_T vl);
vint32m1_t vslide1up_vx_i32m1_vl (vint32m1_t src, int32_t value, _VL_T vl);
vint32m2_t vslide1up_vx_i32m2_vl (vint32m2_t src, int32_t value, _VL_T vl);
vint32m4_t vslide1up_vx_i32m4_vl (vint32m4_t src, int32_t value, _VL_T vl);
vint32m8_t vslide1up_vx_i32m8_vl (vint32m8_t src, int32_t value, _VL_T vl);
vint64m1_t vslide1up_vx_i64m1_vl (vint64m1_t src, int64_t value, _VL_T vl);
vint64m2_t vslide1up_vx_i64m2_vl (vint64m2_t src, int64_t value, _VL_T vl);
vint64m4_t vslide1up_vx_i64m4_vl (vint64m4_t src, int64_t value, _VL_T vl);
vint64m8_t vslide1up_vx_i64m8_vl (vint64m8_t src, int64_t value, _VL_T vl);
vuint8m1_t vslide1up_vx_u8m1_vl (vuint8m1_t src, uint8_t value, _VL_T vl);
vuint8m2_t vslide1up_vx_u8m2_vl (vuint8m2_t src, uint8_t value, _VL_T vl);
vuint8m4_t vslide1up_vx_u8m4_vl (vuint8m4_t src, uint8_t value, _VL_T vl);
vuint8m8_t vslide1up_vx_u8m8_vl (vuint8m8_t src, uint8_t value, _VL_T vl);
vuint16m1_t vslide1up_vx_u16m1_vl (vuint16m1_t src, uint16_t value, _VL_T vl);
vuint16m2_t vslide1up_vx_u16m2_vl (vuint16m2_t src, uint16_t value, _VL_T vl);
vuint16m4_t vslide1up_vx_u16m4_vl (vuint16m4_t src, uint16_t value, _VL_T vl);
vuint16m8_t vslide1up_vx_u16m8_vl (vuint16m8_t src, uint16_t value, _VL_T vl);
vuint32m1_t vslide1up_vx_u32m1_vl (vuint32m1_t src, uint32_t value, _VL_T vl);
vuint32m2_t vslide1up_vx_u32m2_vl (vuint32m2_t src, uint32_t value, _VL_T vl);
vuint32m4_t vslide1up_vx_u32m4_vl (vuint32m4_t src, uint32_t value, _VL_T vl);
vuint32m8_t vslide1up_vx_u32m8_vl (vuint32m8_t src, uint32_t value, _VL_T vl);
vuint64m1_t vslide1up_vx_u64m1_vl (vuint64m1_t src, uint64_t value, _VL_T vl);
vuint64m2_t vslide1up_vx_u64m2_vl (vuint64m2_t src, uint64_t value, _VL_T vl);
vuint64m4_t vslide1up_vx_u64m4_vl (vuint64m4_t src, uint64_t value, _VL_T vl);
vuint64m8_t vslide1up_vx_u64m8_vl (vuint64m8_t src, uint64_t value, _VL_T vl);
vint8m1_t vslide1down_vx_i8m1_vl (vint8m1_t src, int8_t value, _VL_T vl);
vint8m2_t vslide1down_vx_i8m2_vl (vint8m2_t src, int8_t value, _VL_T vl);
vint8m4_t vslide1down_vx_i8m4_vl (vint8m4_t src, int8_t value, _VL_T vl);
vint8m8_t vslide1down_vx_i8m8_vl (vint8m8_t src, int8_t value, _VL_T vl);
vint16m1_t vslide1down_vx_i16m1_vl (vint16m1_t src, int16_t value, _VL_T vl);
vint16m2_t vslide1down_vx_i16m2_vl (vint16m2_t src, int16_t value, _VL_T vl);
vint16m4_t vslide1down_vx_i16m4_vl (vint16m4_t src, int16_t value, _VL_T vl);
vint16m8_t vslide1down_vx_i16m8_vl (vint16m8_t src, int16_t value, _VL_T vl);
vint32m1_t vslide1down_vx_i32m1_vl (vint32m1_t src, int32_t value, _VL_T vl);
vint32m2_t vslide1down_vx_i32m2_vl (vint32m2_t src, int32_t value, _VL_T vl);
vint32m4_t vslide1down_vx_i32m4_vl (vint32m4_t src, int32_t value, _VL_T vl);
vint32m8_t vslide1down_vx_i32m8_vl (vint32m8_t src, int32_t value, _VL_T vl);
vint64m1_t vslide1down_vx_i64m1_vl (vint64m1_t src, int64_t value, _VL_T vl);
vint64m2_t vslide1down_vx_i64m2_vl (vint64m2_t src, int64_t value, _VL_T vl);
vint64m4_t vslide1down_vx_i64m4_vl (vint64m4_t src, int64_t value, _VL_T vl);
vint64m8_t vslide1down_vx_i64m8_vl (vint64m8_t src, int64_t value, _VL_T vl);
vuint8m1_t vslide1down_vx_u8m1_vl (vuint8m1_t src, uint8_t value, _VL_T vl);
vuint8m2_t vslide1down_vx_u8m2_vl (vuint8m2_t src, uint8_t value, _VL_T vl);
vuint8m4_t vslide1down_vx_u8m4_vl (vuint8m4_t src, uint8_t value, _VL_T vl);
vuint8m8_t vslide1down_vx_u8m8_vl (vuint8m8_t src, uint8_t value, _VL_T vl);
vuint16m1_t vslide1down_vx_u16m1_vl (vuint16m1_t src, uint16_t value, _VL_T vl);
vuint16m2_t vslide1down_vx_u16m2_vl (vuint16m2_t src, uint16_t value, _VL_T vl);
vuint16m4_t vslide1down_vx_u16m4_vl (vuint16m4_t src, uint16_t value, _VL_T vl);
vuint16m8_t vslide1down_vx_u16m8_vl (vuint16m8_t src, uint16_t value, _VL_T vl);
vuint32m1_t vslide1down_vx_u32m1_vl (vuint32m1_t src, uint32_t value, _VL_T vl);
vuint32m2_t vslide1down_vx_u32m2_vl (vuint32m2_t src, uint32_t value, _VL_T vl);
vuint32m4_t vslide1down_vx_u32m4_vl (vuint32m4_t src, uint32_t value, _VL_T vl);
vuint32m8_t vslide1down_vx_u32m8_vl (vuint32m8_t src, uint32_t value, _VL_T vl);
vuint64m1_t vslide1down_vx_u64m1_vl (vuint64m1_t src, uint64_t value, _VL_T vl);
vuint64m2_t vslide1down_vx_u64m2_vl (vuint64m2_t src, uint64_t value, _VL_T vl);
vuint64m4_t vslide1down_vx_u64m4_vl (vuint64m4_t src, uint64_t value, _VL_T vl);
vuint64m8_t vslide1down_vx_u64m8_vl (vuint64m8_t src, uint64_t value, _VL_T vl);
// masked functions
vint8m1_t vslide1up_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, _VL_T vl);
vint8m2_t vslide1up_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, _VL_T vl);
vint8m4_t vslide1up_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, _VL_T vl);
vint8m8_t vslide1up_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, _VL_T vl);
vint16m1_t vslide1up_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, _VL_T vl);
vint16m2_t vslide1up_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, _VL_T vl);
vint16m4_t vslide1up_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, _VL_T vl);
vint16m8_t vslide1up_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, _VL_T vl);
vint32m1_t vslide1up_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, _VL_T vl);
vint32m2_t vslide1up_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, _VL_T vl);
vint32m4_t vslide1up_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, _VL_T vl);
vint32m8_t vslide1up_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, _VL_T vl);
vint64m1_t vslide1up_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, _VL_T vl);
vint64m2_t vslide1up_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, _VL_T vl);
vint64m4_t vslide1up_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, _VL_T vl);
vint64m8_t vslide1up_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, _VL_T vl);
vuint8m1_t vslide1up_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, _VL_T vl);
vuint8m2_t vslide1up_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, _VL_T vl);
vuint8m4_t vslide1up_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, _VL_T vl);
vuint8m8_t vslide1up_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, _VL_T vl);
vuint16m1_t vslide1up_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, _VL_T vl);
vuint16m2_t vslide1up_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, _VL_T vl);
vuint16m4_t vslide1up_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, _VL_T vl);
vuint16m8_t vslide1up_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, _VL_T vl);
vuint32m1_t vslide1up_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, _VL_T vl);
vuint32m2_t vslide1up_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, _VL_T vl);
vuint32m4_t vslide1up_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, _VL_T vl);
vuint32m8_t vslide1up_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, _VL_T vl);
vuint64m1_t vslide1up_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, _VL_T vl);
vuint64m2_t vslide1up_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, _VL_T vl);
vuint64m4_t vslide1up_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, _VL_T vl);
vuint64m8_t vslide1up_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, _VL_T vl);
vint8m1_t vslide1down_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, _VL_T vl);
vint8m2_t vslide1down_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, _VL_T vl);
vint8m4_t vslide1down_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, _VL_T vl);
vint8m8_t vslide1down_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, _VL_T vl);
vint16m1_t vslide1down_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, _VL_T vl);
vint16m2_t vslide1down_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, _VL_T vl);
vint16m4_t vslide1down_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, _VL_T vl);
vint16m8_t vslide1down_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, _VL_T vl);
vint32m1_t vslide1down_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, _VL_T vl);
vint32m2_t vslide1down_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, _VL_T vl);
vint32m4_t vslide1down_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, _VL_T vl);
vint32m8_t vslide1down_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, _VL_T vl);
vint64m1_t vslide1down_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, _VL_T vl);
vint64m2_t vslide1down_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, _VL_T vl);
vint64m4_t vslide1down_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, _VL_T vl);
vint64m8_t vslide1down_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, _VL_T vl);
vuint8m1_t vslide1down_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, _VL_T vl);
vuint8m2_t vslide1down_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, _VL_T vl);
vuint8m4_t vslide1down_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, _VL_T vl);
vuint8m8_t vslide1down_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, _VL_T vl);
vuint16m1_t vslide1down_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, _VL_T vl);
vuint16m2_t vslide1down_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, _VL_T vl);
vuint16m4_t vslide1down_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, _VL_T vl);
vuint16m8_t vslide1down_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, _VL_T vl);
vuint32m1_t vslide1down_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, _VL_T vl);
vuint32m2_t vslide1down_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, _VL_T vl);
vuint32m4_t vslide1down_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, _VL_T vl);
vuint32m8_t vslide1down_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, _VL_T vl);
vuint64m1_t vslide1down_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, _VL_T vl);
vuint64m2_t vslide1down_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, _VL_T vl);
vuint64m4_t vslide1down_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, _VL_T vl);
vuint64m8_t vslide1down_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, _VL_T vl);
```
### [Vector Register Gather Functions](rvv-intrinsic-api.md#174-vector-register-gather-operations):

**Prototypes:**
``` C
vint8m1_t vrgather_vv_i8m1_vl (vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vrgather_vx_i8m1_vl (vint8m1_t op1, unsigned long op2, _VL_T vl);
vint8m2_t vrgather_vv_i8m2_vl (vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vrgather_vx_i8m2_vl (vint8m2_t op1, unsigned long op2, _VL_T vl);
vint8m4_t vrgather_vv_i8m4_vl (vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vrgather_vx_i8m4_vl (vint8m4_t op1, unsigned long op2, _VL_T vl);
vint8m8_t vrgather_vv_i8m8_vl (vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vrgather_vx_i8m8_vl (vint8m8_t op1, unsigned long op2, _VL_T vl);
vint16m1_t vrgather_vv_i16m1_vl (vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vrgather_vx_i16m1_vl (vint16m1_t op1, unsigned long op2, _VL_T vl);
vint16m2_t vrgather_vv_i16m2_vl (vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vrgather_vx_i16m2_vl (vint16m2_t op1, unsigned long op2, _VL_T vl);
vint16m4_t vrgather_vv_i16m4_vl (vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vrgather_vx_i16m4_vl (vint16m4_t op1, unsigned long op2, _VL_T vl);
vint16m8_t vrgather_vv_i16m8_vl (vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vrgather_vx_i16m8_vl (vint16m8_t op1, unsigned long op2, _VL_T vl);
vint32m1_t vrgather_vv_i32m1_vl (vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vrgather_vx_i32m1_vl (vint32m1_t op1, unsigned long op2, _VL_T vl);
vint32m2_t vrgather_vv_i32m2_vl (vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vrgather_vx_i32m2_vl (vint32m2_t op1, unsigned long op2, _VL_T vl);
vint32m4_t vrgather_vv_i32m4_vl (vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vrgather_vx_i32m4_vl (vint32m4_t op1, unsigned long op2, _VL_T vl);
vint32m8_t vrgather_vv_i32m8_vl (vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vrgather_vx_i32m8_vl (vint32m8_t op1, unsigned long op2, _VL_T vl);
vint64m1_t vrgather_vv_i64m1_vl (vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vrgather_vx_i64m1_vl (vint64m1_t op1, unsigned long op2, _VL_T vl);
vint64m2_t vrgather_vv_i64m2_vl (vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vrgather_vx_i64m2_vl (vint64m2_t op1, unsigned long op2, _VL_T vl);
vint64m4_t vrgather_vv_i64m4_vl (vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vrgather_vx_i64m4_vl (vint64m4_t op1, unsigned long op2, _VL_T vl);
vint64m8_t vrgather_vv_i64m8_vl (vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vrgather_vx_i64m8_vl (vint64m8_t op1, unsigned long op2, _VL_T vl);
vuint8m1_t vrgather_vv_u8m1_vl (vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vrgather_vx_u8m1_vl (vuint8m1_t op1, unsigned long op2, _VL_T vl);
vuint8m2_t vrgather_vv_u8m2_vl (vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vrgather_vx_u8m2_vl (vuint8m2_t op1, unsigned long op2, _VL_T vl);
vuint8m4_t vrgather_vv_u8m4_vl (vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vrgather_vx_u8m4_vl (vuint8m4_t op1, unsigned long op2, _VL_T vl);
vuint8m8_t vrgather_vv_u8m8_vl (vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vrgather_vx_u8m8_vl (vuint8m8_t op1, unsigned long op2, _VL_T vl);
vuint16m1_t vrgather_vv_u16m1_vl (vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vrgather_vx_u16m1_vl (vuint16m1_t op1, unsigned long op2, _VL_T vl);
vuint16m2_t vrgather_vv_u16m2_vl (vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vrgather_vx_u16m2_vl (vuint16m2_t op1, unsigned long op2, _VL_T vl);
vuint16m4_t vrgather_vv_u16m4_vl (vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vrgather_vx_u16m4_vl (vuint16m4_t op1, unsigned long op2, _VL_T vl);
vuint16m8_t vrgather_vv_u16m8_vl (vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vrgather_vx_u16m8_vl (vuint16m8_t op1, unsigned long op2, _VL_T vl);
vuint32m1_t vrgather_vv_u32m1_vl (vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vrgather_vx_u32m1_vl (vuint32m1_t op1, unsigned long op2, _VL_T vl);
vuint32m2_t vrgather_vv_u32m2_vl (vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vrgather_vx_u32m2_vl (vuint32m2_t op1, unsigned long op2, _VL_T vl);
vuint32m4_t vrgather_vv_u32m4_vl (vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vrgather_vx_u32m4_vl (vuint32m4_t op1, unsigned long op2, _VL_T vl);
vuint32m8_t vrgather_vv_u32m8_vl (vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vrgather_vx_u32m8_vl (vuint32m8_t op1, unsigned long op2, _VL_T vl);
vuint64m1_t vrgather_vv_u64m1_vl (vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vrgather_vx_u64m1_vl (vuint64m1_t op1, unsigned long op2, _VL_T vl);
vuint64m2_t vrgather_vv_u64m2_vl (vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vrgather_vx_u64m2_vl (vuint64m2_t op1, unsigned long op2, _VL_T vl);
vuint64m4_t vrgather_vv_u64m4_vl (vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vrgather_vx_u64m4_vl (vuint64m4_t op1, unsigned long op2, _VL_T vl);
vuint64m8_t vrgather_vv_u64m8_vl (vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vrgather_vx_u64m8_vl (vuint64m8_t op1, unsigned long op2, _VL_T vl);
vfloat16m1_t vrgather_vv_f16m1_vl (vfloat16m1_t op1, vuint16m1_t op2, _VL_T vl);
vfloat16m1_t vrgather_vx_f16m1_vl (vfloat16m1_t op1, unsigned long op2, _VL_T vl);
vfloat16m2_t vrgather_vv_f16m2_vl (vfloat16m2_t op1, vuint16m2_t op2, _VL_T vl);
vfloat16m2_t vrgather_vx_f16m2_vl (vfloat16m2_t op1, unsigned long op2, _VL_T vl);
vfloat16m4_t vrgather_vv_f16m4_vl (vfloat16m4_t op1, vuint16m4_t op2, _VL_T vl);
vfloat16m4_t vrgather_vx_f16m4_vl (vfloat16m4_t op1, unsigned long op2, _VL_T vl);
vfloat16m8_t vrgather_vv_f16m8_vl (vfloat16m8_t op1, vuint16m8_t op2, _VL_T vl);
vfloat16m8_t vrgather_vx_f16m8_vl (vfloat16m8_t op1, unsigned long op2, _VL_T vl);
vfloat32m1_t vrgather_vv_f32m1_vl (vfloat32m1_t op1, vuint32m1_t op2, _VL_T vl);
vfloat32m1_t vrgather_vx_f32m1_vl (vfloat32m1_t op1, unsigned long op2, _VL_T vl);
vfloat32m2_t vrgather_vv_f32m2_vl (vfloat32m2_t op1, vuint32m2_t op2, _VL_T vl);
vfloat32m2_t vrgather_vx_f32m2_vl (vfloat32m2_t op1, unsigned long op2, _VL_T vl);
vfloat32m4_t vrgather_vv_f32m4_vl (vfloat32m4_t op1, vuint32m4_t op2, _VL_T vl);
vfloat32m4_t vrgather_vx_f32m4_vl (vfloat32m4_t op1, unsigned long op2, _VL_T vl);
vfloat32m8_t vrgather_vv_f32m8_vl (vfloat32m8_t op1, vuint32m8_t op2, _VL_T vl);
vfloat32m8_t vrgather_vx_f32m8_vl (vfloat32m8_t op1, unsigned long op2, _VL_T vl);
vfloat64m1_t vrgather_vv_f64m1_vl (vfloat64m1_t op1, vuint64m1_t op2, _VL_T vl);
vfloat64m1_t vrgather_vx_f64m1_vl (vfloat64m1_t op1, unsigned long op2, _VL_T vl);
vfloat64m2_t vrgather_vv_f64m2_vl (vfloat64m2_t op1, vuint64m2_t op2, _VL_T vl);
vfloat64m2_t vrgather_vx_f64m2_vl (vfloat64m2_t op1, unsigned long op2, _VL_T vl);
vfloat64m4_t vrgather_vv_f64m4_vl (vfloat64m4_t op1, vuint64m4_t op2, _VL_T vl);
vfloat64m4_t vrgather_vx_f64m4_vl (vfloat64m4_t op1, unsigned long op2, _VL_T vl);
vfloat64m8_t vrgather_vv_f64m8_vl (vfloat64m8_t op1, vuint64m8_t op2, _VL_T vl);
vfloat64m8_t vrgather_vx_f64m8_vl (vfloat64m8_t op1, unsigned long op2, _VL_T vl);
// masked functions
vint8m1_t vrgather_vv_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vint8m1_t vrgather_vx_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, unsigned long op2, _VL_T vl);
vint8m2_t vrgather_vv_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vint8m2_t vrgather_vx_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, unsigned long op2, _VL_T vl);
vint8m4_t vrgather_vv_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vint8m4_t vrgather_vx_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, unsigned long op2, _VL_T vl);
vint8m8_t vrgather_vv_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vint8m8_t vrgather_vx_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, unsigned long op2, _VL_T vl);
vint16m1_t vrgather_vv_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vint16m1_t vrgather_vx_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, unsigned long op2, _VL_T vl);
vint16m2_t vrgather_vv_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vint16m2_t vrgather_vx_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, unsigned long op2, _VL_T vl);
vint16m4_t vrgather_vv_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vint16m4_t vrgather_vx_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, unsigned long op2, _VL_T vl);
vint16m8_t vrgather_vv_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vint16m8_t vrgather_vx_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, unsigned long op2, _VL_T vl);
vint32m1_t vrgather_vv_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vint32m1_t vrgather_vx_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, unsigned long op2, _VL_T vl);
vint32m2_t vrgather_vv_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vint32m2_t vrgather_vx_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, unsigned long op2, _VL_T vl);
vint32m4_t vrgather_vv_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vint32m4_t vrgather_vx_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, unsigned long op2, _VL_T vl);
vint32m8_t vrgather_vv_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vint32m8_t vrgather_vx_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, unsigned long op2, _VL_T vl);
vint64m1_t vrgather_vv_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vint64m1_t vrgather_vx_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, unsigned long op2, _VL_T vl);
vint64m2_t vrgather_vv_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vint64m2_t vrgather_vx_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, unsigned long op2, _VL_T vl);
vint64m4_t vrgather_vv_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vint64m4_t vrgather_vx_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, unsigned long op2, _VL_T vl);
vint64m8_t vrgather_vv_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vint64m8_t vrgather_vx_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, unsigned long op2, _VL_T vl);
vuint8m1_t vrgather_vv_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2, _VL_T vl);
vuint8m1_t vrgather_vx_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, unsigned long op2, _VL_T vl);
vuint8m2_t vrgather_vv_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2, _VL_T vl);
vuint8m2_t vrgather_vx_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, unsigned long op2, _VL_T vl);
vuint8m4_t vrgather_vv_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2, _VL_T vl);
vuint8m4_t vrgather_vx_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, unsigned long op2, _VL_T vl);
vuint8m8_t vrgather_vv_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2, _VL_T vl);
vuint8m8_t vrgather_vx_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, unsigned long op2, _VL_T vl);
vuint16m1_t vrgather_vv_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, _VL_T vl);
vuint16m1_t vrgather_vx_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, unsigned long op2, _VL_T vl);
vuint16m2_t vrgather_vv_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, _VL_T vl);
vuint16m2_t vrgather_vx_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, unsigned long op2, _VL_T vl);
vuint16m4_t vrgather_vv_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, _VL_T vl);
vuint16m4_t vrgather_vx_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, unsigned long op2, _VL_T vl);
vuint16m8_t vrgather_vv_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, _VL_T vl);
vuint16m8_t vrgather_vx_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, unsigned long op2, _VL_T vl);
vuint32m1_t vrgather_vv_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2, _VL_T vl);
vuint32m1_t vrgather_vx_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, unsigned long op2, _VL_T vl);
vuint32m2_t vrgather_vv_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2, _VL_T vl);
vuint32m2_t vrgather_vx_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, unsigned long op2, _VL_T vl);
vuint32m4_t vrgather_vv_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2, _VL_T vl);
vuint32m4_t vrgather_vx_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, unsigned long op2, _VL_T vl);
vuint32m8_t vrgather_vv_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2, _VL_T vl);
vuint32m8_t vrgather_vx_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, unsigned long op2, _VL_T vl);
vuint64m1_t vrgather_vv_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2, _VL_T vl);
vuint64m1_t vrgather_vx_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, unsigned long op2, _VL_T vl);
vuint64m2_t vrgather_vv_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2, _VL_T vl);
vuint64m2_t vrgather_vx_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, unsigned long op2, _VL_T vl);
vuint64m4_t vrgather_vv_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2, _VL_T vl);
vuint64m4_t vrgather_vx_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, unsigned long op2, _VL_T vl);
vuint64m8_t vrgather_vv_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2, _VL_T vl);
vuint64m8_t vrgather_vx_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, unsigned long op2, _VL_T vl);
vfloat16m1_t vrgather_vv_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t op2, _VL_T vl);
vfloat16m1_t vrgather_vx_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, unsigned long op2, _VL_T vl);
vfloat16m2_t vrgather_vv_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t op2, _VL_T vl);
vfloat16m2_t vrgather_vx_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, unsigned long op2, _VL_T vl);
vfloat16m4_t vrgather_vv_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t op2, _VL_T vl);
vfloat16m4_t vrgather_vx_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, unsigned long op2, _VL_T vl);
vfloat16m8_t vrgather_vv_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t op2, _VL_T vl);
vfloat16m8_t vrgather_vx_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, unsigned long op2, _VL_T vl);
vfloat32m1_t vrgather_vv_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint32m1_t op2, _VL_T vl);
vfloat32m1_t vrgather_vx_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, unsigned long op2, _VL_T vl);
vfloat32m2_t vrgather_vv_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint32m2_t op2, _VL_T vl);
vfloat32m2_t vrgather_vx_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, unsigned long op2, _VL_T vl);
vfloat32m4_t vrgather_vv_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint32m4_t op2, _VL_T vl);
vfloat32m4_t vrgather_vx_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, unsigned long op2, _VL_T vl);
vfloat32m8_t vrgather_vv_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint32m8_t op2, _VL_T vl);
vfloat32m8_t vrgather_vx_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, unsigned long op2, _VL_T vl);
vfloat64m1_t vrgather_vv_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint64m1_t op2, _VL_T vl);
vfloat64m1_t vrgather_vx_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, unsigned long op2, _VL_T vl);
vfloat64m2_t vrgather_vv_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint64m2_t op2, _VL_T vl);
vfloat64m2_t vrgather_vx_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, unsigned long op2, _VL_T vl);
vfloat64m4_t vrgather_vv_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint64m4_t op2, _VL_T vl);
vfloat64m4_t vrgather_vx_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, unsigned long op2, _VL_T vl);
vfloat64m8_t vrgather_vv_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint64m8_t op2, _VL_T vl);
vfloat64m8_t vrgather_vx_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, unsigned long op2, _VL_T vl);
```
### [Vector Compress Functions](rvv-intrinsic-api.md#175-vector-compress-operations):

**Prototypes:**
``` C
// masked functions
vint8m1_t vcompress_vm_i8m1_m_vl (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, _VL_T vl);
vint8m2_t vcompress_vm_i8m2_m_vl (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, _VL_T vl);
vint8m4_t vcompress_vm_i8m4_m_vl (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, _VL_T vl);
vint8m8_t vcompress_vm_i8m8_m_vl (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, _VL_T vl);
vint16m1_t vcompress_vm_i16m1_m_vl (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, _VL_T vl);
vint16m2_t vcompress_vm_i16m2_m_vl (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, _VL_T vl);
vint16m4_t vcompress_vm_i16m4_m_vl (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, _VL_T vl);
vint16m8_t vcompress_vm_i16m8_m_vl (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, _VL_T vl);
vint32m1_t vcompress_vm_i32m1_m_vl (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, _VL_T vl);
vint32m2_t vcompress_vm_i32m2_m_vl (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, _VL_T vl);
vint32m4_t vcompress_vm_i32m4_m_vl (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, _VL_T vl);
vint32m8_t vcompress_vm_i32m8_m_vl (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, _VL_T vl);
vint64m1_t vcompress_vm_i64m1_m_vl (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, _VL_T vl);
vint64m2_t vcompress_vm_i64m2_m_vl (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, _VL_T vl);
vint64m4_t vcompress_vm_i64m4_m_vl (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, _VL_T vl);
vint64m8_t vcompress_vm_i64m8_m_vl (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, _VL_T vl);
vuint8m1_t vcompress_vm_u8m1_m_vl (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, _VL_T vl);
vuint8m2_t vcompress_vm_u8m2_m_vl (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, _VL_T vl);
vuint8m4_t vcompress_vm_u8m4_m_vl (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, _VL_T vl);
vuint8m8_t vcompress_vm_u8m8_m_vl (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, _VL_T vl);
vuint16m1_t vcompress_vm_u16m1_m_vl (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, _VL_T vl);
vuint16m2_t vcompress_vm_u16m2_m_vl (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, _VL_T vl);
vuint16m4_t vcompress_vm_u16m4_m_vl (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, _VL_T vl);
vuint16m8_t vcompress_vm_u16m8_m_vl (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, _VL_T vl);
vuint32m1_t vcompress_vm_u32m1_m_vl (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, _VL_T vl);
vuint32m2_t vcompress_vm_u32m2_m_vl (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, _VL_T vl);
vuint32m4_t vcompress_vm_u32m4_m_vl (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, _VL_T vl);
vuint32m8_t vcompress_vm_u32m8_m_vl (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, _VL_T vl);
vuint64m1_t vcompress_vm_u64m1_m_vl (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, _VL_T vl);
vuint64m2_t vcompress_vm_u64m2_m_vl (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, _VL_T vl);
vuint64m4_t vcompress_vm_u64m4_m_vl (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, _VL_T vl);
vuint64m8_t vcompress_vm_u64m8_m_vl (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, _VL_T vl);
vfloat16m1_t vcompress_vm_f16m1_m_vl (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, _VL_T vl);
vfloat16m2_t vcompress_vm_f16m2_m_vl (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, _VL_T vl);
vfloat16m4_t vcompress_vm_f16m4_m_vl (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, _VL_T vl);
vfloat16m8_t vcompress_vm_f16m8_m_vl (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, _VL_T vl);
vfloat32m1_t vcompress_vm_f32m1_m_vl (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, _VL_T vl);
vfloat32m2_t vcompress_vm_f32m2_m_vl (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, _VL_T vl);
vfloat32m4_t vcompress_vm_f32m4_m_vl (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, _VL_T vl);
vfloat32m8_t vcompress_vm_f32m8_m_vl (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, _VL_T vl);
vfloat64m1_t vcompress_vm_f64m1_m_vl (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, _VL_T vl);
vfloat64m2_t vcompress_vm_f64m2_m_vl (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, _VL_T vl);
vfloat64m4_t vcompress_vm_f64m4_m_vl (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, _VL_T vl);
vfloat64m8_t vcompress_vm_f64m8_m_vl (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, _VL_T vl);
```
## Miscellaneous Vector Functions:

### [Reinterpret Cast Conversion Functions](rvv-intrinsic-api.md#reinterpret-cast-conversion-functions):

**Prototypes:**
``` C
// Reinterpret between different type under the same SEW/LMUL
vuint8m1_t vreinterpret_u8_i8_u8m1_vl (vint8m1_t src, _VL_T vl);
vuint8m2_t vreinterpret_u8_i8_u8m2_vl (vint8m2_t src, _VL_T vl);
vuint8m4_t vreinterpret_u8_i8_u8m4_vl (vint8m4_t src, _VL_T vl);
vuint8m8_t vreinterpret_u8_i8_u8m8_vl (vint8m8_t src, _VL_T vl);
vint8m1_t vreinterpret_i8_u8_i8m1_vl (vuint8m1_t src, _VL_T vl);
vint8m2_t vreinterpret_i8_u8_i8m2_vl (vuint8m2_t src, _VL_T vl);
vint8m4_t vreinterpret_i8_u8_i8m4_vl (vuint8m4_t src, _VL_T vl);
vint8m8_t vreinterpret_i8_u8_i8m8_vl (vuint8m8_t src, _VL_T vl);
vuint16m1_t vreinterpret_u16_i16_u16m1_vl (vint16m1_t src, _VL_T vl);
vuint16m2_t vreinterpret_u16_i16_u16m2_vl (vint16m2_t src, _VL_T vl);
vuint16m4_t vreinterpret_u16_i16_u16m4_vl (vint16m4_t src, _VL_T vl);
vuint16m8_t vreinterpret_u16_i16_u16m8_vl (vint16m8_t src, _VL_T vl);
vint16m1_t vreinterpret_i16_u16_i16m1_vl (vuint16m1_t src, _VL_T vl);
vint16m2_t vreinterpret_i16_u16_i16m2_vl (vuint16m2_t src, _VL_T vl);
vint16m4_t vreinterpret_i16_u16_i16m4_vl (vuint16m4_t src, _VL_T vl);
vint16m8_t vreinterpret_i16_u16_i16m8_vl (vuint16m8_t src, _VL_T vl);
vint16m1_t vreinterpret_i16_f16_i16m1_vl (vfloat16m1_t src, _VL_T vl);
vint16m2_t vreinterpret_i16_f16_i16m2_vl (vfloat16m2_t src, _VL_T vl);
vint16m4_t vreinterpret_i16_f16_i16m4_vl (vfloat16m4_t src, _VL_T vl);
vint16m8_t vreinterpret_i16_f16_i16m8_vl (vfloat16m8_t src, _VL_T vl);
vuint16m1_t vreinterpret_u16_f16_u16m1_vl (vfloat16m1_t src, _VL_T vl);
vuint16m2_t vreinterpret_u16_f16_u16m2_vl (vfloat16m2_t src, _VL_T vl);
vuint16m4_t vreinterpret_u16_f16_u16m4_vl (vfloat16m4_t src, _VL_T vl);
vuint16m8_t vreinterpret_u16_f16_u16m8_vl (vfloat16m8_t src, _VL_T vl);
vfloat16m1_t vreinterpret_f16_i16_f16m1_vl (vint16m1_t src, _VL_T vl);
vfloat16m2_t vreinterpret_f16_i16_f16m2_vl (vint16m2_t src, _VL_T vl);
vfloat16m4_t vreinterpret_f16_i16_f16m4_vl (vint16m4_t src, _VL_T vl);
vfloat16m8_t vreinterpret_f16_i16_f16m8_vl (vint16m8_t src, _VL_T vl);
vfloat16m1_t vreinterpret_f16_u16_f16m1_vl (vuint16m1_t src, _VL_T vl);
vfloat16m2_t vreinterpret_f16_u16_f16m2_vl (vuint16m2_t src, _VL_T vl);
vfloat16m4_t vreinterpret_f16_u16_f16m4_vl (vuint16m4_t src, _VL_T vl);
vfloat16m8_t vreinterpret_f16_u16_f16m8_vl (vuint16m8_t src, _VL_T vl);
vuint32m1_t vreinterpret_u32_i32_u32m1_vl (vint32m1_t src, _VL_T vl);
vuint32m2_t vreinterpret_u32_i32_u32m2_vl (vint32m2_t src, _VL_T vl);
vuint32m4_t vreinterpret_u32_i32_u32m4_vl (vint32m4_t src, _VL_T vl);
vuint32m8_t vreinterpret_u32_i32_u32m8_vl (vint32m8_t src, _VL_T vl);
vint32m1_t vreinterpret_i32_u32_i32m1_vl (vuint32m1_t src, _VL_T vl);
vint32m2_t vreinterpret_i32_u32_i32m2_vl (vuint32m2_t src, _VL_T vl);
vint32m4_t vreinterpret_i32_u32_i32m4_vl (vuint32m4_t src, _VL_T vl);
vint32m8_t vreinterpret_i32_u32_i32m8_vl (vuint32m8_t src, _VL_T vl);
vint32m1_t vreinterpret_i32_f32_i32m1_vl (vfloat32m1_t src, _VL_T vl);
vint32m2_t vreinterpret_i32_f32_i32m2_vl (vfloat32m2_t src, _VL_T vl);
vint32m4_t vreinterpret_i32_f32_i32m4_vl (vfloat32m4_t src, _VL_T vl);
vint32m8_t vreinterpret_i32_f32_i32m8_vl (vfloat32m8_t src, _VL_T vl);
vuint32m1_t vreinterpret_u32_f32_u32m1_vl (vfloat32m1_t src, _VL_T vl);
vuint32m2_t vreinterpret_u32_f32_u32m2_vl (vfloat32m2_t src, _VL_T vl);
vuint32m4_t vreinterpret_u32_f32_u32m4_vl (vfloat32m4_t src, _VL_T vl);
vuint32m8_t vreinterpret_u32_f32_u32m8_vl (vfloat32m8_t src, _VL_T vl);
vfloat32m1_t vreinterpret_f32_i32_f32m1_vl (vint32m1_t src, _VL_T vl);
vfloat32m2_t vreinterpret_f32_i32_f32m2_vl (vint32m2_t src, _VL_T vl);
vfloat32m4_t vreinterpret_f32_i32_f32m4_vl (vint32m4_t src, _VL_T vl);
vfloat32m8_t vreinterpret_f32_i32_f32m8_vl (vint32m8_t src, _VL_T vl);
vfloat32m1_t vreinterpret_f32_u32_f32m1_vl (vuint32m1_t src, _VL_T vl);
vfloat32m2_t vreinterpret_f32_u32_f32m2_vl (vuint32m2_t src, _VL_T vl);
vfloat32m4_t vreinterpret_f32_u32_f32m4_vl (vuint32m4_t src, _VL_T vl);
vfloat32m8_t vreinterpret_f32_u32_f32m8_vl (vuint32m8_t src, _VL_T vl);
vuint64m1_t vreinterpret_u64_i64_u64m1_vl (vint64m1_t src, _VL_T vl);
vuint64m2_t vreinterpret_u64_i64_u64m2_vl (vint64m2_t src, _VL_T vl);
vuint64m4_t vreinterpret_u64_i64_u64m4_vl (vint64m4_t src, _VL_T vl);
vuint64m8_t vreinterpret_u64_i64_u64m8_vl (vint64m8_t src, _VL_T vl);
vint64m1_t vreinterpret_i64_u64_i64m1_vl (vuint64m1_t src, _VL_T vl);
vint64m2_t vreinterpret_i64_u64_i64m2_vl (vuint64m2_t src, _VL_T vl);
vint64m4_t vreinterpret_i64_u64_i64m4_vl (vuint64m4_t src, _VL_T vl);
vint64m8_t vreinterpret_i64_u64_i64m8_vl (vuint64m8_t src, _VL_T vl);
vint64m1_t vreinterpret_i64_f64_i64m1_vl (vfloat64m1_t src, _VL_T vl);
vint64m2_t vreinterpret_i64_f64_i64m2_vl (vfloat64m2_t src, _VL_T vl);
vint64m4_t vreinterpret_i64_f64_i64m4_vl (vfloat64m4_t src, _VL_T vl);
vint64m8_t vreinterpret_i64_f64_i64m8_vl (vfloat64m8_t src, _VL_T vl);
vuint64m1_t vreinterpret_u64_f64_u64m1_vl (vfloat64m1_t src, _VL_T vl);
vuint64m2_t vreinterpret_u64_f64_u64m2_vl (vfloat64m2_t src, _VL_T vl);
vuint64m4_t vreinterpret_u64_f64_u64m4_vl (vfloat64m4_t src, _VL_T vl);
vuint64m8_t vreinterpret_u64_f64_u64m8_vl (vfloat64m8_t src, _VL_T vl);
vfloat64m1_t vreinterpret_f64_i64_f64m1_vl (vint64m1_t src, _VL_T vl);
vfloat64m2_t vreinterpret_f64_i64_f64m2_vl (vint64m2_t src, _VL_T vl);
vfloat64m4_t vreinterpret_f64_i64_f64m4_vl (vint64m4_t src, _VL_T vl);
vfloat64m8_t vreinterpret_f64_i64_f64m8_vl (vint64m8_t src, _VL_T vl);
vfloat64m1_t vreinterpret_f64_u64_f64m1_vl (vuint64m1_t src, _VL_T vl);
vfloat64m2_t vreinterpret_f64_u64_f64m2_vl (vuint64m2_t src, _VL_T vl);
vfloat64m4_t vreinterpret_f64_u64_f64m4_vl (vuint64m4_t src, _VL_T vl);
vfloat64m8_t vreinterpret_f64_u64_f64m8_vl (vuint64m8_t src, _VL_T vl);
// Reinterpret between different SEW under the same LMUL
vint16m1_t vreinterpret_i16_i8_i16m1_vl (vuint8m1_t src, _VL_T vl);
vint16m2_t vreinterpret_i16_i8_i16m2_vl (vuint8m2_t src, _VL_T vl);
vint16m4_t vreinterpret_i16_i8_i16m4_vl (vuint8m4_t src, _VL_T vl);
vint16m8_t vreinterpret_i16_i8_i16m8_vl (vuint8m8_t src, _VL_T vl);
vuint16m1_t vreinterpret_u16_u8_u16m1_vl (vuint8m1_t src, _VL_T vl);
vuint16m2_t vreinterpret_u16_u8_u16m2_vl (vuint8m2_t src, _VL_T vl);
vuint16m4_t vreinterpret_u16_u8_u16m4_vl (vuint8m4_t src, _VL_T vl);
vuint16m8_t vreinterpret_u16_u8_u16m8_vl (vuint8m8_t src, _VL_T vl);
vint32m1_t vreinterpret_i32_i8_i32m1_vl (vuint8m1_t src, _VL_T vl);
vint32m2_t vreinterpret_i32_i8_i32m2_vl (vuint8m2_t src, _VL_T vl);
vint32m4_t vreinterpret_i32_i8_i32m4_vl (vuint8m4_t src, _VL_T vl);
vint32m8_t vreinterpret_i32_i8_i32m8_vl (vuint8m8_t src, _VL_T vl);
vuint32m1_t vreinterpret_u32_u8_u32m1_vl (vuint8m1_t src, _VL_T vl);
vuint32m2_t vreinterpret_u32_u8_u32m2_vl (vuint8m2_t src, _VL_T vl);
vuint32m4_t vreinterpret_u32_u8_u32m4_vl (vuint8m4_t src, _VL_T vl);
vuint32m8_t vreinterpret_u32_u8_u32m8_vl (vuint8m8_t src, _VL_T vl);
vint64m1_t vreinterpret_i64_i8_i64m1_vl (vuint8m1_t src, _VL_T vl);
vint64m2_t vreinterpret_i64_i8_i64m2_vl (vuint8m2_t src, _VL_T vl);
vint64m4_t vreinterpret_i64_i8_i64m4_vl (vuint8m4_t src, _VL_T vl);
vint64m8_t vreinterpret_i64_i8_i64m8_vl (vuint8m8_t src, _VL_T vl);
vuint64m1_t vreinterpret_u64_u8_u64m1_vl (vuint8m1_t src, _VL_T vl);
vuint64m2_t vreinterpret_u64_u8_u64m2_vl (vuint8m2_t src, _VL_T vl);
vuint64m4_t vreinterpret_u64_u8_u64m4_vl (vuint8m4_t src, _VL_T vl);
vuint64m8_t vreinterpret_u64_u8_u64m8_vl (vuint8m8_t src, _VL_T vl);
vint8m1_t vreinterpret_i8_i16_i8m1_vl (vuint16m1_t src, _VL_T vl);
vint8m2_t vreinterpret_i8_i16_i8m2_vl (vuint16m2_t src, _VL_T vl);
vint8m4_t vreinterpret_i8_i16_i8m4_vl (vuint16m4_t src, _VL_T vl);
vint8m8_t vreinterpret_i8_i16_i8m8_vl (vuint16m8_t src, _VL_T vl);
vuint8m1_t vreinterpret_u8_u16_u8m1_vl (vuint16m1_t src, _VL_T vl);
vuint8m2_t vreinterpret_u8_u16_u8m2_vl (vuint16m2_t src, _VL_T vl);
vuint8m4_t vreinterpret_u8_u16_u8m4_vl (vuint16m4_t src, _VL_T vl);
vuint8m8_t vreinterpret_u8_u16_u8m8_vl (vuint16m8_t src, _VL_T vl);
vint32m1_t vreinterpret_i32_i16_i32m1_vl (vuint16m1_t src, _VL_T vl);
vint32m2_t vreinterpret_i32_i16_i32m2_vl (vuint16m2_t src, _VL_T vl);
vint32m4_t vreinterpret_i32_i16_i32m4_vl (vuint16m4_t src, _VL_T vl);
vint32m8_t vreinterpret_i32_i16_i32m8_vl (vuint16m8_t src, _VL_T vl);
vuint32m1_t vreinterpret_u32_u16_u32m1_vl (vuint16m1_t src, _VL_T vl);
vuint32m2_t vreinterpret_u32_u16_u32m2_vl (vuint16m2_t src, _VL_T vl);
vuint32m4_t vreinterpret_u32_u16_u32m4_vl (vuint16m4_t src, _VL_T vl);
vuint32m8_t vreinterpret_u32_u16_u32m8_vl (vuint16m8_t src, _VL_T vl);
vint64m1_t vreinterpret_i64_i16_i64m1_vl (vuint16m1_t src, _VL_T vl);
vint64m2_t vreinterpret_i64_i16_i64m2_vl (vuint16m2_t src, _VL_T vl);
vint64m4_t vreinterpret_i64_i16_i64m4_vl (vuint16m4_t src, _VL_T vl);
vint64m8_t vreinterpret_i64_i16_i64m8_vl (vuint16m8_t src, _VL_T vl);
vuint64m1_t vreinterpret_u64_u16_u64m1_vl (vuint16m1_t src, _VL_T vl);
vuint64m2_t vreinterpret_u64_u16_u64m2_vl (vuint16m2_t src, _VL_T vl);
vuint64m4_t vreinterpret_u64_u16_u64m4_vl (vuint16m4_t src, _VL_T vl);
vuint64m8_t vreinterpret_u64_u16_u64m8_vl (vuint16m8_t src, _VL_T vl);
vint8m1_t vreinterpret_i8_i32_i8m1_vl (vuint32m1_t src, _VL_T vl);
vint8m2_t vreinterpret_i8_i32_i8m2_vl (vuint32m2_t src, _VL_T vl);
vint8m4_t vreinterpret_i8_i32_i8m4_vl (vuint32m4_t src, _VL_T vl);
vint8m8_t vreinterpret_i8_i32_i8m8_vl (vuint32m8_t src, _VL_T vl);
vuint8m1_t vreinterpret_u8_u32_u8m1_vl (vuint32m1_t src, _VL_T vl);
vuint8m2_t vreinterpret_u8_u32_u8m2_vl (vuint32m2_t src, _VL_T vl);
vuint8m4_t vreinterpret_u8_u32_u8m4_vl (vuint32m4_t src, _VL_T vl);
vuint8m8_t vreinterpret_u8_u32_u8m8_vl (vuint32m8_t src, _VL_T vl);
vint16m1_t vreinterpret_i16_i32_i16m1_vl (vuint32m1_t src, _VL_T vl);
vint16m2_t vreinterpret_i16_i32_i16m2_vl (vuint32m2_t src, _VL_T vl);
vint16m4_t vreinterpret_i16_i32_i16m4_vl (vuint32m4_t src, _VL_T vl);
vint16m8_t vreinterpret_i16_i32_i16m8_vl (vuint32m8_t src, _VL_T vl);
vuint16m1_t vreinterpret_u16_u32_u16m1_vl (vuint32m1_t src, _VL_T vl);
vuint16m2_t vreinterpret_u16_u32_u16m2_vl (vuint32m2_t src, _VL_T vl);
vuint16m4_t vreinterpret_u16_u32_u16m4_vl (vuint32m4_t src, _VL_T vl);
vuint16m8_t vreinterpret_u16_u32_u16m8_vl (vuint32m8_t src, _VL_T vl);
vint64m1_t vreinterpret_i64_i32_i64m1_vl (vuint32m1_t src, _VL_T vl);
vint64m2_t vreinterpret_i64_i32_i64m2_vl (vuint32m2_t src, _VL_T vl);
vint64m4_t vreinterpret_i64_i32_i64m4_vl (vuint32m4_t src, _VL_T vl);
vint64m8_t vreinterpret_i64_i32_i64m8_vl (vuint32m8_t src, _VL_T vl);
vuint64m1_t vreinterpret_u64_u32_u64m1_vl (vuint32m1_t src, _VL_T vl);
vuint64m2_t vreinterpret_u64_u32_u64m2_vl (vuint32m2_t src, _VL_T vl);
vuint64m4_t vreinterpret_u64_u32_u64m4_vl (vuint32m4_t src, _VL_T vl);
vuint64m8_t vreinterpret_u64_u32_u64m8_vl (vuint32m8_t src, _VL_T vl);
vint8m1_t vreinterpret_i8_i64_i8m1_vl (vuint64m1_t src, _VL_T vl);
vint8m2_t vreinterpret_i8_i64_i8m2_vl (vuint64m2_t src, _VL_T vl);
vint8m4_t vreinterpret_i8_i64_i8m4_vl (vuint64m4_t src, _VL_T vl);
vint8m8_t vreinterpret_i8_i64_i8m8_vl (vuint64m8_t src, _VL_T vl);
vuint8m1_t vreinterpret_u8_u64_u8m1_vl (vuint64m1_t src, _VL_T vl);
vuint8m2_t vreinterpret_u8_u64_u8m2_vl (vuint64m2_t src, _VL_T vl);
vuint8m4_t vreinterpret_u8_u64_u8m4_vl (vuint64m4_t src, _VL_T vl);
vuint8m8_t vreinterpret_u8_u64_u8m8_vl (vuint64m8_t src, _VL_T vl);
vint16m1_t vreinterpret_i16_i64_i16m1_vl (vuint64m1_t src, _VL_T vl);
vint16m2_t vreinterpret_i16_i64_i16m2_vl (vuint64m2_t src, _VL_T vl);
vint16m4_t vreinterpret_i16_i64_i16m4_vl (vuint64m4_t src, _VL_T vl);
vint16m8_t vreinterpret_i16_i64_i16m8_vl (vuint64m8_t src, _VL_T vl);
vuint16m1_t vreinterpret_u16_u64_u16m1_vl (vuint64m1_t src, _VL_T vl);
vuint16m2_t vreinterpret_u16_u64_u16m2_vl (vuint64m2_t src, _VL_T vl);
vuint16m4_t vreinterpret_u16_u64_u16m4_vl (vuint64m4_t src, _VL_T vl);
vuint16m8_t vreinterpret_u16_u64_u16m8_vl (vuint64m8_t src, _VL_T vl);
vint32m1_t vreinterpret_i32_i64_i32m1_vl (vuint64m1_t src, _VL_T vl);
vint32m2_t vreinterpret_i32_i64_i32m2_vl (vuint64m2_t src, _VL_T vl);
vint32m4_t vreinterpret_i32_i64_i32m4_vl (vuint64m4_t src, _VL_T vl);
vint32m8_t vreinterpret_i32_i64_i32m8_vl (vuint64m8_t src, _VL_T vl);
vuint32m1_t vreinterpret_u32_u64_u32m1_vl (vuint64m1_t src, _VL_T vl);
vuint32m2_t vreinterpret_u32_u64_u32m2_vl (vuint64m2_t src, _VL_T vl);
vuint32m4_t vreinterpret_u32_u64_u32m4_vl (vuint64m4_t src, _VL_T vl);
vuint32m8_t vreinterpret_u32_u64_u32m8_vl (vuint64m8_t src, _VL_T vl);
```
### [Vector Initialization Functions](rvv-intrinsic-api.md#vector-initialization-functions):

**Prototypes:**
``` C
vint8m1_t vzero_i8m1_vl (_VL_T vl);
vint8m2_t vzero_i8m2_vl (_VL_T vl);
vint8m4_t vzero_i8m4_vl (_VL_T vl);
vint8m8_t vzero_i8m8_vl (_VL_T vl);
vint16m1_t vzero_i16m1_vl (_VL_T vl);
vint16m2_t vzero_i16m2_vl (_VL_T vl);
vint16m4_t vzero_i16m4_vl (_VL_T vl);
vint16m8_t vzero_i16m8_vl (_VL_T vl);
vint32m1_t vzero_i32m1_vl (_VL_T vl);
vint32m2_t vzero_i32m2_vl (_VL_T vl);
vint32m4_t vzero_i32m4_vl (_VL_T vl);
vint32m8_t vzero_i32m8_vl (_VL_T vl);
vint64m1_t vzero_i64m1_vl (_VL_T vl);
vint64m2_t vzero_i64m2_vl (_VL_T vl);
vint64m4_t vzero_i64m4_vl (_VL_T vl);
vint64m8_t vzero_i64m8_vl (_VL_T vl);
vuint8m1_t vzero_u8m1_vl (_VL_T vl);
vuint8m2_t vzero_u8m2_vl (_VL_T vl);
vuint8m4_t vzero_u8m4_vl (_VL_T vl);
vuint8m8_t vzero_u8m8_vl (_VL_T vl);
vuint16m1_t vzero_u16m1_vl (_VL_T vl);
vuint16m2_t vzero_u16m2_vl (_VL_T vl);
vuint16m4_t vzero_u16m4_vl (_VL_T vl);
vuint16m8_t vzero_u16m8_vl (_VL_T vl);
vuint32m1_t vzero_u32m1_vl (_VL_T vl);
vuint32m2_t vzero_u32m2_vl (_VL_T vl);
vuint32m4_t vzero_u32m4_vl (_VL_T vl);
vuint32m8_t vzero_u32m8_vl (_VL_T vl);
vuint64m1_t vzero_u64m1_vl (_VL_T vl);
vuint64m2_t vzero_u64m2_vl (_VL_T vl);
vuint64m4_t vzero_u64m4_vl (_VL_T vl);
vuint64m8_t vzero_u64m8_vl (_VL_T vl);
vfloat16m1_t vzero_f16m1_vl (_VL_T vl);
vfloat16m2_t vzero_f16m2_vl (_VL_T vl);
vfloat16m4_t vzero_f16m4_vl (_VL_T vl);
vfloat16m8_t vzero_f16m8_vl (_VL_T vl);
vfloat32m1_t vzero_f32m1_vl (_VL_T vl);
vfloat32m2_t vzero_f32m2_vl (_VL_T vl);
vfloat32m4_t vzero_f32m4_vl (_VL_T vl);
vfloat32m8_t vzero_f32m8_vl (_VL_T vl);
vfloat64m1_t vzero_f64m1_vl (_VL_T vl);
vfloat64m2_t vzero_f64m2_vl (_VL_T vl);
vfloat64m4_t vzero_f64m4_vl (_VL_T vl);
vfloat64m8_t vzero_f64m8_vl (_VL_T vl);
vint8m1_t vundefined_i8m1_vl (_VL_T vl);
vint8m2_t vundefined_i8m2_vl (_VL_T vl);
vint8m4_t vundefined_i8m4_vl (_VL_T vl);
vint8m8_t vundefined_i8m8_vl (_VL_T vl);
vint16m1_t vundefined_i16m1_vl (_VL_T vl);
vint16m2_t vundefined_i16m2_vl (_VL_T vl);
vint16m4_t vundefined_i16m4_vl (_VL_T vl);
vint16m8_t vundefined_i16m8_vl (_VL_T vl);
vint32m1_t vundefined_i32m1_vl (_VL_T vl);
vint32m2_t vundefined_i32m2_vl (_VL_T vl);
vint32m4_t vundefined_i32m4_vl (_VL_T vl);
vint32m8_t vundefined_i32m8_vl (_VL_T vl);
vint64m1_t vundefined_i64m1_vl (_VL_T vl);
vint64m2_t vundefined_i64m2_vl (_VL_T vl);
vint64m4_t vundefined_i64m4_vl (_VL_T vl);
vint64m8_t vundefined_i64m8_vl (_VL_T vl);
vuint8m1_t vundefined_u8m1_vl (_VL_T vl);
vuint8m2_t vundefined_u8m2_vl (_VL_T vl);
vuint8m4_t vundefined_u8m4_vl (_VL_T vl);
vuint8m8_t vundefined_u8m8_vl (_VL_T vl);
vuint16m1_t vundefined_u16m1_vl (_VL_T vl);
vuint16m2_t vundefined_u16m2_vl (_VL_T vl);
vuint16m4_t vundefined_u16m4_vl (_VL_T vl);
vuint16m8_t vundefined_u16m8_vl (_VL_T vl);
vuint32m1_t vundefined_u32m1_vl (_VL_T vl);
vuint32m2_t vundefined_u32m2_vl (_VL_T vl);
vuint32m4_t vundefined_u32m4_vl (_VL_T vl);
vuint32m8_t vundefined_u32m8_vl (_VL_T vl);
vuint64m1_t vundefined_u64m1_vl (_VL_T vl);
vuint64m2_t vundefined_u64m2_vl (_VL_T vl);
vuint64m4_t vundefined_u64m4_vl (_VL_T vl);
vuint64m8_t vundefined_u64m8_vl (_VL_T vl);
vfloat16m1_t vundefined_f16m1_vl (_VL_T vl);
vfloat16m2_t vundefined_f16m2_vl (_VL_T vl);
vfloat16m4_t vundefined_f16m4_vl (_VL_T vl);
vfloat16m8_t vundefined_f16m8_vl (_VL_T vl);
vfloat32m1_t vundefined_f32m1_vl (_VL_T vl);
vfloat32m2_t vundefined_f32m2_vl (_VL_T vl);
vfloat32m4_t vundefined_f32m4_vl (_VL_T vl);
vfloat32m8_t vundefined_f32m8_vl (_VL_T vl);
vfloat64m1_t vundefined_f64m1_vl (_VL_T vl);
vfloat64m2_t vundefined_f64m2_vl (_VL_T vl);
vfloat64m4_t vundefined_f64m4_vl (_VL_T vl);
vfloat64m8_t vundefined_f64m8_vl (_VL_T vl);
```