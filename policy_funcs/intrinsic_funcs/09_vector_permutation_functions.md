<!--NOTE: This file is generated by rvv_intrinsic_gen.py-->

## Vector Permutation Functions:

### [Integer and Floating-Point Scalar Move Functions](../rvv-intrinsic-api.md#171-integer-scalar-move-operations):

**Prototypes:**
``` C
int8_t vmv_x_s_i8mf8_i8 (vint8mf8_t src);
vint8mf8_t vmv_s_x_i8mf8_tu (vint8mf8_t merge, int8_t src, size_t vl);
int8_t vmv_x_s_i8mf4_i8 (vint8mf4_t src);
vint8mf4_t vmv_s_x_i8mf4_tu (vint8mf4_t merge, int8_t src, size_t vl);
int8_t vmv_x_s_i8mf2_i8 (vint8mf2_t src);
vint8mf2_t vmv_s_x_i8mf2_tu (vint8mf2_t merge, int8_t src, size_t vl);
int8_t vmv_x_s_i8m1_i8 (vint8m1_t src);
vint8m1_t vmv_s_x_i8m1_tu (vint8m1_t merge, int8_t src, size_t vl);
int8_t vmv_x_s_i8m2_i8 (vint8m2_t src);
vint8m2_t vmv_s_x_i8m2_tu (vint8m2_t merge, int8_t src, size_t vl);
int8_t vmv_x_s_i8m4_i8 (vint8m4_t src);
vint8m4_t vmv_s_x_i8m4_tu (vint8m4_t merge, int8_t src, size_t vl);
int8_t vmv_x_s_i8m8_i8 (vint8m8_t src);
vint8m8_t vmv_s_x_i8m8_tu (vint8m8_t merge, int8_t src, size_t vl);
int16_t vmv_x_s_i16mf4_i16 (vint16mf4_t src);
vint16mf4_t vmv_s_x_i16mf4_tu (vint16mf4_t merge, int16_t src, size_t vl);
int16_t vmv_x_s_i16mf2_i16 (vint16mf2_t src);
vint16mf2_t vmv_s_x_i16mf2_tu (vint16mf2_t merge, int16_t src, size_t vl);
int16_t vmv_x_s_i16m1_i16 (vint16m1_t src);
vint16m1_t vmv_s_x_i16m1_tu (vint16m1_t merge, int16_t src, size_t vl);
int16_t vmv_x_s_i16m2_i16 (vint16m2_t src);
vint16m2_t vmv_s_x_i16m2_tu (vint16m2_t merge, int16_t src, size_t vl);
int16_t vmv_x_s_i16m4_i16 (vint16m4_t src);
vint16m4_t vmv_s_x_i16m4_tu (vint16m4_t merge, int16_t src, size_t vl);
int16_t vmv_x_s_i16m8_i16 (vint16m8_t src);
vint16m8_t vmv_s_x_i16m8_tu (vint16m8_t merge, int16_t src, size_t vl);
int32_t vmv_x_s_i32mf2_i32 (vint32mf2_t src);
vint32mf2_t vmv_s_x_i32mf2_tu (vint32mf2_t merge, int32_t src, size_t vl);
int32_t vmv_x_s_i32m1_i32 (vint32m1_t src);
vint32m1_t vmv_s_x_i32m1_tu (vint32m1_t merge, int32_t src, size_t vl);
int32_t vmv_x_s_i32m2_i32 (vint32m2_t src);
vint32m2_t vmv_s_x_i32m2_tu (vint32m2_t merge, int32_t src, size_t vl);
int32_t vmv_x_s_i32m4_i32 (vint32m4_t src);
vint32m4_t vmv_s_x_i32m4_tu (vint32m4_t merge, int32_t src, size_t vl);
int32_t vmv_x_s_i32m8_i32 (vint32m8_t src);
vint32m8_t vmv_s_x_i32m8_tu (vint32m8_t merge, int32_t src, size_t vl);
int64_t vmv_x_s_i64m1_i64 (vint64m1_t src);
vint64m1_t vmv_s_x_i64m1_tu (vint64m1_t merge, int64_t src, size_t vl);
int64_t vmv_x_s_i64m2_i64 (vint64m2_t src);
vint64m2_t vmv_s_x_i64m2_tu (vint64m2_t merge, int64_t src, size_t vl);
int64_t vmv_x_s_i64m4_i64 (vint64m4_t src);
vint64m4_t vmv_s_x_i64m4_tu (vint64m4_t merge, int64_t src, size_t vl);
int64_t vmv_x_s_i64m8_i64 (vint64m8_t src);
vint64m8_t vmv_s_x_i64m8_tu (vint64m8_t merge, int64_t src, size_t vl);
uint8_t vmv_x_s_u8mf8_u8 (vuint8mf8_t src);
vuint8mf8_t vmv_s_x_u8mf8_tu (vuint8mf8_t merge, uint8_t src, size_t vl);
uint8_t vmv_x_s_u8mf4_u8 (vuint8mf4_t src);
vuint8mf4_t vmv_s_x_u8mf4_tu (vuint8mf4_t merge, uint8_t src, size_t vl);
uint8_t vmv_x_s_u8mf2_u8 (vuint8mf2_t src);
vuint8mf2_t vmv_s_x_u8mf2_tu (vuint8mf2_t merge, uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m1_u8 (vuint8m1_t src);
vuint8m1_t vmv_s_x_u8m1_tu (vuint8m1_t merge, uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m2_u8 (vuint8m2_t src);
vuint8m2_t vmv_s_x_u8m2_tu (vuint8m2_t merge, uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m4_u8 (vuint8m4_t src);
vuint8m4_t vmv_s_x_u8m4_tu (vuint8m4_t merge, uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m8_u8 (vuint8m8_t src);
vuint8m8_t vmv_s_x_u8m8_tu (vuint8m8_t merge, uint8_t src, size_t vl);
uint16_t vmv_x_s_u16mf4_u16 (vuint16mf4_t src);
vuint16mf4_t vmv_s_x_u16mf4_tu (vuint16mf4_t merge, uint16_t src, size_t vl);
uint16_t vmv_x_s_u16mf2_u16 (vuint16mf2_t src);
vuint16mf2_t vmv_s_x_u16mf2_tu (vuint16mf2_t merge, uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m1_u16 (vuint16m1_t src);
vuint16m1_t vmv_s_x_u16m1_tu (vuint16m1_t merge, uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m2_u16 (vuint16m2_t src);
vuint16m2_t vmv_s_x_u16m2_tu (vuint16m2_t merge, uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m4_u16 (vuint16m4_t src);
vuint16m4_t vmv_s_x_u16m4_tu (vuint16m4_t merge, uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m8_u16 (vuint16m8_t src);
vuint16m8_t vmv_s_x_u16m8_tu (vuint16m8_t merge, uint16_t src, size_t vl);
uint32_t vmv_x_s_u32mf2_u32 (vuint32mf2_t src);
vuint32mf2_t vmv_s_x_u32mf2_tu (vuint32mf2_t merge, uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m1_u32 (vuint32m1_t src);
vuint32m1_t vmv_s_x_u32m1_tu (vuint32m1_t merge, uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m2_u32 (vuint32m2_t src);
vuint32m2_t vmv_s_x_u32m2_tu (vuint32m2_t merge, uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m4_u32 (vuint32m4_t src);
vuint32m4_t vmv_s_x_u32m4_tu (vuint32m4_t merge, uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m8_u32 (vuint32m8_t src);
vuint32m8_t vmv_s_x_u32m8_tu (vuint32m8_t merge, uint32_t src, size_t vl);
uint64_t vmv_x_s_u64m1_u64 (vuint64m1_t src);
vuint64m1_t vmv_s_x_u64m1_tu (vuint64m1_t merge, uint64_t src, size_t vl);
uint64_t vmv_x_s_u64m2_u64 (vuint64m2_t src);
vuint64m2_t vmv_s_x_u64m2_tu (vuint64m2_t merge, uint64_t src, size_t vl);
uint64_t vmv_x_s_u64m4_u64 (vuint64m4_t src);
vuint64m4_t vmv_s_x_u64m4_tu (vuint64m4_t merge, uint64_t src, size_t vl);
uint64_t vmv_x_s_u64m8_u64 (vuint64m8_t src);
vuint64m8_t vmv_s_x_u64m8_tu (vuint64m8_t merge, uint64_t src, size_t vl);
float16_t vfmv_f_s_f16mf4_f16 (vfloat16mf4_t src);
vfloat16mf4_t vfmv_s_f_f16mf4_tu (vfloat16mf4_t merge, float16_t src, size_t vl);
float16_t vfmv_f_s_f16mf2_f16 (vfloat16mf2_t src);
vfloat16mf2_t vfmv_s_f_f16mf2_tu (vfloat16mf2_t merge, float16_t src, size_t vl);
float16_t vfmv_f_s_f16m1_f16 (vfloat16m1_t src);
vfloat16m1_t vfmv_s_f_f16m1_tu (vfloat16m1_t merge, float16_t src, size_t vl);
float16_t vfmv_f_s_f16m2_f16 (vfloat16m2_t src);
vfloat16m2_t vfmv_s_f_f16m2_tu (vfloat16m2_t merge, float16_t src, size_t vl);
float16_t vfmv_f_s_f16m4_f16 (vfloat16m4_t src);
vfloat16m4_t vfmv_s_f_f16m4_tu (vfloat16m4_t merge, float16_t src, size_t vl);
float16_t vfmv_f_s_f16m8_f16 (vfloat16m8_t src);
vfloat16m8_t vfmv_s_f_f16m8_tu (vfloat16m8_t merge, float16_t src, size_t vl);
float32_t vfmv_f_s_f32mf2_f32 (vfloat32mf2_t src);
vfloat32mf2_t vfmv_s_f_f32mf2_tu (vfloat32mf2_t merge, float32_t src, size_t vl);
float32_t vfmv_f_s_f32m1_f32 (vfloat32m1_t src);
vfloat32m1_t vfmv_s_f_f32m1_tu (vfloat32m1_t merge, float32_t src, size_t vl);
float32_t vfmv_f_s_f32m2_f32 (vfloat32m2_t src);
vfloat32m2_t vfmv_s_f_f32m2_tu (vfloat32m2_t merge, float32_t src, size_t vl);
float32_t vfmv_f_s_f32m4_f32 (vfloat32m4_t src);
vfloat32m4_t vfmv_s_f_f32m4_tu (vfloat32m4_t merge, float32_t src, size_t vl);
float32_t vfmv_f_s_f32m8_f32 (vfloat32m8_t src);
vfloat32m8_t vfmv_s_f_f32m8_tu (vfloat32m8_t merge, float32_t src, size_t vl);
float64_t vfmv_f_s_f64m1_f64 (vfloat64m1_t src);
vfloat64m1_t vfmv_s_f_f64m1_tu (vfloat64m1_t merge, float64_t src, size_t vl);
float64_t vfmv_f_s_f64m2_f64 (vfloat64m2_t src);
vfloat64m2_t vfmv_s_f_f64m2_tu (vfloat64m2_t merge, float64_t src, size_t vl);
float64_t vfmv_f_s_f64m4_f64 (vfloat64m4_t src);
vfloat64m4_t vfmv_s_f_f64m4_tu (vfloat64m4_t merge, float64_t src, size_t vl);
float64_t vfmv_f_s_f64m8_f64 (vfloat64m8_t src);
vfloat64m8_t vfmv_s_f_f64m8_tu (vfloat64m8_t merge, float64_t src, size_t vl);
int8_t vmv_x_s_i8mf8_i8 (vint8mf8_t src);
vint8mf8_t vmv_s_x_i8mf8_ta (int8_t src, size_t vl);
int8_t vmv_x_s_i8mf4_i8 (vint8mf4_t src);
vint8mf4_t vmv_s_x_i8mf4_ta (int8_t src, size_t vl);
int8_t vmv_x_s_i8mf2_i8 (vint8mf2_t src);
vint8mf2_t vmv_s_x_i8mf2_ta (int8_t src, size_t vl);
int8_t vmv_x_s_i8m1_i8 (vint8m1_t src);
vint8m1_t vmv_s_x_i8m1_ta (int8_t src, size_t vl);
int8_t vmv_x_s_i8m2_i8 (vint8m2_t src);
vint8m2_t vmv_s_x_i8m2_ta (int8_t src, size_t vl);
int8_t vmv_x_s_i8m4_i8 (vint8m4_t src);
vint8m4_t vmv_s_x_i8m4_ta (int8_t src, size_t vl);
int8_t vmv_x_s_i8m8_i8 (vint8m8_t src);
vint8m8_t vmv_s_x_i8m8_ta (int8_t src, size_t vl);
int16_t vmv_x_s_i16mf4_i16 (vint16mf4_t src);
vint16mf4_t vmv_s_x_i16mf4_ta (int16_t src, size_t vl);
int16_t vmv_x_s_i16mf2_i16 (vint16mf2_t src);
vint16mf2_t vmv_s_x_i16mf2_ta (int16_t src, size_t vl);
int16_t vmv_x_s_i16m1_i16 (vint16m1_t src);
vint16m1_t vmv_s_x_i16m1_ta (int16_t src, size_t vl);
int16_t vmv_x_s_i16m2_i16 (vint16m2_t src);
vint16m2_t vmv_s_x_i16m2_ta (int16_t src, size_t vl);
int16_t vmv_x_s_i16m4_i16 (vint16m4_t src);
vint16m4_t vmv_s_x_i16m4_ta (int16_t src, size_t vl);
int16_t vmv_x_s_i16m8_i16 (vint16m8_t src);
vint16m8_t vmv_s_x_i16m8_ta (int16_t src, size_t vl);
int32_t vmv_x_s_i32mf2_i32 (vint32mf2_t src);
vint32mf2_t vmv_s_x_i32mf2_ta (int32_t src, size_t vl);
int32_t vmv_x_s_i32m1_i32 (vint32m1_t src);
vint32m1_t vmv_s_x_i32m1_ta (int32_t src, size_t vl);
int32_t vmv_x_s_i32m2_i32 (vint32m2_t src);
vint32m2_t vmv_s_x_i32m2_ta (int32_t src, size_t vl);
int32_t vmv_x_s_i32m4_i32 (vint32m4_t src);
vint32m4_t vmv_s_x_i32m4_ta (int32_t src, size_t vl);
int32_t vmv_x_s_i32m8_i32 (vint32m8_t src);
vint32m8_t vmv_s_x_i32m8_ta (int32_t src, size_t vl);
int64_t vmv_x_s_i64m1_i64 (vint64m1_t src);
vint64m1_t vmv_s_x_i64m1_ta (int64_t src, size_t vl);
int64_t vmv_x_s_i64m2_i64 (vint64m2_t src);
vint64m2_t vmv_s_x_i64m2_ta (int64_t src, size_t vl);
int64_t vmv_x_s_i64m4_i64 (vint64m4_t src);
vint64m4_t vmv_s_x_i64m4_ta (int64_t src, size_t vl);
int64_t vmv_x_s_i64m8_i64 (vint64m8_t src);
vint64m8_t vmv_s_x_i64m8_ta (int64_t src, size_t vl);
uint8_t vmv_x_s_u8mf8_u8 (vuint8mf8_t src);
vuint8mf8_t vmv_s_x_u8mf8_ta (uint8_t src, size_t vl);
uint8_t vmv_x_s_u8mf4_u8 (vuint8mf4_t src);
vuint8mf4_t vmv_s_x_u8mf4_ta (uint8_t src, size_t vl);
uint8_t vmv_x_s_u8mf2_u8 (vuint8mf2_t src);
vuint8mf2_t vmv_s_x_u8mf2_ta (uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m1_u8 (vuint8m1_t src);
vuint8m1_t vmv_s_x_u8m1_ta (uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m2_u8 (vuint8m2_t src);
vuint8m2_t vmv_s_x_u8m2_ta (uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m4_u8 (vuint8m4_t src);
vuint8m4_t vmv_s_x_u8m4_ta (uint8_t src, size_t vl);
uint8_t vmv_x_s_u8m8_u8 (vuint8m8_t src);
vuint8m8_t vmv_s_x_u8m8_ta (uint8_t src, size_t vl);
uint16_t vmv_x_s_u16mf4_u16 (vuint16mf4_t src);
vuint16mf4_t vmv_s_x_u16mf4_ta (uint16_t src, size_t vl);
uint16_t vmv_x_s_u16mf2_u16 (vuint16mf2_t src);
vuint16mf2_t vmv_s_x_u16mf2_ta (uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m1_u16 (vuint16m1_t src);
vuint16m1_t vmv_s_x_u16m1_ta (uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m2_u16 (vuint16m2_t src);
vuint16m2_t vmv_s_x_u16m2_ta (uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m4_u16 (vuint16m4_t src);
vuint16m4_t vmv_s_x_u16m4_ta (uint16_t src, size_t vl);
uint16_t vmv_x_s_u16m8_u16 (vuint16m8_t src);
vuint16m8_t vmv_s_x_u16m8_ta (uint16_t src, size_t vl);
uint32_t vmv_x_s_u32mf2_u32 (vuint32mf2_t src);
vuint32mf2_t vmv_s_x_u32mf2_ta (uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m1_u32 (vuint32m1_t src);
vuint32m1_t vmv_s_x_u32m1_ta (uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m2_u32 (vuint32m2_t src);
vuint32m2_t vmv_s_x_u32m2_ta (uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m4_u32 (vuint32m4_t src);
vuint32m4_t vmv_s_x_u32m4_ta (uint32_t src, size_t vl);
uint32_t vmv_x_s_u32m8_u32 (vuint32m8_t src);
vuint32m8_t vmv_s_x_u32m8_ta (uint32_t src, size_t vl);
uint64_t vmv_x_s_u64m1_u64 (vuint64m1_t src);
vuint64m1_t vmv_s_x_u64m1_ta (uint64_t src, size_t vl);
uint64_t vmv_x_s_u64m2_u64 (vuint64m2_t src);
vuint64m2_t vmv_s_x_u64m2_ta (uint64_t src, size_t vl);
uint64_t vmv_x_s_u64m4_u64 (vuint64m4_t src);
vuint64m4_t vmv_s_x_u64m4_ta (uint64_t src, size_t vl);
uint64_t vmv_x_s_u64m8_u64 (vuint64m8_t src);
vuint64m8_t vmv_s_x_u64m8_ta (uint64_t src, size_t vl);
float16_t vfmv_f_s_f16mf4_f16 (vfloat16mf4_t src);
vfloat16mf4_t vfmv_s_f_f16mf4_ta (float16_t src, size_t vl);
float16_t vfmv_f_s_f16mf2_f16 (vfloat16mf2_t src);
vfloat16mf2_t vfmv_s_f_f16mf2_ta (float16_t src, size_t vl);
float16_t vfmv_f_s_f16m1_f16 (vfloat16m1_t src);
vfloat16m1_t vfmv_s_f_f16m1_ta (float16_t src, size_t vl);
float16_t vfmv_f_s_f16m2_f16 (vfloat16m2_t src);
vfloat16m2_t vfmv_s_f_f16m2_ta (float16_t src, size_t vl);
float16_t vfmv_f_s_f16m4_f16 (vfloat16m4_t src);
vfloat16m4_t vfmv_s_f_f16m4_ta (float16_t src, size_t vl);
float16_t vfmv_f_s_f16m8_f16 (vfloat16m8_t src);
vfloat16m8_t vfmv_s_f_f16m8_ta (float16_t src, size_t vl);
float32_t vfmv_f_s_f32mf2_f32 (vfloat32mf2_t src);
vfloat32mf2_t vfmv_s_f_f32mf2_ta (float32_t src, size_t vl);
float32_t vfmv_f_s_f32m1_f32 (vfloat32m1_t src);
vfloat32m1_t vfmv_s_f_f32m1_ta (float32_t src, size_t vl);
float32_t vfmv_f_s_f32m2_f32 (vfloat32m2_t src);
vfloat32m2_t vfmv_s_f_f32m2_ta (float32_t src, size_t vl);
float32_t vfmv_f_s_f32m4_f32 (vfloat32m4_t src);
vfloat32m4_t vfmv_s_f_f32m4_ta (float32_t src, size_t vl);
float32_t vfmv_f_s_f32m8_f32 (vfloat32m8_t src);
vfloat32m8_t vfmv_s_f_f32m8_ta (float32_t src, size_t vl);
float64_t vfmv_f_s_f64m1_f64 (vfloat64m1_t src);
vfloat64m1_t vfmv_s_f_f64m1_ta (float64_t src, size_t vl);
float64_t vfmv_f_s_f64m2_f64 (vfloat64m2_t src);
vfloat64m2_t vfmv_s_f_f64m2_ta (float64_t src, size_t vl);
float64_t vfmv_f_s_f64m4_f64 (vfloat64m4_t src);
vfloat64m4_t vfmv_s_f_f64m4_ta (float64_t src, size_t vl);
float64_t vfmv_f_s_f64m8_f64 (vfloat64m8_t src);
vfloat64m8_t vfmv_s_f_f64m8_ta (float64_t src, size_t vl);
```
### [Vector Slideup and Slidedown Functions](../rvv-intrinsic-api.md#173-vector-slide-operations):

**Prototypes:**
``` C
vint8mf8_t vslideup_vx_i8mf8_tu (vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslideup_vx_i8mf4_tu (vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslideup_vx_i8mf2_tu (vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslideup_vx_i8m1_tu (vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslideup_vx_i8m2_tu (vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslideup_vx_i8m4_tu (vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslideup_vx_i8m8_tu (vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslideup_vx_i16mf4_tu (vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslideup_vx_i16mf2_tu (vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslideup_vx_i16m1_tu (vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslideup_vx_i16m2_tu (vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslideup_vx_i16m4_tu (vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslideup_vx_i16m8_tu (vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslideup_vx_i32mf2_tu (vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslideup_vx_i32m1_tu (vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslideup_vx_i32m2_tu (vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslideup_vx_i32m4_tu (vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslideup_vx_i32m8_tu (vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslideup_vx_i64m1_tu (vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslideup_vx_i64m2_tu (vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslideup_vx_i64m4_tu (vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslideup_vx_i64m8_tu (vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslideup_vx_u8mf8_tu (vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslideup_vx_u8mf4_tu (vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslideup_vx_u8mf2_tu (vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslideup_vx_u8m1_tu (vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslideup_vx_u8m2_tu (vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslideup_vx_u8m4_tu (vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslideup_vx_u8m8_tu (vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslideup_vx_u16mf4_tu (vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslideup_vx_u16mf2_tu (vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslideup_vx_u16m1_tu (vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslideup_vx_u16m2_tu (vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslideup_vx_u16m4_tu (vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslideup_vx_u16m8_tu (vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslideup_vx_u32mf2_tu (vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslideup_vx_u32m1_tu (vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslideup_vx_u32m2_tu (vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslideup_vx_u32m4_tu (vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslideup_vx_u32m8_tu (vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslideup_vx_u64m1_tu (vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslideup_vx_u64m2_tu (vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslideup_vx_u64m4_tu (vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslideup_vx_u64m8_tu (vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslideup_vx_f16mf4_tu (vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslideup_vx_f16mf2_tu (vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslideup_vx_f16m1_tu (vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslideup_vx_f16m2_tu (vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslideup_vx_f16m4_tu (vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslideup_vx_f16m8_tu (vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslideup_vx_f32mf2_tu (vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslideup_vx_f32m1_tu (vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslideup_vx_f32m2_tu (vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslideup_vx_f32m4_tu (vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslideup_vx_f32m8_tu (vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslideup_vx_f64m1_tu (vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslideup_vx_f64m2_tu (vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslideup_vx_f64m4_tu (vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslideup_vx_f64m8_tu (vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t vslidedown_vx_i8mf8_tu (vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslidedown_vx_i8mf4_tu (vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslidedown_vx_i8mf2_tu (vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslidedown_vx_i8m1_tu (vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslidedown_vx_i8m2_tu (vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslidedown_vx_i8m4_tu (vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslidedown_vx_i8m8_tu (vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslidedown_vx_i16mf4_tu (vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslidedown_vx_i16mf2_tu (vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslidedown_vx_i16m1_tu (vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslidedown_vx_i16m2_tu (vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslidedown_vx_i16m4_tu (vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslidedown_vx_i16m8_tu (vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslidedown_vx_i32mf2_tu (vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslidedown_vx_i32m1_tu (vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslidedown_vx_i32m2_tu (vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslidedown_vx_i32m4_tu (vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslidedown_vx_i32m8_tu (vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslidedown_vx_i64m1_tu (vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslidedown_vx_i64m2_tu (vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslidedown_vx_i64m4_tu (vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslidedown_vx_i64m8_tu (vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslidedown_vx_u8mf8_tu (vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslidedown_vx_u8mf4_tu (vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslidedown_vx_u8mf2_tu (vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslidedown_vx_u8m1_tu (vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslidedown_vx_u8m2_tu (vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslidedown_vx_u8m4_tu (vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslidedown_vx_u8m8_tu (vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslidedown_vx_u16mf4_tu (vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslidedown_vx_u16mf2_tu (vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslidedown_vx_u16m1_tu (vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslidedown_vx_u16m2_tu (vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslidedown_vx_u16m4_tu (vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslidedown_vx_u16m8_tu (vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslidedown_vx_u32mf2_tu (vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslidedown_vx_u32m1_tu (vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslidedown_vx_u32m2_tu (vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslidedown_vx_u32m4_tu (vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslidedown_vx_u32m8_tu (vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslidedown_vx_u64m1_tu (vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslidedown_vx_u64m2_tu (vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslidedown_vx_u64m4_tu (vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslidedown_vx_u64m8_tu (vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslidedown_vx_f16mf4_tu (vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslidedown_vx_f16mf2_tu (vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslidedown_vx_f16m1_tu (vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslidedown_vx_f16m2_tu (vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslidedown_vx_f16m4_tu (vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslidedown_vx_f16m8_tu (vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslidedown_vx_f32mf2_tu (vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslidedown_vx_f32m1_tu (vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslidedown_vx_f32m2_tu (vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslidedown_vx_f32m4_tu (vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslidedown_vx_f32m8_tu (vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslidedown_vx_f64m1_tu (vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslidedown_vx_f64m2_tu (vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslidedown_vx_f64m4_tu (vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslidedown_vx_f64m8_tu (vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t vslideup_vx_i8mf8_ta (vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslideup_vx_i8mf4_ta (vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslideup_vx_i8mf2_ta (vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslideup_vx_i8m1_ta (vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslideup_vx_i8m2_ta (vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslideup_vx_i8m4_ta (vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslideup_vx_i8m8_ta (vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslideup_vx_i16mf4_ta (vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslideup_vx_i16mf2_ta (vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslideup_vx_i16m1_ta (vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslideup_vx_i16m2_ta (vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslideup_vx_i16m4_ta (vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslideup_vx_i16m8_ta (vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslideup_vx_i32mf2_ta (vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslideup_vx_i32m1_ta (vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslideup_vx_i32m2_ta (vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslideup_vx_i32m4_ta (vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslideup_vx_i32m8_ta (vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslideup_vx_i64m1_ta (vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslideup_vx_i64m2_ta (vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslideup_vx_i64m4_ta (vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslideup_vx_i64m8_ta (vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslideup_vx_u8mf8_ta (vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslideup_vx_u8mf4_ta (vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslideup_vx_u8mf2_ta (vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslideup_vx_u8m1_ta (vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslideup_vx_u8m2_ta (vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslideup_vx_u8m4_ta (vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslideup_vx_u8m8_ta (vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslideup_vx_u16mf4_ta (vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslideup_vx_u16mf2_ta (vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslideup_vx_u16m1_ta (vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslideup_vx_u16m2_ta (vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslideup_vx_u16m4_ta (vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslideup_vx_u16m8_ta (vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslideup_vx_u32mf2_ta (vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslideup_vx_u32m1_ta (vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslideup_vx_u32m2_ta (vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslideup_vx_u32m4_ta (vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslideup_vx_u32m8_ta (vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslideup_vx_u64m1_ta (vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslideup_vx_u64m2_ta (vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslideup_vx_u64m4_ta (vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslideup_vx_u64m8_ta (vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslideup_vx_f16mf4_ta (vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslideup_vx_f16mf2_ta (vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslideup_vx_f16m1_ta (vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslideup_vx_f16m2_ta (vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslideup_vx_f16m4_ta (vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslideup_vx_f16m8_ta (vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslideup_vx_f32mf2_ta (vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslideup_vx_f32m1_ta (vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslideup_vx_f32m2_ta (vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslideup_vx_f32m4_ta (vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslideup_vx_f32m8_ta (vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslideup_vx_f64m1_ta (vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslideup_vx_f64m2_ta (vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslideup_vx_f64m4_ta (vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslideup_vx_f64m8_ta (vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t vslidedown_vx_i8mf8_ta (vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslidedown_vx_i8mf4_ta (vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslidedown_vx_i8mf2_ta (vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslidedown_vx_i8m1_ta (vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslidedown_vx_i8m2_ta (vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslidedown_vx_i8m4_ta (vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslidedown_vx_i8m8_ta (vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslidedown_vx_i16mf4_ta (vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslidedown_vx_i16mf2_ta (vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslidedown_vx_i16m1_ta (vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslidedown_vx_i16m2_ta (vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslidedown_vx_i16m4_ta (vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslidedown_vx_i16m8_ta (vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslidedown_vx_i32mf2_ta (vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslidedown_vx_i32m1_ta (vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslidedown_vx_i32m2_ta (vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslidedown_vx_i32m4_ta (vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslidedown_vx_i32m8_ta (vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslidedown_vx_i64m1_ta (vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslidedown_vx_i64m2_ta (vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslidedown_vx_i64m4_ta (vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslidedown_vx_i64m8_ta (vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslidedown_vx_u8mf8_ta (vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslidedown_vx_u8mf4_ta (vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslidedown_vx_u8mf2_ta (vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslidedown_vx_u8m1_ta (vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslidedown_vx_u8m2_ta (vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslidedown_vx_u8m4_ta (vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslidedown_vx_u8m8_ta (vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslidedown_vx_u16mf4_ta (vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslidedown_vx_u16mf2_ta (vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslidedown_vx_u16m1_ta (vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslidedown_vx_u16m2_ta (vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslidedown_vx_u16m4_ta (vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslidedown_vx_u16m8_ta (vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslidedown_vx_u32mf2_ta (vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslidedown_vx_u32m1_ta (vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslidedown_vx_u32m2_ta (vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslidedown_vx_u32m4_ta (vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslidedown_vx_u32m8_ta (vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslidedown_vx_u64m1_ta (vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslidedown_vx_u64m2_ta (vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslidedown_vx_u64m4_ta (vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslidedown_vx_u64m8_ta (vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslidedown_vx_f16mf4_ta (vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslidedown_vx_f16mf2_ta (vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslidedown_vx_f16m1_ta (vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslidedown_vx_f16m2_ta (vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslidedown_vx_f16m4_ta (vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslidedown_vx_f16m8_ta (vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslidedown_vx_f32mf2_ta (vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslidedown_vx_f32m1_ta (vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslidedown_vx_f32m2_ta (vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslidedown_vx_f32m4_ta (vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslidedown_vx_f32m8_ta (vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslidedown_vx_f64m1_ta (vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslidedown_vx_f64m2_ta (vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslidedown_vx_f64m4_ta (vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslidedown_vx_f64m8_ta (vfloat64m8_t src, size_t offset, size_t vl);
// masked functions
vint8mf8_t vslideup_vx_i8mf8_tuma (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslideup_vx_i8mf4_tuma (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslideup_vx_i8mf2_tuma (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslideup_vx_i8m1_tuma (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslideup_vx_i8m2_tuma (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslideup_vx_i8m4_tuma (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslideup_vx_i8m8_tuma (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslideup_vx_i16mf4_tuma (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslideup_vx_i16mf2_tuma (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslideup_vx_i16m1_tuma (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslideup_vx_i16m2_tuma (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslideup_vx_i16m4_tuma (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslideup_vx_i16m8_tuma (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslideup_vx_i32mf2_tuma (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslideup_vx_i32m1_tuma (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslideup_vx_i32m2_tuma (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslideup_vx_i32m4_tuma (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslideup_vx_i32m8_tuma (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslideup_vx_i64m1_tuma (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslideup_vx_i64m2_tuma (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslideup_vx_i64m4_tuma (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslideup_vx_i64m8_tuma (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslideup_vx_u8mf8_tuma (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslideup_vx_u8mf4_tuma (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslideup_vx_u8mf2_tuma (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslideup_vx_u8m1_tuma (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslideup_vx_u8m2_tuma (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslideup_vx_u8m4_tuma (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslideup_vx_u8m8_tuma (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslideup_vx_u16mf4_tuma (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslideup_vx_u16mf2_tuma (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslideup_vx_u16m1_tuma (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslideup_vx_u16m2_tuma (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslideup_vx_u16m4_tuma (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslideup_vx_u16m8_tuma (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslideup_vx_u32mf2_tuma (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslideup_vx_u32m1_tuma (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslideup_vx_u32m2_tuma (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslideup_vx_u32m4_tuma (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslideup_vx_u32m8_tuma (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslideup_vx_u64m1_tuma (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslideup_vx_u64m2_tuma (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslideup_vx_u64m4_tuma (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslideup_vx_u64m8_tuma (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslideup_vx_f16mf4_tuma (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslideup_vx_f16mf2_tuma (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslideup_vx_f16m1_tuma (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslideup_vx_f16m2_tuma (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslideup_vx_f16m4_tuma (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslideup_vx_f16m8_tuma (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslideup_vx_f32mf2_tuma (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslideup_vx_f32m1_tuma (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslideup_vx_f32m2_tuma (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslideup_vx_f32m4_tuma (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslideup_vx_f32m8_tuma (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslideup_vx_f64m1_tuma (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslideup_vx_f64m2_tuma (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslideup_vx_f64m4_tuma (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslideup_vx_f64m8_tuma (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t vslidedown_vx_i8mf8_tuma (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslidedown_vx_i8mf4_tuma (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslidedown_vx_i8mf2_tuma (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslidedown_vx_i8m1_tuma (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslidedown_vx_i8m2_tuma (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslidedown_vx_i8m4_tuma (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslidedown_vx_i8m8_tuma (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslidedown_vx_i16mf4_tuma (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslidedown_vx_i16mf2_tuma (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslidedown_vx_i16m1_tuma (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslidedown_vx_i16m2_tuma (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslidedown_vx_i16m4_tuma (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslidedown_vx_i16m8_tuma (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslidedown_vx_i32mf2_tuma (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslidedown_vx_i32m1_tuma (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslidedown_vx_i32m2_tuma (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslidedown_vx_i32m4_tuma (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslidedown_vx_i32m8_tuma (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslidedown_vx_i64m1_tuma (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslidedown_vx_i64m2_tuma (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslidedown_vx_i64m4_tuma (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslidedown_vx_i64m8_tuma (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslidedown_vx_u8mf8_tuma (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslidedown_vx_u8mf4_tuma (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslidedown_vx_u8mf2_tuma (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslidedown_vx_u8m1_tuma (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslidedown_vx_u8m2_tuma (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslidedown_vx_u8m4_tuma (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslidedown_vx_u8m8_tuma (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslidedown_vx_u16mf4_tuma (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslidedown_vx_u16mf2_tuma (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslidedown_vx_u16m1_tuma (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslidedown_vx_u16m2_tuma (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslidedown_vx_u16m4_tuma (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslidedown_vx_u16m8_tuma (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslidedown_vx_u32mf2_tuma (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslidedown_vx_u32m1_tuma (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslidedown_vx_u32m2_tuma (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslidedown_vx_u32m4_tuma (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslidedown_vx_u32m8_tuma (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslidedown_vx_u64m1_tuma (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslidedown_vx_u64m2_tuma (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslidedown_vx_u64m4_tuma (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslidedown_vx_u64m8_tuma (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslidedown_vx_f16mf4_tuma (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslidedown_vx_f16mf2_tuma (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslidedown_vx_f16m1_tuma (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslidedown_vx_f16m2_tuma (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslidedown_vx_f16m4_tuma (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslidedown_vx_f16m8_tuma (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslidedown_vx_f32mf2_tuma (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslidedown_vx_f32m1_tuma (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslidedown_vx_f32m2_tuma (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslidedown_vx_f32m4_tuma (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslidedown_vx_f32m8_tuma (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslidedown_vx_f64m1_tuma (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslidedown_vx_f64m2_tuma (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslidedown_vx_f64m4_tuma (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslidedown_vx_f64m8_tuma (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
// masked functions
vint8mf8_t vslideup_vx_i8mf8_tumu (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslideup_vx_i8mf4_tumu (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslideup_vx_i8mf2_tumu (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslideup_vx_i8m1_tumu (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslideup_vx_i8m2_tumu (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslideup_vx_i8m4_tumu (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslideup_vx_i8m8_tumu (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslideup_vx_i16mf4_tumu (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslideup_vx_i16mf2_tumu (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslideup_vx_i16m1_tumu (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslideup_vx_i16m2_tumu (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslideup_vx_i16m4_tumu (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslideup_vx_i16m8_tumu (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslideup_vx_i32mf2_tumu (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslideup_vx_i32m1_tumu (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslideup_vx_i32m2_tumu (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslideup_vx_i32m4_tumu (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslideup_vx_i32m8_tumu (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslideup_vx_i64m1_tumu (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslideup_vx_i64m2_tumu (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslideup_vx_i64m4_tumu (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslideup_vx_i64m8_tumu (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslideup_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslideup_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslideup_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslideup_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslideup_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslideup_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslideup_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslideup_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslideup_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslideup_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslideup_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslideup_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslideup_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslideup_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslideup_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslideup_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslideup_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslideup_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslideup_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslideup_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslideup_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslideup_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslideup_vx_f16mf4_tumu (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslideup_vx_f16mf2_tumu (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslideup_vx_f16m1_tumu (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslideup_vx_f16m2_tumu (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslideup_vx_f16m4_tumu (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslideup_vx_f16m8_tumu (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslideup_vx_f32mf2_tumu (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslideup_vx_f32m1_tumu (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslideup_vx_f32m2_tumu (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslideup_vx_f32m4_tumu (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslideup_vx_f32m8_tumu (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslideup_vx_f64m1_tumu (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslideup_vx_f64m2_tumu (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslideup_vx_f64m4_tumu (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslideup_vx_f64m8_tumu (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t vslidedown_vx_i8mf8_tumu (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslidedown_vx_i8mf4_tumu (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslidedown_vx_i8mf2_tumu (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslidedown_vx_i8m1_tumu (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslidedown_vx_i8m2_tumu (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslidedown_vx_i8m4_tumu (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslidedown_vx_i8m8_tumu (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslidedown_vx_i16mf4_tumu (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslidedown_vx_i16mf2_tumu (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslidedown_vx_i16m1_tumu (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslidedown_vx_i16m2_tumu (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslidedown_vx_i16m4_tumu (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslidedown_vx_i16m8_tumu (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslidedown_vx_i32mf2_tumu (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslidedown_vx_i32m1_tumu (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslidedown_vx_i32m2_tumu (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslidedown_vx_i32m4_tumu (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslidedown_vx_i32m8_tumu (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslidedown_vx_i64m1_tumu (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslidedown_vx_i64m2_tumu (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslidedown_vx_i64m4_tumu (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslidedown_vx_i64m8_tumu (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslidedown_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslidedown_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslidedown_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslidedown_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslidedown_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslidedown_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslidedown_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslidedown_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslidedown_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslidedown_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslidedown_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslidedown_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslidedown_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslidedown_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslidedown_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslidedown_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslidedown_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslidedown_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslidedown_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslidedown_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslidedown_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslidedown_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslidedown_vx_f16mf4_tumu (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslidedown_vx_f16mf2_tumu (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslidedown_vx_f16m1_tumu (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslidedown_vx_f16m2_tumu (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslidedown_vx_f16m4_tumu (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslidedown_vx_f16m8_tumu (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslidedown_vx_f32mf2_tumu (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslidedown_vx_f32m1_tumu (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslidedown_vx_f32m2_tumu (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslidedown_vx_f32m4_tumu (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslidedown_vx_f32m8_tumu (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslidedown_vx_f64m1_tumu (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslidedown_vx_f64m2_tumu (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslidedown_vx_f64m4_tumu (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslidedown_vx_f64m8_tumu (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
// masked functions
vint8mf8_t vslideup_vx_i8mf8_tama (vbool64_t mask, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslideup_vx_i8mf4_tama (vbool32_t mask, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslideup_vx_i8mf2_tama (vbool16_t mask, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslideup_vx_i8m1_tama (vbool8_t mask, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslideup_vx_i8m2_tama (vbool4_t mask, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslideup_vx_i8m4_tama (vbool2_t mask, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslideup_vx_i8m8_tama (vbool1_t mask, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslideup_vx_i16mf4_tama (vbool64_t mask, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslideup_vx_i16mf2_tama (vbool32_t mask, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslideup_vx_i16m1_tama (vbool16_t mask, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslideup_vx_i16m2_tama (vbool8_t mask, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslideup_vx_i16m4_tama (vbool4_t mask, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslideup_vx_i16m8_tama (vbool2_t mask, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslideup_vx_i32mf2_tama (vbool64_t mask, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslideup_vx_i32m1_tama (vbool32_t mask, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslideup_vx_i32m2_tama (vbool16_t mask, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslideup_vx_i32m4_tama (vbool8_t mask, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslideup_vx_i32m8_tama (vbool4_t mask, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslideup_vx_i64m1_tama (vbool64_t mask, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslideup_vx_i64m2_tama (vbool32_t mask, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslideup_vx_i64m4_tama (vbool16_t mask, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslideup_vx_i64m8_tama (vbool8_t mask, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslideup_vx_u8mf8_tama (vbool64_t mask, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslideup_vx_u8mf4_tama (vbool32_t mask, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslideup_vx_u8mf2_tama (vbool16_t mask, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslideup_vx_u8m1_tama (vbool8_t mask, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslideup_vx_u8m2_tama (vbool4_t mask, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslideup_vx_u8m4_tama (vbool2_t mask, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslideup_vx_u8m8_tama (vbool1_t mask, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslideup_vx_u16mf4_tama (vbool64_t mask, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslideup_vx_u16mf2_tama (vbool32_t mask, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslideup_vx_u16m1_tama (vbool16_t mask, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslideup_vx_u16m2_tama (vbool8_t mask, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslideup_vx_u16m4_tama (vbool4_t mask, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslideup_vx_u16m8_tama (vbool2_t mask, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslideup_vx_u32mf2_tama (vbool64_t mask, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslideup_vx_u32m1_tama (vbool32_t mask, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslideup_vx_u32m2_tama (vbool16_t mask, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslideup_vx_u32m4_tama (vbool8_t mask, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslideup_vx_u32m8_tama (vbool4_t mask, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslideup_vx_u64m1_tama (vbool64_t mask, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslideup_vx_u64m2_tama (vbool32_t mask, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslideup_vx_u64m4_tama (vbool16_t mask, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslideup_vx_u64m8_tama (vbool8_t mask, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslideup_vx_f16mf4_tama (vbool64_t mask, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslideup_vx_f16mf2_tama (vbool32_t mask, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslideup_vx_f16m1_tama (vbool16_t mask, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslideup_vx_f16m2_tama (vbool8_t mask, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslideup_vx_f16m4_tama (vbool4_t mask, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslideup_vx_f16m8_tama (vbool2_t mask, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslideup_vx_f32mf2_tama (vbool64_t mask, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslideup_vx_f32m1_tama (vbool32_t mask, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslideup_vx_f32m2_tama (vbool16_t mask, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslideup_vx_f32m4_tama (vbool8_t mask, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslideup_vx_f32m8_tama (vbool4_t mask, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslideup_vx_f64m1_tama (vbool64_t mask, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslideup_vx_f64m2_tama (vbool32_t mask, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslideup_vx_f64m4_tama (vbool16_t mask, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslideup_vx_f64m8_tama (vbool8_t mask, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t vslidedown_vx_i8mf8_tama (vbool64_t mask, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslidedown_vx_i8mf4_tama (vbool32_t mask, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslidedown_vx_i8mf2_tama (vbool16_t mask, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslidedown_vx_i8m1_tama (vbool8_t mask, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslidedown_vx_i8m2_tama (vbool4_t mask, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslidedown_vx_i8m4_tama (vbool2_t mask, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslidedown_vx_i8m8_tama (vbool1_t mask, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslidedown_vx_i16mf4_tama (vbool64_t mask, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslidedown_vx_i16mf2_tama (vbool32_t mask, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslidedown_vx_i16m1_tama (vbool16_t mask, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslidedown_vx_i16m2_tama (vbool8_t mask, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslidedown_vx_i16m4_tama (vbool4_t mask, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslidedown_vx_i16m8_tama (vbool2_t mask, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslidedown_vx_i32mf2_tama (vbool64_t mask, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslidedown_vx_i32m1_tama (vbool32_t mask, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslidedown_vx_i32m2_tama (vbool16_t mask, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslidedown_vx_i32m4_tama (vbool8_t mask, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslidedown_vx_i32m8_tama (vbool4_t mask, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslidedown_vx_i64m1_tama (vbool64_t mask, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslidedown_vx_i64m2_tama (vbool32_t mask, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslidedown_vx_i64m4_tama (vbool16_t mask, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslidedown_vx_i64m8_tama (vbool8_t mask, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslidedown_vx_u8mf8_tama (vbool64_t mask, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslidedown_vx_u8mf4_tama (vbool32_t mask, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslidedown_vx_u8mf2_tama (vbool16_t mask, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslidedown_vx_u8m1_tama (vbool8_t mask, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslidedown_vx_u8m2_tama (vbool4_t mask, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslidedown_vx_u8m4_tama (vbool2_t mask, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslidedown_vx_u8m8_tama (vbool1_t mask, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslidedown_vx_u16mf4_tama (vbool64_t mask, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslidedown_vx_u16mf2_tama (vbool32_t mask, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslidedown_vx_u16m1_tama (vbool16_t mask, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslidedown_vx_u16m2_tama (vbool8_t mask, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslidedown_vx_u16m4_tama (vbool4_t mask, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslidedown_vx_u16m8_tama (vbool2_t mask, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslidedown_vx_u32mf2_tama (vbool64_t mask, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslidedown_vx_u32m1_tama (vbool32_t mask, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslidedown_vx_u32m2_tama (vbool16_t mask, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslidedown_vx_u32m4_tama (vbool8_t mask, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslidedown_vx_u32m8_tama (vbool4_t mask, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslidedown_vx_u64m1_tama (vbool64_t mask, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslidedown_vx_u64m2_tama (vbool32_t mask, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslidedown_vx_u64m4_tama (vbool16_t mask, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslidedown_vx_u64m8_tama (vbool8_t mask, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslidedown_vx_f16mf4_tama (vbool64_t mask, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslidedown_vx_f16mf2_tama (vbool32_t mask, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslidedown_vx_f16m1_tama (vbool16_t mask, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslidedown_vx_f16m2_tama (vbool8_t mask, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslidedown_vx_f16m4_tama (vbool4_t mask, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslidedown_vx_f16m8_tama (vbool2_t mask, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslidedown_vx_f32mf2_tama (vbool64_t mask, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslidedown_vx_f32m1_tama (vbool32_t mask, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslidedown_vx_f32m2_tama (vbool16_t mask, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslidedown_vx_f32m4_tama (vbool8_t mask, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslidedown_vx_f32m8_tama (vbool4_t mask, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslidedown_vx_f64m1_tama (vbool64_t mask, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslidedown_vx_f64m2_tama (vbool32_t mask, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslidedown_vx_f64m4_tama (vbool16_t mask, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslidedown_vx_f64m8_tama (vbool8_t mask, vfloat64m8_t src, size_t offset, size_t vl);
// masked functions
vint8mf8_t vslideup_vx_i8mf8_tamu (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslideup_vx_i8mf4_tamu (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslideup_vx_i8mf2_tamu (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslideup_vx_i8m1_tamu (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslideup_vx_i8m2_tamu (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslideup_vx_i8m4_tamu (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslideup_vx_i8m8_tamu (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslideup_vx_i16mf4_tamu (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslideup_vx_i16mf2_tamu (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslideup_vx_i16m1_tamu (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslideup_vx_i16m2_tamu (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslideup_vx_i16m4_tamu (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslideup_vx_i16m8_tamu (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslideup_vx_i32mf2_tamu (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslideup_vx_i32m1_tamu (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslideup_vx_i32m2_tamu (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslideup_vx_i32m4_tamu (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslideup_vx_i32m8_tamu (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslideup_vx_i64m1_tamu (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslideup_vx_i64m2_tamu (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslideup_vx_i64m4_tamu (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslideup_vx_i64m8_tamu (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslideup_vx_u8mf8_tamu (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslideup_vx_u8mf4_tamu (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslideup_vx_u8mf2_tamu (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslideup_vx_u8m1_tamu (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslideup_vx_u8m2_tamu (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslideup_vx_u8m4_tamu (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslideup_vx_u8m8_tamu (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslideup_vx_u16mf4_tamu (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslideup_vx_u16mf2_tamu (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslideup_vx_u16m1_tamu (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslideup_vx_u16m2_tamu (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslideup_vx_u16m4_tamu (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslideup_vx_u16m8_tamu (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslideup_vx_u32mf2_tamu (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslideup_vx_u32m1_tamu (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslideup_vx_u32m2_tamu (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslideup_vx_u32m4_tamu (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslideup_vx_u32m8_tamu (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslideup_vx_u64m1_tamu (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslideup_vx_u64m2_tamu (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslideup_vx_u64m4_tamu (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslideup_vx_u64m8_tamu (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslideup_vx_f16mf4_tamu (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslideup_vx_f16mf2_tamu (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslideup_vx_f16m1_tamu (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslideup_vx_f16m2_tamu (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslideup_vx_f16m4_tamu (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslideup_vx_f16m8_tamu (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslideup_vx_f32mf2_tamu (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslideup_vx_f32m1_tamu (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslideup_vx_f32m2_tamu (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslideup_vx_f32m4_tamu (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslideup_vx_f32m8_tamu (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslideup_vx_f64m1_tamu (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslideup_vx_f64m2_tamu (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslideup_vx_f64m4_tamu (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslideup_vx_f64m8_tamu (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t vslidedown_vx_i8mf8_tamu (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t vslidedown_vx_i8mf4_tamu (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t vslidedown_vx_i8mf2_tamu (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t vslidedown_vx_i8m1_tamu (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t vslidedown_vx_i8m2_tamu (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t vslidedown_vx_i8m4_tamu (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t vslidedown_vx_i8m8_tamu (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t vslidedown_vx_i16mf4_tamu (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t vslidedown_vx_i16mf2_tamu (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t vslidedown_vx_i16m1_tamu (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t vslidedown_vx_i16m2_tamu (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t vslidedown_vx_i16m4_tamu (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t vslidedown_vx_i16m8_tamu (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t vslidedown_vx_i32mf2_tamu (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t vslidedown_vx_i32m1_tamu (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t vslidedown_vx_i32m2_tamu (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t vslidedown_vx_i32m4_tamu (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t vslidedown_vx_i32m8_tamu (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t vslidedown_vx_i64m1_tamu (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t vslidedown_vx_i64m2_tamu (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t vslidedown_vx_i64m4_tamu (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t vslidedown_vx_i64m8_tamu (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t vslidedown_vx_u8mf8_tamu (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t vslidedown_vx_u8mf4_tamu (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t vslidedown_vx_u8mf2_tamu (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t vslidedown_vx_u8m1_tamu (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t vslidedown_vx_u8m2_tamu (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t vslidedown_vx_u8m4_tamu (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t vslidedown_vx_u8m8_tamu (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t vslidedown_vx_u16mf4_tamu (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t vslidedown_vx_u16mf2_tamu (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t vslidedown_vx_u16m1_tamu (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t vslidedown_vx_u16m2_tamu (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t vslidedown_vx_u16m4_tamu (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t vslidedown_vx_u16m8_tamu (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t vslidedown_vx_u32mf2_tamu (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t vslidedown_vx_u32m1_tamu (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t vslidedown_vx_u32m2_tamu (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t vslidedown_vx_u32m4_tamu (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t vslidedown_vx_u32m8_tamu (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t vslidedown_vx_u64m1_tamu (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t vslidedown_vx_u64m2_tamu (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t vslidedown_vx_u64m4_tamu (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t vslidedown_vx_u64m8_tamu (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
vfloat16mf4_t vslidedown_vx_f16mf4_tamu (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t vslidedown_vx_f16mf2_tamu (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t vslidedown_vx_f16m1_tamu (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t vslidedown_vx_f16m2_tamu (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t vslidedown_vx_f16m4_tamu (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t vslidedown_vx_f16m8_tamu (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t vslidedown_vx_f32mf2_tamu (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t vslidedown_vx_f32m1_tamu (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t vslidedown_vx_f32m2_tamu (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t vslidedown_vx_f32m4_tamu (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t vslidedown_vx_f32m8_tamu (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t vslidedown_vx_f64m1_tamu (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t vslidedown_vx_f64m2_tamu (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t vslidedown_vx_f64m4_tamu (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t vslidedown_vx_f64m8_tamu (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
```
### [Vector Slide1up and Slide1down Functions](../rvv-intrinsic-api.md#173-vector-slide1up-and-slide1down-functions):

**Prototypes:**
``` C
vint8mf8_t vslide1up_vx_i8mf8_tu (vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1up_vx_i8mf4_tu (vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1up_vx_i8mf2_tu (vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1up_vx_i8m1_tu (vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1up_vx_i8m2_tu (vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1up_vx_i8m4_tu (vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1up_vx_i8m8_tu (vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1up_vx_i16mf4_tu (vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1up_vx_i16mf2_tu (vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1up_vx_i16m1_tu (vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1up_vx_i16m2_tu (vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1up_vx_i16m4_tu (vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1up_vx_i16m8_tu (vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1up_vx_i32mf2_tu (vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1up_vx_i32m1_tu (vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1up_vx_i32m2_tu (vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1up_vx_i32m4_tu (vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1up_vx_i32m8_tu (vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1up_vx_i64m1_tu (vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1up_vx_i64m2_tu (vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1up_vx_i64m4_tu (vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1up_vx_i64m8_tu (vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1up_vx_u8mf8_tu (vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1up_vx_u8mf4_tu (vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1up_vx_u8mf2_tu (vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1up_vx_u8m1_tu (vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1up_vx_u8m2_tu (vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1up_vx_u8m4_tu (vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1up_vx_u8m8_tu (vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1up_vx_u16mf4_tu (vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1up_vx_u16mf2_tu (vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1up_vx_u16m1_tu (vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1up_vx_u16m2_tu (vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1up_vx_u16m4_tu (vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1up_vx_u16m8_tu (vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1up_vx_u32mf2_tu (vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1up_vx_u32m1_tu (vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1up_vx_u32m2_tu (vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1up_vx_u32m4_tu (vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1up_vx_u32m8_tu (vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1up_vx_u64m1_tu (vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1up_vx_u64m2_tu (vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1up_vx_u64m4_tu (vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1up_vx_u64m8_tu (vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1up_vf_f16mf4_tu (vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1up_vf_f16mf2_tu (vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1up_vf_f16m1_tu (vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1up_vf_f16m2_tu (vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1up_vf_f16m4_tu (vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1up_vf_f16m8_tu (vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1up_vf_f32mf2_tu (vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1up_vf_f32m1_tu (vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1up_vf_f32m2_tu (vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1up_vf_f32m4_tu (vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1up_vf_f32m8_tu (vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1up_vf_f64m1_tu (vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1up_vf_f64m2_tu (vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1up_vf_f64m4_tu (vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1up_vf_f64m8_tu (vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t vslide1down_vx_i8mf8_tu (vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1down_vx_i8mf4_tu (vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1down_vx_i8mf2_tu (vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1down_vx_i8m1_tu (vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1down_vx_i8m2_tu (vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1down_vx_i8m4_tu (vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1down_vx_i8m8_tu (vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1down_vx_i16mf4_tu (vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1down_vx_i16mf2_tu (vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1down_vx_i16m1_tu (vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1down_vx_i16m2_tu (vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1down_vx_i16m4_tu (vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1down_vx_i16m8_tu (vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1down_vx_i32mf2_tu (vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1down_vx_i32m1_tu (vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1down_vx_i32m2_tu (vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1down_vx_i32m4_tu (vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1down_vx_i32m8_tu (vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1down_vx_i64m1_tu (vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1down_vx_i64m2_tu (vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1down_vx_i64m4_tu (vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1down_vx_i64m8_tu (vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1down_vx_u8mf8_tu (vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1down_vx_u8mf4_tu (vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1down_vx_u8mf2_tu (vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1down_vx_u8m1_tu (vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1down_vx_u8m2_tu (vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1down_vx_u8m4_tu (vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1down_vx_u8m8_tu (vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1down_vx_u16mf4_tu (vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1down_vx_u16mf2_tu (vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1down_vx_u16m1_tu (vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1down_vx_u16m2_tu (vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1down_vx_u16m4_tu (vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1down_vx_u16m8_tu (vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1down_vx_u32mf2_tu (vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1down_vx_u32m1_tu (vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1down_vx_u32m2_tu (vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1down_vx_u32m4_tu (vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1down_vx_u32m8_tu (vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1down_vx_u64m1_tu (vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1down_vx_u64m2_tu (vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1down_vx_u64m4_tu (vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1down_vx_u64m8_tu (vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1down_vf_f16mf4_tu (vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1down_vf_f16mf2_tu (vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1down_vf_f16m1_tu (vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1down_vf_f16m2_tu (vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1down_vf_f16m4_tu (vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1down_vf_f16m8_tu (vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1down_vf_f32mf2_tu (vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1down_vf_f32m1_tu (vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1down_vf_f32m2_tu (vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1down_vf_f32m4_tu (vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1down_vf_f32m8_tu (vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1down_vf_f64m1_tu (vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1down_vf_f64m2_tu (vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1down_vf_f64m4_tu (vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1down_vf_f64m8_tu (vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t vslide1up_vx_i8mf8_ta (vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1up_vx_i8mf4_ta (vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1up_vx_i8mf2_ta (vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1up_vx_i8m1_ta (vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1up_vx_i8m2_ta (vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1up_vx_i8m4_ta (vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1up_vx_i8m8_ta (vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1up_vx_i16mf4_ta (vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1up_vx_i16mf2_ta (vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1up_vx_i16m1_ta (vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1up_vx_i16m2_ta (vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1up_vx_i16m4_ta (vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1up_vx_i16m8_ta (vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1up_vx_i32mf2_ta (vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1up_vx_i32m1_ta (vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1up_vx_i32m2_ta (vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1up_vx_i32m4_ta (vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1up_vx_i32m8_ta (vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1up_vx_i64m1_ta (vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1up_vx_i64m2_ta (vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1up_vx_i64m4_ta (vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1up_vx_i64m8_ta (vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1up_vx_u8mf8_ta (vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1up_vx_u8mf4_ta (vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1up_vx_u8mf2_ta (vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1up_vx_u8m1_ta (vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1up_vx_u8m2_ta (vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1up_vx_u8m4_ta (vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1up_vx_u8m8_ta (vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1up_vx_u16mf4_ta (vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1up_vx_u16mf2_ta (vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1up_vx_u16m1_ta (vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1up_vx_u16m2_ta (vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1up_vx_u16m4_ta (vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1up_vx_u16m8_ta (vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1up_vx_u32mf2_ta (vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1up_vx_u32m1_ta (vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1up_vx_u32m2_ta (vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1up_vx_u32m4_ta (vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1up_vx_u32m8_ta (vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1up_vx_u64m1_ta (vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1up_vx_u64m2_ta (vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1up_vx_u64m4_ta (vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1up_vx_u64m8_ta (vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1up_vf_f16mf4_ta (vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1up_vf_f16mf2_ta (vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1up_vf_f16m1_ta (vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1up_vf_f16m2_ta (vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1up_vf_f16m4_ta (vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1up_vf_f16m8_ta (vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1up_vf_f32mf2_ta (vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1up_vf_f32m1_ta (vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1up_vf_f32m2_ta (vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1up_vf_f32m4_ta (vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1up_vf_f32m8_ta (vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1up_vf_f64m1_ta (vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1up_vf_f64m2_ta (vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1up_vf_f64m4_ta (vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1up_vf_f64m8_ta (vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t vslide1down_vx_i8mf8_ta (vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1down_vx_i8mf4_ta (vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1down_vx_i8mf2_ta (vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1down_vx_i8m1_ta (vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1down_vx_i8m2_ta (vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1down_vx_i8m4_ta (vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1down_vx_i8m8_ta (vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1down_vx_i16mf4_ta (vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1down_vx_i16mf2_ta (vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1down_vx_i16m1_ta (vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1down_vx_i16m2_ta (vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1down_vx_i16m4_ta (vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1down_vx_i16m8_ta (vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1down_vx_i32mf2_ta (vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1down_vx_i32m1_ta (vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1down_vx_i32m2_ta (vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1down_vx_i32m4_ta (vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1down_vx_i32m8_ta (vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1down_vx_i64m1_ta (vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1down_vx_i64m2_ta (vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1down_vx_i64m4_ta (vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1down_vx_i64m8_ta (vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1down_vx_u8mf8_ta (vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1down_vx_u8mf4_ta (vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1down_vx_u8mf2_ta (vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1down_vx_u8m1_ta (vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1down_vx_u8m2_ta (vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1down_vx_u8m4_ta (vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1down_vx_u8m8_ta (vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1down_vx_u16mf4_ta (vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1down_vx_u16mf2_ta (vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1down_vx_u16m1_ta (vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1down_vx_u16m2_ta (vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1down_vx_u16m4_ta (vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1down_vx_u16m8_ta (vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1down_vx_u32mf2_ta (vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1down_vx_u32m1_ta (vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1down_vx_u32m2_ta (vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1down_vx_u32m4_ta (vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1down_vx_u32m8_ta (vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1down_vx_u64m1_ta (vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1down_vx_u64m2_ta (vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1down_vx_u64m4_ta (vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1down_vx_u64m8_ta (vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1down_vf_f16mf4_ta (vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1down_vf_f16mf2_ta (vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1down_vf_f16m1_ta (vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1down_vf_f16m2_ta (vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1down_vf_f16m4_ta (vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1down_vf_f16m8_ta (vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1down_vf_f32mf2_ta (vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1down_vf_f32m1_ta (vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1down_vf_f32m2_ta (vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1down_vf_f32m4_ta (vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1down_vf_f32m8_ta (vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1down_vf_f64m1_ta (vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1down_vf_f64m2_ta (vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1down_vf_f64m4_ta (vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1down_vf_f64m8_ta (vfloat64m8_t src, float64_t value, size_t vl);
// masked functions
vint8mf8_t vslide1up_vx_i8mf8_tuma (vbool64_t mask, vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1up_vx_i8mf4_tuma (vbool32_t mask, vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1up_vx_i8mf2_tuma (vbool16_t mask, vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1up_vx_i8m1_tuma (vbool8_t mask, vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1up_vx_i8m2_tuma (vbool4_t mask, vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1up_vx_i8m4_tuma (vbool2_t mask, vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1up_vx_i8m8_tuma (vbool1_t mask, vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1up_vx_i16mf4_tuma (vbool64_t mask, vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1up_vx_i16mf2_tuma (vbool32_t mask, vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1up_vx_i16m1_tuma (vbool16_t mask, vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1up_vx_i16m2_tuma (vbool8_t mask, vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1up_vx_i16m4_tuma (vbool4_t mask, vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1up_vx_i16m8_tuma (vbool2_t mask, vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1up_vx_i32mf2_tuma (vbool64_t mask, vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1up_vx_i32m1_tuma (vbool32_t mask, vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1up_vx_i32m2_tuma (vbool16_t mask, vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1up_vx_i32m4_tuma (vbool8_t mask, vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1up_vx_i32m8_tuma (vbool4_t mask, vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1up_vx_i64m1_tuma (vbool64_t mask, vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1up_vx_i64m2_tuma (vbool32_t mask, vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1up_vx_i64m4_tuma (vbool16_t mask, vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1up_vx_i64m8_tuma (vbool8_t mask, vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1up_vx_u8mf8_tuma (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1up_vx_u8mf4_tuma (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1up_vx_u8mf2_tuma (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1up_vx_u8m1_tuma (vbool8_t mask, vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1up_vx_u8m2_tuma (vbool4_t mask, vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1up_vx_u8m4_tuma (vbool2_t mask, vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1up_vx_u8m8_tuma (vbool1_t mask, vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1up_vx_u16mf4_tuma (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1up_vx_u16mf2_tuma (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1up_vx_u16m1_tuma (vbool16_t mask, vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1up_vx_u16m2_tuma (vbool8_t mask, vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1up_vx_u16m4_tuma (vbool4_t mask, vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1up_vx_u16m8_tuma (vbool2_t mask, vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1up_vx_u32mf2_tuma (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1up_vx_u32m1_tuma (vbool32_t mask, vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1up_vx_u32m2_tuma (vbool16_t mask, vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1up_vx_u32m4_tuma (vbool8_t mask, vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1up_vx_u32m8_tuma (vbool4_t mask, vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1up_vx_u64m1_tuma (vbool64_t mask, vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1up_vx_u64m2_tuma (vbool32_t mask, vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1up_vx_u64m4_tuma (vbool16_t mask, vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1up_vx_u64m8_tuma (vbool8_t mask, vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1up_vf_f16mf4_tuma (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1up_vf_f16mf2_tuma (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1up_vf_f16m1_tuma (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1up_vf_f16m2_tuma (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1up_vf_f16m4_tuma (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1up_vf_f16m8_tuma (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1up_vf_f32mf2_tuma (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1up_vf_f32m1_tuma (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1up_vf_f32m2_tuma (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1up_vf_f32m4_tuma (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1up_vf_f32m8_tuma (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1up_vf_f64m1_tuma (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1up_vf_f64m2_tuma (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1up_vf_f64m4_tuma (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1up_vf_f64m8_tuma (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t vslide1down_vx_i8mf8_tuma (vbool64_t mask, vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1down_vx_i8mf4_tuma (vbool32_t mask, vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1down_vx_i8mf2_tuma (vbool16_t mask, vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1down_vx_i8m1_tuma (vbool8_t mask, vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1down_vx_i8m2_tuma (vbool4_t mask, vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1down_vx_i8m4_tuma (vbool2_t mask, vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1down_vx_i8m8_tuma (vbool1_t mask, vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1down_vx_i16mf4_tuma (vbool64_t mask, vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1down_vx_i16mf2_tuma (vbool32_t mask, vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1down_vx_i16m1_tuma (vbool16_t mask, vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1down_vx_i16m2_tuma (vbool8_t mask, vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1down_vx_i16m4_tuma (vbool4_t mask, vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1down_vx_i16m8_tuma (vbool2_t mask, vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1down_vx_i32mf2_tuma (vbool64_t mask, vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1down_vx_i32m1_tuma (vbool32_t mask, vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1down_vx_i32m2_tuma (vbool16_t mask, vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1down_vx_i32m4_tuma (vbool8_t mask, vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1down_vx_i32m8_tuma (vbool4_t mask, vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1down_vx_i64m1_tuma (vbool64_t mask, vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1down_vx_i64m2_tuma (vbool32_t mask, vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1down_vx_i64m4_tuma (vbool16_t mask, vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1down_vx_i64m8_tuma (vbool8_t mask, vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1down_vx_u8mf8_tuma (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1down_vx_u8mf4_tuma (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1down_vx_u8mf2_tuma (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1down_vx_u8m1_tuma (vbool8_t mask, vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1down_vx_u8m2_tuma (vbool4_t mask, vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1down_vx_u8m4_tuma (vbool2_t mask, vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1down_vx_u8m8_tuma (vbool1_t mask, vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1down_vx_u16mf4_tuma (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1down_vx_u16mf2_tuma (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1down_vx_u16m1_tuma (vbool16_t mask, vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1down_vx_u16m2_tuma (vbool8_t mask, vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1down_vx_u16m4_tuma (vbool4_t mask, vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1down_vx_u16m8_tuma (vbool2_t mask, vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1down_vx_u32mf2_tuma (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1down_vx_u32m1_tuma (vbool32_t mask, vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1down_vx_u32m2_tuma (vbool16_t mask, vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1down_vx_u32m4_tuma (vbool8_t mask, vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1down_vx_u32m8_tuma (vbool4_t mask, vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1down_vx_u64m1_tuma (vbool64_t mask, vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1down_vx_u64m2_tuma (vbool32_t mask, vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1down_vx_u64m4_tuma (vbool16_t mask, vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1down_vx_u64m8_tuma (vbool8_t mask, vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1down_vf_f16mf4_tuma (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1down_vf_f16mf2_tuma (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1down_vf_f16m1_tuma (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1down_vf_f16m2_tuma (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1down_vf_f16m4_tuma (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1down_vf_f16m8_tuma (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1down_vf_f32mf2_tuma (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1down_vf_f32m1_tuma (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1down_vf_f32m2_tuma (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1down_vf_f32m4_tuma (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1down_vf_f32m8_tuma (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1down_vf_f64m1_tuma (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1down_vf_f64m2_tuma (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1down_vf_f64m4_tuma (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1down_vf_f64m8_tuma (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
// masked functions
vint8mf8_t vslide1up_vx_i8mf8_tumu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1up_vx_i8mf4_tumu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1up_vx_i8mf2_tumu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1up_vx_i8m1_tumu (vbool8_t mask, vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1up_vx_i8m2_tumu (vbool4_t mask, vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1up_vx_i8m4_tumu (vbool2_t mask, vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1up_vx_i8m8_tumu (vbool1_t mask, vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1up_vx_i16mf4_tumu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1up_vx_i16mf2_tumu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1up_vx_i16m1_tumu (vbool16_t mask, vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1up_vx_i16m2_tumu (vbool8_t mask, vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1up_vx_i16m4_tumu (vbool4_t mask, vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1up_vx_i16m8_tumu (vbool2_t mask, vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1up_vx_i32mf2_tumu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1up_vx_i32m1_tumu (vbool32_t mask, vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1up_vx_i32m2_tumu (vbool16_t mask, vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1up_vx_i32m4_tumu (vbool8_t mask, vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1up_vx_i32m8_tumu (vbool4_t mask, vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1up_vx_i64m1_tumu (vbool64_t mask, vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1up_vx_i64m2_tumu (vbool32_t mask, vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1up_vx_i64m4_tumu (vbool16_t mask, vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1up_vx_i64m8_tumu (vbool8_t mask, vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1up_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1up_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1up_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1up_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1up_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1up_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1up_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1up_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1up_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1up_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1up_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1up_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1up_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1up_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1up_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1up_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1up_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1up_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1up_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1up_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1up_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1up_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1up_vf_f16mf4_tumu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1up_vf_f16mf2_tumu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1up_vf_f16m1_tumu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1up_vf_f16m2_tumu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1up_vf_f16m4_tumu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1up_vf_f16m8_tumu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1up_vf_f32mf2_tumu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1up_vf_f32m1_tumu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1up_vf_f32m2_tumu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1up_vf_f32m4_tumu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1up_vf_f32m8_tumu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1up_vf_f64m1_tumu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1up_vf_f64m2_tumu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1up_vf_f64m4_tumu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1up_vf_f64m8_tumu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t vslide1down_vx_i8mf8_tumu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1down_vx_i8mf4_tumu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1down_vx_i8mf2_tumu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1down_vx_i8m1_tumu (vbool8_t mask, vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1down_vx_i8m2_tumu (vbool4_t mask, vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1down_vx_i8m4_tumu (vbool2_t mask, vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1down_vx_i8m8_tumu (vbool1_t mask, vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1down_vx_i16mf4_tumu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1down_vx_i16mf2_tumu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1down_vx_i16m1_tumu (vbool16_t mask, vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1down_vx_i16m2_tumu (vbool8_t mask, vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1down_vx_i16m4_tumu (vbool4_t mask, vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1down_vx_i16m8_tumu (vbool2_t mask, vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1down_vx_i32mf2_tumu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1down_vx_i32m1_tumu (vbool32_t mask, vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1down_vx_i32m2_tumu (vbool16_t mask, vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1down_vx_i32m4_tumu (vbool8_t mask, vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1down_vx_i32m8_tumu (vbool4_t mask, vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1down_vx_i64m1_tumu (vbool64_t mask, vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1down_vx_i64m2_tumu (vbool32_t mask, vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1down_vx_i64m4_tumu (vbool16_t mask, vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1down_vx_i64m8_tumu (vbool8_t mask, vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1down_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1down_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1down_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1down_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1down_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1down_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1down_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1down_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1down_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1down_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1down_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1down_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1down_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1down_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1down_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1down_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1down_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1down_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1down_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1down_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1down_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1down_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1down_vf_f16mf4_tumu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1down_vf_f16mf2_tumu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1down_vf_f16m1_tumu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1down_vf_f16m2_tumu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1down_vf_f16m4_tumu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1down_vf_f16m8_tumu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1down_vf_f32mf2_tumu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1down_vf_f32m1_tumu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1down_vf_f32m2_tumu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1down_vf_f32m4_tumu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1down_vf_f32m8_tumu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1down_vf_f64m1_tumu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1down_vf_f64m2_tumu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1down_vf_f64m4_tumu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1down_vf_f64m8_tumu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
// masked functions
vint8mf8_t vslide1up_vx_i8mf8_tama (vbool64_t mask, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1up_vx_i8mf4_tama (vbool32_t mask, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1up_vx_i8mf2_tama (vbool16_t mask, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1up_vx_i8m1_tama (vbool8_t mask, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1up_vx_i8m2_tama (vbool4_t mask, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1up_vx_i8m4_tama (vbool2_t mask, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1up_vx_i8m8_tama (vbool1_t mask, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1up_vx_i16mf4_tama (vbool64_t mask, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1up_vx_i16mf2_tama (vbool32_t mask, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1up_vx_i16m1_tama (vbool16_t mask, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1up_vx_i16m2_tama (vbool8_t mask, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1up_vx_i16m4_tama (vbool4_t mask, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1up_vx_i16m8_tama (vbool2_t mask, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1up_vx_i32mf2_tama (vbool64_t mask, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1up_vx_i32m1_tama (vbool32_t mask, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1up_vx_i32m2_tama (vbool16_t mask, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1up_vx_i32m4_tama (vbool8_t mask, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1up_vx_i32m8_tama (vbool4_t mask, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1up_vx_i64m1_tama (vbool64_t mask, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1up_vx_i64m2_tama (vbool32_t mask, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1up_vx_i64m4_tama (vbool16_t mask, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1up_vx_i64m8_tama (vbool8_t mask, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1up_vx_u8mf8_tama (vbool64_t mask, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1up_vx_u8mf4_tama (vbool32_t mask, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1up_vx_u8mf2_tama (vbool16_t mask, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1up_vx_u8m1_tama (vbool8_t mask, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1up_vx_u8m2_tama (vbool4_t mask, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1up_vx_u8m4_tama (vbool2_t mask, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1up_vx_u8m8_tama (vbool1_t mask, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1up_vx_u16mf4_tama (vbool64_t mask, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1up_vx_u16mf2_tama (vbool32_t mask, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1up_vx_u16m1_tama (vbool16_t mask, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1up_vx_u16m2_tama (vbool8_t mask, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1up_vx_u16m4_tama (vbool4_t mask, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1up_vx_u16m8_tama (vbool2_t mask, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1up_vx_u32mf2_tama (vbool64_t mask, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1up_vx_u32m1_tama (vbool32_t mask, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1up_vx_u32m2_tama (vbool16_t mask, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1up_vx_u32m4_tama (vbool8_t mask, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1up_vx_u32m8_tama (vbool4_t mask, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1up_vx_u64m1_tama (vbool64_t mask, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1up_vx_u64m2_tama (vbool32_t mask, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1up_vx_u64m4_tama (vbool16_t mask, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1up_vx_u64m8_tama (vbool8_t mask, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1up_vf_f16mf4_tama (vbool64_t mask, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1up_vf_f16mf2_tama (vbool32_t mask, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1up_vf_f16m1_tama (vbool16_t mask, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1up_vf_f16m2_tama (vbool8_t mask, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1up_vf_f16m4_tama (vbool4_t mask, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1up_vf_f16m8_tama (vbool2_t mask, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1up_vf_f32mf2_tama (vbool64_t mask, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1up_vf_f32m1_tama (vbool32_t mask, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1up_vf_f32m2_tama (vbool16_t mask, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1up_vf_f32m4_tama (vbool8_t mask, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1up_vf_f32m8_tama (vbool4_t mask, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1up_vf_f64m1_tama (vbool64_t mask, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1up_vf_f64m2_tama (vbool32_t mask, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1up_vf_f64m4_tama (vbool16_t mask, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1up_vf_f64m8_tama (vbool8_t mask, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t vslide1down_vx_i8mf8_tama (vbool64_t mask, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1down_vx_i8mf4_tama (vbool32_t mask, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1down_vx_i8mf2_tama (vbool16_t mask, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1down_vx_i8m1_tama (vbool8_t mask, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1down_vx_i8m2_tama (vbool4_t mask, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1down_vx_i8m4_tama (vbool2_t mask, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1down_vx_i8m8_tama (vbool1_t mask, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1down_vx_i16mf4_tama (vbool64_t mask, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1down_vx_i16mf2_tama (vbool32_t mask, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1down_vx_i16m1_tama (vbool16_t mask, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1down_vx_i16m2_tama (vbool8_t mask, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1down_vx_i16m4_tama (vbool4_t mask, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1down_vx_i16m8_tama (vbool2_t mask, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1down_vx_i32mf2_tama (vbool64_t mask, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1down_vx_i32m1_tama (vbool32_t mask, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1down_vx_i32m2_tama (vbool16_t mask, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1down_vx_i32m4_tama (vbool8_t mask, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1down_vx_i32m8_tama (vbool4_t mask, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1down_vx_i64m1_tama (vbool64_t mask, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1down_vx_i64m2_tama (vbool32_t mask, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1down_vx_i64m4_tama (vbool16_t mask, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1down_vx_i64m8_tama (vbool8_t mask, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1down_vx_u8mf8_tama (vbool64_t mask, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1down_vx_u8mf4_tama (vbool32_t mask, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1down_vx_u8mf2_tama (vbool16_t mask, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1down_vx_u8m1_tama (vbool8_t mask, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1down_vx_u8m2_tama (vbool4_t mask, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1down_vx_u8m4_tama (vbool2_t mask, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1down_vx_u8m8_tama (vbool1_t mask, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1down_vx_u16mf4_tama (vbool64_t mask, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1down_vx_u16mf2_tama (vbool32_t mask, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1down_vx_u16m1_tama (vbool16_t mask, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1down_vx_u16m2_tama (vbool8_t mask, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1down_vx_u16m4_tama (vbool4_t mask, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1down_vx_u16m8_tama (vbool2_t mask, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1down_vx_u32mf2_tama (vbool64_t mask, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1down_vx_u32m1_tama (vbool32_t mask, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1down_vx_u32m2_tama (vbool16_t mask, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1down_vx_u32m4_tama (vbool8_t mask, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1down_vx_u32m8_tama (vbool4_t mask, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1down_vx_u64m1_tama (vbool64_t mask, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1down_vx_u64m2_tama (vbool32_t mask, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1down_vx_u64m4_tama (vbool16_t mask, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1down_vx_u64m8_tama (vbool8_t mask, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1down_vf_f16mf4_tama (vbool64_t mask, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1down_vf_f16mf2_tama (vbool32_t mask, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1down_vf_f16m1_tama (vbool16_t mask, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1down_vf_f16m2_tama (vbool8_t mask, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1down_vf_f16m4_tama (vbool4_t mask, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1down_vf_f16m8_tama (vbool2_t mask, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1down_vf_f32mf2_tama (vbool64_t mask, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1down_vf_f32m1_tama (vbool32_t mask, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1down_vf_f32m2_tama (vbool16_t mask, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1down_vf_f32m4_tama (vbool8_t mask, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1down_vf_f32m8_tama (vbool4_t mask, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1down_vf_f64m1_tama (vbool64_t mask, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1down_vf_f64m2_tama (vbool32_t mask, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1down_vf_f64m4_tama (vbool16_t mask, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1down_vf_f64m8_tama (vbool8_t mask, vfloat64m8_t src, float64_t value, size_t vl);
// masked functions
vint8mf8_t vslide1up_vx_i8mf8_tamu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1up_vx_i8mf4_tamu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1up_vx_i8mf2_tamu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1up_vx_i8m1_tamu (vbool8_t mask, vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1up_vx_i8m2_tamu (vbool4_t mask, vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1up_vx_i8m4_tamu (vbool2_t mask, vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1up_vx_i8m8_tamu (vbool1_t mask, vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1up_vx_i16mf4_tamu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1up_vx_i16mf2_tamu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1up_vx_i16m1_tamu (vbool16_t mask, vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1up_vx_i16m2_tamu (vbool8_t mask, vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1up_vx_i16m4_tamu (vbool4_t mask, vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1up_vx_i16m8_tamu (vbool2_t mask, vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1up_vx_i32mf2_tamu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1up_vx_i32m1_tamu (vbool32_t mask, vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1up_vx_i32m2_tamu (vbool16_t mask, vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1up_vx_i32m4_tamu (vbool8_t mask, vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1up_vx_i32m8_tamu (vbool4_t mask, vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1up_vx_i64m1_tamu (vbool64_t mask, vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1up_vx_i64m2_tamu (vbool32_t mask, vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1up_vx_i64m4_tamu (vbool16_t mask, vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1up_vx_i64m8_tamu (vbool8_t mask, vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1up_vx_u8mf8_tamu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1up_vx_u8mf4_tamu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1up_vx_u8mf2_tamu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1up_vx_u8m1_tamu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1up_vx_u8m2_tamu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1up_vx_u8m4_tamu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1up_vx_u8m8_tamu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1up_vx_u16mf4_tamu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1up_vx_u16mf2_tamu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1up_vx_u16m1_tamu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1up_vx_u16m2_tamu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1up_vx_u16m4_tamu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1up_vx_u16m8_tamu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1up_vx_u32mf2_tamu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1up_vx_u32m1_tamu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1up_vx_u32m2_tamu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1up_vx_u32m4_tamu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1up_vx_u32m8_tamu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1up_vx_u64m1_tamu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1up_vx_u64m2_tamu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1up_vx_u64m4_tamu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1up_vx_u64m8_tamu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1up_vf_f16mf4_tamu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1up_vf_f16mf2_tamu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1up_vf_f16m1_tamu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1up_vf_f16m2_tamu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1up_vf_f16m4_tamu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1up_vf_f16m8_tamu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1up_vf_f32mf2_tamu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1up_vf_f32m1_tamu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1up_vf_f32m2_tamu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1up_vf_f32m4_tamu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1up_vf_f32m8_tamu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1up_vf_f64m1_tamu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1up_vf_f64m2_tamu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1up_vf_f64m4_tamu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1up_vf_f64m8_tamu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t vslide1down_vx_i8mf8_tamu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t vslide1down_vx_i8mf4_tamu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t vslide1down_vx_i8mf2_tamu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t vslide1down_vx_i8m1_tamu (vbool8_t mask, vint8m1_t merge, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t vslide1down_vx_i8m2_tamu (vbool4_t mask, vint8m2_t merge, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t vslide1down_vx_i8m4_tamu (vbool2_t mask, vint8m4_t merge, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t vslide1down_vx_i8m8_tamu (vbool1_t mask, vint8m8_t merge, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t vslide1down_vx_i16mf4_tamu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t vslide1down_vx_i16mf2_tamu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t vslide1down_vx_i16m1_tamu (vbool16_t mask, vint16m1_t merge, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t vslide1down_vx_i16m2_tamu (vbool8_t mask, vint16m2_t merge, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t vslide1down_vx_i16m4_tamu (vbool4_t mask, vint16m4_t merge, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t vslide1down_vx_i16m8_tamu (vbool2_t mask, vint16m8_t merge, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t vslide1down_vx_i32mf2_tamu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t vslide1down_vx_i32m1_tamu (vbool32_t mask, vint32m1_t merge, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t vslide1down_vx_i32m2_tamu (vbool16_t mask, vint32m2_t merge, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t vslide1down_vx_i32m4_tamu (vbool8_t mask, vint32m4_t merge, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t vslide1down_vx_i32m8_tamu (vbool4_t mask, vint32m8_t merge, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t vslide1down_vx_i64m1_tamu (vbool64_t mask, vint64m1_t merge, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t vslide1down_vx_i64m2_tamu (vbool32_t mask, vint64m2_t merge, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t vslide1down_vx_i64m4_tamu (vbool16_t mask, vint64m4_t merge, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t vslide1down_vx_i64m8_tamu (vbool8_t mask, vint64m8_t merge, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t vslide1down_vx_u8mf8_tamu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t vslide1down_vx_u8mf4_tamu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t vslide1down_vx_u8mf2_tamu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t vslide1down_vx_u8m1_tamu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t vslide1down_vx_u8m2_tamu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t vslide1down_vx_u8m4_tamu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t vslide1down_vx_u8m8_tamu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t vslide1down_vx_u16mf4_tamu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t vslide1down_vx_u16mf2_tamu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t vslide1down_vx_u16m1_tamu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t vslide1down_vx_u16m2_tamu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t vslide1down_vx_u16m4_tamu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t vslide1down_vx_u16m8_tamu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t vslide1down_vx_u32mf2_tamu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t vslide1down_vx_u32m1_tamu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t vslide1down_vx_u32m2_tamu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t vslide1down_vx_u32m4_tamu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t vslide1down_vx_u32m8_tamu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t vslide1down_vx_u64m1_tamu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t vslide1down_vx_u64m2_tamu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t vslide1down_vx_u64m4_tamu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t vslide1down_vx_u64m8_tamu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t src, uint64_t value, size_t vl);
vfloat16mf4_t vfslide1down_vf_f16mf4_tamu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t vfslide1down_vf_f16mf2_tamu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t vfslide1down_vf_f16m1_tamu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t vfslide1down_vf_f16m2_tamu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t vfslide1down_vf_f16m4_tamu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t vfslide1down_vf_f16m8_tamu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t vfslide1down_vf_f32mf2_tamu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t vfslide1down_vf_f32m1_tamu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t vfslide1down_vf_f32m2_tamu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t vfslide1down_vf_f32m4_tamu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t vfslide1down_vf_f32m8_tamu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t vfslide1down_vf_f64m1_tamu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t vfslide1down_vf_f64m2_tamu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t vfslide1down_vf_f64m4_tamu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t vfslide1down_vf_f64m8_tamu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t src, float64_t value, size_t vl);
```
### [Vector Register Gather Functions](../rvv-intrinsic-api.md#174-vector-register-gather-operations):

**Prototypes:**
``` C
vint8mf8_t vrgather_vv_i8mf8_tu (vint8mf8_t merge, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t vrgather_vx_i8mf8_tu (vint8mf8_t merge, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t vrgather_vv_i8mf4_tu (vint8mf4_t merge, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t vrgather_vx_i8mf4_tu (vint8mf4_t merge, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t vrgather_vv_i8mf2_tu (vint8mf2_t merge, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t vrgather_vx_i8mf2_tu (vint8mf2_t merge, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t vrgather_vv_i8m1_tu (vint8m1_t merge, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t vrgather_vx_i8m1_tu (vint8m1_t merge, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t vrgather_vv_i8m2_tu (vint8m2_t merge, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t vrgather_vx_i8m2_tu (vint8m2_t merge, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t vrgather_vv_i8m4_tu (vint8m4_t merge, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t vrgather_vx_i8m4_tu (vint8m4_t merge, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t vrgather_vv_i8m8_tu (vint8m8_t merge, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t vrgather_vx_i8m8_tu (vint8m8_t merge, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t vrgather_vv_i16mf4_tu (vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t vrgather_vx_i16mf4_tu (vint16mf4_t merge, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t vrgather_vv_i16mf2_tu (vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t vrgather_vx_i16mf2_tu (vint16mf2_t merge, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t vrgather_vv_i16m1_tu (vint16m1_t merge, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t vrgather_vx_i16m1_tu (vint16m1_t merge, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t vrgather_vv_i16m2_tu (vint16m2_t merge, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t vrgather_vx_i16m2_tu (vint16m2_t merge, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t vrgather_vv_i16m4_tu (vint16m4_t merge, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t vrgather_vx_i16m4_tu (vint16m4_t merge, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t vrgather_vv_i16m8_tu (vint16m8_t merge, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t vrgather_vx_i16m8_tu (vint16m8_t merge, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t vrgather_vv_i32mf2_tu (vint32mf2_t merge, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t vrgather_vx_i32mf2_tu (vint32mf2_t merge, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t vrgather_vv_i32m1_tu (vint32m1_t merge, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t vrgather_vx_i32m1_tu (vint32m1_t merge, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t vrgather_vv_i32m2_tu (vint32m2_t merge, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t vrgather_vx_i32m2_tu (vint32m2_t merge, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t vrgather_vv_i32m4_tu (vint32m4_t merge, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t vrgather_vx_i32m4_tu (vint32m4_t merge, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t vrgather_vv_i32m8_tu (vint32m8_t merge, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t vrgather_vx_i32m8_tu (vint32m8_t merge, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t vrgather_vv_i64m1_tu (vint64m1_t merge, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t vrgather_vx_i64m1_tu (vint64m1_t merge, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t vrgather_vv_i64m2_tu (vint64m2_t merge, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t vrgather_vx_i64m2_tu (vint64m2_t merge, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t vrgather_vv_i64m4_tu (vint64m4_t merge, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t vrgather_vx_i64m4_tu (vint64m4_t merge, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t vrgather_vv_i64m8_tu (vint64m8_t merge, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t vrgather_vx_i64m8_tu (vint64m8_t merge, vint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t vrgather_vv_u8mf8_tu (vuint8mf8_t merge, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t vrgather_vx_u8mf8_tu (vuint8mf8_t merge, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t vrgather_vv_u8mf4_tu (vuint8mf4_t merge, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t vrgather_vx_u8mf4_tu (vuint8mf4_t merge, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t vrgather_vv_u8mf2_tu (vuint8mf2_t merge, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t vrgather_vx_u8mf2_tu (vuint8mf2_t merge, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t vrgather_vv_u8m1_tu (vuint8m1_t merge, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t vrgather_vx_u8m1_tu (vuint8m1_t merge, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t vrgather_vv_u8m2_tu (vuint8m2_t merge, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t vrgather_vx_u8m2_tu (vuint8m2_t merge, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t vrgather_vv_u8m4_tu (vuint8m4_t merge, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t vrgather_vx_u8m4_tu (vuint8m4_t merge, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t vrgather_vv_u8m8_tu (vuint8m8_t merge, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t vrgather_vx_u8m8_tu (vuint8m8_t merge, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t vrgather_vv_u16mf4_tu (vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t vrgather_vx_u16mf4_tu (vuint16mf4_t merge, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t vrgather_vv_u16mf2_tu (vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t vrgather_vx_u16mf2_tu (vuint16mf2_t merge, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t vrgather_vv_u16m1_tu (vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t vrgather_vx_u16m1_tu (vuint16m1_t merge, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t vrgather_vv_u16m2_tu (vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t vrgather_vx_u16m2_tu (vuint16m2_t merge, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t vrgather_vv_u16m4_tu (vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t vrgather_vx_u16m4_tu (vuint16m4_t merge, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t vrgather_vv_u16m8_tu (vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t vrgather_vx_u16m8_tu (vuint16m8_t merge, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t vrgather_vv_u32mf2_tu (vuint32mf2_t merge, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t vrgather_vx_u32mf2_tu (vuint32mf2_t merge, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t vrgather_vv_u32m1_tu (vuint32m1_t merge, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t vrgather_vx_u32m1_tu (vuint32m1_t merge, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t vrgather_vv_u32m2_tu (vuint32m2_t merge, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t vrgather_vx_u32m2_tu (vuint32m2_t merge, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t vrgather_vv_u32m4_tu (vuint32m4_t merge, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t vrgather_vx_u32m4_tu (vuint32m4_t merge, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t vrgather_vv_u32m8_tu (vuint32m8_t merge, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t vrgather_vx_u32m8_tu (vuint32m8_t merge, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t vrgather_vv_u64m1_tu (vuint64m1_t merge, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t vrgather_vx_u64m1_tu (vuint64m1_t merge, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t vrgather_vv_u64m2_tu (vuint64m2_t merge, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t vrgather_vx_u64m2_tu (vuint64m2_t merge, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t vrgather_vv_u64m4_tu (vuint64m4_t merge, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t vrgather_vx_u64m4_tu (vuint64m4_t merge, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t vrgather_vv_u64m8_tu (vuint64m8_t merge, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t vrgather_vx_u64m8_tu (vuint64m8_t merge, vuint64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t vrgather_vv_f16mf4_tu (vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t vrgather_vx_f16mf4_tu (vfloat16mf4_t merge, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t vrgather_vv_f16mf2_tu (vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t vrgather_vx_f16mf2_tu (vfloat16mf2_t merge, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t vrgather_vv_f16m1_tu (vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t vrgather_vx_f16m1_tu (vfloat16m1_t merge, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t vrgather_vv_f16m2_tu (vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t vrgather_vx_f16m2_tu (vfloat16m2_t merge, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t vrgather_vv_f16m4_tu (vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t vrgather_vx_f16m4_tu (vfloat16m4_t merge, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t vrgather_vv_f16m8_tu (vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t vrgather_vx_f16m8_tu (vfloat16m8_t merge, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t vrgather_vv_f32mf2_tu (vfloat32mf2_t merge, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t vrgather_vx_f32mf2_tu (vfloat32mf2_t merge, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t vrgather_vv_f32m1_tu (vfloat32m1_t merge, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t vrgather_vx_f32m1_tu (vfloat32m1_t merge, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t vrgather_vv_f32m2_tu (vfloat32m2_t merge, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t vrgather_vx_f32m2_tu (vfloat32m2_t merge, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t vrgather_vv_f32m4_tu (vfloat32m4_t merge, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t vrgather_vx_f32m4_tu (vfloat32m4_t merge, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t vrgather_vv_f32m8_tu (vfloat32m8_t merge, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t vrgather_vx_f32m8_tu (vfloat32m8_t merge, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t vrgather_vv_f64m1_tu (vfloat64m1_t merge, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t vrgather_vx_f64m1_tu (vfloat64m1_t merge, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t vrgather_vv_f64m2_tu (vfloat64m2_t merge, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t vrgather_vx_f64m2_tu (vfloat64m2_t merge, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t vrgather_vv_f64m4_tu (vfloat64m4_t merge, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t vrgather_vx_f64m4_tu (vfloat64m4_t merge, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t vrgather_vv_f64m8_tu (vfloat64m8_t merge, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t vrgather_vx_f64m8_tu (vfloat64m8_t merge, vfloat64m8_t op1, size_t index, size_t vl);
vint8mf8_t vrgatherei16_vv_i8mf8_tu (vint8mf8_t merge, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t vrgatherei16_vv_i8mf4_tu (vint8mf4_t merge, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t vrgatherei16_vv_i8mf2_tu (vint8mf2_t merge, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t vrgatherei16_vv_i8m1_tu (vint8m1_t merge, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t vrgatherei16_vv_i8m2_tu (vint8m2_t merge, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t vrgatherei16_vv_i8m4_tu (vint8m4_t merge, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t vrgatherei16_vv_i16mf4_tu (vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t vrgatherei16_vv_i16mf2_tu (vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t vrgatherei16_vv_i16m1_tu (vint16m1_t merge, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t vrgatherei16_vv_i16m2_tu (vint16m2_t merge, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t vrgatherei16_vv_i16m4_tu (vint16m4_t merge, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t vrgatherei16_vv_i16m8_tu (vint16m8_t merge, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t vrgatherei16_vv_i32mf2_tu (vint32mf2_t merge, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t vrgatherei16_vv_i32m1_tu (vint32m1_t merge, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t vrgatherei16_vv_i32m2_tu (vint32m2_t merge, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t vrgatherei16_vv_i32m4_tu (vint32m4_t merge, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t vrgatherei16_vv_i32m8_tu (vint32m8_t merge, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t vrgatherei16_vv_i64m1_tu (vint64m1_t merge, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t vrgatherei16_vv_i64m2_tu (vint64m2_t merge, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t vrgatherei16_vv_i64m4_tu (vint64m4_t merge, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t vrgatherei16_vv_i64m8_tu (vint64m8_t merge, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t vrgatherei16_vv_u8mf8_tu (vuint8mf8_t merge, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t vrgatherei16_vv_u8mf4_tu (vuint8mf4_t merge, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t vrgatherei16_vv_u8mf2_tu (vuint8mf2_t merge, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t vrgatherei16_vv_u8m1_tu (vuint8m1_t merge, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t vrgatherei16_vv_u8m2_tu (vuint8m2_t merge, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t vrgatherei16_vv_u8m4_tu (vuint8m4_t merge, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t vrgatherei16_vv_u16mf4_tu (vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t vrgatherei16_vv_u16mf2_tu (vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t vrgatherei16_vv_u16m1_tu (vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t vrgatherei16_vv_u16m2_tu (vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t vrgatherei16_vv_u16m4_tu (vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t vrgatherei16_vv_u16m8_tu (vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t vrgatherei16_vv_u32mf2_tu (vuint32mf2_t merge, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t vrgatherei16_vv_u32m1_tu (vuint32m1_t merge, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t vrgatherei16_vv_u32m2_tu (vuint32m2_t merge, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t vrgatherei16_vv_u32m4_tu (vuint32m4_t merge, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t vrgatherei16_vv_u32m8_tu (vuint32m8_t merge, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t vrgatherei16_vv_u64m1_tu (vuint64m1_t merge, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t vrgatherei16_vv_u64m2_tu (vuint64m2_t merge, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t vrgatherei16_vv_u64m4_tu (vuint64m4_t merge, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t vrgatherei16_vv_u64m8_tu (vuint64m8_t merge, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
vfloat16mf4_t vrgatherei16_vv_f16mf4_tu (vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t vrgatherei16_vv_f16mf2_tu (vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t vrgatherei16_vv_f16m1_tu (vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t vrgatherei16_vv_f16m2_tu (vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t vrgatherei16_vv_f16m4_tu (vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t vrgatherei16_vv_f16m8_tu (vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t vrgatherei16_vv_f32mf2_tu (vfloat32mf2_t merge, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t vrgatherei16_vv_f32m1_tu (vfloat32m1_t merge, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t vrgatherei16_vv_f32m2_tu (vfloat32m2_t merge, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t vrgatherei16_vv_f32m4_tu (vfloat32m4_t merge, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t vrgatherei16_vv_f32m8_tu (vfloat32m8_t merge, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t vrgatherei16_vv_f64m1_tu (vfloat64m1_t merge, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t vrgatherei16_vv_f64m2_tu (vfloat64m2_t merge, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t vrgatherei16_vv_f64m4_tu (vfloat64m4_t merge, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t vrgatherei16_vv_f64m8_tu (vfloat64m8_t merge, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
vint8mf8_t vrgather_vv_i8mf8_ta (vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t vrgather_vx_i8mf8_ta (vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t vrgather_vv_i8mf4_ta (vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t vrgather_vx_i8mf4_ta (vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t vrgather_vv_i8mf2_ta (vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t vrgather_vx_i8mf2_ta (vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t vrgather_vv_i8m1_ta (vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t vrgather_vx_i8m1_ta (vint8m1_t op1, size_t index, size_t vl);
vint8m2_t vrgather_vv_i8m2_ta (vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t vrgather_vx_i8m2_ta (vint8m2_t op1, size_t index, size_t vl);
vint8m4_t vrgather_vv_i8m4_ta (vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t vrgather_vx_i8m4_ta (vint8m4_t op1, size_t index, size_t vl);
vint8m8_t vrgather_vv_i8m8_ta (vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t vrgather_vx_i8m8_ta (vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t vrgather_vv_i16mf4_ta (vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t vrgather_vx_i16mf4_ta (vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t vrgather_vv_i16mf2_ta (vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t vrgather_vx_i16mf2_ta (vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t vrgather_vv_i16m1_ta (vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t vrgather_vx_i16m1_ta (vint16m1_t op1, size_t index, size_t vl);
vint16m2_t vrgather_vv_i16m2_ta (vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t vrgather_vx_i16m2_ta (vint16m2_t op1, size_t index, size_t vl);
vint16m4_t vrgather_vv_i16m4_ta (vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t vrgather_vx_i16m4_ta (vint16m4_t op1, size_t index, size_t vl);
vint16m8_t vrgather_vv_i16m8_ta (vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t vrgather_vx_i16m8_ta (vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t vrgather_vv_i32mf2_ta (vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t vrgather_vx_i32mf2_ta (vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t vrgather_vv_i32m1_ta (vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t vrgather_vx_i32m1_ta (vint32m1_t op1, size_t index, size_t vl);
vint32m2_t vrgather_vv_i32m2_ta (vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t vrgather_vx_i32m2_ta (vint32m2_t op1, size_t index, size_t vl);
vint32m4_t vrgather_vv_i32m4_ta (vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t vrgather_vx_i32m4_ta (vint32m4_t op1, size_t index, size_t vl);
vint32m8_t vrgather_vv_i32m8_ta (vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t vrgather_vx_i32m8_ta (vint32m8_t op1, size_t index, size_t vl);
vint64m1_t vrgather_vv_i64m1_ta (vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t vrgather_vx_i64m1_ta (vint64m1_t op1, size_t index, size_t vl);
vint64m2_t vrgather_vv_i64m2_ta (vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t vrgather_vx_i64m2_ta (vint64m2_t op1, size_t index, size_t vl);
vint64m4_t vrgather_vv_i64m4_ta (vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t vrgather_vx_i64m4_ta (vint64m4_t op1, size_t index, size_t vl);
vint64m8_t vrgather_vv_i64m8_ta (vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t vrgather_vx_i64m8_ta (vint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t vrgather_vv_u8mf8_ta (vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t vrgather_vx_u8mf8_ta (vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t vrgather_vv_u8mf4_ta (vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t vrgather_vx_u8mf4_ta (vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t vrgather_vv_u8mf2_ta (vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t vrgather_vx_u8mf2_ta (vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t vrgather_vv_u8m1_ta (vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t vrgather_vx_u8m1_ta (vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t vrgather_vv_u8m2_ta (vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t vrgather_vx_u8m2_ta (vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t vrgather_vv_u8m4_ta (vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t vrgather_vx_u8m4_ta (vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t vrgather_vv_u8m8_ta (vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t vrgather_vx_u8m8_ta (vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t vrgather_vv_u16mf4_ta (vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t vrgather_vx_u16mf4_ta (vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t vrgather_vv_u16mf2_ta (vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t vrgather_vx_u16mf2_ta (vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t vrgather_vv_u16m1_ta (vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t vrgather_vx_u16m1_ta (vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t vrgather_vv_u16m2_ta (vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t vrgather_vx_u16m2_ta (vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t vrgather_vv_u16m4_ta (vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t vrgather_vx_u16m4_ta (vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t vrgather_vv_u16m8_ta (vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t vrgather_vx_u16m8_ta (vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t vrgather_vv_u32mf2_ta (vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t vrgather_vx_u32mf2_ta (vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t vrgather_vv_u32m1_ta (vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t vrgather_vx_u32m1_ta (vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t vrgather_vv_u32m2_ta (vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t vrgather_vx_u32m2_ta (vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t vrgather_vv_u32m4_ta (vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t vrgather_vx_u32m4_ta (vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t vrgather_vv_u32m8_ta (vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t vrgather_vx_u32m8_ta (vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t vrgather_vv_u64m1_ta (vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t vrgather_vx_u64m1_ta (vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t vrgather_vv_u64m2_ta (vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t vrgather_vx_u64m2_ta (vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t vrgather_vv_u64m4_ta (vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t vrgather_vx_u64m4_ta (vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t vrgather_vv_u64m8_ta (vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t vrgather_vx_u64m8_ta (vuint64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t vrgather_vv_f16mf4_ta (vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t vrgather_vx_f16mf4_ta (vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t vrgather_vv_f16mf2_ta (vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t vrgather_vx_f16mf2_ta (vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t vrgather_vv_f16m1_ta (vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t vrgather_vx_f16m1_ta (vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t vrgather_vv_f16m2_ta (vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t vrgather_vx_f16m2_ta (vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t vrgather_vv_f16m4_ta (vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t vrgather_vx_f16m4_ta (vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t vrgather_vv_f16m8_ta (vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t vrgather_vx_f16m8_ta (vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t vrgather_vv_f32mf2_ta (vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t vrgather_vx_f32mf2_ta (vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t vrgather_vv_f32m1_ta (vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t vrgather_vx_f32m1_ta (vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t vrgather_vv_f32m2_ta (vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t vrgather_vx_f32m2_ta (vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t vrgather_vv_f32m4_ta (vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t vrgather_vx_f32m4_ta (vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t vrgather_vv_f32m8_ta (vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t vrgather_vx_f32m8_ta (vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t vrgather_vv_f64m1_ta (vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t vrgather_vx_f64m1_ta (vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t vrgather_vv_f64m2_ta (vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t vrgather_vx_f64m2_ta (vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t vrgather_vv_f64m4_ta (vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t vrgather_vx_f64m4_ta (vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t vrgather_vv_f64m8_ta (vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t vrgather_vx_f64m8_ta (vfloat64m8_t op1, size_t index, size_t vl);
vint8mf8_t vrgatherei16_vv_i8mf8_ta (vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t vrgatherei16_vv_i8mf4_ta (vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t vrgatherei16_vv_i8mf2_ta (vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t vrgatherei16_vv_i8m1_ta (vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t vrgatherei16_vv_i8m2_ta (vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t vrgatherei16_vv_i8m4_ta (vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t vrgatherei16_vv_i16mf4_ta (vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t vrgatherei16_vv_i16mf2_ta (vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t vrgatherei16_vv_i16m1_ta (vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t vrgatherei16_vv_i16m2_ta (vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t vrgatherei16_vv_i16m4_ta (vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t vrgatherei16_vv_i16m8_ta (vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t vrgatherei16_vv_i32mf2_ta (vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t vrgatherei16_vv_i32m1_ta (vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t vrgatherei16_vv_i32m2_ta (vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t vrgatherei16_vv_i32m4_ta (vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t vrgatherei16_vv_i32m8_ta (vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t vrgatherei16_vv_i64m1_ta (vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t vrgatherei16_vv_i64m2_ta (vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t vrgatherei16_vv_i64m4_ta (vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t vrgatherei16_vv_i64m8_ta (vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t vrgatherei16_vv_u8mf8_ta (vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t vrgatherei16_vv_u8mf4_ta (vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t vrgatherei16_vv_u8mf2_ta (vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t vrgatherei16_vv_u8m1_ta (vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t vrgatherei16_vv_u8m2_ta (vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t vrgatherei16_vv_u8m4_ta (vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t vrgatherei16_vv_u16mf4_ta (vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t vrgatherei16_vv_u16mf2_ta (vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t vrgatherei16_vv_u16m1_ta (vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t vrgatherei16_vv_u16m2_ta (vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t vrgatherei16_vv_u16m4_ta (vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t vrgatherei16_vv_u16m8_ta (vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t vrgatherei16_vv_u32mf2_ta (vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t vrgatherei16_vv_u32m1_ta (vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t vrgatherei16_vv_u32m2_ta (vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t vrgatherei16_vv_u32m4_ta (vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t vrgatherei16_vv_u32m8_ta (vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t vrgatherei16_vv_u64m1_ta (vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t vrgatherei16_vv_u64m2_ta (vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t vrgatherei16_vv_u64m4_ta (vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t vrgatherei16_vv_u64m8_ta (vuint64m8_t op1, vuint16m2_t op2, size_t vl);
vfloat16mf4_t vrgatherei16_vv_f16mf4_ta (vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t vrgatherei16_vv_f16mf2_ta (vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t vrgatherei16_vv_f16m1_ta (vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t vrgatherei16_vv_f16m2_ta (vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t vrgatherei16_vv_f16m4_ta (vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t vrgatherei16_vv_f16m8_ta (vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t vrgatherei16_vv_f32mf2_ta (vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t vrgatherei16_vv_f32m1_ta (vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t vrgatherei16_vv_f32m2_ta (vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t vrgatherei16_vv_f32m4_ta (vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t vrgatherei16_vv_f32m8_ta (vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t vrgatherei16_vv_f64m1_ta (vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t vrgatherei16_vv_f64m2_ta (vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t vrgatherei16_vv_f64m4_ta (vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t vrgatherei16_vv_f64m8_ta (vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vint8mf8_t vrgather_vv_i8mf8_tuma (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t vrgather_vx_i8mf8_tuma (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t vrgather_vv_i8mf4_tuma (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t vrgather_vx_i8mf4_tuma (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t vrgather_vv_i8mf2_tuma (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t vrgather_vx_i8mf2_tuma (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t vrgather_vv_i8m1_tuma (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t vrgather_vx_i8m1_tuma (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t vrgather_vv_i8m2_tuma (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t vrgather_vx_i8m2_tuma (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t vrgather_vv_i8m4_tuma (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t vrgather_vx_i8m4_tuma (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t vrgather_vv_i8m8_tuma (vbool1_t mask, vint8m8_t merge, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t vrgather_vx_i8m8_tuma (vbool1_t mask, vint8m8_t merge, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t vrgather_vv_i16mf4_tuma (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t vrgather_vx_i16mf4_tuma (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t vrgather_vv_i16mf2_tuma (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t vrgather_vx_i16mf2_tuma (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t vrgather_vv_i16m1_tuma (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t vrgather_vx_i16m1_tuma (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t vrgather_vv_i16m2_tuma (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t vrgather_vx_i16m2_tuma (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t vrgather_vv_i16m4_tuma (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t vrgather_vx_i16m4_tuma (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t vrgather_vv_i16m8_tuma (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t vrgather_vx_i16m8_tuma (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t vrgather_vv_i32mf2_tuma (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t vrgather_vx_i32mf2_tuma (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t vrgather_vv_i32m1_tuma (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t vrgather_vx_i32m1_tuma (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t vrgather_vv_i32m2_tuma (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t vrgather_vx_i32m2_tuma (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t vrgather_vv_i32m4_tuma (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t vrgather_vx_i32m4_tuma (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t vrgather_vv_i32m8_tuma (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t vrgather_vx_i32m8_tuma (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t vrgather_vv_i64m1_tuma (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t vrgather_vx_i64m1_tuma (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t vrgather_vv_i64m2_tuma (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t vrgather_vx_i64m2_tuma (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t vrgather_vv_i64m4_tuma (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t vrgather_vx_i64m4_tuma (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t vrgather_vv_i64m8_tuma (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t vrgather_vx_i64m8_tuma (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t vrgather_vv_u8mf8_tuma (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t vrgather_vx_u8mf8_tuma (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t vrgather_vv_u8mf4_tuma (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t vrgather_vx_u8mf4_tuma (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t vrgather_vv_u8mf2_tuma (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t vrgather_vx_u8mf2_tuma (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t vrgather_vv_u8m1_tuma (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t vrgather_vx_u8m1_tuma (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t vrgather_vv_u8m2_tuma (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t vrgather_vx_u8m2_tuma (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t vrgather_vv_u8m4_tuma (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t vrgather_vx_u8m4_tuma (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t vrgather_vv_u8m8_tuma (vbool1_t mask, vuint8m8_t merge, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t vrgather_vx_u8m8_tuma (vbool1_t mask, vuint8m8_t merge, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t vrgather_vv_u16mf4_tuma (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t vrgather_vx_u16mf4_tuma (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t vrgather_vv_u16mf2_tuma (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t vrgather_vx_u16mf2_tuma (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t vrgather_vv_u16m1_tuma (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t vrgather_vx_u16m1_tuma (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t vrgather_vv_u16m2_tuma (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t vrgather_vx_u16m2_tuma (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t vrgather_vv_u16m4_tuma (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t vrgather_vx_u16m4_tuma (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t vrgather_vv_u16m8_tuma (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t vrgather_vx_u16m8_tuma (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t vrgather_vv_u32mf2_tuma (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t vrgather_vx_u32mf2_tuma (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t vrgather_vv_u32m1_tuma (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t vrgather_vx_u32m1_tuma (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t vrgather_vv_u32m2_tuma (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t vrgather_vx_u32m2_tuma (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t vrgather_vv_u32m4_tuma (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t vrgather_vx_u32m4_tuma (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t vrgather_vv_u32m8_tuma (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t vrgather_vx_u32m8_tuma (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t vrgather_vv_u64m1_tuma (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t vrgather_vx_u64m1_tuma (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t vrgather_vv_u64m2_tuma (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t vrgather_vx_u64m2_tuma (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t vrgather_vv_u64m4_tuma (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t vrgather_vx_u64m4_tuma (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t vrgather_vv_u64m8_tuma (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t vrgather_vx_u64m8_tuma (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t vrgather_vv_f16mf4_tuma (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t vrgather_vx_f16mf4_tuma (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t vrgather_vv_f16mf2_tuma (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t vrgather_vx_f16mf2_tuma (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t vrgather_vv_f16m1_tuma (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t vrgather_vx_f16m1_tuma (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t vrgather_vv_f16m2_tuma (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t vrgather_vx_f16m2_tuma (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t vrgather_vv_f16m4_tuma (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t vrgather_vx_f16m4_tuma (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t vrgather_vv_f16m8_tuma (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t vrgather_vx_f16m8_tuma (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t vrgather_vv_f32mf2_tuma (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t vrgather_vx_f32mf2_tuma (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t vrgather_vv_f32m1_tuma (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t vrgather_vx_f32m1_tuma (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t vrgather_vv_f32m2_tuma (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t vrgather_vx_f32m2_tuma (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t vrgather_vv_f32m4_tuma (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t vrgather_vx_f32m4_tuma (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t vrgather_vv_f32m8_tuma (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t vrgather_vx_f32m8_tuma (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t vrgather_vv_f64m1_tuma (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t vrgather_vx_f64m1_tuma (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t vrgather_vv_f64m2_tuma (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t vrgather_vx_f64m2_tuma (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t vrgather_vv_f64m4_tuma (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t vrgather_vx_f64m4_tuma (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t vrgather_vv_f64m8_tuma (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t vrgather_vx_f64m8_tuma (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, size_t index, size_t vl);
vint8mf8_t vrgatherei16_vv_i8mf8_tuma (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t vrgatherei16_vv_i8mf4_tuma (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t vrgatherei16_vv_i8mf2_tuma (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t vrgatherei16_vv_i8m1_tuma (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t vrgatherei16_vv_i8m2_tuma (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t vrgatherei16_vv_i8m4_tuma (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t vrgatherei16_vv_i16mf4_tuma (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t vrgatherei16_vv_i16mf2_tuma (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t vrgatherei16_vv_i16m1_tuma (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t vrgatherei16_vv_i16m2_tuma (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t vrgatherei16_vv_i16m4_tuma (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t vrgatherei16_vv_i16m8_tuma (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t vrgatherei16_vv_i32mf2_tuma (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t vrgatherei16_vv_i32m1_tuma (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t vrgatherei16_vv_i32m2_tuma (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t vrgatherei16_vv_i32m4_tuma (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t vrgatherei16_vv_i32m8_tuma (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t vrgatherei16_vv_i64m1_tuma (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t vrgatherei16_vv_i64m2_tuma (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t vrgatherei16_vv_i64m4_tuma (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t vrgatherei16_vv_i64m8_tuma (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t vrgatherei16_vv_u8mf8_tuma (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t vrgatherei16_vv_u8mf4_tuma (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t vrgatherei16_vv_u8mf2_tuma (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t vrgatherei16_vv_u8m1_tuma (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t vrgatherei16_vv_u8m2_tuma (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t vrgatherei16_vv_u8m4_tuma (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t vrgatherei16_vv_u16mf4_tuma (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t vrgatherei16_vv_u16mf2_tuma (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t vrgatherei16_vv_u16m1_tuma (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t vrgatherei16_vv_u16m2_tuma (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t vrgatherei16_vv_u16m4_tuma (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t vrgatherei16_vv_u16m8_tuma (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t vrgatherei16_vv_u32mf2_tuma (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t vrgatherei16_vv_u32m1_tuma (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t vrgatherei16_vv_u32m2_tuma (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t vrgatherei16_vv_u32m4_tuma (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t vrgatherei16_vv_u32m8_tuma (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t vrgatherei16_vv_u64m1_tuma (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t vrgatherei16_vv_u64m2_tuma (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t vrgatherei16_vv_u64m4_tuma (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t vrgatherei16_vv_u64m8_tuma (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
vfloat16mf4_t vrgatherei16_vv_f16mf4_tuma (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t vrgatherei16_vv_f16mf2_tuma (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t vrgatherei16_vv_f16m1_tuma (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t vrgatherei16_vv_f16m2_tuma (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t vrgatherei16_vv_f16m4_tuma (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t vrgatherei16_vv_f16m8_tuma (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t vrgatherei16_vv_f32mf2_tuma (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t vrgatherei16_vv_f32m1_tuma (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t vrgatherei16_vv_f32m2_tuma (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t vrgatherei16_vv_f32m4_tuma (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t vrgatherei16_vv_f32m8_tuma (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t vrgatherei16_vv_f64m1_tuma (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t vrgatherei16_vv_f64m2_tuma (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t vrgatherei16_vv_f64m4_tuma (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t vrgatherei16_vv_f64m8_tuma (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vint8mf8_t vrgather_vv_i8mf8_tumu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t vrgather_vx_i8mf8_tumu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t vrgather_vv_i8mf4_tumu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t vrgather_vx_i8mf4_tumu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t vrgather_vv_i8mf2_tumu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t vrgather_vx_i8mf2_tumu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t vrgather_vv_i8m1_tumu (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t vrgather_vx_i8m1_tumu (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t vrgather_vv_i8m2_tumu (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t vrgather_vx_i8m2_tumu (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t vrgather_vv_i8m4_tumu (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t vrgather_vx_i8m4_tumu (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t vrgather_vv_i8m8_tumu (vbool1_t mask, vint8m8_t merge, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t vrgather_vx_i8m8_tumu (vbool1_t mask, vint8m8_t merge, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t vrgather_vv_i16mf4_tumu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t vrgather_vx_i16mf4_tumu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t vrgather_vv_i16mf2_tumu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t vrgather_vx_i16mf2_tumu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t vrgather_vv_i16m1_tumu (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t vrgather_vx_i16m1_tumu (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t vrgather_vv_i16m2_tumu (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t vrgather_vx_i16m2_tumu (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t vrgather_vv_i16m4_tumu (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t vrgather_vx_i16m4_tumu (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t vrgather_vv_i16m8_tumu (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t vrgather_vx_i16m8_tumu (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t vrgather_vv_i32mf2_tumu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t vrgather_vx_i32mf2_tumu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t vrgather_vv_i32m1_tumu (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t vrgather_vx_i32m1_tumu (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t vrgather_vv_i32m2_tumu (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t vrgather_vx_i32m2_tumu (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t vrgather_vv_i32m4_tumu (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t vrgather_vx_i32m4_tumu (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t vrgather_vv_i32m8_tumu (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t vrgather_vx_i32m8_tumu (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t vrgather_vv_i64m1_tumu (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t vrgather_vx_i64m1_tumu (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t vrgather_vv_i64m2_tumu (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t vrgather_vx_i64m2_tumu (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t vrgather_vv_i64m4_tumu (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t vrgather_vx_i64m4_tumu (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t vrgather_vv_i64m8_tumu (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t vrgather_vx_i64m8_tumu (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t vrgather_vv_u8mf8_tumu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t vrgather_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t vrgather_vv_u8mf4_tumu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t vrgather_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t vrgather_vv_u8mf2_tumu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t vrgather_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t vrgather_vv_u8m1_tumu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t vrgather_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t vrgather_vv_u8m2_tumu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t vrgather_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t vrgather_vv_u8m4_tumu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t vrgather_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t vrgather_vv_u8m8_tumu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t vrgather_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t vrgather_vv_u16mf4_tumu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t vrgather_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t vrgather_vv_u16mf2_tumu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t vrgather_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t vrgather_vv_u16m1_tumu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t vrgather_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t vrgather_vv_u16m2_tumu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t vrgather_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t vrgather_vv_u16m4_tumu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t vrgather_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t vrgather_vv_u16m8_tumu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t vrgather_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t vrgather_vv_u32mf2_tumu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t vrgather_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t vrgather_vv_u32m1_tumu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t vrgather_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t vrgather_vv_u32m2_tumu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t vrgather_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t vrgather_vv_u32m4_tumu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t vrgather_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t vrgather_vv_u32m8_tumu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t vrgather_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t vrgather_vv_u64m1_tumu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t vrgather_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t vrgather_vv_u64m2_tumu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t vrgather_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t vrgather_vv_u64m4_tumu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t vrgather_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t vrgather_vv_u64m8_tumu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t vrgather_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t vrgather_vv_f16mf4_tumu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t vrgather_vx_f16mf4_tumu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t vrgather_vv_f16mf2_tumu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t vrgather_vx_f16mf2_tumu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t vrgather_vv_f16m1_tumu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t vrgather_vx_f16m1_tumu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t vrgather_vv_f16m2_tumu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t vrgather_vx_f16m2_tumu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t vrgather_vv_f16m4_tumu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t vrgather_vx_f16m4_tumu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t vrgather_vv_f16m8_tumu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t vrgather_vx_f16m8_tumu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t vrgather_vv_f32mf2_tumu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t vrgather_vx_f32mf2_tumu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t vrgather_vv_f32m1_tumu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t vrgather_vx_f32m1_tumu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t vrgather_vv_f32m2_tumu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t vrgather_vx_f32m2_tumu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t vrgather_vv_f32m4_tumu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t vrgather_vx_f32m4_tumu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t vrgather_vv_f32m8_tumu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t vrgather_vx_f32m8_tumu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t vrgather_vv_f64m1_tumu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t vrgather_vx_f64m1_tumu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t vrgather_vv_f64m2_tumu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t vrgather_vx_f64m2_tumu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t vrgather_vv_f64m4_tumu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t vrgather_vx_f64m4_tumu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t vrgather_vv_f64m8_tumu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t vrgather_vx_f64m8_tumu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, size_t index, size_t vl);
vint8mf8_t vrgatherei16_vv_i8mf8_tumu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t vrgatherei16_vv_i8mf4_tumu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t vrgatherei16_vv_i8mf2_tumu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t vrgatherei16_vv_i8m1_tumu (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t vrgatherei16_vv_i8m2_tumu (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t vrgatherei16_vv_i8m4_tumu (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t vrgatherei16_vv_i16mf4_tumu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t vrgatherei16_vv_i16mf2_tumu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t vrgatherei16_vv_i16m1_tumu (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t vrgatherei16_vv_i16m2_tumu (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t vrgatherei16_vv_i16m4_tumu (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t vrgatherei16_vv_i16m8_tumu (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t vrgatherei16_vv_i32mf2_tumu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t vrgatherei16_vv_i32m1_tumu (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t vrgatherei16_vv_i32m2_tumu (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t vrgatherei16_vv_i32m4_tumu (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t vrgatherei16_vv_i32m8_tumu (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t vrgatherei16_vv_i64m1_tumu (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t vrgatherei16_vv_i64m2_tumu (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t vrgatherei16_vv_i64m4_tumu (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t vrgatherei16_vv_i64m8_tumu (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t vrgatherei16_vv_u8mf8_tumu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t vrgatherei16_vv_u8mf4_tumu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t vrgatherei16_vv_u8mf2_tumu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t vrgatherei16_vv_u8m1_tumu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t vrgatherei16_vv_u8m2_tumu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t vrgatherei16_vv_u8m4_tumu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t vrgatherei16_vv_u16mf4_tumu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t vrgatherei16_vv_u16mf2_tumu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t vrgatherei16_vv_u16m1_tumu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t vrgatherei16_vv_u16m2_tumu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t vrgatherei16_vv_u16m4_tumu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t vrgatherei16_vv_u16m8_tumu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t vrgatherei16_vv_u32mf2_tumu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t vrgatherei16_vv_u32m1_tumu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t vrgatherei16_vv_u32m2_tumu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t vrgatherei16_vv_u32m4_tumu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t vrgatherei16_vv_u32m8_tumu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t vrgatherei16_vv_u64m1_tumu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t vrgatherei16_vv_u64m2_tumu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t vrgatherei16_vv_u64m4_tumu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t vrgatherei16_vv_u64m8_tumu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
vfloat16mf4_t vrgatherei16_vv_f16mf4_tumu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t vrgatherei16_vv_f16mf2_tumu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t vrgatherei16_vv_f16m1_tumu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t vrgatherei16_vv_f16m2_tumu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t vrgatherei16_vv_f16m4_tumu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t vrgatherei16_vv_f16m8_tumu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t vrgatherei16_vv_f32mf2_tumu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t vrgatherei16_vv_f32m1_tumu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t vrgatherei16_vv_f32m2_tumu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t vrgatherei16_vv_f32m4_tumu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t vrgatherei16_vv_f32m8_tumu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t vrgatherei16_vv_f64m1_tumu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t vrgatherei16_vv_f64m2_tumu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t vrgatherei16_vv_f64m4_tumu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t vrgatherei16_vv_f64m8_tumu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vint8mf8_t vrgather_vv_i8mf8_tama (vbool64_t mask, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t vrgather_vx_i8mf8_tama (vbool64_t mask, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t vrgather_vv_i8mf4_tama (vbool32_t mask, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t vrgather_vx_i8mf4_tama (vbool32_t mask, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t vrgather_vv_i8mf2_tama (vbool16_t mask, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t vrgather_vx_i8mf2_tama (vbool16_t mask, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t vrgather_vv_i8m1_tama (vbool8_t mask, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t vrgather_vx_i8m1_tama (vbool8_t mask, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t vrgather_vv_i8m2_tama (vbool4_t mask, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t vrgather_vx_i8m2_tama (vbool4_t mask, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t vrgather_vv_i8m4_tama (vbool2_t mask, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t vrgather_vx_i8m4_tama (vbool2_t mask, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t vrgather_vv_i8m8_tama (vbool1_t mask, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t vrgather_vx_i8m8_tama (vbool1_t mask, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t vrgather_vv_i16mf4_tama (vbool64_t mask, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t vrgather_vx_i16mf4_tama (vbool64_t mask, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t vrgather_vv_i16mf2_tama (vbool32_t mask, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t vrgather_vx_i16mf2_tama (vbool32_t mask, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t vrgather_vv_i16m1_tama (vbool16_t mask, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t vrgather_vx_i16m1_tama (vbool16_t mask, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t vrgather_vv_i16m2_tama (vbool8_t mask, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t vrgather_vx_i16m2_tama (vbool8_t mask, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t vrgather_vv_i16m4_tama (vbool4_t mask, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t vrgather_vx_i16m4_tama (vbool4_t mask, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t vrgather_vv_i16m8_tama (vbool2_t mask, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t vrgather_vx_i16m8_tama (vbool2_t mask, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t vrgather_vv_i32mf2_tama (vbool64_t mask, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t vrgather_vx_i32mf2_tama (vbool64_t mask, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t vrgather_vv_i32m1_tama (vbool32_t mask, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t vrgather_vx_i32m1_tama (vbool32_t mask, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t vrgather_vv_i32m2_tama (vbool16_t mask, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t vrgather_vx_i32m2_tama (vbool16_t mask, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t vrgather_vv_i32m4_tama (vbool8_t mask, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t vrgather_vx_i32m4_tama (vbool8_t mask, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t vrgather_vv_i32m8_tama (vbool4_t mask, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t vrgather_vx_i32m8_tama (vbool4_t mask, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t vrgather_vv_i64m1_tama (vbool64_t mask, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t vrgather_vx_i64m1_tama (vbool64_t mask, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t vrgather_vv_i64m2_tama (vbool32_t mask, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t vrgather_vx_i64m2_tama (vbool32_t mask, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t vrgather_vv_i64m4_tama (vbool16_t mask, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t vrgather_vx_i64m4_tama (vbool16_t mask, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t vrgather_vv_i64m8_tama (vbool8_t mask, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t vrgather_vx_i64m8_tama (vbool8_t mask, vint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t vrgather_vv_u8mf8_tama (vbool64_t mask, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t vrgather_vx_u8mf8_tama (vbool64_t mask, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t vrgather_vv_u8mf4_tama (vbool32_t mask, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t vrgather_vx_u8mf4_tama (vbool32_t mask, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t vrgather_vv_u8mf2_tama (vbool16_t mask, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t vrgather_vx_u8mf2_tama (vbool16_t mask, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t vrgather_vv_u8m1_tama (vbool8_t mask, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t vrgather_vx_u8m1_tama (vbool8_t mask, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t vrgather_vv_u8m2_tama (vbool4_t mask, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t vrgather_vx_u8m2_tama (vbool4_t mask, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t vrgather_vv_u8m4_tama (vbool2_t mask, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t vrgather_vx_u8m4_tama (vbool2_t mask, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t vrgather_vv_u8m8_tama (vbool1_t mask, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t vrgather_vx_u8m8_tama (vbool1_t mask, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t vrgather_vv_u16mf4_tama (vbool64_t mask, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t vrgather_vx_u16mf4_tama (vbool64_t mask, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t vrgather_vv_u16mf2_tama (vbool32_t mask, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t vrgather_vx_u16mf2_tama (vbool32_t mask, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t vrgather_vv_u16m1_tama (vbool16_t mask, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t vrgather_vx_u16m1_tama (vbool16_t mask, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t vrgather_vv_u16m2_tama (vbool8_t mask, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t vrgather_vx_u16m2_tama (vbool8_t mask, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t vrgather_vv_u16m4_tama (vbool4_t mask, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t vrgather_vx_u16m4_tama (vbool4_t mask, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t vrgather_vv_u16m8_tama (vbool2_t mask, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t vrgather_vx_u16m8_tama (vbool2_t mask, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t vrgather_vv_u32mf2_tama (vbool64_t mask, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t vrgather_vx_u32mf2_tama (vbool64_t mask, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t vrgather_vv_u32m1_tama (vbool32_t mask, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t vrgather_vx_u32m1_tama (vbool32_t mask, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t vrgather_vv_u32m2_tama (vbool16_t mask, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t vrgather_vx_u32m2_tama (vbool16_t mask, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t vrgather_vv_u32m4_tama (vbool8_t mask, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t vrgather_vx_u32m4_tama (vbool8_t mask, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t vrgather_vv_u32m8_tama (vbool4_t mask, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t vrgather_vx_u32m8_tama (vbool4_t mask, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t vrgather_vv_u64m1_tama (vbool64_t mask, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t vrgather_vx_u64m1_tama (vbool64_t mask, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t vrgather_vv_u64m2_tama (vbool32_t mask, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t vrgather_vx_u64m2_tama (vbool32_t mask, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t vrgather_vv_u64m4_tama (vbool16_t mask, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t vrgather_vx_u64m4_tama (vbool16_t mask, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t vrgather_vv_u64m8_tama (vbool8_t mask, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t vrgather_vx_u64m8_tama (vbool8_t mask, vuint64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t vrgather_vv_f16mf4_tama (vbool64_t mask, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t vrgather_vx_f16mf4_tama (vbool64_t mask, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t vrgather_vv_f16mf2_tama (vbool32_t mask, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t vrgather_vx_f16mf2_tama (vbool32_t mask, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t vrgather_vv_f16m1_tama (vbool16_t mask, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t vrgather_vx_f16m1_tama (vbool16_t mask, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t vrgather_vv_f16m2_tama (vbool8_t mask, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t vrgather_vx_f16m2_tama (vbool8_t mask, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t vrgather_vv_f16m4_tama (vbool4_t mask, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t vrgather_vx_f16m4_tama (vbool4_t mask, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t vrgather_vv_f16m8_tama (vbool2_t mask, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t vrgather_vx_f16m8_tama (vbool2_t mask, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t vrgather_vv_f32mf2_tama (vbool64_t mask, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t vrgather_vx_f32mf2_tama (vbool64_t mask, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t vrgather_vv_f32m1_tama (vbool32_t mask, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t vrgather_vx_f32m1_tama (vbool32_t mask, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t vrgather_vv_f32m2_tama (vbool16_t mask, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t vrgather_vx_f32m2_tama (vbool16_t mask, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t vrgather_vv_f32m4_tama (vbool8_t mask, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t vrgather_vx_f32m4_tama (vbool8_t mask, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t vrgather_vv_f32m8_tama (vbool4_t mask, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t vrgather_vx_f32m8_tama (vbool4_t mask, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t vrgather_vv_f64m1_tama (vbool64_t mask, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t vrgather_vx_f64m1_tama (vbool64_t mask, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t vrgather_vv_f64m2_tama (vbool32_t mask, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t vrgather_vx_f64m2_tama (vbool32_t mask, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t vrgather_vv_f64m4_tama (vbool16_t mask, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t vrgather_vx_f64m4_tama (vbool16_t mask, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t vrgather_vv_f64m8_tama (vbool8_t mask, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t vrgather_vx_f64m8_tama (vbool8_t mask, vfloat64m8_t op1, size_t index, size_t vl);
vint8mf8_t vrgatherei16_vv_i8mf8_tama (vbool64_t mask, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t vrgatherei16_vv_i8mf4_tama (vbool32_t mask, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t vrgatherei16_vv_i8mf2_tama (vbool16_t mask, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t vrgatherei16_vv_i8m1_tama (vbool8_t mask, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t vrgatherei16_vv_i8m2_tama (vbool4_t mask, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t vrgatherei16_vv_i8m4_tama (vbool2_t mask, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t vrgatherei16_vv_i16mf4_tama (vbool64_t mask, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t vrgatherei16_vv_i16mf2_tama (vbool32_t mask, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t vrgatherei16_vv_i16m1_tama (vbool16_t mask, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t vrgatherei16_vv_i16m2_tama (vbool8_t mask, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t vrgatherei16_vv_i16m4_tama (vbool4_t mask, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t vrgatherei16_vv_i16m8_tama (vbool2_t mask, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t vrgatherei16_vv_i32mf2_tama (vbool64_t mask, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t vrgatherei16_vv_i32m1_tama (vbool32_t mask, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t vrgatherei16_vv_i32m2_tama (vbool16_t mask, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t vrgatherei16_vv_i32m4_tama (vbool8_t mask, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t vrgatherei16_vv_i32m8_tama (vbool4_t mask, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t vrgatherei16_vv_i64m1_tama (vbool64_t mask, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t vrgatherei16_vv_i64m2_tama (vbool32_t mask, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t vrgatherei16_vv_i64m4_tama (vbool16_t mask, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t vrgatherei16_vv_i64m8_tama (vbool8_t mask, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t vrgatherei16_vv_u8mf8_tama (vbool64_t mask, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t vrgatherei16_vv_u8mf4_tama (vbool32_t mask, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t vrgatherei16_vv_u8mf2_tama (vbool16_t mask, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t vrgatherei16_vv_u8m1_tama (vbool8_t mask, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t vrgatherei16_vv_u8m2_tama (vbool4_t mask, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t vrgatherei16_vv_u8m4_tama (vbool2_t mask, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t vrgatherei16_vv_u16mf4_tama (vbool64_t mask, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t vrgatherei16_vv_u16mf2_tama (vbool32_t mask, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t vrgatherei16_vv_u16m1_tama (vbool16_t mask, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t vrgatherei16_vv_u16m2_tama (vbool8_t mask, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t vrgatherei16_vv_u16m4_tama (vbool4_t mask, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t vrgatherei16_vv_u16m8_tama (vbool2_t mask, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t vrgatherei16_vv_u32mf2_tama (vbool64_t mask, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t vrgatherei16_vv_u32m1_tama (vbool32_t mask, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t vrgatherei16_vv_u32m2_tama (vbool16_t mask, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t vrgatherei16_vv_u32m4_tama (vbool8_t mask, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t vrgatherei16_vv_u32m8_tama (vbool4_t mask, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t vrgatherei16_vv_u64m1_tama (vbool64_t mask, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t vrgatherei16_vv_u64m2_tama (vbool32_t mask, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t vrgatherei16_vv_u64m4_tama (vbool16_t mask, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t vrgatherei16_vv_u64m8_tama (vbool8_t mask, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
vfloat16mf4_t vrgatherei16_vv_f16mf4_tama (vbool64_t mask, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t vrgatherei16_vv_f16mf2_tama (vbool32_t mask, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t vrgatherei16_vv_f16m1_tama (vbool16_t mask, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t vrgatherei16_vv_f16m2_tama (vbool8_t mask, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t vrgatherei16_vv_f16m4_tama (vbool4_t mask, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t vrgatherei16_vv_f16m8_tama (vbool2_t mask, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t vrgatherei16_vv_f32mf2_tama (vbool64_t mask, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t vrgatherei16_vv_f32m1_tama (vbool32_t mask, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t vrgatherei16_vv_f32m2_tama (vbool16_t mask, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t vrgatherei16_vv_f32m4_tama (vbool8_t mask, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t vrgatherei16_vv_f32m8_tama (vbool4_t mask, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t vrgatherei16_vv_f64m1_tama (vbool64_t mask, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t vrgatherei16_vv_f64m2_tama (vbool32_t mask, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t vrgatherei16_vv_f64m4_tama (vbool16_t mask, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t vrgatherei16_vv_f64m8_tama (vbool8_t mask, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vint8mf8_t vrgather_vv_i8mf8_tamu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t vrgather_vx_i8mf8_tamu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t vrgather_vv_i8mf4_tamu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t vrgather_vx_i8mf4_tamu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t vrgather_vv_i8mf2_tamu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t vrgather_vx_i8mf2_tamu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t vrgather_vv_i8m1_tamu (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t vrgather_vx_i8m1_tamu (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t vrgather_vv_i8m2_tamu (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t vrgather_vx_i8m2_tamu (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t vrgather_vv_i8m4_tamu (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t vrgather_vx_i8m4_tamu (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t vrgather_vv_i8m8_tamu (vbool1_t mask, vint8m8_t merge, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t vrgather_vx_i8m8_tamu (vbool1_t mask, vint8m8_t merge, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t vrgather_vv_i16mf4_tamu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t vrgather_vx_i16mf4_tamu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t vrgather_vv_i16mf2_tamu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t vrgather_vx_i16mf2_tamu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t vrgather_vv_i16m1_tamu (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t vrgather_vx_i16m1_tamu (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t vrgather_vv_i16m2_tamu (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t vrgather_vx_i16m2_tamu (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t vrgather_vv_i16m4_tamu (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t vrgather_vx_i16m4_tamu (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t vrgather_vv_i16m8_tamu (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t vrgather_vx_i16m8_tamu (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t vrgather_vv_i32mf2_tamu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t vrgather_vx_i32mf2_tamu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t vrgather_vv_i32m1_tamu (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t vrgather_vx_i32m1_tamu (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t vrgather_vv_i32m2_tamu (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t vrgather_vx_i32m2_tamu (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t vrgather_vv_i32m4_tamu (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t vrgather_vx_i32m4_tamu (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t vrgather_vv_i32m8_tamu (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t vrgather_vx_i32m8_tamu (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t vrgather_vv_i64m1_tamu (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t vrgather_vx_i64m1_tamu (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t vrgather_vv_i64m2_tamu (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t vrgather_vx_i64m2_tamu (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t vrgather_vv_i64m4_tamu (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t vrgather_vx_i64m4_tamu (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t vrgather_vv_i64m8_tamu (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t vrgather_vx_i64m8_tamu (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t vrgather_vv_u8mf8_tamu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t vrgather_vx_u8mf8_tamu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t vrgather_vv_u8mf4_tamu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t vrgather_vx_u8mf4_tamu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t vrgather_vv_u8mf2_tamu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t vrgather_vx_u8mf2_tamu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t vrgather_vv_u8m1_tamu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t vrgather_vx_u8m1_tamu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t vrgather_vv_u8m2_tamu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t vrgather_vx_u8m2_tamu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t vrgather_vv_u8m4_tamu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t vrgather_vx_u8m4_tamu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t vrgather_vv_u8m8_tamu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t vrgather_vx_u8m8_tamu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t vrgather_vv_u16mf4_tamu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t vrgather_vx_u16mf4_tamu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t vrgather_vv_u16mf2_tamu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t vrgather_vx_u16mf2_tamu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t vrgather_vv_u16m1_tamu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t vrgather_vx_u16m1_tamu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t vrgather_vv_u16m2_tamu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t vrgather_vx_u16m2_tamu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t vrgather_vv_u16m4_tamu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t vrgather_vx_u16m4_tamu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t vrgather_vv_u16m8_tamu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t vrgather_vx_u16m8_tamu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t vrgather_vv_u32mf2_tamu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t vrgather_vx_u32mf2_tamu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t vrgather_vv_u32m1_tamu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t vrgather_vx_u32m1_tamu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t vrgather_vv_u32m2_tamu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t vrgather_vx_u32m2_tamu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t vrgather_vv_u32m4_tamu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t vrgather_vx_u32m4_tamu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t vrgather_vv_u32m8_tamu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t vrgather_vx_u32m8_tamu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t vrgather_vv_u64m1_tamu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t vrgather_vx_u64m1_tamu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t vrgather_vv_u64m2_tamu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t vrgather_vx_u64m2_tamu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t vrgather_vv_u64m4_tamu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t vrgather_vx_u64m4_tamu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t vrgather_vv_u64m8_tamu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t vrgather_vx_u64m8_tamu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t vrgather_vv_f16mf4_tamu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t vrgather_vx_f16mf4_tamu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t vrgather_vv_f16mf2_tamu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t vrgather_vx_f16mf2_tamu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t vrgather_vv_f16m1_tamu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t vrgather_vx_f16m1_tamu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t vrgather_vv_f16m2_tamu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t vrgather_vx_f16m2_tamu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t vrgather_vv_f16m4_tamu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t vrgather_vx_f16m4_tamu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t vrgather_vv_f16m8_tamu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t vrgather_vx_f16m8_tamu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t vrgather_vv_f32mf2_tamu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t vrgather_vx_f32mf2_tamu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t vrgather_vv_f32m1_tamu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t vrgather_vx_f32m1_tamu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t vrgather_vv_f32m2_tamu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t vrgather_vx_f32m2_tamu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t vrgather_vv_f32m4_tamu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t vrgather_vx_f32m4_tamu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t vrgather_vv_f32m8_tamu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t vrgather_vx_f32m8_tamu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t vrgather_vv_f64m1_tamu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t vrgather_vx_f64m1_tamu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t vrgather_vv_f64m2_tamu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t vrgather_vx_f64m2_tamu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t vrgather_vv_f64m4_tamu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t vrgather_vx_f64m4_tamu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t vrgather_vv_f64m8_tamu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t vrgather_vx_f64m8_tamu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, size_t index, size_t vl);
vint8mf8_t vrgatherei16_vv_i8mf8_tamu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t vrgatherei16_vv_i8mf4_tamu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t vrgatherei16_vv_i8mf2_tamu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t vrgatherei16_vv_i8m1_tamu (vbool8_t mask, vint8m1_t merge, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t vrgatherei16_vv_i8m2_tamu (vbool4_t mask, vint8m2_t merge, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t vrgatherei16_vv_i8m4_tamu (vbool2_t mask, vint8m4_t merge, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t vrgatherei16_vv_i16mf4_tamu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t vrgatherei16_vv_i16mf2_tamu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t vrgatherei16_vv_i16m1_tamu (vbool16_t mask, vint16m1_t merge, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t vrgatherei16_vv_i16m2_tamu (vbool8_t mask, vint16m2_t merge, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t vrgatherei16_vv_i16m4_tamu (vbool4_t mask, vint16m4_t merge, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t vrgatherei16_vv_i16m8_tamu (vbool2_t mask, vint16m8_t merge, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t vrgatherei16_vv_i32mf2_tamu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t vrgatherei16_vv_i32m1_tamu (vbool32_t mask, vint32m1_t merge, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t vrgatherei16_vv_i32m2_tamu (vbool16_t mask, vint32m2_t merge, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t vrgatherei16_vv_i32m4_tamu (vbool8_t mask, vint32m4_t merge, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t vrgatherei16_vv_i32m8_tamu (vbool4_t mask, vint32m8_t merge, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t vrgatherei16_vv_i64m1_tamu (vbool64_t mask, vint64m1_t merge, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t vrgatherei16_vv_i64m2_tamu (vbool32_t mask, vint64m2_t merge, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t vrgatherei16_vv_i64m4_tamu (vbool16_t mask, vint64m4_t merge, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t vrgatherei16_vv_i64m8_tamu (vbool8_t mask, vint64m8_t merge, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t vrgatherei16_vv_u8mf8_tamu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t vrgatherei16_vv_u8mf4_tamu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t vrgatherei16_vv_u8mf2_tamu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t vrgatherei16_vv_u8m1_tamu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t vrgatherei16_vv_u8m2_tamu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t vrgatherei16_vv_u8m4_tamu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t vrgatherei16_vv_u16mf4_tamu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t vrgatherei16_vv_u16mf2_tamu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t vrgatherei16_vv_u16m1_tamu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t vrgatherei16_vv_u16m2_tamu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t vrgatherei16_vv_u16m4_tamu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t vrgatherei16_vv_u16m8_tamu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t vrgatherei16_vv_u32mf2_tamu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t vrgatherei16_vv_u32m1_tamu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t vrgatherei16_vv_u32m2_tamu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t vrgatherei16_vv_u32m4_tamu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t vrgatherei16_vv_u32m8_tamu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t vrgatherei16_vv_u64m1_tamu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t vrgatherei16_vv_u64m2_tamu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t vrgatherei16_vv_u64m4_tamu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t vrgatherei16_vv_u64m8_tamu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
vfloat16mf4_t vrgatherei16_vv_f16mf4_tamu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t vrgatherei16_vv_f16mf2_tamu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t vrgatherei16_vv_f16m1_tamu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t vrgatherei16_vv_f16m2_tamu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t vrgatherei16_vv_f16m4_tamu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t vrgatherei16_vv_f16m8_tamu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t vrgatherei16_vv_f32mf2_tamu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t vrgatherei16_vv_f32m1_tamu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t vrgatherei16_vv_f32m2_tamu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t vrgatherei16_vv_f32m4_tamu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t vrgatherei16_vv_f32m8_tamu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t vrgatherei16_vv_f64m1_tamu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t vrgatherei16_vv_f64m2_tamu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t vrgatherei16_vv_f64m4_tamu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t vrgatherei16_vv_f64m8_tamu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
```
### [Vector Compress Functions](../rvv-intrinsic-api.md#175-vector-compress-operations):

**Prototypes:**
``` C
vint8mf8_t vcompress_vm_i8mf8_tu (vbool64_t mask, vint8mf8_t merge, vint8mf8_t src, size_t vl);
vint8mf4_t vcompress_vm_i8mf4_tu (vbool32_t mask, vint8mf4_t merge, vint8mf4_t src, size_t vl);
vint8mf2_t vcompress_vm_i8mf2_tu (vbool16_t mask, vint8mf2_t merge, vint8mf2_t src, size_t vl);
vint8m1_t vcompress_vm_i8m1_tu (vbool8_t mask, vint8m1_t merge, vint8m1_t src, size_t vl);
vint8m2_t vcompress_vm_i8m2_tu (vbool4_t mask, vint8m2_t merge, vint8m2_t src, size_t vl);
vint8m4_t vcompress_vm_i8m4_tu (vbool2_t mask, vint8m4_t merge, vint8m4_t src, size_t vl);
vint8m8_t vcompress_vm_i8m8_tu (vbool1_t mask, vint8m8_t merge, vint8m8_t src, size_t vl);
vint16mf4_t vcompress_vm_i16mf4_tu (vbool64_t mask, vint16mf4_t merge, vint16mf4_t src, size_t vl);
vint16mf2_t vcompress_vm_i16mf2_tu (vbool32_t mask, vint16mf2_t merge, vint16mf2_t src, size_t vl);
vint16m1_t vcompress_vm_i16m1_tu (vbool16_t mask, vint16m1_t merge, vint16m1_t src, size_t vl);
vint16m2_t vcompress_vm_i16m2_tu (vbool8_t mask, vint16m2_t merge, vint16m2_t src, size_t vl);
vint16m4_t vcompress_vm_i16m4_tu (vbool4_t mask, vint16m4_t merge, vint16m4_t src, size_t vl);
vint16m8_t vcompress_vm_i16m8_tu (vbool2_t mask, vint16m8_t merge, vint16m8_t src, size_t vl);
vint32mf2_t vcompress_vm_i32mf2_tu (vbool64_t mask, vint32mf2_t merge, vint32mf2_t src, size_t vl);
vint32m1_t vcompress_vm_i32m1_tu (vbool32_t mask, vint32m1_t merge, vint32m1_t src, size_t vl);
vint32m2_t vcompress_vm_i32m2_tu (vbool16_t mask, vint32m2_t merge, vint32m2_t src, size_t vl);
vint32m4_t vcompress_vm_i32m4_tu (vbool8_t mask, vint32m4_t merge, vint32m4_t src, size_t vl);
vint32m8_t vcompress_vm_i32m8_tu (vbool4_t mask, vint32m8_t merge, vint32m8_t src, size_t vl);
vint64m1_t vcompress_vm_i64m1_tu (vbool64_t mask, vint64m1_t merge, vint64m1_t src, size_t vl);
vint64m2_t vcompress_vm_i64m2_tu (vbool32_t mask, vint64m2_t merge, vint64m2_t src, size_t vl);
vint64m4_t vcompress_vm_i64m4_tu (vbool16_t mask, vint64m4_t merge, vint64m4_t src, size_t vl);
vint64m8_t vcompress_vm_i64m8_tu (vbool8_t mask, vint64m8_t merge, vint64m8_t src, size_t vl);
vuint8mf8_t vcompress_vm_u8mf8_tu (vbool64_t mask, vuint8mf8_t merge, vuint8mf8_t src, size_t vl);
vuint8mf4_t vcompress_vm_u8mf4_tu (vbool32_t mask, vuint8mf4_t merge, vuint8mf4_t src, size_t vl);
vuint8mf2_t vcompress_vm_u8mf2_tu (vbool16_t mask, vuint8mf2_t merge, vuint8mf2_t src, size_t vl);
vuint8m1_t vcompress_vm_u8m1_tu (vbool8_t mask, vuint8m1_t merge, vuint8m1_t src, size_t vl);
vuint8m2_t vcompress_vm_u8m2_tu (vbool4_t mask, vuint8m2_t merge, vuint8m2_t src, size_t vl);
vuint8m4_t vcompress_vm_u8m4_tu (vbool2_t mask, vuint8m4_t merge, vuint8m4_t src, size_t vl);
vuint8m8_t vcompress_vm_u8m8_tu (vbool1_t mask, vuint8m8_t merge, vuint8m8_t src, size_t vl);
vuint16mf4_t vcompress_vm_u16mf4_tu (vbool64_t mask, vuint16mf4_t merge, vuint16mf4_t src, size_t vl);
vuint16mf2_t vcompress_vm_u16mf2_tu (vbool32_t mask, vuint16mf2_t merge, vuint16mf2_t src, size_t vl);
vuint16m1_t vcompress_vm_u16m1_tu (vbool16_t mask, vuint16m1_t merge, vuint16m1_t src, size_t vl);
vuint16m2_t vcompress_vm_u16m2_tu (vbool8_t mask, vuint16m2_t merge, vuint16m2_t src, size_t vl);
vuint16m4_t vcompress_vm_u16m4_tu (vbool4_t mask, vuint16m4_t merge, vuint16m4_t src, size_t vl);
vuint16m8_t vcompress_vm_u16m8_tu (vbool2_t mask, vuint16m8_t merge, vuint16m8_t src, size_t vl);
vuint32mf2_t vcompress_vm_u32mf2_tu (vbool64_t mask, vuint32mf2_t merge, vuint32mf2_t src, size_t vl);
vuint32m1_t vcompress_vm_u32m1_tu (vbool32_t mask, vuint32m1_t merge, vuint32m1_t src, size_t vl);
vuint32m2_t vcompress_vm_u32m2_tu (vbool16_t mask, vuint32m2_t merge, vuint32m2_t src, size_t vl);
vuint32m4_t vcompress_vm_u32m4_tu (vbool8_t mask, vuint32m4_t merge, vuint32m4_t src, size_t vl);
vuint32m8_t vcompress_vm_u32m8_tu (vbool4_t mask, vuint32m8_t merge, vuint32m8_t src, size_t vl);
vuint64m1_t vcompress_vm_u64m1_tu (vbool64_t mask, vuint64m1_t merge, vuint64m1_t src, size_t vl);
vuint64m2_t vcompress_vm_u64m2_tu (vbool32_t mask, vuint64m2_t merge, vuint64m2_t src, size_t vl);
vuint64m4_t vcompress_vm_u64m4_tu (vbool16_t mask, vuint64m4_t merge, vuint64m4_t src, size_t vl);
vuint64m8_t vcompress_vm_u64m8_tu (vbool8_t mask, vuint64m8_t merge, vuint64m8_t src, size_t vl);
vfloat16mf4_t vcompress_vm_f16mf4_tu (vbool64_t mask, vfloat16mf4_t merge, vfloat16mf4_t src, size_t vl);
vfloat16mf2_t vcompress_vm_f16mf2_tu (vbool32_t mask, vfloat16mf2_t merge, vfloat16mf2_t src, size_t vl);
vfloat16m1_t vcompress_vm_f16m1_tu (vbool16_t mask, vfloat16m1_t merge, vfloat16m1_t src, size_t vl);
vfloat16m2_t vcompress_vm_f16m2_tu (vbool8_t mask, vfloat16m2_t merge, vfloat16m2_t src, size_t vl);
vfloat16m4_t vcompress_vm_f16m4_tu (vbool4_t mask, vfloat16m4_t merge, vfloat16m4_t src, size_t vl);
vfloat16m8_t vcompress_vm_f16m8_tu (vbool2_t mask, vfloat16m8_t merge, vfloat16m8_t src, size_t vl);
vfloat32mf2_t vcompress_vm_f32mf2_tu (vbool64_t mask, vfloat32mf2_t merge, vfloat32mf2_t src, size_t vl);
vfloat32m1_t vcompress_vm_f32m1_tu (vbool32_t mask, vfloat32m1_t merge, vfloat32m1_t src, size_t vl);
vfloat32m2_t vcompress_vm_f32m2_tu (vbool16_t mask, vfloat32m2_t merge, vfloat32m2_t src, size_t vl);
vfloat32m4_t vcompress_vm_f32m4_tu (vbool8_t mask, vfloat32m4_t merge, vfloat32m4_t src, size_t vl);
vfloat32m8_t vcompress_vm_f32m8_tu (vbool4_t mask, vfloat32m8_t merge, vfloat32m8_t src, size_t vl);
vfloat64m1_t vcompress_vm_f64m1_tu (vbool64_t mask, vfloat64m1_t merge, vfloat64m1_t src, size_t vl);
vfloat64m2_t vcompress_vm_f64m2_tu (vbool32_t mask, vfloat64m2_t merge, vfloat64m2_t src, size_t vl);
vfloat64m4_t vcompress_vm_f64m4_tu (vbool16_t mask, vfloat64m4_t merge, vfloat64m4_t src, size_t vl);
vfloat64m8_t vcompress_vm_f64m8_tu (vbool8_t mask, vfloat64m8_t merge, vfloat64m8_t src, size_t vl);
vint8mf8_t vcompress_vm_i8mf8_ta (vbool64_t mask, vint8mf8_t src, size_t vl);
vint8mf4_t vcompress_vm_i8mf4_ta (vbool32_t mask, vint8mf4_t src, size_t vl);
vint8mf2_t vcompress_vm_i8mf2_ta (vbool16_t mask, vint8mf2_t src, size_t vl);
vint8m1_t vcompress_vm_i8m1_ta (vbool8_t mask, vint8m1_t src, size_t vl);
vint8m2_t vcompress_vm_i8m2_ta (vbool4_t mask, vint8m2_t src, size_t vl);
vint8m4_t vcompress_vm_i8m4_ta (vbool2_t mask, vint8m4_t src, size_t vl);
vint8m8_t vcompress_vm_i8m8_ta (vbool1_t mask, vint8m8_t src, size_t vl);
vint16mf4_t vcompress_vm_i16mf4_ta (vbool64_t mask, vint16mf4_t src, size_t vl);
vint16mf2_t vcompress_vm_i16mf2_ta (vbool32_t mask, vint16mf2_t src, size_t vl);
vint16m1_t vcompress_vm_i16m1_ta (vbool16_t mask, vint16m1_t src, size_t vl);
vint16m2_t vcompress_vm_i16m2_ta (vbool8_t mask, vint16m2_t src, size_t vl);
vint16m4_t vcompress_vm_i16m4_ta (vbool4_t mask, vint16m4_t src, size_t vl);
vint16m8_t vcompress_vm_i16m8_ta (vbool2_t mask, vint16m8_t src, size_t vl);
vint32mf2_t vcompress_vm_i32mf2_ta (vbool64_t mask, vint32mf2_t src, size_t vl);
vint32m1_t vcompress_vm_i32m1_ta (vbool32_t mask, vint32m1_t src, size_t vl);
vint32m2_t vcompress_vm_i32m2_ta (vbool16_t mask, vint32m2_t src, size_t vl);
vint32m4_t vcompress_vm_i32m4_ta (vbool8_t mask, vint32m4_t src, size_t vl);
vint32m8_t vcompress_vm_i32m8_ta (vbool4_t mask, vint32m8_t src, size_t vl);
vint64m1_t vcompress_vm_i64m1_ta (vbool64_t mask, vint64m1_t src, size_t vl);
vint64m2_t vcompress_vm_i64m2_ta (vbool32_t mask, vint64m2_t src, size_t vl);
vint64m4_t vcompress_vm_i64m4_ta (vbool16_t mask, vint64m4_t src, size_t vl);
vint64m8_t vcompress_vm_i64m8_ta (vbool8_t mask, vint64m8_t src, size_t vl);
vuint8mf8_t vcompress_vm_u8mf8_ta (vbool64_t mask, vuint8mf8_t src, size_t vl);
vuint8mf4_t vcompress_vm_u8mf4_ta (vbool32_t mask, vuint8mf4_t src, size_t vl);
vuint8mf2_t vcompress_vm_u8mf2_ta (vbool16_t mask, vuint8mf2_t src, size_t vl);
vuint8m1_t vcompress_vm_u8m1_ta (vbool8_t mask, vuint8m1_t src, size_t vl);
vuint8m2_t vcompress_vm_u8m2_ta (vbool4_t mask, vuint8m2_t src, size_t vl);
vuint8m4_t vcompress_vm_u8m4_ta (vbool2_t mask, vuint8m4_t src, size_t vl);
vuint8m8_t vcompress_vm_u8m8_ta (vbool1_t mask, vuint8m8_t src, size_t vl);
vuint16mf4_t vcompress_vm_u16mf4_ta (vbool64_t mask, vuint16mf4_t src, size_t vl);
vuint16mf2_t vcompress_vm_u16mf2_ta (vbool32_t mask, vuint16mf2_t src, size_t vl);
vuint16m1_t vcompress_vm_u16m1_ta (vbool16_t mask, vuint16m1_t src, size_t vl);
vuint16m2_t vcompress_vm_u16m2_ta (vbool8_t mask, vuint16m2_t src, size_t vl);
vuint16m4_t vcompress_vm_u16m4_ta (vbool4_t mask, vuint16m4_t src, size_t vl);
vuint16m8_t vcompress_vm_u16m8_ta (vbool2_t mask, vuint16m8_t src, size_t vl);
vuint32mf2_t vcompress_vm_u32mf2_ta (vbool64_t mask, vuint32mf2_t src, size_t vl);
vuint32m1_t vcompress_vm_u32m1_ta (vbool32_t mask, vuint32m1_t src, size_t vl);
vuint32m2_t vcompress_vm_u32m2_ta (vbool16_t mask, vuint32m2_t src, size_t vl);
vuint32m4_t vcompress_vm_u32m4_ta (vbool8_t mask, vuint32m4_t src, size_t vl);
vuint32m8_t vcompress_vm_u32m8_ta (vbool4_t mask, vuint32m8_t src, size_t vl);
vuint64m1_t vcompress_vm_u64m1_ta (vbool64_t mask, vuint64m1_t src, size_t vl);
vuint64m2_t vcompress_vm_u64m2_ta (vbool32_t mask, vuint64m2_t src, size_t vl);
vuint64m4_t vcompress_vm_u64m4_ta (vbool16_t mask, vuint64m4_t src, size_t vl);
vuint64m8_t vcompress_vm_u64m8_ta (vbool8_t mask, vuint64m8_t src, size_t vl);
vfloat16mf4_t vcompress_vm_f16mf4_ta (vbool64_t mask, vfloat16mf4_t src, size_t vl);
vfloat16mf2_t vcompress_vm_f16mf2_ta (vbool32_t mask, vfloat16mf2_t src, size_t vl);
vfloat16m1_t vcompress_vm_f16m1_ta (vbool16_t mask, vfloat16m1_t src, size_t vl);
vfloat16m2_t vcompress_vm_f16m2_ta (vbool8_t mask, vfloat16m2_t src, size_t vl);
vfloat16m4_t vcompress_vm_f16m4_ta (vbool4_t mask, vfloat16m4_t src, size_t vl);
vfloat16m8_t vcompress_vm_f16m8_ta (vbool2_t mask, vfloat16m8_t src, size_t vl);
vfloat32mf2_t vcompress_vm_f32mf2_ta (vbool64_t mask, vfloat32mf2_t src, size_t vl);
vfloat32m1_t vcompress_vm_f32m1_ta (vbool32_t mask, vfloat32m1_t src, size_t vl);
vfloat32m2_t vcompress_vm_f32m2_ta (vbool16_t mask, vfloat32m2_t src, size_t vl);
vfloat32m4_t vcompress_vm_f32m4_ta (vbool8_t mask, vfloat32m4_t src, size_t vl);
vfloat32m8_t vcompress_vm_f32m8_ta (vbool4_t mask, vfloat32m8_t src, size_t vl);
vfloat64m1_t vcompress_vm_f64m1_ta (vbool64_t mask, vfloat64m1_t src, size_t vl);
vfloat64m2_t vcompress_vm_f64m2_ta (vbool32_t mask, vfloat64m2_t src, size_t vl);
vfloat64m4_t vcompress_vm_f64m4_ta (vbool16_t mask, vfloat64m4_t src, size_t vl);
vfloat64m8_t vcompress_vm_f64m8_ta (vbool8_t mask, vfloat64m8_t src, size_t vl);
```