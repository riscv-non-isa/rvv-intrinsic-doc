
== Vector Permutation Instructions

[[overloaded-integer-scalar-move]]
=== Integer and Floating-Point Scalar Move Intrinsics

``` C
float16_t __riscv_vfmv_f (vfloat16mf4_t src);
vfloat16mf4_t __riscv_vfmv_s_f_f16mf4 (float16_t src, size_t vl);
float16_t __riscv_vfmv_f (vfloat16mf2_t src);
vfloat16mf2_t __riscv_vfmv_s_f_f16mf2 (float16_t src, size_t vl);
float16_t __riscv_vfmv_f (vfloat16m1_t src);
vfloat16m1_t __riscv_vfmv_s_f_f16m1 (float16_t src, size_t vl);
float16_t __riscv_vfmv_f (vfloat16m2_t src);
vfloat16m2_t __riscv_vfmv_s_f_f16m2 (float16_t src, size_t vl);
float16_t __riscv_vfmv_f (vfloat16m4_t src);
vfloat16m4_t __riscv_vfmv_s_f_f16m4 (float16_t src, size_t vl);
float16_t __riscv_vfmv_f (vfloat16m8_t src);
vfloat16m8_t __riscv_vfmv_s_f_f16m8 (float16_t src, size_t vl);
float32_t __riscv_vfmv_f (vfloat32mf2_t src);
vfloat32mf2_t __riscv_vfmv_s_f_f32mf2 (float32_t src, size_t vl);
float32_t __riscv_vfmv_f (vfloat32m1_t src);
vfloat32m1_t __riscv_vfmv_s_f_f32m1 (float32_t src, size_t vl);
float32_t __riscv_vfmv_f (vfloat32m2_t src);
vfloat32m2_t __riscv_vfmv_s_f_f32m2 (float32_t src, size_t vl);
float32_t __riscv_vfmv_f (vfloat32m4_t src);
vfloat32m4_t __riscv_vfmv_s_f_f32m4 (float32_t src, size_t vl);
float32_t __riscv_vfmv_f (vfloat32m8_t src);
vfloat32m8_t __riscv_vfmv_s_f_f32m8 (float32_t src, size_t vl);
float64_t __riscv_vfmv_f (vfloat64m1_t src);
vfloat64m1_t __riscv_vfmv_s_f_f64m1 (float64_t src, size_t vl);
float64_t __riscv_vfmv_f (vfloat64m2_t src);
vfloat64m2_t __riscv_vfmv_s_f_f64m2 (float64_t src, size_t vl);
float64_t __riscv_vfmv_f (vfloat64m4_t src);
vfloat64m4_t __riscv_vfmv_s_f_f64m4 (float64_t src, size_t vl);
float64_t __riscv_vfmv_f (vfloat64m8_t src);
vfloat64m8_t __riscv_vfmv_s_f_f64m8 (float64_t src, size_t vl);
int8_t __riscv_vmv_x (vint8mf8_t src);
vint8mf8_t __riscv_vmv_s_x_i8mf8 (int8_t src, size_t vl);
int8_t __riscv_vmv_x (vint8mf4_t src);
vint8mf4_t __riscv_vmv_s_x_i8mf4 (int8_t src, size_t vl);
int8_t __riscv_vmv_x (vint8mf2_t src);
vint8mf2_t __riscv_vmv_s_x_i8mf2 (int8_t src, size_t vl);
int8_t __riscv_vmv_x (vint8m1_t src);
vint8m1_t __riscv_vmv_s_x_i8m1 (int8_t src, size_t vl);
int8_t __riscv_vmv_x (vint8m2_t src);
vint8m2_t __riscv_vmv_s_x_i8m2 (int8_t src, size_t vl);
int8_t __riscv_vmv_x (vint8m4_t src);
vint8m4_t __riscv_vmv_s_x_i8m4 (int8_t src, size_t vl);
int8_t __riscv_vmv_x (vint8m8_t src);
vint8m8_t __riscv_vmv_s_x_i8m8 (int8_t src, size_t vl);
int16_t __riscv_vmv_x (vint16mf4_t src);
vint16mf4_t __riscv_vmv_s_x_i16mf4 (int16_t src, size_t vl);
int16_t __riscv_vmv_x (vint16mf2_t src);
vint16mf2_t __riscv_vmv_s_x_i16mf2 (int16_t src, size_t vl);
int16_t __riscv_vmv_x (vint16m1_t src);
vint16m1_t __riscv_vmv_s_x_i16m1 (int16_t src, size_t vl);
int16_t __riscv_vmv_x (vint16m2_t src);
vint16m2_t __riscv_vmv_s_x_i16m2 (int16_t src, size_t vl);
int16_t __riscv_vmv_x (vint16m4_t src);
vint16m4_t __riscv_vmv_s_x_i16m4 (int16_t src, size_t vl);
int16_t __riscv_vmv_x (vint16m8_t src);
vint16m8_t __riscv_vmv_s_x_i16m8 (int16_t src, size_t vl);
int32_t __riscv_vmv_x (vint32mf2_t src);
vint32mf2_t __riscv_vmv_s_x_i32mf2 (int32_t src, size_t vl);
int32_t __riscv_vmv_x (vint32m1_t src);
vint32m1_t __riscv_vmv_s_x_i32m1 (int32_t src, size_t vl);
int32_t __riscv_vmv_x (vint32m2_t src);
vint32m2_t __riscv_vmv_s_x_i32m2 (int32_t src, size_t vl);
int32_t __riscv_vmv_x (vint32m4_t src);
vint32m4_t __riscv_vmv_s_x_i32m4 (int32_t src, size_t vl);
int32_t __riscv_vmv_x (vint32m8_t src);
vint32m8_t __riscv_vmv_s_x_i32m8 (int32_t src, size_t vl);
int64_t __riscv_vmv_x (vint64m1_t src);
vint64m1_t __riscv_vmv_s_x_i64m1 (int64_t src, size_t vl);
int64_t __riscv_vmv_x (vint64m2_t src);
vint64m2_t __riscv_vmv_s_x_i64m2 (int64_t src, size_t vl);
int64_t __riscv_vmv_x (vint64m4_t src);
vint64m4_t __riscv_vmv_s_x_i64m4 (int64_t src, size_t vl);
int64_t __riscv_vmv_x (vint64m8_t src);
vint64m8_t __riscv_vmv_s_x_i64m8 (int64_t src, size_t vl);
uint8_t __riscv_vmv_x (vuint8mf8_t src);
vuint8mf8_t __riscv_vmv_s_x_u8mf8 (uint8_t src, size_t vl);
uint8_t __riscv_vmv_x (vuint8mf4_t src);
vuint8mf4_t __riscv_vmv_s_x_u8mf4 (uint8_t src, size_t vl);
uint8_t __riscv_vmv_x (vuint8mf2_t src);
vuint8mf2_t __riscv_vmv_s_x_u8mf2 (uint8_t src, size_t vl);
uint8_t __riscv_vmv_x (vuint8m1_t src);
vuint8m1_t __riscv_vmv_s_x_u8m1 (uint8_t src, size_t vl);
uint8_t __riscv_vmv_x (vuint8m2_t src);
vuint8m2_t __riscv_vmv_s_x_u8m2 (uint8_t src, size_t vl);
uint8_t __riscv_vmv_x (vuint8m4_t src);
vuint8m4_t __riscv_vmv_s_x_u8m4 (uint8_t src, size_t vl);
uint8_t __riscv_vmv_x (vuint8m8_t src);
vuint8m8_t __riscv_vmv_s_x_u8m8 (uint8_t src, size_t vl);
uint16_t __riscv_vmv_x (vuint16mf4_t src);
vuint16mf4_t __riscv_vmv_s_x_u16mf4 (uint16_t src, size_t vl);
uint16_t __riscv_vmv_x (vuint16mf2_t src);
vuint16mf2_t __riscv_vmv_s_x_u16mf2 (uint16_t src, size_t vl);
uint16_t __riscv_vmv_x (vuint16m1_t src);
vuint16m1_t __riscv_vmv_s_x_u16m1 (uint16_t src, size_t vl);
uint16_t __riscv_vmv_x (vuint16m2_t src);
vuint16m2_t __riscv_vmv_s_x_u16m2 (uint16_t src, size_t vl);
uint16_t __riscv_vmv_x (vuint16m4_t src);
vuint16m4_t __riscv_vmv_s_x_u16m4 (uint16_t src, size_t vl);
uint16_t __riscv_vmv_x (vuint16m8_t src);
vuint16m8_t __riscv_vmv_s_x_u16m8 (uint16_t src, size_t vl);
uint32_t __riscv_vmv_x (vuint32mf2_t src);
vuint32mf2_t __riscv_vmv_s_x_u32mf2 (uint32_t src, size_t vl);
uint32_t __riscv_vmv_x (vuint32m1_t src);
vuint32m1_t __riscv_vmv_s_x_u32m1 (uint32_t src, size_t vl);
uint32_t __riscv_vmv_x (vuint32m2_t src);
vuint32m2_t __riscv_vmv_s_x_u32m2 (uint32_t src, size_t vl);
uint32_t __riscv_vmv_x (vuint32m4_t src);
vuint32m4_t __riscv_vmv_s_x_u32m4 (uint32_t src, size_t vl);
uint32_t __riscv_vmv_x (vuint32m8_t src);
vuint32m8_t __riscv_vmv_s_x_u32m8 (uint32_t src, size_t vl);
uint64_t __riscv_vmv_x (vuint64m1_t src);
vuint64m1_t __riscv_vmv_s_x_u64m1 (uint64_t src, size_t vl);
uint64_t __riscv_vmv_x (vuint64m2_t src);
vuint64m2_t __riscv_vmv_s_x_u64m2 (uint64_t src, size_t vl);
uint64_t __riscv_vmv_x (vuint64m4_t src);
vuint64m4_t __riscv_vmv_s_x_u64m4 (uint64_t src, size_t vl);
uint64_t __riscv_vmv_x (vuint64m8_t src);
vuint64m8_t __riscv_vmv_s_x_u64m8 (uint64_t src, size_t vl);
```

[[overloaded-vector-slideup]]
=== Vector Slideup Intrinsics

``` C
vfloat16mf4_t __riscv_vslideup (vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslideup (vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslideup (vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslideup (vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslideup (vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslideup (vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslideup (vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslideup (vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslideup (vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslideup (vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslideup (vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslideup (vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslideup (vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslideup (vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslideup (vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslideup (vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslideup (vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslideup (vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslideup (vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslideup (vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslideup (vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslideup (vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslideup (vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslideup (vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslideup (vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslideup (vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslideup (vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslideup (vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslideup (vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslideup (vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslideup (vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslideup (vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslideup (vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslideup (vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslideup (vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslideup (vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslideup (vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslideup (vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslideup (vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslideup (vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslideup (vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslideup (vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslideup (vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslideup (vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslideup (vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslideup (vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslideup (vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslideup (vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslideup (vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslideup (vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslideup (vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslideup (vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslideup (vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslideup (vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslideup (vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslideup (vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslideup (vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslideup (vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslideup (vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslideup (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslideup (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslideup (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslideup (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslideup (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslideup (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslideup (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslideup (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslideup (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslideup (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslideup (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslideup (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslideup (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslideup (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslideup (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslideup (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslideup (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslideup (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslideup (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslideup (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslideup (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslideup (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslideup (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslideup (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslideup (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslideup (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslideup (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslideup (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslideup (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslideup (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslideup (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslideup (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslideup (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslideup (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslideup (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslideup (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslideup (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslideup (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslideup (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslideup (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslideup (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslideup (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslideup (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslideup (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslideup (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslideup (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslideup (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslideup (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslideup (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslideup (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslideup (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslideup (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslideup (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslideup (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslideup (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslideup (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslideup (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslideup (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
```

[[overloaded-vector-slidedown]]
=== Vector Slidedown Intrinsics

``` C
vfloat16mf4_t __riscv_vslidedown (vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslidedown (vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslidedown (vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslidedown (vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslidedown (vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslidedown (vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslidedown (vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslidedown (vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslidedown (vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslidedown (vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslidedown (vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslidedown (vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslidedown (vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslidedown (vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslidedown (vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslidedown (vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslidedown (vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslidedown (vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslidedown (vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslidedown (vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslidedown (vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslidedown (vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslidedown (vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslidedown (vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslidedown (vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslidedown (vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslidedown (vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslidedown (vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslidedown (vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslidedown (vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslidedown (vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslidedown (vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslidedown (vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslidedown (vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslidedown (vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslidedown (vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslidedown (vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslidedown (vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslidedown (vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslidedown (vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslidedown (vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslidedown (vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslidedown (vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslidedown (vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslidedown (vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslidedown (vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslidedown (vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslidedown (vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslidedown (vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslidedown (vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslidedown (vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslidedown (vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslidedown (vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslidedown (vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslidedown (vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslidedown (vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslidedown (vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslidedown (vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslidedown (vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown (vbool64_t mask, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslidedown (vbool32_t mask, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslidedown (vbool16_t mask, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslidedown (vbool8_t mask, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslidedown (vbool4_t mask, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslidedown (vbool2_t mask, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslidedown (vbool64_t mask, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslidedown (vbool32_t mask, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslidedown (vbool16_t mask, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslidedown (vbool8_t mask, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslidedown (vbool4_t mask, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslidedown (vbool64_t mask, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslidedown (vbool32_t mask, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslidedown (vbool16_t mask, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslidedown (vbool8_t mask, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslidedown (vbool64_t mask, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslidedown (vbool32_t mask, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslidedown (vbool16_t mask, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslidedown (vbool8_t mask, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslidedown (vbool4_t mask, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslidedown (vbool2_t mask, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslidedown (vbool1_t mask, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslidedown (vbool64_t mask, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslidedown (vbool32_t mask, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslidedown (vbool16_t mask, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslidedown (vbool8_t mask, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslidedown (vbool4_t mask, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslidedown (vbool2_t mask, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslidedown (vbool64_t mask, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslidedown (vbool32_t mask, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslidedown (vbool16_t mask, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslidedown (vbool8_t mask, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslidedown (vbool4_t mask, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslidedown (vbool64_t mask, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslidedown (vbool32_t mask, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslidedown (vbool16_t mask, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslidedown (vbool8_t mask, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslidedown (vbool64_t mask, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslidedown (vbool32_t mask, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslidedown (vbool16_t mask, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslidedown (vbool8_t mask, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslidedown (vbool4_t mask, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslidedown (vbool2_t mask, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslidedown (vbool1_t mask, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslidedown (vbool64_t mask, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslidedown (vbool32_t mask, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslidedown (vbool16_t mask, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslidedown (vbool8_t mask, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslidedown (vbool4_t mask, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslidedown (vbool2_t mask, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslidedown (vbool64_t mask, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslidedown (vbool32_t mask, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslidedown (vbool16_t mask, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslidedown (vbool8_t mask, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslidedown (vbool4_t mask, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslidedown (vbool64_t mask, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslidedown (vbool32_t mask, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslidedown (vbool16_t mask, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslidedown (vbool8_t mask, vuint64m8_t src, size_t offset, size_t vl);
```

[[overloaded-vector-slide1up-and-slide1down]]
=== Vector Slide1up and Slide1down Intrinsics

``` C
vfloat16mf4_t __riscv_vfslide1up (vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1up (vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1up (vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1up (vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1up (vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1up (vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1up (vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1up (vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1up (vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1up (vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1up (vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1up (vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1up (vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1up (vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1up (vfloat64m8_t src, float64_t value, size_t vl);
vfloat16mf4_t __riscv_vfslide1down (vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1down (vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1down (vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1down (vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1down (vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1down (vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1down (vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1down (vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1down (vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1down (vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1down (vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1down (vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1down (vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1down (vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1down (vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t __riscv_vslide1up (vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1up (vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1up (vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1up (vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1up (vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1up (vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1up (vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1up (vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1up (vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1up (vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1up (vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1up (vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1up (vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1up (vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1up (vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1up (vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1up (vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1up (vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1up (vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1up (vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1up (vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1up (vint64m8_t src, int64_t value, size_t vl);
vint8mf8_t __riscv_vslide1down (vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1down (vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1down (vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1down (vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1down (vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1down (vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1down (vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1down (vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1down (vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1down (vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1down (vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1down (vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1down (vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1down (vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1down (vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1down (vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1down (vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1down (vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1down (vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1down (vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1down (vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1down (vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1up (vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1up (vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1up (vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1up (vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1up (vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1up (vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1up (vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1up (vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1up (vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1up (vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1up (vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1up (vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1up (vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1up (vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1up (vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1up (vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1up (vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1up (vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1up (vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1up (vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1up (vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1up (vuint64m8_t src, uint64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1down (vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1down (vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1down (vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1down (vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1down (vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1down (vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1down (vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1down (vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1down (vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1down (vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1down (vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1down (vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1down (vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1down (vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1down (vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1down (vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1down (vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1down (vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1down (vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1down (vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1down (vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1down (vuint64m8_t src, uint64_t value, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up (vbool64_t mask, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1up (vbool32_t mask, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1up (vbool16_t mask, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1up (vbool8_t mask, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1up (vbool4_t mask, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1up (vbool2_t mask, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1up (vbool64_t mask, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1up (vbool32_t mask, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1up (vbool16_t mask, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1up (vbool8_t mask, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1up (vbool4_t mask, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1up (vbool64_t mask, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1up (vbool32_t mask, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1up (vbool16_t mask, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1up (vbool8_t mask, vfloat64m8_t src, float64_t value, size_t vl);
vfloat16mf4_t __riscv_vfslide1down (vbool64_t mask, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1down (vbool32_t mask, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1down (vbool16_t mask, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1down (vbool8_t mask, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1down (vbool4_t mask, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1down (vbool2_t mask, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1down (vbool64_t mask, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1down (vbool32_t mask, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1down (vbool16_t mask, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1down (vbool8_t mask, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1down (vbool4_t mask, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1down (vbool64_t mask, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1down (vbool32_t mask, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1down (vbool16_t mask, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1down (vbool8_t mask, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t __riscv_vslide1up (vbool64_t mask, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1up (vbool32_t mask, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1up (vbool16_t mask, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1up (vbool8_t mask, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1up (vbool4_t mask, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1up (vbool2_t mask, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1up (vbool1_t mask, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1up (vbool64_t mask, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1up (vbool32_t mask, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1up (vbool16_t mask, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1up (vbool8_t mask, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1up (vbool4_t mask, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1up (vbool2_t mask, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1up (vbool64_t mask, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1up (vbool32_t mask, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1up (vbool16_t mask, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1up (vbool8_t mask, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1up (vbool4_t mask, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1up (vbool64_t mask, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1up (vbool32_t mask, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1up (vbool16_t mask, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1up (vbool8_t mask, vint64m8_t src, int64_t value, size_t vl);
vint8mf8_t __riscv_vslide1down (vbool64_t mask, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1down (vbool32_t mask, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1down (vbool16_t mask, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1down (vbool8_t mask, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1down (vbool4_t mask, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1down (vbool2_t mask, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1down (vbool1_t mask, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1down (vbool64_t mask, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1down (vbool32_t mask, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1down (vbool16_t mask, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1down (vbool8_t mask, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1down (vbool4_t mask, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1down (vbool2_t mask, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1down (vbool64_t mask, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1down (vbool32_t mask, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1down (vbool16_t mask, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1down (vbool8_t mask, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1down (vbool4_t mask, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1down (vbool64_t mask, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1down (vbool32_t mask, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1down (vbool16_t mask, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1down (vbool8_t mask, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1up (vbool64_t mask, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1up (vbool32_t mask, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1up (vbool16_t mask, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1up (vbool8_t mask, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1up (vbool4_t mask, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1up (vbool2_t mask, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1up (vbool1_t mask, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1up (vbool64_t mask, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1up (vbool32_t mask, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1up (vbool16_t mask, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1up (vbool8_t mask, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1up (vbool4_t mask, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1up (vbool2_t mask, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1up (vbool64_t mask, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1up (vbool32_t mask, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1up (vbool16_t mask, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1up (vbool8_t mask, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1up (vbool4_t mask, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1up (vbool64_t mask, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1up (vbool32_t mask, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1up (vbool16_t mask, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1up (vbool8_t mask, vuint64m8_t src, uint64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1down (vbool64_t mask, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1down (vbool32_t mask, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1down (vbool16_t mask, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1down (vbool8_t mask, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1down (vbool4_t mask, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1down (vbool2_t mask, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1down (vbool1_t mask, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1down (vbool64_t mask, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1down (vbool32_t mask, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1down (vbool16_t mask, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1down (vbool8_t mask, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1down (vbool4_t mask, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1down (vbool2_t mask, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1down (vbool64_t mask, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1down (vbool32_t mask, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1down (vbool16_t mask, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1down (vbool8_t mask, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1down (vbool4_t mask, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1down (vbool64_t mask, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1down (vbool32_t mask, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1down (vbool16_t mask, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1down (vbool8_t mask, vuint64m8_t src, uint64_t value, size_t vl);
```

[[overloaded-vector-register-gather]]
=== Vector Register Gather Intrinsics

``` C
vfloat16mf4_t __riscv_vrgather (vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t __riscv_vrgather (vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather (vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather (vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t __riscv_vrgather (vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t __riscv_vrgather (vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t __riscv_vrgather (vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t __riscv_vrgather (vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t __riscv_vrgather (vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t __riscv_vrgather (vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t __riscv_vrgather (vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t __riscv_vrgather (vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather (vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather (vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t __riscv_vrgather (vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t __riscv_vrgather (vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t __riscv_vrgather (vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t __riscv_vrgather (vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t __riscv_vrgather (vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t __riscv_vrgather (vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t __riscv_vrgather (vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t __riscv_vrgather (vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t __riscv_vrgather (vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t __riscv_vrgather (vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t __riscv_vrgather (vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t __riscv_vrgather (vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t __riscv_vrgather (vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t __riscv_vrgather (vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t __riscv_vrgather (vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t __riscv_vrgather (vfloat64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t __riscv_vrgatherei16 (vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t __riscv_vrgatherei16 (vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t __riscv_vrgatherei16 (vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t __riscv_vrgatherei16 (vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t __riscv_vrgatherei16 (vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t __riscv_vrgatherei16 (vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t __riscv_vrgatherei16 (vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t __riscv_vrgatherei16 (vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t __riscv_vrgatherei16 (vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t __riscv_vrgatherei16 (vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t __riscv_vrgatherei16 (vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t __riscv_vrgatherei16 (vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t __riscv_vrgatherei16 (vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t __riscv_vrgatherei16 (vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t __riscv_vrgatherei16 (vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
vint8mf8_t __riscv_vrgather (vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t __riscv_vrgather (vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t __riscv_vrgather (vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t __riscv_vrgather (vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t __riscv_vrgather (vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t __riscv_vrgather (vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t __riscv_vrgather (vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t __riscv_vrgather (vint8m1_t op1, size_t index, size_t vl);
vint8m2_t __riscv_vrgather (vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t __riscv_vrgather (vint8m2_t op1, size_t index, size_t vl);
vint8m4_t __riscv_vrgather (vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t __riscv_vrgather (vint8m4_t op1, size_t index, size_t vl);
vint8m8_t __riscv_vrgather (vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t __riscv_vrgather (vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t __riscv_vrgather (vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t __riscv_vrgather (vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t __riscv_vrgather (vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t __riscv_vrgather (vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t __riscv_vrgather (vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t __riscv_vrgather (vint16m1_t op1, size_t index, size_t vl);
vint16m2_t __riscv_vrgather (vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t __riscv_vrgather (vint16m2_t op1, size_t index, size_t vl);
vint16m4_t __riscv_vrgather (vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t __riscv_vrgather (vint16m4_t op1, size_t index, size_t vl);
vint16m8_t __riscv_vrgather (vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t __riscv_vrgather (vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t __riscv_vrgather (vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t __riscv_vrgather (vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t __riscv_vrgather (vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t __riscv_vrgather (vint32m1_t op1, size_t index, size_t vl);
vint32m2_t __riscv_vrgather (vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t __riscv_vrgather (vint32m2_t op1, size_t index, size_t vl);
vint32m4_t __riscv_vrgather (vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t __riscv_vrgather (vint32m4_t op1, size_t index, size_t vl);
vint32m8_t __riscv_vrgather (vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t __riscv_vrgather (vint32m8_t op1, size_t index, size_t vl);
vint64m1_t __riscv_vrgather (vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t __riscv_vrgather (vint64m1_t op1, size_t index, size_t vl);
vint64m2_t __riscv_vrgather (vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t __riscv_vrgather (vint64m2_t op1, size_t index, size_t vl);
vint64m4_t __riscv_vrgather (vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t __riscv_vrgather (vint64m4_t op1, size_t index, size_t vl);
vint64m8_t __riscv_vrgather (vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t __riscv_vrgather (vint64m8_t op1, size_t index, size_t vl);
vint8mf8_t __riscv_vrgatherei16 (vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t __riscv_vrgatherei16 (vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t __riscv_vrgatherei16 (vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t __riscv_vrgatherei16 (vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t __riscv_vrgatherei16 (vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t __riscv_vrgatherei16 (vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t __riscv_vrgatherei16 (vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t __riscv_vrgatherei16 (vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t __riscv_vrgatherei16 (vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t __riscv_vrgatherei16 (vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t __riscv_vrgatherei16 (vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t __riscv_vrgatherei16 (vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t __riscv_vrgatherei16 (vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t __riscv_vrgatherei16 (vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t __riscv_vrgatherei16 (vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t __riscv_vrgatherei16 (vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t __riscv_vrgatherei16 (vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t __riscv_vrgatherei16 (vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t __riscv_vrgatherei16 (vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t __riscv_vrgatherei16 (vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t __riscv_vrgatherei16 (vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t __riscv_vrgather (vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t __riscv_vrgather (vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t __riscv_vrgather (vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t __riscv_vrgather (vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t __riscv_vrgather (vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t __riscv_vrgather (vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t __riscv_vrgather (vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t __riscv_vrgather (vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t __riscv_vrgather (vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t __riscv_vrgather (vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t __riscv_vrgather (vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t __riscv_vrgather (vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t __riscv_vrgather (vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t __riscv_vrgather (vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t __riscv_vrgather (vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t __riscv_vrgather (vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t __riscv_vrgather (vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t __riscv_vrgather (vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t __riscv_vrgather (vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t __riscv_vrgather (vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t __riscv_vrgather (vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t __riscv_vrgather (vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t __riscv_vrgather (vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t __riscv_vrgather (vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t __riscv_vrgather (vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t __riscv_vrgather (vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t __riscv_vrgather (vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t __riscv_vrgather (vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t __riscv_vrgather (vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t __riscv_vrgather (vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t __riscv_vrgather (vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t __riscv_vrgather (vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t __riscv_vrgather (vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t __riscv_vrgather (vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t __riscv_vrgather (vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t __riscv_vrgather (vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t __riscv_vrgather (vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t __riscv_vrgather (vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t __riscv_vrgather (vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t __riscv_vrgather (vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t __riscv_vrgather (vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t __riscv_vrgather (vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t __riscv_vrgather (vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t __riscv_vrgather (vuint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t __riscv_vrgatherei16 (vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t __riscv_vrgatherei16 (vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t __riscv_vrgatherei16 (vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t __riscv_vrgatherei16 (vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t __riscv_vrgatherei16 (vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t __riscv_vrgatherei16 (vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t __riscv_vrgatherei16 (vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t __riscv_vrgatherei16 (vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t __riscv_vrgatherei16 (vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t __riscv_vrgatherei16 (vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t __riscv_vrgatherei16 (vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t __riscv_vrgatherei16 (vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t __riscv_vrgatherei16 (vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t __riscv_vrgatherei16 (vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t __riscv_vrgatherei16 (vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t __riscv_vrgatherei16 (vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t __riscv_vrgatherei16 (vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t __riscv_vrgatherei16 (vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t __riscv_vrgatherei16 (vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t __riscv_vrgatherei16 (vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t __riscv_vrgatherei16 (vuint64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather (vbool64_t mask, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t __riscv_vrgather (vbool64_t mask, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather (vbool32_t mask, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather (vbool32_t mask, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t __riscv_vrgather (vbool16_t mask, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t __riscv_vrgather (vbool16_t mask, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t __riscv_vrgather (vbool8_t mask, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t __riscv_vrgather (vbool8_t mask, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t __riscv_vrgather (vbool4_t mask, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t __riscv_vrgather (vbool4_t mask, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t __riscv_vrgather (vbool2_t mask, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t __riscv_vrgather (vbool2_t mask, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather (vbool64_t mask, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather (vbool64_t mask, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t __riscv_vrgather (vbool32_t mask, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t __riscv_vrgather (vbool32_t mask, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t __riscv_vrgather (vbool16_t mask, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t __riscv_vrgather (vbool16_t mask, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t __riscv_vrgather (vbool8_t mask, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t __riscv_vrgather (vbool8_t mask, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t __riscv_vrgather (vbool4_t mask, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t __riscv_vrgather (vbool4_t mask, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t __riscv_vrgather (vbool64_t mask, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t __riscv_vrgather (vbool64_t mask, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t __riscv_vrgather (vbool32_t mask, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t __riscv_vrgather (vbool32_t mask, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t __riscv_vrgather (vbool16_t mask, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t __riscv_vrgather (vbool16_t mask, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t __riscv_vrgather (vbool8_t mask, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t __riscv_vrgather (vbool8_t mask, vfloat64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t __riscv_vrgatherei16 (vbool64_t mask, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t __riscv_vrgatherei16 (vbool32_t mask, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t __riscv_vrgatherei16 (vbool16_t mask, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t __riscv_vrgatherei16 (vbool8_t mask, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t __riscv_vrgatherei16 (vbool4_t mask, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t __riscv_vrgatherei16 (vbool2_t mask, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t __riscv_vrgatherei16 (vbool64_t mask, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t __riscv_vrgatherei16 (vbool32_t mask, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t __riscv_vrgatherei16 (vbool16_t mask, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t __riscv_vrgatherei16 (vbool8_t mask, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t __riscv_vrgatherei16 (vbool4_t mask, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t __riscv_vrgatherei16 (vbool64_t mask, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t __riscv_vrgatherei16 (vbool32_t mask, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t __riscv_vrgatherei16 (vbool16_t mask, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t __riscv_vrgatherei16 (vbool8_t mask, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
vint8mf8_t __riscv_vrgather (vbool64_t mask, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t __riscv_vrgather (vbool64_t mask, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t __riscv_vrgather (vbool32_t mask, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t __riscv_vrgather (vbool32_t mask, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t __riscv_vrgather (vbool16_t mask, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t __riscv_vrgather (vbool16_t mask, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t __riscv_vrgather (vbool8_t mask, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t __riscv_vrgather (vbool8_t mask, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t __riscv_vrgather (vbool4_t mask, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t __riscv_vrgather (vbool4_t mask, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t __riscv_vrgather (vbool2_t mask, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t __riscv_vrgather (vbool2_t mask, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t __riscv_vrgather (vbool1_t mask, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t __riscv_vrgather (vbool1_t mask, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t __riscv_vrgather (vbool64_t mask, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t __riscv_vrgather (vbool64_t mask, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t __riscv_vrgather (vbool32_t mask, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t __riscv_vrgather (vbool32_t mask, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t __riscv_vrgather (vbool16_t mask, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t __riscv_vrgather (vbool16_t mask, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t __riscv_vrgather (vbool8_t mask, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t __riscv_vrgather (vbool8_t mask, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t __riscv_vrgather (vbool4_t mask, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t __riscv_vrgather (vbool4_t mask, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t __riscv_vrgather (vbool2_t mask, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t __riscv_vrgather (vbool2_t mask, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t __riscv_vrgather (vbool64_t mask, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t __riscv_vrgather (vbool64_t mask, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t __riscv_vrgather (vbool32_t mask, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t __riscv_vrgather (vbool32_t mask, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t __riscv_vrgather (vbool16_t mask, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t __riscv_vrgather (vbool16_t mask, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t __riscv_vrgather (vbool8_t mask, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t __riscv_vrgather (vbool8_t mask, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t __riscv_vrgather (vbool4_t mask, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t __riscv_vrgather (vbool4_t mask, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t __riscv_vrgather (vbool64_t mask, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t __riscv_vrgather (vbool64_t mask, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t __riscv_vrgather (vbool32_t mask, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t __riscv_vrgather (vbool32_t mask, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t __riscv_vrgather (vbool16_t mask, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t __riscv_vrgather (vbool16_t mask, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t __riscv_vrgather (vbool8_t mask, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t __riscv_vrgather (vbool8_t mask, vint64m8_t op1, size_t index, size_t vl);
vint8mf8_t __riscv_vrgatherei16 (vbool64_t mask, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t __riscv_vrgatherei16 (vbool32_t mask, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t __riscv_vrgatherei16 (vbool16_t mask, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t __riscv_vrgatherei16 (vbool8_t mask, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t __riscv_vrgatherei16 (vbool4_t mask, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t __riscv_vrgatherei16 (vbool2_t mask, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t __riscv_vrgatherei16 (vbool64_t mask, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t __riscv_vrgatherei16 (vbool32_t mask, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t __riscv_vrgatherei16 (vbool16_t mask, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t __riscv_vrgatherei16 (vbool8_t mask, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t __riscv_vrgatherei16 (vbool4_t mask, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t __riscv_vrgatherei16 (vbool2_t mask, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t __riscv_vrgatherei16 (vbool64_t mask, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t __riscv_vrgatherei16 (vbool32_t mask, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t __riscv_vrgatherei16 (vbool16_t mask, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t __riscv_vrgatherei16 (vbool8_t mask, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t __riscv_vrgatherei16 (vbool4_t mask, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t __riscv_vrgatherei16 (vbool64_t mask, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t __riscv_vrgatherei16 (vbool32_t mask, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t __riscv_vrgatherei16 (vbool16_t mask, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t __riscv_vrgatherei16 (vbool8_t mask, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t __riscv_vrgather (vbool64_t mask, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t __riscv_vrgather (vbool64_t mask, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t __riscv_vrgather (vbool32_t mask, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t __riscv_vrgather (vbool32_t mask, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t __riscv_vrgather (vbool16_t mask, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t __riscv_vrgather (vbool16_t mask, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t __riscv_vrgather (vbool8_t mask, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t __riscv_vrgather (vbool8_t mask, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t __riscv_vrgather (vbool4_t mask, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t __riscv_vrgather (vbool4_t mask, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t __riscv_vrgather (vbool2_t mask, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t __riscv_vrgather (vbool2_t mask, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t __riscv_vrgather (vbool1_t mask, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t __riscv_vrgather (vbool1_t mask, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t __riscv_vrgather (vbool64_t mask, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t __riscv_vrgather (vbool64_t mask, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t __riscv_vrgather (vbool32_t mask, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t __riscv_vrgather (vbool32_t mask, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t __riscv_vrgather (vbool16_t mask, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t __riscv_vrgather (vbool16_t mask, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t __riscv_vrgather (vbool8_t mask, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t __riscv_vrgather (vbool8_t mask, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t __riscv_vrgather (vbool4_t mask, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t __riscv_vrgather (vbool4_t mask, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t __riscv_vrgather (vbool2_t mask, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t __riscv_vrgather (vbool2_t mask, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t __riscv_vrgather (vbool64_t mask, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t __riscv_vrgather (vbool64_t mask, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t __riscv_vrgather (vbool32_t mask, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t __riscv_vrgather (vbool32_t mask, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t __riscv_vrgather (vbool16_t mask, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t __riscv_vrgather (vbool16_t mask, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t __riscv_vrgather (vbool8_t mask, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t __riscv_vrgather (vbool8_t mask, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t __riscv_vrgather (vbool4_t mask, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t __riscv_vrgather (vbool4_t mask, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t __riscv_vrgather (vbool64_t mask, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t __riscv_vrgather (vbool64_t mask, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t __riscv_vrgather (vbool32_t mask, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t __riscv_vrgather (vbool32_t mask, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t __riscv_vrgather (vbool16_t mask, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t __riscv_vrgather (vbool16_t mask, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t __riscv_vrgather (vbool8_t mask, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t __riscv_vrgather (vbool8_t mask, vuint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t __riscv_vrgatherei16 (vbool64_t mask, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t __riscv_vrgatherei16 (vbool32_t mask, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t __riscv_vrgatherei16 (vbool16_t mask, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t __riscv_vrgatherei16 (vbool8_t mask, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t __riscv_vrgatherei16 (vbool4_t mask, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t __riscv_vrgatherei16 (vbool2_t mask, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t __riscv_vrgatherei16 (vbool64_t mask, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t __riscv_vrgatherei16 (vbool32_t mask, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t __riscv_vrgatherei16 (vbool16_t mask, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t __riscv_vrgatherei16 (vbool8_t mask, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t __riscv_vrgatherei16 (vbool4_t mask, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t __riscv_vrgatherei16 (vbool2_t mask, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t __riscv_vrgatherei16 (vbool64_t mask, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t __riscv_vrgatherei16 (vbool32_t mask, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t __riscv_vrgatherei16 (vbool16_t mask, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t __riscv_vrgatherei16 (vbool8_t mask, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t __riscv_vrgatherei16 (vbool4_t mask, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t __riscv_vrgatherei16 (vbool64_t mask, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t __riscv_vrgatherei16 (vbool32_t mask, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t __riscv_vrgatherei16 (vbool16_t mask, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t __riscv_vrgatherei16 (vbool8_t mask, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
```

[[overloaded-vector-compress]]
=== Vector Compress Intrinsics

``` C
vfloat16mf4_t __riscv_vcompress (vfloat16mf4_t src, vbool64_t mask, size_t vl);
vfloat16mf2_t __riscv_vcompress (vfloat16mf2_t src, vbool32_t mask, size_t vl);
vfloat16m1_t __riscv_vcompress (vfloat16m1_t src, vbool16_t mask, size_t vl);
vfloat16m2_t __riscv_vcompress (vfloat16m2_t src, vbool8_t mask, size_t vl);
vfloat16m4_t __riscv_vcompress (vfloat16m4_t src, vbool4_t mask, size_t vl);
vfloat16m8_t __riscv_vcompress (vfloat16m8_t src, vbool2_t mask, size_t vl);
vfloat32mf2_t __riscv_vcompress (vfloat32mf2_t src, vbool64_t mask, size_t vl);
vfloat32m1_t __riscv_vcompress (vfloat32m1_t src, vbool32_t mask, size_t vl);
vfloat32m2_t __riscv_vcompress (vfloat32m2_t src, vbool16_t mask, size_t vl);
vfloat32m4_t __riscv_vcompress (vfloat32m4_t src, vbool8_t mask, size_t vl);
vfloat32m8_t __riscv_vcompress (vfloat32m8_t src, vbool4_t mask, size_t vl);
vfloat64m1_t __riscv_vcompress (vfloat64m1_t src, vbool64_t mask, size_t vl);
vfloat64m2_t __riscv_vcompress (vfloat64m2_t src, vbool32_t mask, size_t vl);
vfloat64m4_t __riscv_vcompress (vfloat64m4_t src, vbool16_t mask, size_t vl);
vfloat64m8_t __riscv_vcompress (vfloat64m8_t src, vbool8_t mask, size_t vl);
vint8mf8_t __riscv_vcompress (vint8mf8_t src, vbool64_t mask, size_t vl);
vint8mf4_t __riscv_vcompress (vint8mf4_t src, vbool32_t mask, size_t vl);
vint8mf2_t __riscv_vcompress (vint8mf2_t src, vbool16_t mask, size_t vl);
vint8m1_t __riscv_vcompress (vint8m1_t src, vbool8_t mask, size_t vl);
vint8m2_t __riscv_vcompress (vint8m2_t src, vbool4_t mask, size_t vl);
vint8m4_t __riscv_vcompress (vint8m4_t src, vbool2_t mask, size_t vl);
vint8m8_t __riscv_vcompress (vint8m8_t src, vbool1_t mask, size_t vl);
vint16mf4_t __riscv_vcompress (vint16mf4_t src, vbool64_t mask, size_t vl);
vint16mf2_t __riscv_vcompress (vint16mf2_t src, vbool32_t mask, size_t vl);
vint16m1_t __riscv_vcompress (vint16m1_t src, vbool16_t mask, size_t vl);
vint16m2_t __riscv_vcompress (vint16m2_t src, vbool8_t mask, size_t vl);
vint16m4_t __riscv_vcompress (vint16m4_t src, vbool4_t mask, size_t vl);
vint16m8_t __riscv_vcompress (vint16m8_t src, vbool2_t mask, size_t vl);
vint32mf2_t __riscv_vcompress (vint32mf2_t src, vbool64_t mask, size_t vl);
vint32m1_t __riscv_vcompress (vint32m1_t src, vbool32_t mask, size_t vl);
vint32m2_t __riscv_vcompress (vint32m2_t src, vbool16_t mask, size_t vl);
vint32m4_t __riscv_vcompress (vint32m4_t src, vbool8_t mask, size_t vl);
vint32m8_t __riscv_vcompress (vint32m8_t src, vbool4_t mask, size_t vl);
vint64m1_t __riscv_vcompress (vint64m1_t src, vbool64_t mask, size_t vl);
vint64m2_t __riscv_vcompress (vint64m2_t src, vbool32_t mask, size_t vl);
vint64m4_t __riscv_vcompress (vint64m4_t src, vbool16_t mask, size_t vl);
vint64m8_t __riscv_vcompress (vint64m8_t src, vbool8_t mask, size_t vl);
vuint8mf8_t __riscv_vcompress (vuint8mf8_t src, vbool64_t mask, size_t vl);
vuint8mf4_t __riscv_vcompress (vuint8mf4_t src, vbool32_t mask, size_t vl);
vuint8mf2_t __riscv_vcompress (vuint8mf2_t src, vbool16_t mask, size_t vl);
vuint8m1_t __riscv_vcompress (vuint8m1_t src, vbool8_t mask, size_t vl);
vuint8m2_t __riscv_vcompress (vuint8m2_t src, vbool4_t mask, size_t vl);
vuint8m4_t __riscv_vcompress (vuint8m4_t src, vbool2_t mask, size_t vl);
vuint8m8_t __riscv_vcompress (vuint8m8_t src, vbool1_t mask, size_t vl);
vuint16mf4_t __riscv_vcompress (vuint16mf4_t src, vbool64_t mask, size_t vl);
vuint16mf2_t __riscv_vcompress (vuint16mf2_t src, vbool32_t mask, size_t vl);
vuint16m1_t __riscv_vcompress (vuint16m1_t src, vbool16_t mask, size_t vl);
vuint16m2_t __riscv_vcompress (vuint16m2_t src, vbool8_t mask, size_t vl);
vuint16m4_t __riscv_vcompress (vuint16m4_t src, vbool4_t mask, size_t vl);
vuint16m8_t __riscv_vcompress (vuint16m8_t src, vbool2_t mask, size_t vl);
vuint32mf2_t __riscv_vcompress (vuint32mf2_t src, vbool64_t mask, size_t vl);
vuint32m1_t __riscv_vcompress (vuint32m1_t src, vbool32_t mask, size_t vl);
vuint32m2_t __riscv_vcompress (vuint32m2_t src, vbool16_t mask, size_t vl);
vuint32m4_t __riscv_vcompress (vuint32m4_t src, vbool8_t mask, size_t vl);
vuint32m8_t __riscv_vcompress (vuint32m8_t src, vbool4_t mask, size_t vl);
vuint64m1_t __riscv_vcompress (vuint64m1_t src, vbool64_t mask, size_t vl);
vuint64m2_t __riscv_vcompress (vuint64m2_t src, vbool32_t mask, size_t vl);
vuint64m4_t __riscv_vcompress (vuint64m4_t src, vbool16_t mask, size_t vl);
vuint64m8_t __riscv_vcompress (vuint64m8_t src, vbool8_t mask, size_t vl);
```
