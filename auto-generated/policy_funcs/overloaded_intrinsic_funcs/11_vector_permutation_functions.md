
## Vector Permutation Functions:

### [Integer and Floating-Point Scalar Move Functions](../rvv-intrinsic-api.md#171-integer-scalar-move-operations):

**Prototypes:**
``` C
vfloat16mf4_t __riscv_vfmv_s_tu (vfloat16mf4_t maskedoff, float16_t src, size_t vl);
vfloat16mf2_t __riscv_vfmv_s_tu (vfloat16mf2_t maskedoff, float16_t src, size_t vl);
vfloat16m1_t __riscv_vfmv_s_tu (vfloat16m1_t maskedoff, float16_t src, size_t vl);
vfloat16m2_t __riscv_vfmv_s_tu (vfloat16m2_t maskedoff, float16_t src, size_t vl);
vfloat16m4_t __riscv_vfmv_s_tu (vfloat16m4_t maskedoff, float16_t src, size_t vl);
vfloat16m8_t __riscv_vfmv_s_tu (vfloat16m8_t maskedoff, float16_t src, size_t vl);
vfloat32mf2_t __riscv_vfmv_s_tu (vfloat32mf2_t maskedoff, float32_t src, size_t vl);
vfloat32m1_t __riscv_vfmv_s_tu (vfloat32m1_t maskedoff, float32_t src, size_t vl);
vfloat32m2_t __riscv_vfmv_s_tu (vfloat32m2_t maskedoff, float32_t src, size_t vl);
vfloat32m4_t __riscv_vfmv_s_tu (vfloat32m4_t maskedoff, float32_t src, size_t vl);
vfloat32m8_t __riscv_vfmv_s_tu (vfloat32m8_t maskedoff, float32_t src, size_t vl);
vfloat64m1_t __riscv_vfmv_s_tu (vfloat64m1_t maskedoff, float64_t src, size_t vl);
vfloat64m2_t __riscv_vfmv_s_tu (vfloat64m2_t maskedoff, float64_t src, size_t vl);
vfloat64m4_t __riscv_vfmv_s_tu (vfloat64m4_t maskedoff, float64_t src, size_t vl);
vfloat64m8_t __riscv_vfmv_s_tu (vfloat64m8_t maskedoff, float64_t src, size_t vl);
vint8mf8_t __riscv_vmv_s_tu (vint8mf8_t maskedoff, int8_t src, size_t vl);
vint8mf4_t __riscv_vmv_s_tu (vint8mf4_t maskedoff, int8_t src, size_t vl);
vint8mf2_t __riscv_vmv_s_tu (vint8mf2_t maskedoff, int8_t src, size_t vl);
vint8m1_t __riscv_vmv_s_tu (vint8m1_t maskedoff, int8_t src, size_t vl);
vint8m2_t __riscv_vmv_s_tu (vint8m2_t maskedoff, int8_t src, size_t vl);
vint8m4_t __riscv_vmv_s_tu (vint8m4_t maskedoff, int8_t src, size_t vl);
vint8m8_t __riscv_vmv_s_tu (vint8m8_t maskedoff, int8_t src, size_t vl);
vint16mf4_t __riscv_vmv_s_tu (vint16mf4_t maskedoff, int16_t src, size_t vl);
vint16mf2_t __riscv_vmv_s_tu (vint16mf2_t maskedoff, int16_t src, size_t vl);
vint16m1_t __riscv_vmv_s_tu (vint16m1_t maskedoff, int16_t src, size_t vl);
vint16m2_t __riscv_vmv_s_tu (vint16m2_t maskedoff, int16_t src, size_t vl);
vint16m4_t __riscv_vmv_s_tu (vint16m4_t maskedoff, int16_t src, size_t vl);
vint16m8_t __riscv_vmv_s_tu (vint16m8_t maskedoff, int16_t src, size_t vl);
vint32mf2_t __riscv_vmv_s_tu (vint32mf2_t maskedoff, int32_t src, size_t vl);
vint32m1_t __riscv_vmv_s_tu (vint32m1_t maskedoff, int32_t src, size_t vl);
vint32m2_t __riscv_vmv_s_tu (vint32m2_t maskedoff, int32_t src, size_t vl);
vint32m4_t __riscv_vmv_s_tu (vint32m4_t maskedoff, int32_t src, size_t vl);
vint32m8_t __riscv_vmv_s_tu (vint32m8_t maskedoff, int32_t src, size_t vl);
vint64m1_t __riscv_vmv_s_tu (vint64m1_t maskedoff, int64_t src, size_t vl);
vint64m2_t __riscv_vmv_s_tu (vint64m2_t maskedoff, int64_t src, size_t vl);
vint64m4_t __riscv_vmv_s_tu (vint64m4_t maskedoff, int64_t src, size_t vl);
vint64m8_t __riscv_vmv_s_tu (vint64m8_t maskedoff, int64_t src, size_t vl);
vuint8mf8_t __riscv_vmv_s_tu (vuint8mf8_t maskedoff, uint8_t src, size_t vl);
vuint8mf4_t __riscv_vmv_s_tu (vuint8mf4_t maskedoff, uint8_t src, size_t vl);
vuint8mf2_t __riscv_vmv_s_tu (vuint8mf2_t maskedoff, uint8_t src, size_t vl);
vuint8m1_t __riscv_vmv_s_tu (vuint8m1_t maskedoff, uint8_t src, size_t vl);
vuint8m2_t __riscv_vmv_s_tu (vuint8m2_t maskedoff, uint8_t src, size_t vl);
vuint8m4_t __riscv_vmv_s_tu (vuint8m4_t maskedoff, uint8_t src, size_t vl);
vuint8m8_t __riscv_vmv_s_tu (vuint8m8_t maskedoff, uint8_t src, size_t vl);
vuint16mf4_t __riscv_vmv_s_tu (vuint16mf4_t maskedoff, uint16_t src, size_t vl);
vuint16mf2_t __riscv_vmv_s_tu (vuint16mf2_t maskedoff, uint16_t src, size_t vl);
vuint16m1_t __riscv_vmv_s_tu (vuint16m1_t maskedoff, uint16_t src, size_t vl);
vuint16m2_t __riscv_vmv_s_tu (vuint16m2_t maskedoff, uint16_t src, size_t vl);
vuint16m4_t __riscv_vmv_s_tu (vuint16m4_t maskedoff, uint16_t src, size_t vl);
vuint16m8_t __riscv_vmv_s_tu (vuint16m8_t maskedoff, uint16_t src, size_t vl);
vuint32mf2_t __riscv_vmv_s_tu (vuint32mf2_t maskedoff, uint32_t src, size_t vl);
vuint32m1_t __riscv_vmv_s_tu (vuint32m1_t maskedoff, uint32_t src, size_t vl);
vuint32m2_t __riscv_vmv_s_tu (vuint32m2_t maskedoff, uint32_t src, size_t vl);
vuint32m4_t __riscv_vmv_s_tu (vuint32m4_t maskedoff, uint32_t src, size_t vl);
vuint32m8_t __riscv_vmv_s_tu (vuint32m8_t maskedoff, uint32_t src, size_t vl);
vuint64m1_t __riscv_vmv_s_tu (vuint64m1_t maskedoff, uint64_t src, size_t vl);
vuint64m2_t __riscv_vmv_s_tu (vuint64m2_t maskedoff, uint64_t src, size_t vl);
vuint64m4_t __riscv_vmv_s_tu (vuint64m4_t maskedoff, uint64_t src, size_t vl);
vuint64m8_t __riscv_vmv_s_tu (vuint64m8_t maskedoff, uint64_t src, size_t vl);
```

### [Vector Slideup Functions](../rvv-intrinsic-api.md#173-vector-slide-operations):

**Prototypes:**
``` C
vfloat16mf4_t __riscv_vslideup_tu (vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslideup_tu (vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslideup_tu (vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslideup_tu (vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslideup_tu (vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslideup_tu (vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslideup_tu (vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslideup_tu (vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslideup_tu (vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslideup_tu (vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslideup_tu (vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslideup_tu (vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslideup_tu (vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslideup_tu (vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslideup_tu (vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslideup_tu (vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslideup_tu (vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslideup_tu (vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslideup_tu (vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslideup_tu (vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslideup_tu (vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslideup_tu (vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslideup_tu (vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslideup_tu (vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslideup_tu (vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslideup_tu (vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslideup_tu (vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslideup_tu (vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslideup_tu (vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslideup_tu (vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslideup_tu (vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslideup_tu (vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslideup_tu (vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslideup_tu (vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslideup_tu (vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslideup_tu (vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslideup_tu (vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslideup_tu (vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslideup_tu (vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslideup_tu (vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslideup_tu (vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslideup_tu (vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslideup_tu (vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslideup_tu (vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslideup_tu (vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslideup_tu (vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslideup_tu (vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslideup_tu (vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslideup_tu (vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslideup_tu (vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslideup_tu (vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslideup_tu (vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslideup_tu (vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslideup_tu (vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslideup_tu (vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslideup_tu (vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslideup_tu (vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslideup_tu (vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslideup_tu (vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup_tum (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslideup_tum (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslideup_tum (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslideup_tum (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslideup_tum (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslideup_tum (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslideup_tum (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslideup_tum (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslideup_tum (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslideup_tum (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslideup_tum (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslideup_tum (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslideup_tum (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslideup_tum (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslideup_tum (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslideup_tum (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslideup_tum (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslideup_tum (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslideup_tum (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslideup_tum (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslideup_tum (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslideup_tum (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslideup_tum (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslideup_tum (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslideup_tum (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslideup_tum (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslideup_tum (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslideup_tum (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslideup_tum (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslideup_tum (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslideup_tum (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslideup_tum (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslideup_tum (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslideup_tum (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslideup_tum (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslideup_tum (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslideup_tum (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslideup_tum (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslideup_tum (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslideup_tum (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslideup_tum (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslideup_tum (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslideup_tum (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslideup_tum (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslideup_tum (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslideup_tum (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslideup_tum (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslideup_tum (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslideup_tum (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslideup_tum (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslideup_tum (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslideup_tum (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslideup_tum (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslideup_tum (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslideup_tum (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslideup_tum (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslideup_tum (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslideup_tum (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslideup_tum (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup_tumu (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslideup_tumu (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslideup_tumu (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslideup_tumu (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslideup_tumu (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslideup_tumu (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslideup_tumu (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslideup_tumu (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslideup_tumu (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslideup_tumu (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslideup_tumu (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslideup_tumu (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslideup_tumu (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslideup_tumu (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslideup_tumu (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslideup_tumu (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslideup_tumu (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslideup_tumu (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslideup_tumu (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslideup_tumu (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslideup_tumu (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslideup_tumu (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslideup_tumu (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslideup_tumu (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslideup_tumu (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslideup_tumu (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslideup_tumu (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslideup_tumu (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslideup_tumu (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslideup_tumu (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslideup_tumu (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslideup_tumu (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslideup_tumu (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslideup_tumu (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslideup_tumu (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslideup_tumu (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslideup_tumu (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslideup_tumu (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslideup_tumu (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslideup_tumu (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslideup_tumu (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslideup_tumu (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslideup_tumu (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslideup_tumu (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslideup_tumu (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslideup_tumu (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslideup_tumu (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslideup_tumu (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslideup_tumu (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslideup_tumu (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslideup_tumu (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslideup_tumu (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslideup_tumu (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslideup_tumu (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslideup_tumu (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslideup_tumu (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslideup_tumu (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslideup_tumu (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslideup_tumu (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup_mu (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslideup_mu (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslideup_mu (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslideup_mu (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslideup_mu (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslideup_mu (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslideup_mu (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslideup_mu (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslideup_mu (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslideup_mu (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslideup_mu (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslideup_mu (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslideup_mu (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslideup_mu (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslideup_mu (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslideup_mu (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslideup_mu (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslideup_mu (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslideup_mu (vbool8_t mask, vint8m1_t dest, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslideup_mu (vbool4_t mask, vint8m2_t dest, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslideup_mu (vbool2_t mask, vint8m4_t dest, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslideup_mu (vbool1_t mask, vint8m8_t dest, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslideup_mu (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslideup_mu (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslideup_mu (vbool16_t mask, vint16m1_t dest, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslideup_mu (vbool8_t mask, vint16m2_t dest, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslideup_mu (vbool4_t mask, vint16m4_t dest, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslideup_mu (vbool2_t mask, vint16m8_t dest, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslideup_mu (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslideup_mu (vbool32_t mask, vint32m1_t dest, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslideup_mu (vbool16_t mask, vint32m2_t dest, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslideup_mu (vbool8_t mask, vint32m4_t dest, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslideup_mu (vbool4_t mask, vint32m8_t dest, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslideup_mu (vbool64_t mask, vint64m1_t dest, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslideup_mu (vbool32_t mask, vint64m2_t dest, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslideup_mu (vbool16_t mask, vint64m4_t dest, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslideup_mu (vbool8_t mask, vint64m8_t dest, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslideup_mu (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslideup_mu (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslideup_mu (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslideup_mu (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslideup_mu (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslideup_mu (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslideup_mu (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslideup_mu (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslideup_mu (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslideup_mu (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslideup_mu (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslideup_mu (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslideup_mu (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslideup_mu (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslideup_mu (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslideup_mu (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslideup_mu (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslideup_mu (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslideup_mu (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslideup_mu (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslideup_mu (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslideup_mu (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src, size_t offset, size_t vl);
```

### [Vector Slidedown Functions](../rvv-intrinsic-api.md#173-vector-slide-operations):

**Prototypes:**
``` C
vfloat16mf4_t __riscv_vslidedown_tu (vfloat16mf4_t maskedoff, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslidedown_tu (vfloat16mf2_t maskedoff, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslidedown_tu (vfloat16m1_t maskedoff, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslidedown_tu (vfloat16m2_t maskedoff, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslidedown_tu (vfloat16m4_t maskedoff, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslidedown_tu (vfloat16m8_t maskedoff, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslidedown_tu (vfloat32mf2_t maskedoff, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslidedown_tu (vfloat32m1_t maskedoff, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslidedown_tu (vfloat32m2_t maskedoff, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslidedown_tu (vfloat32m4_t maskedoff, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslidedown_tu (vfloat32m8_t maskedoff, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslidedown_tu (vfloat64m1_t maskedoff, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslidedown_tu (vfloat64m2_t maskedoff, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslidedown_tu (vfloat64m4_t maskedoff, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslidedown_tu (vfloat64m8_t maskedoff, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslidedown_tu (vint8mf8_t maskedoff, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslidedown_tu (vint8mf4_t maskedoff, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslidedown_tu (vint8mf2_t maskedoff, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslidedown_tu (vint8m1_t maskedoff, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslidedown_tu (vint8m2_t maskedoff, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslidedown_tu (vint8m4_t maskedoff, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslidedown_tu (vint8m8_t maskedoff, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslidedown_tu (vint16mf4_t maskedoff, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslidedown_tu (vint16mf2_t maskedoff, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslidedown_tu (vint16m1_t maskedoff, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslidedown_tu (vint16m2_t maskedoff, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslidedown_tu (vint16m4_t maskedoff, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslidedown_tu (vint16m8_t maskedoff, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslidedown_tu (vint32mf2_t maskedoff, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslidedown_tu (vint32m1_t maskedoff, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslidedown_tu (vint32m2_t maskedoff, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslidedown_tu (vint32m4_t maskedoff, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslidedown_tu (vint32m8_t maskedoff, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslidedown_tu (vint64m1_t maskedoff, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslidedown_tu (vint64m2_t maskedoff, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslidedown_tu (vint64m4_t maskedoff, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslidedown_tu (vint64m8_t maskedoff, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslidedown_tu (vuint8mf8_t maskedoff, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslidedown_tu (vuint8mf4_t maskedoff, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslidedown_tu (vuint8mf2_t maskedoff, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslidedown_tu (vuint8m1_t maskedoff, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslidedown_tu (vuint8m2_t maskedoff, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslidedown_tu (vuint8m4_t maskedoff, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslidedown_tu (vuint8m8_t maskedoff, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslidedown_tu (vuint16mf4_t maskedoff, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslidedown_tu (vuint16mf2_t maskedoff, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslidedown_tu (vuint16m1_t maskedoff, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslidedown_tu (vuint16m2_t maskedoff, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslidedown_tu (vuint16m4_t maskedoff, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslidedown_tu (vuint16m8_t maskedoff, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslidedown_tu (vuint32mf2_t maskedoff, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslidedown_tu (vuint32m1_t maskedoff, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslidedown_tu (vuint32m2_t maskedoff, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslidedown_tu (vuint32m4_t maskedoff, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslidedown_tu (vuint32m8_t maskedoff, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslidedown_tu (vuint64m1_t maskedoff, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslidedown_tu (vuint64m2_t maskedoff, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslidedown_tu (vuint64m4_t maskedoff, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslidedown_tu (vuint64m8_t maskedoff, vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown_tum (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslidedown_tum (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslidedown_tum (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslidedown_tum (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslidedown_tum (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslidedown_tum (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslidedown_tum (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslidedown_tum (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslidedown_tum (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslidedown_tum (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslidedown_tum (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslidedown_tum (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslidedown_tum (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslidedown_tum (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslidedown_tum (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslidedown_tum (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslidedown_tum (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslidedown_tum (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslidedown_tum (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslidedown_tum (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslidedown_tum (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslidedown_tum (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslidedown_tum (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslidedown_tum (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslidedown_tum (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslidedown_tum (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslidedown_tum (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslidedown_tum (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslidedown_tum (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslidedown_tum (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslidedown_tum (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslidedown_tum (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslidedown_tum (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslidedown_tum (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslidedown_tum (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslidedown_tum (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslidedown_tum (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslidedown_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslidedown_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslidedown_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslidedown_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslidedown_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslidedown_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslidedown_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslidedown_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslidedown_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslidedown_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslidedown_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslidedown_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslidedown_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslidedown_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslidedown_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslidedown_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslidedown_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslidedown_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslidedown_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslidedown_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslidedown_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslidedown_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown_tumu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslidedown_tumu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslidedown_tumu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslidedown_tumu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslidedown_tumu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslidedown_tumu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslidedown_tumu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslidedown_tumu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslidedown_tumu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslidedown_tumu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslidedown_tumu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslidedown_tumu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslidedown_tumu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslidedown_tumu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslidedown_tumu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslidedown_tumu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslidedown_tumu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslidedown_tumu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslidedown_tumu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslidedown_tumu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslidedown_tumu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslidedown_tumu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslidedown_tumu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslidedown_tumu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslidedown_tumu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslidedown_tumu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslidedown_tumu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslidedown_tumu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslidedown_tumu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslidedown_tumu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslidedown_tumu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslidedown_tumu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslidedown_tumu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslidedown_tumu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslidedown_tumu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslidedown_tumu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslidedown_tumu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslidedown_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslidedown_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslidedown_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslidedown_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslidedown_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslidedown_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslidedown_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslidedown_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslidedown_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslidedown_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslidedown_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslidedown_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslidedown_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslidedown_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslidedown_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslidedown_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslidedown_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslidedown_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslidedown_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslidedown_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslidedown_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslidedown_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, size_t offset, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown_mu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, size_t offset, size_t vl);
vfloat16mf2_t __riscv_vslidedown_mu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, size_t offset, size_t vl);
vfloat16m1_t __riscv_vslidedown_mu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, size_t offset, size_t vl);
vfloat16m2_t __riscv_vslidedown_mu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, size_t offset, size_t vl);
vfloat16m4_t __riscv_vslidedown_mu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, size_t offset, size_t vl);
vfloat16m8_t __riscv_vslidedown_mu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, size_t offset, size_t vl);
vfloat32mf2_t __riscv_vslidedown_mu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, size_t offset, size_t vl);
vfloat32m1_t __riscv_vslidedown_mu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, size_t offset, size_t vl);
vfloat32m2_t __riscv_vslidedown_mu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, size_t offset, size_t vl);
vfloat32m4_t __riscv_vslidedown_mu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, size_t offset, size_t vl);
vfloat32m8_t __riscv_vslidedown_mu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, size_t offset, size_t vl);
vfloat64m1_t __riscv_vslidedown_mu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, size_t offset, size_t vl);
vfloat64m2_t __riscv_vslidedown_mu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, size_t offset, size_t vl);
vfloat64m4_t __riscv_vslidedown_mu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, size_t offset, size_t vl);
vfloat64m8_t __riscv_vslidedown_mu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, size_t offset, size_t vl);
vint8mf8_t __riscv_vslidedown_mu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, size_t offset, size_t vl);
vint8mf4_t __riscv_vslidedown_mu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, size_t offset, size_t vl);
vint8mf2_t __riscv_vslidedown_mu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, size_t offset, size_t vl);
vint8m1_t __riscv_vslidedown_mu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, size_t offset, size_t vl);
vint8m2_t __riscv_vslidedown_mu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, size_t offset, size_t vl);
vint8m4_t __riscv_vslidedown_mu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, size_t offset, size_t vl);
vint8m8_t __riscv_vslidedown_mu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, size_t offset, size_t vl);
vint16mf4_t __riscv_vslidedown_mu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, size_t offset, size_t vl);
vint16mf2_t __riscv_vslidedown_mu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, size_t offset, size_t vl);
vint16m1_t __riscv_vslidedown_mu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, size_t offset, size_t vl);
vint16m2_t __riscv_vslidedown_mu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, size_t offset, size_t vl);
vint16m4_t __riscv_vslidedown_mu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, size_t offset, size_t vl);
vint16m8_t __riscv_vslidedown_mu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, size_t offset, size_t vl);
vint32mf2_t __riscv_vslidedown_mu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, size_t offset, size_t vl);
vint32m1_t __riscv_vslidedown_mu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, size_t offset, size_t vl);
vint32m2_t __riscv_vslidedown_mu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, size_t offset, size_t vl);
vint32m4_t __riscv_vslidedown_mu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, size_t offset, size_t vl);
vint32m8_t __riscv_vslidedown_mu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, size_t offset, size_t vl);
vint64m1_t __riscv_vslidedown_mu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, size_t offset, size_t vl);
vint64m2_t __riscv_vslidedown_mu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, size_t offset, size_t vl);
vint64m4_t __riscv_vslidedown_mu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, size_t offset, size_t vl);
vint64m8_t __riscv_vslidedown_mu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, size_t offset, size_t vl);
vuint8mf8_t __riscv_vslidedown_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, size_t offset, size_t vl);
vuint8mf4_t __riscv_vslidedown_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, size_t offset, size_t vl);
vuint8mf2_t __riscv_vslidedown_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, size_t offset, size_t vl);
vuint8m1_t __riscv_vslidedown_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, size_t offset, size_t vl);
vuint8m2_t __riscv_vslidedown_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, size_t offset, size_t vl);
vuint8m4_t __riscv_vslidedown_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, size_t offset, size_t vl);
vuint8m8_t __riscv_vslidedown_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, size_t offset, size_t vl);
vuint16mf4_t __riscv_vslidedown_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, size_t offset, size_t vl);
vuint16mf2_t __riscv_vslidedown_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, size_t offset, size_t vl);
vuint16m1_t __riscv_vslidedown_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, size_t offset, size_t vl);
vuint16m2_t __riscv_vslidedown_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, size_t offset, size_t vl);
vuint16m4_t __riscv_vslidedown_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, size_t offset, size_t vl);
vuint16m8_t __riscv_vslidedown_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, size_t offset, size_t vl);
vuint32mf2_t __riscv_vslidedown_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, size_t offset, size_t vl);
vuint32m1_t __riscv_vslidedown_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, size_t offset, size_t vl);
vuint32m2_t __riscv_vslidedown_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, size_t offset, size_t vl);
vuint32m4_t __riscv_vslidedown_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, size_t offset, size_t vl);
vuint32m8_t __riscv_vslidedown_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, size_t offset, size_t vl);
vuint64m1_t __riscv_vslidedown_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, size_t offset, size_t vl);
vuint64m2_t __riscv_vslidedown_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, size_t offset, size_t vl);
vuint64m4_t __riscv_vslidedown_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, size_t offset, size_t vl);
vuint64m8_t __riscv_vslidedown_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, size_t offset, size_t vl);
```

### [Vector Slide1up and Slide1down Functions](../rvv-intrinsic-api.md#173-vector-slide1up-and-slide1down-functions):

**Prototypes:**
``` C
vfloat16mf4_t __riscv_vfslide1up_tu (vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1up_tu (vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1up_tu (vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1up_tu (vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1up_tu (vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1up_tu (vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1up_tu (vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1up_tu (vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1up_tu (vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1up_tu (vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1up_tu (vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1up_tu (vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1up_tu (vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1up_tu (vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1up_tu (vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vfloat16mf4_t __riscv_vfslide1down_tu (vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1down_tu (vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1down_tu (vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1down_tu (vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1down_tu (vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1down_tu (vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1down_tu (vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1down_tu (vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1down_tu (vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1down_tu (vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1down_tu (vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1down_tu (vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1down_tu (vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1down_tu (vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1down_tu (vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t __riscv_vslide1up_tu (vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1up_tu (vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1up_tu (vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1up_tu (vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1up_tu (vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1up_tu (vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1up_tu (vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1up_tu (vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1up_tu (vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1up_tu (vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1up_tu (vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1up_tu (vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1up_tu (vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1up_tu (vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1up_tu (vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1up_tu (vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1up_tu (vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1up_tu (vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1up_tu (vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1up_tu (vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1up_tu (vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1up_tu (vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vint8mf8_t __riscv_vslide1down_tu (vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1down_tu (vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1down_tu (vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1down_tu (vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1down_tu (vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1down_tu (vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1down_tu (vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1down_tu (vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1down_tu (vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1down_tu (vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1down_tu (vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1down_tu (vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1down_tu (vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1down_tu (vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1down_tu (vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1down_tu (vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1down_tu (vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1down_tu (vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1down_tu (vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1down_tu (vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1down_tu (vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1down_tu (vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1up_tu (vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1up_tu (vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1up_tu (vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1up_tu (vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1up_tu (vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1up_tu (vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1up_tu (vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1up_tu (vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1up_tu (vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1up_tu (vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1up_tu (vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1up_tu (vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1up_tu (vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1up_tu (vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1up_tu (vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1up_tu (vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1up_tu (vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1up_tu (vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1up_tu (vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1up_tu (vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1up_tu (vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1up_tu (vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1down_tu (vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1down_tu (vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1down_tu (vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1down_tu (vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1down_tu (vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1down_tu (vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1down_tu (vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1down_tu (vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1down_tu (vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1down_tu (vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1down_tu (vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1down_tu (vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1down_tu (vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1down_tu (vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1down_tu (vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1down_tu (vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1down_tu (vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1down_tu (vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1down_tu (vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1down_tu (vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1down_tu (vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1down_tu (vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up_tum (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1up_tum (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1up_tum (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1up_tum (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1up_tum (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1up_tum (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1up_tum (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1up_tum (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1up_tum (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1up_tum (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1up_tum (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1up_tum (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1up_tum (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1up_tum (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1up_tum (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vfloat16mf4_t __riscv_vfslide1down_tum (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1down_tum (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1down_tum (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1down_tum (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1down_tum (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1down_tum (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1down_tum (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1down_tum (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1down_tum (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1down_tum (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1down_tum (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1down_tum (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1down_tum (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1down_tum (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1down_tum (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t __riscv_vslide1up_tum (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1up_tum (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1up_tum (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1up_tum (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1up_tum (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1up_tum (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1up_tum (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1up_tum (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1up_tum (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1up_tum (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1up_tum (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1up_tum (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1up_tum (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1up_tum (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1up_tum (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1up_tum (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1up_tum (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1up_tum (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1up_tum (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1up_tum (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1up_tum (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1up_tum (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vint8mf8_t __riscv_vslide1down_tum (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1down_tum (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1down_tum (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1down_tum (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1down_tum (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1down_tum (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1down_tum (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1down_tum (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1down_tum (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1down_tum (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1down_tum (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1down_tum (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1down_tum (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1down_tum (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1down_tum (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1down_tum (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1down_tum (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1down_tum (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1down_tum (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1down_tum (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1down_tum (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1down_tum (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1up_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1up_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1up_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1up_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1up_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1up_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1up_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1up_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1up_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1up_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1up_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1up_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1up_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1up_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1up_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1up_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1up_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1up_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1up_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1up_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1up_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1up_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1down_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1down_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1down_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1down_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1down_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1down_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1down_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1down_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1down_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1down_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1down_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1down_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1down_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1down_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1down_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1down_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1down_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1down_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1down_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1down_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1down_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1down_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up_tumu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1up_tumu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1up_tumu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1up_tumu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1up_tumu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1up_tumu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1up_tumu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1up_tumu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1up_tumu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1up_tumu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1up_tumu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1up_tumu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1up_tumu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1up_tumu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1up_tumu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vfloat16mf4_t __riscv_vfslide1down_tumu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1down_tumu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1down_tumu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1down_tumu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1down_tumu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1down_tumu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1down_tumu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1down_tumu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1down_tumu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1down_tumu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1down_tumu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1down_tumu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1down_tumu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1down_tumu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1down_tumu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t __riscv_vslide1up_tumu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1up_tumu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1up_tumu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1up_tumu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1up_tumu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1up_tumu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1up_tumu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1up_tumu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1up_tumu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1up_tumu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1up_tumu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1up_tumu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1up_tumu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1up_tumu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1up_tumu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1up_tumu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1up_tumu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1up_tumu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1up_tumu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1up_tumu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1up_tumu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1up_tumu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vint8mf8_t __riscv_vslide1down_tumu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1down_tumu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1down_tumu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1down_tumu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1down_tumu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1down_tumu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1down_tumu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1down_tumu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1down_tumu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1down_tumu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1down_tumu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1down_tumu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1down_tumu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1down_tumu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1down_tumu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1down_tumu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1down_tumu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1down_tumu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1down_tumu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1down_tumu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1down_tumu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1down_tumu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1up_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1up_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1up_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1up_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1up_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1up_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1up_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1up_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1up_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1up_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1up_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1up_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1up_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1up_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1up_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1up_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1up_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1up_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1up_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1up_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1up_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1up_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1down_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1down_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1down_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1down_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1down_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1down_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1down_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1down_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1down_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1down_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1down_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1down_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1down_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1down_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1down_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1down_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1down_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1down_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1down_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1down_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1down_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1down_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up_mu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1up_mu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1up_mu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1up_mu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1up_mu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1up_mu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1up_mu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1up_mu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1up_mu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1up_mu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1up_mu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1up_mu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1up_mu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1up_mu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1up_mu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vfloat16mf4_t __riscv_vfslide1down_mu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value, size_t vl);
vfloat16mf2_t __riscv_vfslide1down_mu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value, size_t vl);
vfloat16m1_t __riscv_vfslide1down_mu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value, size_t vl);
vfloat16m2_t __riscv_vfslide1down_mu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value, size_t vl);
vfloat16m4_t __riscv_vfslide1down_mu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value, size_t vl);
vfloat16m8_t __riscv_vfslide1down_mu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value, size_t vl);
vfloat32mf2_t __riscv_vfslide1down_mu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value, size_t vl);
vfloat32m1_t __riscv_vfslide1down_mu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value, size_t vl);
vfloat32m2_t __riscv_vfslide1down_mu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value, size_t vl);
vfloat32m4_t __riscv_vfslide1down_mu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value, size_t vl);
vfloat32m8_t __riscv_vfslide1down_mu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value, size_t vl);
vfloat64m1_t __riscv_vfslide1down_mu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value, size_t vl);
vfloat64m2_t __riscv_vfslide1down_mu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value, size_t vl);
vfloat64m4_t __riscv_vfslide1down_mu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value, size_t vl);
vfloat64m8_t __riscv_vfslide1down_mu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value, size_t vl);
vint8mf8_t __riscv_vslide1up_mu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1up_mu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1up_mu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1up_mu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1up_mu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1up_mu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1up_mu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1up_mu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1up_mu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1up_mu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1up_mu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1up_mu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1up_mu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1up_mu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1up_mu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1up_mu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1up_mu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1up_mu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1up_mu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1up_mu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1up_mu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1up_mu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vint8mf8_t __riscv_vslide1down_mu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value, size_t vl);
vint8mf4_t __riscv_vslide1down_mu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value, size_t vl);
vint8mf2_t __riscv_vslide1down_mu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value, size_t vl);
vint8m1_t __riscv_vslide1down_mu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value, size_t vl);
vint8m2_t __riscv_vslide1down_mu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value, size_t vl);
vint8m4_t __riscv_vslide1down_mu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value, size_t vl);
vint8m8_t __riscv_vslide1down_mu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value, size_t vl);
vint16mf4_t __riscv_vslide1down_mu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value, size_t vl);
vint16mf2_t __riscv_vslide1down_mu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value, size_t vl);
vint16m1_t __riscv_vslide1down_mu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value, size_t vl);
vint16m2_t __riscv_vslide1down_mu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value, size_t vl);
vint16m4_t __riscv_vslide1down_mu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value, size_t vl);
vint16m8_t __riscv_vslide1down_mu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value, size_t vl);
vint32mf2_t __riscv_vslide1down_mu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value, size_t vl);
vint32m1_t __riscv_vslide1down_mu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value, size_t vl);
vint32m2_t __riscv_vslide1down_mu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value, size_t vl);
vint32m4_t __riscv_vslide1down_mu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value, size_t vl);
vint32m8_t __riscv_vslide1down_mu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value, size_t vl);
vint64m1_t __riscv_vslide1down_mu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value, size_t vl);
vint64m2_t __riscv_vslide1down_mu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value, size_t vl);
vint64m4_t __riscv_vslide1down_mu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value, size_t vl);
vint64m8_t __riscv_vslide1down_mu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1up_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1up_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1up_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1up_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1up_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1up_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1up_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1up_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1up_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1up_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1up_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1up_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1up_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1up_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1up_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1up_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1up_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1up_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1up_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1up_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1up_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1up_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
vuint8mf8_t __riscv_vslide1down_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value, size_t vl);
vuint8mf4_t __riscv_vslide1down_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value, size_t vl);
vuint8mf2_t __riscv_vslide1down_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value, size_t vl);
vuint8m1_t __riscv_vslide1down_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value, size_t vl);
vuint8m2_t __riscv_vslide1down_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value, size_t vl);
vuint8m4_t __riscv_vslide1down_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value, size_t vl);
vuint8m8_t __riscv_vslide1down_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value, size_t vl);
vuint16mf4_t __riscv_vslide1down_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value, size_t vl);
vuint16mf2_t __riscv_vslide1down_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value, size_t vl);
vuint16m1_t __riscv_vslide1down_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value, size_t vl);
vuint16m2_t __riscv_vslide1down_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value, size_t vl);
vuint16m4_t __riscv_vslide1down_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value, size_t vl);
vuint16m8_t __riscv_vslide1down_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value, size_t vl);
vuint32mf2_t __riscv_vslide1down_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value, size_t vl);
vuint32m1_t __riscv_vslide1down_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value, size_t vl);
vuint32m2_t __riscv_vslide1down_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value, size_t vl);
vuint32m4_t __riscv_vslide1down_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value, size_t vl);
vuint32m8_t __riscv_vslide1down_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value, size_t vl);
vuint64m1_t __riscv_vslide1down_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value, size_t vl);
vuint64m2_t __riscv_vslide1down_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value, size_t vl);
vuint64m4_t __riscv_vslide1down_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value, size_t vl);
vuint64m8_t __riscv_vslide1down_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value, size_t vl);
```

### [Vector Register Gather Functions](../rvv-intrinsic-api.md#174-vector-register-gather-operations):

**Prototypes:**
``` C
vfloat16mf4_t __riscv_vrgather_tu (vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t __riscv_vrgather_tu (vfloat16mf4_t maskedoff, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_tu (vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_tu (vfloat16mf2_t maskedoff, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_tu (vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_tu (vfloat16m1_t maskedoff, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_tu (vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_tu (vfloat16m2_t maskedoff, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_tu (vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_tu (vfloat16m4_t maskedoff, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_tu (vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_tu (vfloat16m8_t maskedoff, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_tu (vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_tu (vfloat32mf2_t maskedoff, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_tu (vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_tu (vfloat32m1_t maskedoff, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_tu (vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_tu (vfloat32m2_t maskedoff, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_tu (vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_tu (vfloat32m4_t maskedoff, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_tu (vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_tu (vfloat32m8_t maskedoff, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_tu (vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_tu (vfloat64m1_t maskedoff, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_tu (vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_tu (vfloat64m2_t maskedoff, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_tu (vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_tu (vfloat64m4_t maskedoff, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_tu (vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_tu (vfloat64m8_t maskedoff, vfloat64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t __riscv_vrgatherei16_tu (vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t __riscv_vrgatherei16_tu (vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t __riscv_vrgatherei16_tu (vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t __riscv_vrgatherei16_tu (vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t __riscv_vrgatherei16_tu (vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t __riscv_vrgatherei16_tu (vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t __riscv_vrgatherei16_tu (vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t __riscv_vrgatherei16_tu (vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t __riscv_vrgatherei16_tu (vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t __riscv_vrgatherei16_tu (vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t __riscv_vrgatherei16_tu (vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t __riscv_vrgatherei16_tu (vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t __riscv_vrgatherei16_tu (vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t __riscv_vrgatherei16_tu (vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t __riscv_vrgatherei16_tu (vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
vint8mf8_t __riscv_vrgather_tu (vint8mf8_t maskedoff, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t __riscv_vrgather_tu (vint8mf8_t maskedoff, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t __riscv_vrgather_tu (vint8mf4_t maskedoff, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t __riscv_vrgather_tu (vint8mf4_t maskedoff, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t __riscv_vrgather_tu (vint8mf2_t maskedoff, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t __riscv_vrgather_tu (vint8mf2_t maskedoff, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t __riscv_vrgather_tu (vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t __riscv_vrgather_tu (vint8m1_t maskedoff, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t __riscv_vrgather_tu (vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t __riscv_vrgather_tu (vint8m2_t maskedoff, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t __riscv_vrgather_tu (vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t __riscv_vrgather_tu (vint8m4_t maskedoff, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t __riscv_vrgather_tu (vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t __riscv_vrgather_tu (vint8m8_t maskedoff, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t __riscv_vrgather_tu (vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t __riscv_vrgather_tu (vint16mf4_t maskedoff, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t __riscv_vrgather_tu (vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t __riscv_vrgather_tu (vint16mf2_t maskedoff, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t __riscv_vrgather_tu (vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t __riscv_vrgather_tu (vint16m1_t maskedoff, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t __riscv_vrgather_tu (vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t __riscv_vrgather_tu (vint16m2_t maskedoff, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t __riscv_vrgather_tu (vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t __riscv_vrgather_tu (vint16m4_t maskedoff, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t __riscv_vrgather_tu (vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t __riscv_vrgather_tu (vint16m8_t maskedoff, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t __riscv_vrgather_tu (vint32mf2_t maskedoff, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t __riscv_vrgather_tu (vint32mf2_t maskedoff, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t __riscv_vrgather_tu (vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t __riscv_vrgather_tu (vint32m1_t maskedoff, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t __riscv_vrgather_tu (vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t __riscv_vrgather_tu (vint32m2_t maskedoff, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t __riscv_vrgather_tu (vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t __riscv_vrgather_tu (vint32m4_t maskedoff, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t __riscv_vrgather_tu (vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t __riscv_vrgather_tu (vint32m8_t maskedoff, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t __riscv_vrgather_tu (vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t __riscv_vrgather_tu (vint64m1_t maskedoff, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t __riscv_vrgather_tu (vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t __riscv_vrgather_tu (vint64m2_t maskedoff, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t __riscv_vrgather_tu (vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t __riscv_vrgather_tu (vint64m4_t maskedoff, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t __riscv_vrgather_tu (vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t __riscv_vrgather_tu (vint64m8_t maskedoff, vint64m8_t op1, size_t index, size_t vl);
vint8mf8_t __riscv_vrgatherei16_tu (vint8mf8_t maskedoff, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t __riscv_vrgatherei16_tu (vint8mf4_t maskedoff, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t __riscv_vrgatherei16_tu (vint8mf2_t maskedoff, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t __riscv_vrgatherei16_tu (vint8m1_t maskedoff, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t __riscv_vrgatherei16_tu (vint8m2_t maskedoff, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t __riscv_vrgatherei16_tu (vint8m4_t maskedoff, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t __riscv_vrgatherei16_tu (vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t __riscv_vrgatherei16_tu (vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t __riscv_vrgatherei16_tu (vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t __riscv_vrgatherei16_tu (vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t __riscv_vrgatherei16_tu (vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t __riscv_vrgatherei16_tu (vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t __riscv_vrgatherei16_tu (vint32mf2_t maskedoff, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t __riscv_vrgatherei16_tu (vint32m1_t maskedoff, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t __riscv_vrgatherei16_tu (vint32m2_t maskedoff, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t __riscv_vrgatherei16_tu (vint32m4_t maskedoff, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t __riscv_vrgatherei16_tu (vint32m8_t maskedoff, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t __riscv_vrgatherei16_tu (vint64m1_t maskedoff, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t __riscv_vrgatherei16_tu (vint64m2_t maskedoff, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t __riscv_vrgatherei16_tu (vint64m4_t maskedoff, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t __riscv_vrgatherei16_tu (vint64m8_t maskedoff, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t __riscv_vrgather_tu (vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t __riscv_vrgather_tu (vuint8mf8_t maskedoff, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_tu (vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_tu (vuint8mf4_t maskedoff, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_tu (vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_tu (vuint8mf2_t maskedoff, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t __riscv_vrgather_tu (vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t __riscv_vrgather_tu (vuint8m1_t maskedoff, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t __riscv_vrgather_tu (vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t __riscv_vrgather_tu (vuint8m2_t maskedoff, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t __riscv_vrgather_tu (vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t __riscv_vrgather_tu (vuint8m4_t maskedoff, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t __riscv_vrgather_tu (vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t __riscv_vrgather_tu (vuint8m8_t maskedoff, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_tu (vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_tu (vuint16mf4_t maskedoff, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_tu (vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_tu (vuint16mf2_t maskedoff, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t __riscv_vrgather_tu (vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t __riscv_vrgather_tu (vuint16m1_t maskedoff, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t __riscv_vrgather_tu (vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t __riscv_vrgather_tu (vuint16m2_t maskedoff, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t __riscv_vrgather_tu (vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t __riscv_vrgather_tu (vuint16m4_t maskedoff, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t __riscv_vrgather_tu (vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t __riscv_vrgather_tu (vuint16m8_t maskedoff, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_tu (vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_tu (vuint32mf2_t maskedoff, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t __riscv_vrgather_tu (vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t __riscv_vrgather_tu (vuint32m1_t maskedoff, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t __riscv_vrgather_tu (vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t __riscv_vrgather_tu (vuint32m2_t maskedoff, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t __riscv_vrgather_tu (vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t __riscv_vrgather_tu (vuint32m4_t maskedoff, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t __riscv_vrgather_tu (vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t __riscv_vrgather_tu (vuint32m8_t maskedoff, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t __riscv_vrgather_tu (vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t __riscv_vrgather_tu (vuint64m1_t maskedoff, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t __riscv_vrgather_tu (vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t __riscv_vrgather_tu (vuint64m2_t maskedoff, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t __riscv_vrgather_tu (vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t __riscv_vrgather_tu (vuint64m4_t maskedoff, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t __riscv_vrgather_tu (vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t __riscv_vrgather_tu (vuint64m8_t maskedoff, vuint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t __riscv_vrgatherei16_tu (vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t __riscv_vrgatherei16_tu (vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t __riscv_vrgatherei16_tu (vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t __riscv_vrgatherei16_tu (vuint8m1_t maskedoff, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t __riscv_vrgatherei16_tu (vuint8m2_t maskedoff, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t __riscv_vrgatherei16_tu (vuint8m4_t maskedoff, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t __riscv_vrgatherei16_tu (vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t __riscv_vrgatherei16_tu (vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t __riscv_vrgatherei16_tu (vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t __riscv_vrgatherei16_tu (vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t __riscv_vrgatherei16_tu (vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t __riscv_vrgatherei16_tu (vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t __riscv_vrgatherei16_tu (vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t __riscv_vrgatherei16_tu (vuint32m1_t maskedoff, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t __riscv_vrgatherei16_tu (vuint32m2_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t __riscv_vrgatherei16_tu (vuint32m4_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t __riscv_vrgatherei16_tu (vuint32m8_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t __riscv_vrgatherei16_tu (vuint64m1_t maskedoff, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t __riscv_vrgatherei16_tu (vuint64m2_t maskedoff, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t __riscv_vrgatherei16_tu (vuint64m4_t maskedoff, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t __riscv_vrgatherei16_tu (vuint64m8_t maskedoff, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather_tum (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t __riscv_vrgather_tum (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_tum (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_tum (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_tum (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_tum (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_tum (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_tum (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_tum (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_tum (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_tum (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_tum (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_tum (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_tum (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_tum (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_tum (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_tum (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_tum (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_tum (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_tum (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_tum (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_tum (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_tum (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_tum (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_tum (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_tum (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_tum (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_tum (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_tum (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_tum (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t __riscv_vrgatherei16_tum (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t __riscv_vrgatherei16_tum (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t __riscv_vrgatherei16_tum (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t __riscv_vrgatherei16_tum (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t __riscv_vrgatherei16_tum (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t __riscv_vrgatherei16_tum (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t __riscv_vrgatherei16_tum (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t __riscv_vrgatherei16_tum (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t __riscv_vrgatherei16_tum (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t __riscv_vrgatherei16_tum (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t __riscv_vrgatherei16_tum (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t __riscv_vrgatherei16_tum (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t __riscv_vrgatherei16_tum (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t __riscv_vrgatherei16_tum (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t __riscv_vrgatherei16_tum (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
vint8mf8_t __riscv_vrgather_tum (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t __riscv_vrgather_tum (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t __riscv_vrgather_tum (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t __riscv_vrgather_tum (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t __riscv_vrgather_tum (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t __riscv_vrgather_tum (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t __riscv_vrgather_tum (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t __riscv_vrgather_tum (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t __riscv_vrgather_tum (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t __riscv_vrgather_tum (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t __riscv_vrgather_tum (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t __riscv_vrgather_tum (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t __riscv_vrgather_tum (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t __riscv_vrgather_tum (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t __riscv_vrgather_tum (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t __riscv_vrgather_tum (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t __riscv_vrgather_tum (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t __riscv_vrgather_tum (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t __riscv_vrgather_tum (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t __riscv_vrgather_tum (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t __riscv_vrgather_tum (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t __riscv_vrgather_tum (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t __riscv_vrgather_tum (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t __riscv_vrgather_tum (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t __riscv_vrgather_tum (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t __riscv_vrgather_tum (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t __riscv_vrgather_tum (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t __riscv_vrgather_tum (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t __riscv_vrgather_tum (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t __riscv_vrgather_tum (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t __riscv_vrgather_tum (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t __riscv_vrgather_tum (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t __riscv_vrgather_tum (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t __riscv_vrgather_tum (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t __riscv_vrgather_tum (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t __riscv_vrgather_tum (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t __riscv_vrgather_tum (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t __riscv_vrgather_tum (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t __riscv_vrgather_tum (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t __riscv_vrgather_tum (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t __riscv_vrgather_tum (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t __riscv_vrgather_tum (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t __riscv_vrgather_tum (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t __riscv_vrgather_tum (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, size_t index, size_t vl);
vint8mf8_t __riscv_vrgatherei16_tum (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t __riscv_vrgatherei16_tum (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t __riscv_vrgatherei16_tum (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t __riscv_vrgatherei16_tum (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t __riscv_vrgatherei16_tum (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t __riscv_vrgatherei16_tum (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t __riscv_vrgatherei16_tum (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t __riscv_vrgatherei16_tum (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t __riscv_vrgatherei16_tum (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t __riscv_vrgatherei16_tum (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t __riscv_vrgatherei16_tum (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t __riscv_vrgatherei16_tum (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t __riscv_vrgatherei16_tum (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t __riscv_vrgatherei16_tum (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t __riscv_vrgatherei16_tum (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t __riscv_vrgatherei16_tum (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t __riscv_vrgatherei16_tum (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t __riscv_vrgatherei16_tum (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t __riscv_vrgatherei16_tum (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t __riscv_vrgatherei16_tum (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t __riscv_vrgatherei16_tum (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t __riscv_vrgather_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t __riscv_vrgather_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t __riscv_vrgather_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t __riscv_vrgather_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t __riscv_vrgather_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t __riscv_vrgather_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t __riscv_vrgather_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t __riscv_vrgather_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t __riscv_vrgather_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t __riscv_vrgather_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t __riscv_vrgather_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t __riscv_vrgather_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t __riscv_vrgather_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t __riscv_vrgather_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t __riscv_vrgather_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t __riscv_vrgather_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t __riscv_vrgather_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t __riscv_vrgather_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t __riscv_vrgather_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t __riscv_vrgather_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t __riscv_vrgather_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t __riscv_vrgather_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t __riscv_vrgather_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t __riscv_vrgather_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t __riscv_vrgather_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t __riscv_vrgather_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t __riscv_vrgather_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t __riscv_vrgather_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t __riscv_vrgather_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t __riscv_vrgather_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t __riscv_vrgather_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t __riscv_vrgather_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t __riscv_vrgather_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t __riscv_vrgather_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t __riscv_vrgatherei16_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t __riscv_vrgatherei16_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t __riscv_vrgatherei16_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t __riscv_vrgatherei16_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t __riscv_vrgatherei16_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t __riscv_vrgatherei16_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t __riscv_vrgatherei16_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t __riscv_vrgatherei16_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t __riscv_vrgatherei16_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t __riscv_vrgatherei16_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t __riscv_vrgatherei16_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t __riscv_vrgatherei16_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t __riscv_vrgatherei16_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t __riscv_vrgatherei16_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t __riscv_vrgatherei16_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t __riscv_vrgatherei16_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t __riscv_vrgatherei16_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t __riscv_vrgatherei16_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t __riscv_vrgatherei16_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t __riscv_vrgatherei16_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t __riscv_vrgatherei16_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather_tumu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t __riscv_vrgather_tumu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_tumu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_tumu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_tumu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_tumu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_tumu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_tumu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_tumu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_tumu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_tumu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_tumu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_tumu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_tumu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_tumu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_tumu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_tumu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_tumu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_tumu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_tumu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_tumu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_tumu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_tumu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_tumu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_tumu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_tumu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_tumu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_tumu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_tumu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_tumu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t __riscv_vrgatherei16_tumu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t __riscv_vrgatherei16_tumu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t __riscv_vrgatherei16_tumu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t __riscv_vrgatherei16_tumu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t __riscv_vrgatherei16_tumu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t __riscv_vrgatherei16_tumu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t __riscv_vrgatherei16_tumu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t __riscv_vrgatherei16_tumu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t __riscv_vrgatherei16_tumu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t __riscv_vrgatherei16_tumu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t __riscv_vrgatherei16_tumu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t __riscv_vrgatherei16_tumu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t __riscv_vrgatherei16_tumu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t __riscv_vrgatherei16_tumu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t __riscv_vrgatherei16_tumu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
vint8mf8_t __riscv_vrgather_tumu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t __riscv_vrgather_tumu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t __riscv_vrgather_tumu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t __riscv_vrgather_tumu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t __riscv_vrgather_tumu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t __riscv_vrgather_tumu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t __riscv_vrgather_tumu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t __riscv_vrgather_tumu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t __riscv_vrgather_tumu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t __riscv_vrgather_tumu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t __riscv_vrgather_tumu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t __riscv_vrgather_tumu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t __riscv_vrgather_tumu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t __riscv_vrgather_tumu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t __riscv_vrgather_tumu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t __riscv_vrgather_tumu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t __riscv_vrgather_tumu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t __riscv_vrgather_tumu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t __riscv_vrgather_tumu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t __riscv_vrgather_tumu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t __riscv_vrgather_tumu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t __riscv_vrgather_tumu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t __riscv_vrgather_tumu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t __riscv_vrgather_tumu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t __riscv_vrgather_tumu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t __riscv_vrgather_tumu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t __riscv_vrgather_tumu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t __riscv_vrgather_tumu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t __riscv_vrgather_tumu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t __riscv_vrgather_tumu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t __riscv_vrgather_tumu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t __riscv_vrgather_tumu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t __riscv_vrgather_tumu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t __riscv_vrgather_tumu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t __riscv_vrgather_tumu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t __riscv_vrgather_tumu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t __riscv_vrgather_tumu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t __riscv_vrgather_tumu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t __riscv_vrgather_tumu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t __riscv_vrgather_tumu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t __riscv_vrgather_tumu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t __riscv_vrgather_tumu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t __riscv_vrgather_tumu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t __riscv_vrgather_tumu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, size_t index, size_t vl);
vint8mf8_t __riscv_vrgatherei16_tumu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t __riscv_vrgatherei16_tumu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t __riscv_vrgatherei16_tumu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t __riscv_vrgatherei16_tumu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t __riscv_vrgatherei16_tumu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t __riscv_vrgatherei16_tumu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t __riscv_vrgatherei16_tumu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t __riscv_vrgatherei16_tumu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t __riscv_vrgatherei16_tumu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t __riscv_vrgatherei16_tumu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t __riscv_vrgatherei16_tumu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t __riscv_vrgatherei16_tumu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t __riscv_vrgatherei16_tumu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t __riscv_vrgatherei16_tumu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t __riscv_vrgatherei16_tumu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t __riscv_vrgatherei16_tumu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t __riscv_vrgatherei16_tumu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t __riscv_vrgatherei16_tumu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t __riscv_vrgatherei16_tumu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t __riscv_vrgatherei16_tumu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t __riscv_vrgatherei16_tumu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t __riscv_vrgather_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t __riscv_vrgather_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t __riscv_vrgather_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t __riscv_vrgather_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t __riscv_vrgather_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t __riscv_vrgather_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t __riscv_vrgather_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t __riscv_vrgather_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t __riscv_vrgather_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t __riscv_vrgather_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t __riscv_vrgather_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t __riscv_vrgather_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t __riscv_vrgather_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t __riscv_vrgather_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t __riscv_vrgather_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t __riscv_vrgather_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t __riscv_vrgather_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t __riscv_vrgather_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t __riscv_vrgather_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t __riscv_vrgather_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t __riscv_vrgather_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t __riscv_vrgather_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t __riscv_vrgather_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t __riscv_vrgather_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t __riscv_vrgather_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t __riscv_vrgather_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t __riscv_vrgather_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t __riscv_vrgather_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t __riscv_vrgather_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t __riscv_vrgather_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t __riscv_vrgather_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t __riscv_vrgather_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t __riscv_vrgather_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t __riscv_vrgather_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t __riscv_vrgatherei16_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t __riscv_vrgatherei16_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t __riscv_vrgatherei16_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t __riscv_vrgatherei16_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t __riscv_vrgatherei16_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t __riscv_vrgatherei16_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t __riscv_vrgatherei16_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t __riscv_vrgatherei16_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t __riscv_vrgatherei16_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t __riscv_vrgatherei16_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t __riscv_vrgatherei16_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t __riscv_vrgatherei16_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t __riscv_vrgatherei16_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t __riscv_vrgatherei16_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t __riscv_vrgatherei16_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t __riscv_vrgatherei16_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t __riscv_vrgatherei16_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t __riscv_vrgatherei16_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t __riscv_vrgatherei16_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t __riscv_vrgatherei16_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t __riscv_vrgatherei16_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather_mu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t index, size_t vl);
vfloat16mf4_t __riscv_vrgather_mu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, size_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_mu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t index, size_t vl);
vfloat16mf2_t __riscv_vrgather_mu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, size_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_mu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t index, size_t vl);
vfloat16m1_t __riscv_vrgather_mu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, size_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_mu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t index, size_t vl);
vfloat16m2_t __riscv_vrgather_mu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, size_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_mu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t index, size_t vl);
vfloat16m4_t __riscv_vrgather_mu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, size_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_mu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t index, size_t vl);
vfloat16m8_t __riscv_vrgather_mu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, size_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_mu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint32mf2_t index, size_t vl);
vfloat32mf2_t __riscv_vrgather_mu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, size_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_mu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint32m1_t index, size_t vl);
vfloat32m1_t __riscv_vrgather_mu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, size_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_mu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint32m2_t index, size_t vl);
vfloat32m2_t __riscv_vrgather_mu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, size_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_mu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint32m4_t index, size_t vl);
vfloat32m4_t __riscv_vrgather_mu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, size_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_mu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint32m8_t index, size_t vl);
vfloat32m8_t __riscv_vrgather_mu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, size_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_mu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint64m1_t index, size_t vl);
vfloat64m1_t __riscv_vrgather_mu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, size_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_mu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint64m2_t index, size_t vl);
vfloat64m2_t __riscv_vrgather_mu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, size_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_mu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint64m4_t index, size_t vl);
vfloat64m4_t __riscv_vrgather_mu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, size_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_mu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint64m8_t index, size_t vl);
vfloat64m8_t __riscv_vrgather_mu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, size_t index, size_t vl);
vfloat16mf4_t __riscv_vrgatherei16_mu (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t op2, size_t vl);
vfloat16mf2_t __riscv_vrgatherei16_mu (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t op2, size_t vl);
vfloat16m1_t __riscv_vrgatherei16_mu (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t op2, size_t vl);
vfloat16m2_t __riscv_vrgatherei16_mu (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t op2, size_t vl);
vfloat16m4_t __riscv_vrgatherei16_mu (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t op2, size_t vl);
vfloat16m8_t __riscv_vrgatherei16_mu (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t op2, size_t vl);
vfloat32mf2_t __riscv_vrgatherei16_mu (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint16mf4_t op2, size_t vl);
vfloat32m1_t __riscv_vrgatherei16_mu (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint16mf2_t op2, size_t vl);
vfloat32m2_t __riscv_vrgatherei16_mu (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint16m1_t op2, size_t vl);
vfloat32m4_t __riscv_vrgatherei16_mu (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint16m2_t op2, size_t vl);
vfloat32m8_t __riscv_vrgatherei16_mu (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint16m4_t op2, size_t vl);
vfloat64m1_t __riscv_vrgatherei16_mu (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint16mf4_t op2, size_t vl);
vfloat64m2_t __riscv_vrgatherei16_mu (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint16mf2_t op2, size_t vl);
vfloat64m4_t __riscv_vrgatherei16_mu (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint16m1_t op2, size_t vl);
vfloat64m8_t __riscv_vrgatherei16_mu (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint16m2_t op2, size_t vl);
vint8mf8_t __riscv_vrgather_mu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint8mf8_t index, size_t vl);
vint8mf8_t __riscv_vrgather_mu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, size_t index, size_t vl);
vint8mf4_t __riscv_vrgather_mu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint8mf4_t index, size_t vl);
vint8mf4_t __riscv_vrgather_mu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, size_t index, size_t vl);
vint8mf2_t __riscv_vrgather_mu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint8mf2_t index, size_t vl);
vint8mf2_t __riscv_vrgather_mu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, size_t index, size_t vl);
vint8m1_t __riscv_vrgather_mu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t index, size_t vl);
vint8m1_t __riscv_vrgather_mu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, size_t index, size_t vl);
vint8m2_t __riscv_vrgather_mu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t index, size_t vl);
vint8m2_t __riscv_vrgather_mu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, size_t index, size_t vl);
vint8m4_t __riscv_vrgather_mu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t index, size_t vl);
vint8m4_t __riscv_vrgather_mu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, size_t index, size_t vl);
vint8m8_t __riscv_vrgather_mu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t index, size_t vl);
vint8m8_t __riscv_vrgather_mu (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, size_t index, size_t vl);
vint16mf4_t __riscv_vrgather_mu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t index, size_t vl);
vint16mf4_t __riscv_vrgather_mu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, size_t index, size_t vl);
vint16mf2_t __riscv_vrgather_mu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t index, size_t vl);
vint16mf2_t __riscv_vrgather_mu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, size_t index, size_t vl);
vint16m1_t __riscv_vrgather_mu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t index, size_t vl);
vint16m1_t __riscv_vrgather_mu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, size_t index, size_t vl);
vint16m2_t __riscv_vrgather_mu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t index, size_t vl);
vint16m2_t __riscv_vrgather_mu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, size_t index, size_t vl);
vint16m4_t __riscv_vrgather_mu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t index, size_t vl);
vint16m4_t __riscv_vrgather_mu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, size_t index, size_t vl);
vint16m8_t __riscv_vrgather_mu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t index, size_t vl);
vint16m8_t __riscv_vrgather_mu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, size_t index, size_t vl);
vint32mf2_t __riscv_vrgather_mu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint32mf2_t index, size_t vl);
vint32mf2_t __riscv_vrgather_mu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, size_t index, size_t vl);
vint32m1_t __riscv_vrgather_mu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t index, size_t vl);
vint32m1_t __riscv_vrgather_mu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, size_t index, size_t vl);
vint32m2_t __riscv_vrgather_mu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t index, size_t vl);
vint32m2_t __riscv_vrgather_mu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, size_t index, size_t vl);
vint32m4_t __riscv_vrgather_mu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t index, size_t vl);
vint32m4_t __riscv_vrgather_mu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, size_t index, size_t vl);
vint32m8_t __riscv_vrgather_mu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t index, size_t vl);
vint32m8_t __riscv_vrgather_mu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, size_t index, size_t vl);
vint64m1_t __riscv_vrgather_mu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t index, size_t vl);
vint64m1_t __riscv_vrgather_mu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, size_t index, size_t vl);
vint64m2_t __riscv_vrgather_mu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t index, size_t vl);
vint64m2_t __riscv_vrgather_mu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, size_t index, size_t vl);
vint64m4_t __riscv_vrgather_mu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t index, size_t vl);
vint64m4_t __riscv_vrgather_mu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, size_t index, size_t vl);
vint64m8_t __riscv_vrgather_mu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t index, size_t vl);
vint64m8_t __riscv_vrgather_mu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, size_t index, size_t vl);
vint8mf8_t __riscv_vrgatherei16_mu (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vint8mf4_t __riscv_vrgatherei16_mu (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vint8mf2_t __riscv_vrgatherei16_mu (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint16m1_t op2, size_t vl);
vint8m1_t __riscv_vrgatherei16_mu (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint16m2_t op2, size_t vl);
vint8m2_t __riscv_vrgatherei16_mu (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint16m4_t op2, size_t vl);
vint8m4_t __riscv_vrgatherei16_mu (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint16m8_t op2, size_t vl);
vint16mf4_t __riscv_vrgatherei16_mu (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vint16mf2_t __riscv_vrgatherei16_mu (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vint16m1_t __riscv_vrgatherei16_mu (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2, size_t vl);
vint16m2_t __riscv_vrgatherei16_mu (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2, size_t vl);
vint16m4_t __riscv_vrgatherei16_mu (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2, size_t vl);
vint16m8_t __riscv_vrgatherei16_mu (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2, size_t vl);
vint32mf2_t __riscv_vrgatherei16_mu (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vint32m1_t __riscv_vrgatherei16_mu (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint16mf2_t op2, size_t vl);
vint32m2_t __riscv_vrgatherei16_mu (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint16m1_t op2, size_t vl);
vint32m4_t __riscv_vrgatherei16_mu (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint16m2_t op2, size_t vl);
vint32m8_t __riscv_vrgatherei16_mu (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint16m4_t op2, size_t vl);
vint64m1_t __riscv_vrgatherei16_mu (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint16mf4_t op2, size_t vl);
vint64m2_t __riscv_vrgatherei16_mu (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint16mf2_t op2, size_t vl);
vint64m4_t __riscv_vrgatherei16_mu (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint16m1_t op2, size_t vl);
vint64m8_t __riscv_vrgatherei16_mu (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint16m2_t op2, size_t vl);
vuint8mf8_t __riscv_vrgather_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint8mf8_t index, size_t vl);
vuint8mf8_t __riscv_vrgather_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, size_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint8mf4_t index, size_t vl);
vuint8mf4_t __riscv_vrgather_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, size_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint8mf2_t index, size_t vl);
vuint8mf2_t __riscv_vrgather_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, size_t index, size_t vl);
vuint8m1_t __riscv_vrgather_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t index, size_t vl);
vuint8m1_t __riscv_vrgather_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, size_t index, size_t vl);
vuint8m2_t __riscv_vrgather_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t index, size_t vl);
vuint8m2_t __riscv_vrgather_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, size_t index, size_t vl);
vuint8m4_t __riscv_vrgather_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t index, size_t vl);
vuint8m4_t __riscv_vrgather_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, size_t index, size_t vl);
vuint8m8_t __riscv_vrgather_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t index, size_t vl);
vuint8m8_t __riscv_vrgather_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, size_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t index, size_t vl);
vuint16mf4_t __riscv_vrgather_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, size_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t index, size_t vl);
vuint16mf2_t __riscv_vrgather_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, size_t index, size_t vl);
vuint16m1_t __riscv_vrgather_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t index, size_t vl);
vuint16m1_t __riscv_vrgather_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, size_t index, size_t vl);
vuint16m2_t __riscv_vrgather_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t index, size_t vl);
vuint16m2_t __riscv_vrgather_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, size_t index, size_t vl);
vuint16m4_t __riscv_vrgather_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t index, size_t vl);
vuint16m4_t __riscv_vrgather_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, size_t index, size_t vl);
vuint16m8_t __riscv_vrgather_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t index, size_t vl);
vuint16m8_t __riscv_vrgather_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, size_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint32mf2_t index, size_t vl);
vuint32mf2_t __riscv_vrgather_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, size_t index, size_t vl);
vuint32m1_t __riscv_vrgather_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t index, size_t vl);
vuint32m1_t __riscv_vrgather_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, size_t index, size_t vl);
vuint32m2_t __riscv_vrgather_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t index, size_t vl);
vuint32m2_t __riscv_vrgather_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, size_t index, size_t vl);
vuint32m4_t __riscv_vrgather_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t index, size_t vl);
vuint32m4_t __riscv_vrgather_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, size_t index, size_t vl);
vuint32m8_t __riscv_vrgather_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t index, size_t vl);
vuint32m8_t __riscv_vrgather_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, size_t index, size_t vl);
vuint64m1_t __riscv_vrgather_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t index, size_t vl);
vuint64m1_t __riscv_vrgather_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, size_t index, size_t vl);
vuint64m2_t __riscv_vrgather_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t index, size_t vl);
vuint64m2_t __riscv_vrgather_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, size_t index, size_t vl);
vuint64m4_t __riscv_vrgather_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t index, size_t vl);
vuint64m4_t __riscv_vrgather_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, size_t index, size_t vl);
vuint64m8_t __riscv_vrgather_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t index, size_t vl);
vuint64m8_t __riscv_vrgather_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, size_t index, size_t vl);
vuint8mf8_t __riscv_vrgatherei16_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint16mf4_t op2, size_t vl);
vuint8mf4_t __riscv_vrgatherei16_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint16mf2_t op2, size_t vl);
vuint8mf2_t __riscv_vrgatherei16_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint16m1_t op2, size_t vl);
vuint8m1_t __riscv_vrgatherei16_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint16m2_t op2, size_t vl);
vuint8m2_t __riscv_vrgatherei16_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint16m4_t op2, size_t vl);
vuint8m4_t __riscv_vrgatherei16_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint16m8_t op2, size_t vl);
vuint16mf4_t __riscv_vrgatherei16_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t op2, size_t vl);
vuint16mf2_t __riscv_vrgatherei16_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t op2, size_t vl);
vuint16m1_t __riscv_vrgatherei16_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2, size_t vl);
vuint16m2_t __riscv_vrgatherei16_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2, size_t vl);
vuint16m4_t __riscv_vrgatherei16_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2, size_t vl);
vuint16m8_t __riscv_vrgatherei16_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2, size_t vl);
vuint32mf2_t __riscv_vrgatherei16_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint16mf4_t op2, size_t vl);
vuint32m1_t __riscv_vrgatherei16_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint16mf2_t op2, size_t vl);
vuint32m2_t __riscv_vrgatherei16_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint16m1_t op2, size_t vl);
vuint32m4_t __riscv_vrgatherei16_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint16m2_t op2, size_t vl);
vuint32m8_t __riscv_vrgatherei16_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint16m4_t op2, size_t vl);
vuint64m1_t __riscv_vrgatherei16_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint16mf4_t op2, size_t vl);
vuint64m2_t __riscv_vrgatherei16_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint16mf2_t op2, size_t vl);
vuint64m4_t __riscv_vrgatherei16_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint16m1_t op2, size_t vl);
vuint64m8_t __riscv_vrgatherei16_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint16m2_t op2, size_t vl);
```

### [Vector Compress Functions](../rvv-intrinsic-api.md#175-vector-compress-operations):

**Prototypes:**
``` C
vfloat16mf4_t __riscv_vcompress_tu (vfloat16mf4_t maskedoff, vfloat16mf4_t src, vbool64_t mask, size_t vl);
vfloat16mf2_t __riscv_vcompress_tu (vfloat16mf2_t maskedoff, vfloat16mf2_t src, vbool32_t mask, size_t vl);
vfloat16m1_t __riscv_vcompress_tu (vfloat16m1_t maskedoff, vfloat16m1_t src, vbool16_t mask, size_t vl);
vfloat16m2_t __riscv_vcompress_tu (vfloat16m2_t maskedoff, vfloat16m2_t src, vbool8_t mask, size_t vl);
vfloat16m4_t __riscv_vcompress_tu (vfloat16m4_t maskedoff, vfloat16m4_t src, vbool4_t mask, size_t vl);
vfloat16m8_t __riscv_vcompress_tu (vfloat16m8_t maskedoff, vfloat16m8_t src, vbool2_t mask, size_t vl);
vfloat32mf2_t __riscv_vcompress_tu (vfloat32mf2_t maskedoff, vfloat32mf2_t src, vbool64_t mask, size_t vl);
vfloat32m1_t __riscv_vcompress_tu (vfloat32m1_t maskedoff, vfloat32m1_t src, vbool32_t mask, size_t vl);
vfloat32m2_t __riscv_vcompress_tu (vfloat32m2_t maskedoff, vfloat32m2_t src, vbool16_t mask, size_t vl);
vfloat32m4_t __riscv_vcompress_tu (vfloat32m4_t maskedoff, vfloat32m4_t src, vbool8_t mask, size_t vl);
vfloat32m8_t __riscv_vcompress_tu (vfloat32m8_t maskedoff, vfloat32m8_t src, vbool4_t mask, size_t vl);
vfloat64m1_t __riscv_vcompress_tu (vfloat64m1_t maskedoff, vfloat64m1_t src, vbool64_t mask, size_t vl);
vfloat64m2_t __riscv_vcompress_tu (vfloat64m2_t maskedoff, vfloat64m2_t src, vbool32_t mask, size_t vl);
vfloat64m4_t __riscv_vcompress_tu (vfloat64m4_t maskedoff, vfloat64m4_t src, vbool16_t mask, size_t vl);
vfloat64m8_t __riscv_vcompress_tu (vfloat64m8_t maskedoff, vfloat64m8_t src, vbool8_t mask, size_t vl);
vint8mf8_t __riscv_vcompress_tu (vint8mf8_t maskedoff, vint8mf8_t src, vbool64_t mask, size_t vl);
vint8mf4_t __riscv_vcompress_tu (vint8mf4_t maskedoff, vint8mf4_t src, vbool32_t mask, size_t vl);
vint8mf2_t __riscv_vcompress_tu (vint8mf2_t maskedoff, vint8mf2_t src, vbool16_t mask, size_t vl);
vint8m1_t __riscv_vcompress_tu (vint8m1_t maskedoff, vint8m1_t src, vbool8_t mask, size_t vl);
vint8m2_t __riscv_vcompress_tu (vint8m2_t maskedoff, vint8m2_t src, vbool4_t mask, size_t vl);
vint8m4_t __riscv_vcompress_tu (vint8m4_t maskedoff, vint8m4_t src, vbool2_t mask, size_t vl);
vint8m8_t __riscv_vcompress_tu (vint8m8_t maskedoff, vint8m8_t src, vbool1_t mask, size_t vl);
vint16mf4_t __riscv_vcompress_tu (vint16mf4_t maskedoff, vint16mf4_t src, vbool64_t mask, size_t vl);
vint16mf2_t __riscv_vcompress_tu (vint16mf2_t maskedoff, vint16mf2_t src, vbool32_t mask, size_t vl);
vint16m1_t __riscv_vcompress_tu (vint16m1_t maskedoff, vint16m1_t src, vbool16_t mask, size_t vl);
vint16m2_t __riscv_vcompress_tu (vint16m2_t maskedoff, vint16m2_t src, vbool8_t mask, size_t vl);
vint16m4_t __riscv_vcompress_tu (vint16m4_t maskedoff, vint16m4_t src, vbool4_t mask, size_t vl);
vint16m8_t __riscv_vcompress_tu (vint16m8_t maskedoff, vint16m8_t src, vbool2_t mask, size_t vl);
vint32mf2_t __riscv_vcompress_tu (vint32mf2_t maskedoff, vint32mf2_t src, vbool64_t mask, size_t vl);
vint32m1_t __riscv_vcompress_tu (vint32m1_t maskedoff, vint32m1_t src, vbool32_t mask, size_t vl);
vint32m2_t __riscv_vcompress_tu (vint32m2_t maskedoff, vint32m2_t src, vbool16_t mask, size_t vl);
vint32m4_t __riscv_vcompress_tu (vint32m4_t maskedoff, vint32m4_t src, vbool8_t mask, size_t vl);
vint32m8_t __riscv_vcompress_tu (vint32m8_t maskedoff, vint32m8_t src, vbool4_t mask, size_t vl);
vint64m1_t __riscv_vcompress_tu (vint64m1_t maskedoff, vint64m1_t src, vbool64_t mask, size_t vl);
vint64m2_t __riscv_vcompress_tu (vint64m2_t maskedoff, vint64m2_t src, vbool32_t mask, size_t vl);
vint64m4_t __riscv_vcompress_tu (vint64m4_t maskedoff, vint64m4_t src, vbool16_t mask, size_t vl);
vint64m8_t __riscv_vcompress_tu (vint64m8_t maskedoff, vint64m8_t src, vbool8_t mask, size_t vl);
vuint8mf8_t __riscv_vcompress_tu (vuint8mf8_t maskedoff, vuint8mf8_t src, vbool64_t mask, size_t vl);
vuint8mf4_t __riscv_vcompress_tu (vuint8mf4_t maskedoff, vuint8mf4_t src, vbool32_t mask, size_t vl);
vuint8mf2_t __riscv_vcompress_tu (vuint8mf2_t maskedoff, vuint8mf2_t src, vbool16_t mask, size_t vl);
vuint8m1_t __riscv_vcompress_tu (vuint8m1_t maskedoff, vuint8m1_t src, vbool8_t mask, size_t vl);
vuint8m2_t __riscv_vcompress_tu (vuint8m2_t maskedoff, vuint8m2_t src, vbool4_t mask, size_t vl);
vuint8m4_t __riscv_vcompress_tu (vuint8m4_t maskedoff, vuint8m4_t src, vbool2_t mask, size_t vl);
vuint8m8_t __riscv_vcompress_tu (vuint8m8_t maskedoff, vuint8m8_t src, vbool1_t mask, size_t vl);
vuint16mf4_t __riscv_vcompress_tu (vuint16mf4_t maskedoff, vuint16mf4_t src, vbool64_t mask, size_t vl);
vuint16mf2_t __riscv_vcompress_tu (vuint16mf2_t maskedoff, vuint16mf2_t src, vbool32_t mask, size_t vl);
vuint16m1_t __riscv_vcompress_tu (vuint16m1_t maskedoff, vuint16m1_t src, vbool16_t mask, size_t vl);
vuint16m2_t __riscv_vcompress_tu (vuint16m2_t maskedoff, vuint16m2_t src, vbool8_t mask, size_t vl);
vuint16m4_t __riscv_vcompress_tu (vuint16m4_t maskedoff, vuint16m4_t src, vbool4_t mask, size_t vl);
vuint16m8_t __riscv_vcompress_tu (vuint16m8_t maskedoff, vuint16m8_t src, vbool2_t mask, size_t vl);
vuint32mf2_t __riscv_vcompress_tu (vuint32mf2_t maskedoff, vuint32mf2_t src, vbool64_t mask, size_t vl);
vuint32m1_t __riscv_vcompress_tu (vuint32m1_t maskedoff, vuint32m1_t src, vbool32_t mask, size_t vl);
vuint32m2_t __riscv_vcompress_tu (vuint32m2_t maskedoff, vuint32m2_t src, vbool16_t mask, size_t vl);
vuint32m4_t __riscv_vcompress_tu (vuint32m4_t maskedoff, vuint32m4_t src, vbool8_t mask, size_t vl);
vuint32m8_t __riscv_vcompress_tu (vuint32m8_t maskedoff, vuint32m8_t src, vbool4_t mask, size_t vl);
vuint64m1_t __riscv_vcompress_tu (vuint64m1_t maskedoff, vuint64m1_t src, vbool64_t mask, size_t vl);
vuint64m2_t __riscv_vcompress_tu (vuint64m2_t maskedoff, vuint64m2_t src, vbool32_t mask, size_t vl);
vuint64m4_t __riscv_vcompress_tu (vuint64m4_t maskedoff, vuint64m4_t src, vbool16_t mask, size_t vl);
vuint64m8_t __riscv_vcompress_tu (vuint64m8_t maskedoff, vuint64m8_t src, vbool8_t mask, size_t vl);
```
