
=== Vector Loads and Stores Segment Instructions

[[policy-variant-vector-unit-stride-segment-load]]
=== Vector Unit-Stride Segment Load Intrinsics

[,c]
----
vfloat16mf4x2_t __riscv_vlseg2e16_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x3_t __riscv_vlseg3e16_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x4_t __riscv_vlseg4e16_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x5_t __riscv_vlseg5e16_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x6_t __riscv_vlseg6e16_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x7_t __riscv_vlseg7e16_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x8_t __riscv_vlseg8e16_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x2_t __riscv_vlseg2e16_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x3_t __riscv_vlseg3e16_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x4_t __riscv_vlseg4e16_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x5_t __riscv_vlseg5e16_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x6_t __riscv_vlseg6e16_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x7_t __riscv_vlseg7e16_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x8_t __riscv_vlseg8e16_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat32mf2x2_t __riscv_vlseg2e32_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x3_t __riscv_vlseg3e32_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x4_t __riscv_vlseg4e32_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x5_t __riscv_vlseg5e32_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x6_t __riscv_vlseg6e32_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x7_t __riscv_vlseg7e32_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x8_t __riscv_vlseg8e32_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat16mf4x2_t
__riscv_vlseg2e16ff_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf4x3_t
__riscv_vlseg3e16ff_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf4x4_t
__riscv_vlseg4e16ff_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf4x5_t
__riscv_vlseg5e16ff_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf4x6_t
__riscv_vlseg6e16ff_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf4x7_t
__riscv_vlseg7e16ff_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf4x8_t
__riscv_vlseg8e16ff_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf2x2_t
__riscv_vlseg2e16ff_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf2x3_t
__riscv_vlseg3e16ff_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf2x4_t
__riscv_vlseg4e16ff_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf2x5_t
__riscv_vlseg5e16ff_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf2x6_t
__riscv_vlseg6e16ff_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf2x7_t
__riscv_vlseg7e16ff_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16mf2x8_t
__riscv_vlseg8e16ff_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                  const float16_t *base, size_t *new_vl,
                                  size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16ff_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16ff_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16ff_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16ff_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16ff_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16ff_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16ff_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16ff_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16ff_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16ff_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16ff_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32mf2x2_t
__riscv_vlseg2e32ff_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                  const float32_t *base, size_t *new_vl,
                                  size_t vl);
vfloat32mf2x3_t
__riscv_vlseg3e32ff_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                  const float32_t *base, size_t *new_vl,
                                  size_t vl);
vfloat32mf2x4_t
__riscv_vlseg4e32ff_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                  const float32_t *base, size_t *new_vl,
                                  size_t vl);
vfloat32mf2x5_t
__riscv_vlseg5e32ff_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                  const float32_t *base, size_t *new_vl,
                                  size_t vl);
vfloat32mf2x6_t
__riscv_vlseg6e32ff_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                  const float32_t *base, size_t *new_vl,
                                  size_t vl);
vfloat32mf2x7_t
__riscv_vlseg7e32ff_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                  const float32_t *base, size_t *new_vl,
                                  size_t vl);
vfloat32mf2x8_t
__riscv_vlseg8e32ff_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                  const float32_t *base, size_t *new_vl,
                                  size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32ff_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32ff_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32ff_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32ff_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32ff_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32ff_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32ff_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32ff_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32ff_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32ff_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32ff_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64ff_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64ff_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64ff_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64ff_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64ff_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64ff_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64ff_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64ff_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64ff_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64ff_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64ff_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x3_t __riscv_vlseg3e8_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x4_t __riscv_vlseg4e8_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x5_t __riscv_vlseg5e8_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x6_t __riscv_vlseg6e8_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x7_t __riscv_vlseg7e8_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x8_t __riscv_vlseg8e8_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x2_t __riscv_vlseg2e8_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x3_t __riscv_vlseg3e8_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x4_t __riscv_vlseg4e8_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x5_t __riscv_vlseg5e8_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x6_t __riscv_vlseg6e8_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x7_t __riscv_vlseg7e8_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x8_t __riscv_vlseg8e8_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x2_t __riscv_vlseg2e8_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x3_t __riscv_vlseg3e8_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x4_t __riscv_vlseg4e8_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x5_t __riscv_vlseg5e8_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x6_t __riscv_vlseg6e8_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x7_t __riscv_vlseg7e8_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x8_t __riscv_vlseg8e8_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x2_t __riscv_vlseg2e8_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x3_t __riscv_vlseg3e8_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x4_t __riscv_vlseg4e8_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x5_t __riscv_vlseg5e8_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x6_t __riscv_vlseg6e8_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x7_t __riscv_vlseg7e8_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x8_t __riscv_vlseg8e8_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m2x2_t __riscv_vlseg2e8_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m2x3_t __riscv_vlseg3e8_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m2x4_t __riscv_vlseg4e8_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m4x2_t __riscv_vlseg2e8_v_i8m4x2_tu(vint8m4x2_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint16mf4x2_t __riscv_vlseg2e16_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x2_t __riscv_vlseg2e16_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x3_t __riscv_vlseg3e16_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x4_t __riscv_vlseg4e16_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x5_t __riscv_vlseg5e16_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x6_t __riscv_vlseg6e16_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x7_t __riscv_vlseg7e16_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x8_t __riscv_vlseg8e16_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m2x2_t __riscv_vlseg2e16_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m2x3_t __riscv_vlseg3e16_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m2x4_t __riscv_vlseg4e16_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m4x2_t __riscv_vlseg2e16_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x2_t __riscv_vlseg2e32_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x3_t __riscv_vlseg3e32_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x4_t __riscv_vlseg4e32_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x5_t __riscv_vlseg5e32_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x6_t __riscv_vlseg6e32_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x7_t __riscv_vlseg7e32_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x8_t __riscv_vlseg8e32_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m2x2_t __riscv_vlseg2e32_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m2x3_t __riscv_vlseg3e32_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m2x4_t __riscv_vlseg4e32_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m4x2_t __riscv_vlseg2e32_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint64m1x2_t __riscv_vlseg2e64_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x3_t __riscv_vlseg3e64_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x4_t __riscv_vlseg4e64_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x5_t __riscv_vlseg5e64_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x6_t __riscv_vlseg6e64_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x7_t __riscv_vlseg7e64_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x8_t __riscv_vlseg8e64_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m2x2_t __riscv_vlseg2e64_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m2x3_t __riscv_vlseg3e64_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m2x4_t __riscv_vlseg4e64_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m4x2_t __riscv_vlseg2e64_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8ff_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x3_t __riscv_vlseg3e8ff_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x4_t __riscv_vlseg4e8ff_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x5_t __riscv_vlseg5e8ff_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x6_t __riscv_vlseg6e8ff_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x7_t __riscv_vlseg7e8ff_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x8_t __riscv_vlseg8e8ff_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x2_t __riscv_vlseg2e8ff_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x3_t __riscv_vlseg3e8ff_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x4_t __riscv_vlseg4e8ff_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x5_t __riscv_vlseg5e8ff_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x6_t __riscv_vlseg6e8ff_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x7_t __riscv_vlseg7e8ff_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x8_t __riscv_vlseg8e8ff_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x2_t __riscv_vlseg2e8ff_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x3_t __riscv_vlseg3e8ff_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x4_t __riscv_vlseg4e8ff_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x5_t __riscv_vlseg5e8ff_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x6_t __riscv_vlseg6e8ff_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x7_t __riscv_vlseg7e8ff_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x8_t __riscv_vlseg8e8ff_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x2_t __riscv_vlseg2e8ff_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x3_t __riscv_vlseg3e8ff_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x4_t __riscv_vlseg4e8ff_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x5_t __riscv_vlseg5e8ff_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x6_t __riscv_vlseg6e8ff_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x7_t __riscv_vlseg7e8ff_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x8_t __riscv_vlseg8e8ff_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m2x2_t __riscv_vlseg2e8ff_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m2x3_t __riscv_vlseg3e8ff_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m2x4_t __riscv_vlseg4e8ff_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m4x2_t __riscv_vlseg2e8ff_v_i8m4x2_tu(vint8m4x2_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint16mf4x2_t __riscv_vlseg2e16ff_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16ff_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16ff_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16ff_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16ff_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16ff_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16ff_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16ff_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16ff_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16ff_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16ff_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16ff_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16ff_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16ff_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x2_t __riscv_vlseg2e16ff_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x3_t __riscv_vlseg3e16ff_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x4_t __riscv_vlseg4e16ff_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x5_t __riscv_vlseg5e16ff_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x6_t __riscv_vlseg6e16ff_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x7_t __riscv_vlseg7e16ff_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x8_t __riscv_vlseg8e16ff_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m2x2_t __riscv_vlseg2e16ff_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m2x3_t __riscv_vlseg3e16ff_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m2x4_t __riscv_vlseg4e16ff_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m4x2_t __riscv_vlseg2e16ff_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32ff_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32ff_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32ff_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32ff_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32ff_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32ff_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32ff_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x2_t __riscv_vlseg2e32ff_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x3_t __riscv_vlseg3e32ff_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x4_t __riscv_vlseg4e32ff_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x5_t __riscv_vlseg5e32ff_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x6_t __riscv_vlseg6e32ff_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x7_t __riscv_vlseg7e32ff_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x8_t __riscv_vlseg8e32ff_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m2x2_t __riscv_vlseg2e32ff_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m2x3_t __riscv_vlseg3e32ff_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m2x4_t __riscv_vlseg4e32ff_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m4x2_t __riscv_vlseg2e32ff_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x2_t __riscv_vlseg2e64ff_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x3_t __riscv_vlseg3e64ff_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x4_t __riscv_vlseg4e64ff_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x5_t __riscv_vlseg5e64ff_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x6_t __riscv_vlseg6e64ff_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x7_t __riscv_vlseg7e64ff_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x8_t __riscv_vlseg8e64ff_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m2x2_t __riscv_vlseg2e64ff_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m2x3_t __riscv_vlseg3e64ff_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m2x4_t __riscv_vlseg4e64ff_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m4x2_t __riscv_vlseg2e64ff_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x3_t __riscv_vlseg3e8_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x4_t __riscv_vlseg4e8_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x5_t __riscv_vlseg5e8_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x6_t __riscv_vlseg6e8_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x7_t __riscv_vlseg7e8_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x8_t __riscv_vlseg8e8_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m2x2_t __riscv_vlseg2e8_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m2x3_t __riscv_vlseg3e8_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m2x4_t __riscv_vlseg4e8_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m4x2_t __riscv_vlseg2e8_v_u8m4x2_tu(vuint8m4x2_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x2_t __riscv_vlseg2e16_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x2_t __riscv_vlseg2e32_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8ff_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8ff_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8ff_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8ff_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8ff_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8ff_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8ff_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8ff_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8ff_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8ff_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8ff_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8ff_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8ff_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8ff_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8ff_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8ff_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8ff_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8ff_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8ff_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8ff_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8ff_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8ff_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x3_t __riscv_vlseg3e8ff_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x4_t __riscv_vlseg4e8ff_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x5_t __riscv_vlseg5e8ff_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x6_t __riscv_vlseg6e8ff_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x7_t __riscv_vlseg7e8ff_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x8_t __riscv_vlseg8e8ff_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m2x2_t __riscv_vlseg2e8ff_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m2x3_t __riscv_vlseg3e8ff_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m2x4_t __riscv_vlseg4e8ff_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m4x2_t __riscv_vlseg2e8ff_v_u8m4x2_tu(vuint8m4x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16ff_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16ff_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16ff_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16ff_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16ff_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16ff_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16ff_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16ff_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16ff_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16ff_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16ff_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16ff_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16ff_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16ff_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x2_t __riscv_vlseg2e16ff_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16ff_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16ff_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16ff_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16ff_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16ff_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16ff_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16ff_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16ff_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16ff_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16ff_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32ff_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32ff_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32ff_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32ff_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32ff_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32ff_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32ff_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x2_t __riscv_vlseg2e32ff_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32ff_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32ff_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32ff_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32ff_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32ff_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32ff_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32ff_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32ff_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32ff_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32ff_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64ff_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64ff_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64ff_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64ff_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64ff_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64ff_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64ff_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64ff_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64ff_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64ff_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64ff_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
// masked functions
vfloat16mf4x2_t
__riscv_vlseg2e16_v_f16mf4x2_tum(vbool64_t mask,
                                 vfloat16mf4x2_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf4x3_t
__riscv_vlseg3e16_v_f16mf4x3_tum(vbool64_t mask,
                                 vfloat16mf4x3_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf4x4_t
__riscv_vlseg4e16_v_f16mf4x4_tum(vbool64_t mask,
                                 vfloat16mf4x4_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf4x5_t
__riscv_vlseg5e16_v_f16mf4x5_tum(vbool64_t mask,
                                 vfloat16mf4x5_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf4x6_t
__riscv_vlseg6e16_v_f16mf4x6_tum(vbool64_t mask,
                                 vfloat16mf4x6_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf4x7_t
__riscv_vlseg7e16_v_f16mf4x7_tum(vbool64_t mask,
                                 vfloat16mf4x7_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf4x8_t
__riscv_vlseg8e16_v_f16mf4x8_tum(vbool64_t mask,
                                 vfloat16mf4x8_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf2x2_t
__riscv_vlseg2e16_v_f16mf2x2_tum(vbool32_t mask,
                                 vfloat16mf2x2_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf2x3_t
__riscv_vlseg3e16_v_f16mf2x3_tum(vbool32_t mask,
                                 vfloat16mf2x3_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf2x4_t
__riscv_vlseg4e16_v_f16mf2x4_tum(vbool32_t mask,
                                 vfloat16mf2x4_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf2x5_t
__riscv_vlseg5e16_v_f16mf2x5_tum(vbool32_t mask,
                                 vfloat16mf2x5_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf2x6_t
__riscv_vlseg6e16_v_f16mf2x6_tum(vbool32_t mask,
                                 vfloat16mf2x6_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf2x7_t
__riscv_vlseg7e16_v_f16mf2x7_tum(vbool32_t mask,
                                 vfloat16mf2x7_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16mf2x8_t
__riscv_vlseg8e16_v_f16mf2x8_tum(vbool32_t mask,
                                 vfloat16mf2x8_t maskedoff_tuple,
                                 const float16_t *base, size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16_v_f16m1x2_tum(vbool16_t mask,
                                               vfloat16m1x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16_v_f16m1x3_tum(vbool16_t mask,
                                               vfloat16m1x3_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16_v_f16m1x4_tum(vbool16_t mask,
                                               vfloat16m1x4_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16_v_f16m1x5_tum(vbool16_t mask,
                                               vfloat16m1x5_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16_v_f16m1x6_tum(vbool16_t mask,
                                               vfloat16m1x6_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16_v_f16m1x7_tum(vbool16_t mask,
                                               vfloat16m1x7_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16_v_f16m1x8_tum(vbool16_t mask,
                                               vfloat16m1x8_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16_v_f16m2x2_tum(vbool8_t mask,
                                               vfloat16m2x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16_v_f16m2x3_tum(vbool8_t mask,
                                               vfloat16m2x3_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16_v_f16m2x4_tum(vbool8_t mask,
                                               vfloat16m2x4_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16_v_f16m4x2_tum(vbool4_t mask,
                                               vfloat16m4x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               size_t vl);
vfloat32mf2x2_t
__riscv_vlseg2e32_v_f32mf2x2_tum(vbool64_t mask,
                                 vfloat32mf2x2_t maskedoff_tuple,
                                 const float32_t *base, size_t vl);
vfloat32mf2x3_t
__riscv_vlseg3e32_v_f32mf2x3_tum(vbool64_t mask,
                                 vfloat32mf2x3_t maskedoff_tuple,
                                 const float32_t *base, size_t vl);
vfloat32mf2x4_t
__riscv_vlseg4e32_v_f32mf2x4_tum(vbool64_t mask,
                                 vfloat32mf2x4_t maskedoff_tuple,
                                 const float32_t *base, size_t vl);
vfloat32mf2x5_t
__riscv_vlseg5e32_v_f32mf2x5_tum(vbool64_t mask,
                                 vfloat32mf2x5_t maskedoff_tuple,
                                 const float32_t *base, size_t vl);
vfloat32mf2x6_t
__riscv_vlseg6e32_v_f32mf2x6_tum(vbool64_t mask,
                                 vfloat32mf2x6_t maskedoff_tuple,
                                 const float32_t *base, size_t vl);
vfloat32mf2x7_t
__riscv_vlseg7e32_v_f32mf2x7_tum(vbool64_t mask,
                                 vfloat32mf2x7_t maskedoff_tuple,
                                 const float32_t *base, size_t vl);
vfloat32mf2x8_t
__riscv_vlseg8e32_v_f32mf2x8_tum(vbool64_t mask,
                                 vfloat32mf2x8_t maskedoff_tuple,
                                 const float32_t *base, size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32_v_f32m1x2_tum(vbool32_t mask,
                                               vfloat32m1x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32_v_f32m1x3_tum(vbool32_t mask,
                                               vfloat32m1x3_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32_v_f32m1x4_tum(vbool32_t mask,
                                               vfloat32m1x4_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32_v_f32m1x5_tum(vbool32_t mask,
                                               vfloat32m1x5_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32_v_f32m1x6_tum(vbool32_t mask,
                                               vfloat32m1x6_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32_v_f32m1x7_tum(vbool32_t mask,
                                               vfloat32m1x7_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32_v_f32m1x8_tum(vbool32_t mask,
                                               vfloat32m1x8_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32_v_f32m2x2_tum(vbool16_t mask,
                                               vfloat32m2x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32_v_f32m2x3_tum(vbool16_t mask,
                                               vfloat32m2x3_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32_v_f32m2x4_tum(vbool16_t mask,
                                               vfloat32m2x4_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32_v_f32m4x2_tum(vbool8_t mask,
                                               vfloat32m4x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64_v_f64m1x2_tum(vbool64_t mask,
                                               vfloat64m1x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64_v_f64m1x3_tum(vbool64_t mask,
                                               vfloat64m1x3_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64_v_f64m1x4_tum(vbool64_t mask,
                                               vfloat64m1x4_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64_v_f64m1x5_tum(vbool64_t mask,
                                               vfloat64m1x5_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64_v_f64m1x6_tum(vbool64_t mask,
                                               vfloat64m1x6_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64_v_f64m1x7_tum(vbool64_t mask,
                                               vfloat64m1x7_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64_v_f64m1x8_tum(vbool64_t mask,
                                               vfloat64m1x8_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64_v_f64m2x2_tum(vbool32_t mask,
                                               vfloat64m2x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64_v_f64m2x3_tum(vbool32_t mask,
                                               vfloat64m2x3_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64_v_f64m2x4_tum(vbool32_t mask,
                                               vfloat64m2x4_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64_v_f64m4x2_tum(vbool16_t mask,
                                               vfloat64m4x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               size_t vl);
vfloat16mf4x2_t __riscv_vlseg2e16ff_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x3_t __riscv_vlseg3e16ff_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x4_t __riscv_vlseg4e16ff_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x5_t __riscv_vlseg5e16ff_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x6_t __riscv_vlseg6e16ff_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x7_t __riscv_vlseg7e16ff_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x8_t __riscv_vlseg8e16ff_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x2_t __riscv_vlseg2e16ff_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x3_t __riscv_vlseg3e16ff_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x4_t __riscv_vlseg4e16ff_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x5_t __riscv_vlseg5e16ff_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x6_t __riscv_vlseg6e16ff_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x7_t __riscv_vlseg7e16ff_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x8_t __riscv_vlseg8e16ff_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16ff_v_f16m1x2_tum(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16ff_v_f16m1x3_tum(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16ff_v_f16m1x4_tum(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16ff_v_f16m1x5_tum(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16ff_v_f16m1x6_tum(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16ff_v_f16m1x7_tum(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16ff_v_f16m1x8_tum(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16ff_v_f16m2x2_tum(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16ff_v_f16m2x3_tum(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16ff_v_f16m2x4_tum(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16ff_v_f16m4x2_tum(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32mf2x2_t __riscv_vlseg2e32ff_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x3_t __riscv_vlseg3e32ff_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x4_t __riscv_vlseg4e32ff_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x5_t __riscv_vlseg5e32ff_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x6_t __riscv_vlseg6e32ff_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x7_t __riscv_vlseg7e32ff_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x8_t __riscv_vlseg8e32ff_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32ff_v_f32m1x2_tum(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32ff_v_f32m1x3_tum(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32ff_v_f32m1x4_tum(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32ff_v_f32m1x5_tum(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32ff_v_f32m1x6_tum(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32ff_v_f32m1x7_tum(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32ff_v_f32m1x8_tum(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32ff_v_f32m2x2_tum(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32ff_v_f32m2x3_tum(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32ff_v_f32m2x4_tum(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32ff_v_f32m4x2_tum(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64ff_v_f64m1x2_tum(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64ff_v_f64m1x3_tum(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64ff_v_f64m1x4_tum(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64ff_v_f64m1x5_tum(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64ff_v_f64m1x6_tum(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64ff_v_f64m1x7_tum(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64ff_v_f64m1x8_tum(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64ff_v_f64m2x2_tum(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64ff_v_f64m2x3_tum(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64ff_v_f64m2x4_tum(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64ff_v_f64m4x2_tum(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 size_t *new_vl, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8_v_i8mf8x2_tum(vbool64_t mask,
                                            vint8mf8x2_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf8x3_t __riscv_vlseg3e8_v_i8mf8x3_tum(vbool64_t mask,
                                            vint8mf8x3_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf8x4_t __riscv_vlseg4e8_v_i8mf8x4_tum(vbool64_t mask,
                                            vint8mf8x4_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf8x5_t __riscv_vlseg5e8_v_i8mf8x5_tum(vbool64_t mask,
                                            vint8mf8x5_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf8x6_t __riscv_vlseg6e8_v_i8mf8x6_tum(vbool64_t mask,
                                            vint8mf8x6_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf8x7_t __riscv_vlseg7e8_v_i8mf8x7_tum(vbool64_t mask,
                                            vint8mf8x7_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf8x8_t __riscv_vlseg8e8_v_i8mf8x8_tum(vbool64_t mask,
                                            vint8mf8x8_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf4x2_t __riscv_vlseg2e8_v_i8mf4x2_tum(vbool32_t mask,
                                            vint8mf4x2_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf4x3_t __riscv_vlseg3e8_v_i8mf4x3_tum(vbool32_t mask,
                                            vint8mf4x3_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf4x4_t __riscv_vlseg4e8_v_i8mf4x4_tum(vbool32_t mask,
                                            vint8mf4x4_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf4x5_t __riscv_vlseg5e8_v_i8mf4x5_tum(vbool32_t mask,
                                            vint8mf4x5_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf4x6_t __riscv_vlseg6e8_v_i8mf4x6_tum(vbool32_t mask,
                                            vint8mf4x6_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf4x7_t __riscv_vlseg7e8_v_i8mf4x7_tum(vbool32_t mask,
                                            vint8mf4x7_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf4x8_t __riscv_vlseg8e8_v_i8mf4x8_tum(vbool32_t mask,
                                            vint8mf4x8_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf2x2_t __riscv_vlseg2e8_v_i8mf2x2_tum(vbool16_t mask,
                                            vint8mf2x2_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf2x3_t __riscv_vlseg3e8_v_i8mf2x3_tum(vbool16_t mask,
                                            vint8mf2x3_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf2x4_t __riscv_vlseg4e8_v_i8mf2x4_tum(vbool16_t mask,
                                            vint8mf2x4_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf2x5_t __riscv_vlseg5e8_v_i8mf2x5_tum(vbool16_t mask,
                                            vint8mf2x5_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf2x6_t __riscv_vlseg6e8_v_i8mf2x6_tum(vbool16_t mask,
                                            vint8mf2x6_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf2x7_t __riscv_vlseg7e8_v_i8mf2x7_tum(vbool16_t mask,
                                            vint8mf2x7_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8mf2x8_t __riscv_vlseg8e8_v_i8mf2x8_tum(vbool16_t mask,
                                            vint8mf2x8_t maskedoff_tuple,
                                            const int8_t *base, size_t vl);
vint8m1x2_t __riscv_vlseg2e8_v_i8m1x2_tum(vbool8_t mask,
                                          vint8m1x2_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m1x3_t __riscv_vlseg3e8_v_i8m1x3_tum(vbool8_t mask,
                                          vint8m1x3_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m1x4_t __riscv_vlseg4e8_v_i8m1x4_tum(vbool8_t mask,
                                          vint8m1x4_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m1x5_t __riscv_vlseg5e8_v_i8m1x5_tum(vbool8_t mask,
                                          vint8m1x5_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m1x6_t __riscv_vlseg6e8_v_i8m1x6_tum(vbool8_t mask,
                                          vint8m1x6_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m1x7_t __riscv_vlseg7e8_v_i8m1x7_tum(vbool8_t mask,
                                          vint8m1x7_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m1x8_t __riscv_vlseg8e8_v_i8m1x8_tum(vbool8_t mask,
                                          vint8m1x8_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m2x2_t __riscv_vlseg2e8_v_i8m2x2_tum(vbool4_t mask,
                                          vint8m2x2_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m2x3_t __riscv_vlseg3e8_v_i8m2x3_tum(vbool4_t mask,
                                          vint8m2x3_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m2x4_t __riscv_vlseg4e8_v_i8m2x4_tum(vbool4_t mask,
                                          vint8m2x4_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint8m4x2_t __riscv_vlseg2e8_v_i8m4x2_tum(vbool2_t mask,
                                          vint8m4x2_t maskedoff_tuple,
                                          const int8_t *base, size_t vl);
vint16mf4x2_t __riscv_vlseg2e16_v_i16mf4x2_tum(vbool64_t mask,
                                               vint16mf4x2_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16_v_i16mf4x3_tum(vbool64_t mask,
                                               vint16mf4x3_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16_v_i16mf4x4_tum(vbool64_t mask,
                                               vint16mf4x4_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16_v_i16mf4x5_tum(vbool64_t mask,
                                               vint16mf4x5_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16_v_i16mf4x6_tum(vbool64_t mask,
                                               vint16mf4x6_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16_v_i16mf4x7_tum(vbool64_t mask,
                                               vint16mf4x7_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16_v_i16mf4x8_tum(vbool64_t mask,
                                               vint16mf4x8_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16_v_i16mf2x2_tum(vbool32_t mask,
                                               vint16mf2x2_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16_v_i16mf2x3_tum(vbool32_t mask,
                                               vint16mf2x3_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16_v_i16mf2x4_tum(vbool32_t mask,
                                               vint16mf2x4_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16_v_i16mf2x5_tum(vbool32_t mask,
                                               vint16mf2x5_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16_v_i16mf2x6_tum(vbool32_t mask,
                                               vint16mf2x6_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16_v_i16mf2x7_tum(vbool32_t mask,
                                               vint16mf2x7_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16_v_i16mf2x8_tum(vbool32_t mask,
                                               vint16mf2x8_t maskedoff_tuple,
                                               const int16_t *base, size_t vl);
vint16m1x2_t __riscv_vlseg2e16_v_i16m1x2_tum(vbool16_t mask,
                                             vint16m1x2_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m1x3_t __riscv_vlseg3e16_v_i16m1x3_tum(vbool16_t mask,
                                             vint16m1x3_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m1x4_t __riscv_vlseg4e16_v_i16m1x4_tum(vbool16_t mask,
                                             vint16m1x4_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m1x5_t __riscv_vlseg5e16_v_i16m1x5_tum(vbool16_t mask,
                                             vint16m1x5_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m1x6_t __riscv_vlseg6e16_v_i16m1x6_tum(vbool16_t mask,
                                             vint16m1x6_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m1x7_t __riscv_vlseg7e16_v_i16m1x7_tum(vbool16_t mask,
                                             vint16m1x7_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m1x8_t __riscv_vlseg8e16_v_i16m1x8_tum(vbool16_t mask,
                                             vint16m1x8_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m2x2_t __riscv_vlseg2e16_v_i16m2x2_tum(vbool8_t mask,
                                             vint16m2x2_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m2x3_t __riscv_vlseg3e16_v_i16m2x3_tum(vbool8_t mask,
                                             vint16m2x3_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m2x4_t __riscv_vlseg4e16_v_i16m2x4_tum(vbool8_t mask,
                                             vint16m2x4_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint16m4x2_t __riscv_vlseg2e16_v_i16m4x2_tum(vbool4_t mask,
                                             vint16m4x2_t maskedoff_tuple,
                                             const int16_t *base, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32_v_i32mf2x2_tum(vbool64_t mask,
                                               vint32mf2x2_t maskedoff_tuple,
                                               const int32_t *base, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32_v_i32mf2x3_tum(vbool64_t mask,
                                               vint32mf2x3_t maskedoff_tuple,
                                               const int32_t *base, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32_v_i32mf2x4_tum(vbool64_t mask,
                                               vint32mf2x4_t maskedoff_tuple,
                                               const int32_t *base, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32_v_i32mf2x5_tum(vbool64_t mask,
                                               vint32mf2x5_t maskedoff_tuple,
                                               const int32_t *base, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32_v_i32mf2x6_tum(vbool64_t mask,
                                               vint32mf2x6_t maskedoff_tuple,
                                               const int32_t *base, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32_v_i32mf2x7_tum(vbool64_t mask,
                                               vint32mf2x7_t maskedoff_tuple,
                                               const int32_t *base, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32_v_i32mf2x8_tum(vbool64_t mask,
                                               vint32mf2x8_t maskedoff_tuple,
                                               const int32_t *base, size_t vl);
vint32m1x2_t __riscv_vlseg2e32_v_i32m1x2_tum(vbool32_t mask,
                                             vint32m1x2_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m1x3_t __riscv_vlseg3e32_v_i32m1x3_tum(vbool32_t mask,
                                             vint32m1x3_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m1x4_t __riscv_vlseg4e32_v_i32m1x4_tum(vbool32_t mask,
                                             vint32m1x4_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m1x5_t __riscv_vlseg5e32_v_i32m1x5_tum(vbool32_t mask,
                                             vint32m1x5_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m1x6_t __riscv_vlseg6e32_v_i32m1x6_tum(vbool32_t mask,
                                             vint32m1x6_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m1x7_t __riscv_vlseg7e32_v_i32m1x7_tum(vbool32_t mask,
                                             vint32m1x7_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m1x8_t __riscv_vlseg8e32_v_i32m1x8_tum(vbool32_t mask,
                                             vint32m1x8_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m2x2_t __riscv_vlseg2e32_v_i32m2x2_tum(vbool16_t mask,
                                             vint32m2x2_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m2x3_t __riscv_vlseg3e32_v_i32m2x3_tum(vbool16_t mask,
                                             vint32m2x3_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m2x4_t __riscv_vlseg4e32_v_i32m2x4_tum(vbool16_t mask,
                                             vint32m2x4_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint32m4x2_t __riscv_vlseg2e32_v_i32m4x2_tum(vbool8_t mask,
                                             vint32m4x2_t maskedoff_tuple,
                                             const int32_t *base, size_t vl);
vint64m1x2_t __riscv_vlseg2e64_v_i64m1x2_tum(vbool64_t mask,
                                             vint64m1x2_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m1x3_t __riscv_vlseg3e64_v_i64m1x3_tum(vbool64_t mask,
                                             vint64m1x3_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m1x4_t __riscv_vlseg4e64_v_i64m1x4_tum(vbool64_t mask,
                                             vint64m1x4_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m1x5_t __riscv_vlseg5e64_v_i64m1x5_tum(vbool64_t mask,
                                             vint64m1x5_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m1x6_t __riscv_vlseg6e64_v_i64m1x6_tum(vbool64_t mask,
                                             vint64m1x6_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m1x7_t __riscv_vlseg7e64_v_i64m1x7_tum(vbool64_t mask,
                                             vint64m1x7_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m1x8_t __riscv_vlseg8e64_v_i64m1x8_tum(vbool64_t mask,
                                             vint64m1x8_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m2x2_t __riscv_vlseg2e64_v_i64m2x2_tum(vbool32_t mask,
                                             vint64m2x2_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m2x3_t __riscv_vlseg3e64_v_i64m2x3_tum(vbool32_t mask,
                                             vint64m2x3_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m2x4_t __riscv_vlseg4e64_v_i64m2x4_tum(vbool32_t mask,
                                             vint64m2x4_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint64m4x2_t __riscv_vlseg2e64_v_i64m4x2_tum(vbool16_t mask,
                                             vint64m4x2_t maskedoff_tuple,
                                             const int64_t *base, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8ff_v_i8mf8x2_tum(vbool64_t mask,
                                              vint8mf8x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf8x3_t __riscv_vlseg3e8ff_v_i8mf8x3_tum(vbool64_t mask,
                                              vint8mf8x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf8x4_t __riscv_vlseg4e8ff_v_i8mf8x4_tum(vbool64_t mask,
                                              vint8mf8x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf8x5_t __riscv_vlseg5e8ff_v_i8mf8x5_tum(vbool64_t mask,
                                              vint8mf8x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf8x6_t __riscv_vlseg6e8ff_v_i8mf8x6_tum(vbool64_t mask,
                                              vint8mf8x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf8x7_t __riscv_vlseg7e8ff_v_i8mf8x7_tum(vbool64_t mask,
                                              vint8mf8x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf8x8_t __riscv_vlseg8e8ff_v_i8mf8x8_tum(vbool64_t mask,
                                              vint8mf8x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf4x2_t __riscv_vlseg2e8ff_v_i8mf4x2_tum(vbool32_t mask,
                                              vint8mf4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf4x3_t __riscv_vlseg3e8ff_v_i8mf4x3_tum(vbool32_t mask,
                                              vint8mf4x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf4x4_t __riscv_vlseg4e8ff_v_i8mf4x4_tum(vbool32_t mask,
                                              vint8mf4x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf4x5_t __riscv_vlseg5e8ff_v_i8mf4x5_tum(vbool32_t mask,
                                              vint8mf4x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf4x6_t __riscv_vlseg6e8ff_v_i8mf4x6_tum(vbool32_t mask,
                                              vint8mf4x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf4x7_t __riscv_vlseg7e8ff_v_i8mf4x7_tum(vbool32_t mask,
                                              vint8mf4x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf4x8_t __riscv_vlseg8e8ff_v_i8mf4x8_tum(vbool32_t mask,
                                              vint8mf4x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf2x2_t __riscv_vlseg2e8ff_v_i8mf2x2_tum(vbool16_t mask,
                                              vint8mf2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf2x3_t __riscv_vlseg3e8ff_v_i8mf2x3_tum(vbool16_t mask,
                                              vint8mf2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf2x4_t __riscv_vlseg4e8ff_v_i8mf2x4_tum(vbool16_t mask,
                                              vint8mf2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf2x5_t __riscv_vlseg5e8ff_v_i8mf2x5_tum(vbool16_t mask,
                                              vint8mf2x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf2x6_t __riscv_vlseg6e8ff_v_i8mf2x6_tum(vbool16_t mask,
                                              vint8mf2x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf2x7_t __riscv_vlseg7e8ff_v_i8mf2x7_tum(vbool16_t mask,
                                              vint8mf2x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8mf2x8_t __riscv_vlseg8e8ff_v_i8mf2x8_tum(vbool16_t mask,
                                              vint8mf2x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              size_t *new_vl, size_t vl);
vint8m1x2_t __riscv_vlseg2e8ff_v_i8m1x2_tum(vbool8_t mask,
                                            vint8m1x2_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m1x3_t __riscv_vlseg3e8ff_v_i8m1x3_tum(vbool8_t mask,
                                            vint8m1x3_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m1x4_t __riscv_vlseg4e8ff_v_i8m1x4_tum(vbool8_t mask,
                                            vint8m1x4_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m1x5_t __riscv_vlseg5e8ff_v_i8m1x5_tum(vbool8_t mask,
                                            vint8m1x5_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m1x6_t __riscv_vlseg6e8ff_v_i8m1x6_tum(vbool8_t mask,
                                            vint8m1x6_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m1x7_t __riscv_vlseg7e8ff_v_i8m1x7_tum(vbool8_t mask,
                                            vint8m1x7_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m1x8_t __riscv_vlseg8e8ff_v_i8m1x8_tum(vbool8_t mask,
                                            vint8m1x8_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m2x2_t __riscv_vlseg2e8ff_v_i8m2x2_tum(vbool4_t mask,
                                            vint8m2x2_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m2x3_t __riscv_vlseg3e8ff_v_i8m2x3_tum(vbool4_t mask,
                                            vint8m2x3_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m2x4_t __riscv_vlseg4e8ff_v_i8m2x4_tum(vbool4_t mask,
                                            vint8m2x4_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint8m4x2_t __riscv_vlseg2e8ff_v_i8m4x2_tum(vbool2_t mask,
                                            vint8m4x2_t maskedoff_tuple,
                                            const int8_t *base, size_t *new_vl,
                                            size_t vl);
vint16mf4x2_t __riscv_vlseg2e16ff_v_i16mf4x2_tum(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16ff_v_i16mf4x3_tum(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16ff_v_i16mf4x4_tum(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16ff_v_i16mf4x5_tum(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16ff_v_i16mf4x6_tum(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16ff_v_i16mf4x7_tum(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16ff_v_i16mf4x8_tum(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16ff_v_i16mf2x2_tum(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16ff_v_i16mf2x3_tum(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16ff_v_i16mf2x4_tum(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16ff_v_i16mf2x5_tum(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16ff_v_i16mf2x6_tum(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16ff_v_i16mf2x7_tum(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16ff_v_i16mf2x8_tum(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 size_t *new_vl, size_t vl);
vint16m1x2_t __riscv_vlseg2e16ff_v_i16m1x2_tum(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m1x3_t __riscv_vlseg3e16ff_v_i16m1x3_tum(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m1x4_t __riscv_vlseg4e16ff_v_i16m1x4_tum(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m1x5_t __riscv_vlseg5e16ff_v_i16m1x5_tum(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m1x6_t __riscv_vlseg6e16ff_v_i16m1x6_tum(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m1x7_t __riscv_vlseg7e16ff_v_i16m1x7_tum(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m1x8_t __riscv_vlseg8e16ff_v_i16m1x8_tum(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m2x2_t __riscv_vlseg2e16ff_v_i16m2x2_tum(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m2x3_t __riscv_vlseg3e16ff_v_i16m2x3_tum(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m2x4_t __riscv_vlseg4e16ff_v_i16m2x4_tum(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint16m4x2_t __riscv_vlseg2e16ff_v_i16m4x2_tum(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               size_t *new_vl, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32ff_v_i32mf2x2_tum(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 size_t *new_vl, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32ff_v_i32mf2x3_tum(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 size_t *new_vl, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32ff_v_i32mf2x4_tum(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 size_t *new_vl, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32ff_v_i32mf2x5_tum(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 size_t *new_vl, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32ff_v_i32mf2x6_tum(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 size_t *new_vl, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32ff_v_i32mf2x7_tum(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 size_t *new_vl, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32ff_v_i32mf2x8_tum(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 size_t *new_vl, size_t vl);
vint32m1x2_t __riscv_vlseg2e32ff_v_i32m1x2_tum(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m1x3_t __riscv_vlseg3e32ff_v_i32m1x3_tum(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m1x4_t __riscv_vlseg4e32ff_v_i32m1x4_tum(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m1x5_t __riscv_vlseg5e32ff_v_i32m1x5_tum(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m1x6_t __riscv_vlseg6e32ff_v_i32m1x6_tum(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m1x7_t __riscv_vlseg7e32ff_v_i32m1x7_tum(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m1x8_t __riscv_vlseg8e32ff_v_i32m1x8_tum(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m2x2_t __riscv_vlseg2e32ff_v_i32m2x2_tum(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m2x3_t __riscv_vlseg3e32ff_v_i32m2x3_tum(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m2x4_t __riscv_vlseg4e32ff_v_i32m2x4_tum(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint32m4x2_t __riscv_vlseg2e32ff_v_i32m4x2_tum(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               size_t *new_vl, size_t vl);
vint64m1x2_t __riscv_vlseg2e64ff_v_i64m1x2_tum(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m1x3_t __riscv_vlseg3e64ff_v_i64m1x3_tum(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m1x4_t __riscv_vlseg4e64ff_v_i64m1x4_tum(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m1x5_t __riscv_vlseg5e64ff_v_i64m1x5_tum(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m1x6_t __riscv_vlseg6e64ff_v_i64m1x6_tum(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m1x7_t __riscv_vlseg7e64ff_v_i64m1x7_tum(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m1x8_t __riscv_vlseg8e64ff_v_i64m1x8_tum(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m2x2_t __riscv_vlseg2e64ff_v_i64m2x2_tum(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m2x3_t __riscv_vlseg3e64ff_v_i64m2x3_tum(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m2x4_t __riscv_vlseg4e64ff_v_i64m2x4_tum(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vint64m4x2_t __riscv_vlseg2e64ff_v_i64m4x2_tum(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8_v_u8mf8x2_tum(vbool64_t mask,
                                             vuint8mf8x2_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8_v_u8mf8x3_tum(vbool64_t mask,
                                             vuint8mf8x3_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8_v_u8mf8x4_tum(vbool64_t mask,
                                             vuint8mf8x4_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8_v_u8mf8x5_tum(vbool64_t mask,
                                             vuint8mf8x5_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8_v_u8mf8x6_tum(vbool64_t mask,
                                             vuint8mf8x6_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8_v_u8mf8x7_tum(vbool64_t mask,
                                             vuint8mf8x7_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8_v_u8mf8x8_tum(vbool64_t mask,
                                             vuint8mf8x8_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8_v_u8mf4x2_tum(vbool32_t mask,
                                             vuint8mf4x2_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8_v_u8mf4x3_tum(vbool32_t mask,
                                             vuint8mf4x3_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8_v_u8mf4x4_tum(vbool32_t mask,
                                             vuint8mf4x4_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8_v_u8mf4x5_tum(vbool32_t mask,
                                             vuint8mf4x5_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8_v_u8mf4x6_tum(vbool32_t mask,
                                             vuint8mf4x6_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8_v_u8mf4x7_tum(vbool32_t mask,
                                             vuint8mf4x7_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8_v_u8mf4x8_tum(vbool32_t mask,
                                             vuint8mf4x8_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8_v_u8mf2x2_tum(vbool16_t mask,
                                             vuint8mf2x2_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8_v_u8mf2x3_tum(vbool16_t mask,
                                             vuint8mf2x3_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8_v_u8mf2x4_tum(vbool16_t mask,
                                             vuint8mf2x4_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8_v_u8mf2x5_tum(vbool16_t mask,
                                             vuint8mf2x5_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8_v_u8mf2x6_tum(vbool16_t mask,
                                             vuint8mf2x6_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8_v_u8mf2x7_tum(vbool16_t mask,
                                             vuint8mf2x7_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8_v_u8mf2x8_tum(vbool16_t mask,
                                             vuint8mf2x8_t maskedoff_tuple,
                                             const uint8_t *base, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8_v_u8m1x2_tum(vbool8_t mask,
                                           vuint8m1x2_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m1x3_t __riscv_vlseg3e8_v_u8m1x3_tum(vbool8_t mask,
                                           vuint8m1x3_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m1x4_t __riscv_vlseg4e8_v_u8m1x4_tum(vbool8_t mask,
                                           vuint8m1x4_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m1x5_t __riscv_vlseg5e8_v_u8m1x5_tum(vbool8_t mask,
                                           vuint8m1x5_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m1x6_t __riscv_vlseg6e8_v_u8m1x6_tum(vbool8_t mask,
                                           vuint8m1x6_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m1x7_t __riscv_vlseg7e8_v_u8m1x7_tum(vbool8_t mask,
                                           vuint8m1x7_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m1x8_t __riscv_vlseg8e8_v_u8m1x8_tum(vbool8_t mask,
                                           vuint8m1x8_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m2x2_t __riscv_vlseg2e8_v_u8m2x2_tum(vbool4_t mask,
                                           vuint8m2x2_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m2x3_t __riscv_vlseg3e8_v_u8m2x3_tum(vbool4_t mask,
                                           vuint8m2x3_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m2x4_t __riscv_vlseg4e8_v_u8m2x4_tum(vbool4_t mask,
                                           vuint8m2x4_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint8m4x2_t __riscv_vlseg2e8_v_u8m4x2_tum(vbool2_t mask,
                                           vuint8m4x2_t maskedoff_tuple,
                                           const uint8_t *base, size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16_v_u16mf4x2_tum(vbool64_t mask,
                                                vuint16mf4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16_v_u16mf4x3_tum(vbool64_t mask,
                                                vuint16mf4x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16_v_u16mf4x4_tum(vbool64_t mask,
                                                vuint16mf4x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16_v_u16mf4x5_tum(vbool64_t mask,
                                                vuint16mf4x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16_v_u16mf4x6_tum(vbool64_t mask,
                                                vuint16mf4x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16_v_u16mf4x7_tum(vbool64_t mask,
                                                vuint16mf4x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16_v_u16mf4x8_tum(vbool64_t mask,
                                                vuint16mf4x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16_v_u16mf2x2_tum(vbool32_t mask,
                                                vuint16mf2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16_v_u16mf2x3_tum(vbool32_t mask,
                                                vuint16mf2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16_v_u16mf2x4_tum(vbool32_t mask,
                                                vuint16mf2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16_v_u16mf2x5_tum(vbool32_t mask,
                                                vuint16mf2x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16_v_u16mf2x6_tum(vbool32_t mask,
                                                vuint16mf2x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16_v_u16mf2x7_tum(vbool32_t mask,
                                                vuint16mf2x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16_v_u16mf2x8_tum(vbool32_t mask,
                                                vuint16mf2x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t vl);
vuint16m1x2_t __riscv_vlseg2e16_v_u16m1x2_tum(vbool16_t mask,
                                              vuint16m1x2_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16_v_u16m1x3_tum(vbool16_t mask,
                                              vuint16m1x3_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16_v_u16m1x4_tum(vbool16_t mask,
                                              vuint16m1x4_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16_v_u16m1x5_tum(vbool16_t mask,
                                              vuint16m1x5_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16_v_u16m1x6_tum(vbool16_t mask,
                                              vuint16m1x6_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16_v_u16m1x7_tum(vbool16_t mask,
                                              vuint16m1x7_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16_v_u16m1x8_tum(vbool16_t mask,
                                              vuint16m1x8_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16_v_u16m2x2_tum(vbool8_t mask,
                                              vuint16m2x2_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16_v_u16m2x3_tum(vbool8_t mask,
                                              vuint16m2x3_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16_v_u16m2x4_tum(vbool8_t mask,
                                              vuint16m2x4_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16_v_u16m4x2_tum(vbool4_t mask,
                                              vuint16m4x2_t maskedoff_tuple,
                                              const uint16_t *base, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32_v_u32mf2x2_tum(vbool64_t mask,
                                                vuint32mf2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32_v_u32mf2x3_tum(vbool64_t mask,
                                                vuint32mf2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32_v_u32mf2x4_tum(vbool64_t mask,
                                                vuint32mf2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32_v_u32mf2x5_tum(vbool64_t mask,
                                                vuint32mf2x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32_v_u32mf2x6_tum(vbool64_t mask,
                                                vuint32mf2x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32_v_u32mf2x7_tum(vbool64_t mask,
                                                vuint32mf2x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32_v_u32mf2x8_tum(vbool64_t mask,
                                                vuint32mf2x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t vl);
vuint32m1x2_t __riscv_vlseg2e32_v_u32m1x2_tum(vbool32_t mask,
                                              vuint32m1x2_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32_v_u32m1x3_tum(vbool32_t mask,
                                              vuint32m1x3_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32_v_u32m1x4_tum(vbool32_t mask,
                                              vuint32m1x4_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32_v_u32m1x5_tum(vbool32_t mask,
                                              vuint32m1x5_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32_v_u32m1x6_tum(vbool32_t mask,
                                              vuint32m1x6_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32_v_u32m1x7_tum(vbool32_t mask,
                                              vuint32m1x7_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32_v_u32m1x8_tum(vbool32_t mask,
                                              vuint32m1x8_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32_v_u32m2x2_tum(vbool16_t mask,
                                              vuint32m2x2_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32_v_u32m2x3_tum(vbool16_t mask,
                                              vuint32m2x3_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32_v_u32m2x4_tum(vbool16_t mask,
                                              vuint32m2x4_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32_v_u32m4x2_tum(vbool8_t mask,
                                              vuint32m4x2_t maskedoff_tuple,
                                              const uint32_t *base, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64_v_u64m1x2_tum(vbool64_t mask,
                                              vuint64m1x2_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64_v_u64m1x3_tum(vbool64_t mask,
                                              vuint64m1x3_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64_v_u64m1x4_tum(vbool64_t mask,
                                              vuint64m1x4_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64_v_u64m1x5_tum(vbool64_t mask,
                                              vuint64m1x5_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64_v_u64m1x6_tum(vbool64_t mask,
                                              vuint64m1x6_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64_v_u64m1x7_tum(vbool64_t mask,
                                              vuint64m1x7_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64_v_u64m1x8_tum(vbool64_t mask,
                                              vuint64m1x8_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64_v_u64m2x2_tum(vbool32_t mask,
                                              vuint64m2x2_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64_v_u64m2x3_tum(vbool32_t mask,
                                              vuint64m2x3_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64_v_u64m2x4_tum(vbool32_t mask,
                                              vuint64m2x4_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64_v_u64m4x2_tum(vbool16_t mask,
                                              vuint64m4x2_t maskedoff_tuple,
                                              const uint64_t *base, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8ff_v_u8mf8x2_tum(vbool64_t mask,
                                               vuint8mf8x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8ff_v_u8mf8x3_tum(vbool64_t mask,
                                               vuint8mf8x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8ff_v_u8mf8x4_tum(vbool64_t mask,
                                               vuint8mf8x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8ff_v_u8mf8x5_tum(vbool64_t mask,
                                               vuint8mf8x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8ff_v_u8mf8x6_tum(vbool64_t mask,
                                               vuint8mf8x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8ff_v_u8mf8x7_tum(vbool64_t mask,
                                               vuint8mf8x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8ff_v_u8mf8x8_tum(vbool64_t mask,
                                               vuint8mf8x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8ff_v_u8mf4x2_tum(vbool32_t mask,
                                               vuint8mf4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8ff_v_u8mf4x3_tum(vbool32_t mask,
                                               vuint8mf4x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8ff_v_u8mf4x4_tum(vbool32_t mask,
                                               vuint8mf4x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8ff_v_u8mf4x5_tum(vbool32_t mask,
                                               vuint8mf4x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8ff_v_u8mf4x6_tum(vbool32_t mask,
                                               vuint8mf4x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8ff_v_u8mf4x7_tum(vbool32_t mask,
                                               vuint8mf4x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8ff_v_u8mf4x8_tum(vbool32_t mask,
                                               vuint8mf4x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8ff_v_u8mf2x2_tum(vbool16_t mask,
                                               vuint8mf2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8ff_v_u8mf2x3_tum(vbool16_t mask,
                                               vuint8mf2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8ff_v_u8mf2x4_tum(vbool16_t mask,
                                               vuint8mf2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8ff_v_u8mf2x5_tum(vbool16_t mask,
                                               vuint8mf2x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8ff_v_u8mf2x6_tum(vbool16_t mask,
                                               vuint8mf2x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8ff_v_u8mf2x7_tum(vbool16_t mask,
                                               vuint8mf2x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8ff_v_u8mf2x8_tum(vbool16_t mask,
                                               vuint8mf2x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               size_t *new_vl, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8ff_v_u8m1x2_tum(vbool8_t mask,
                                             vuint8m1x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m1x3_t __riscv_vlseg3e8ff_v_u8m1x3_tum(vbool8_t mask,
                                             vuint8m1x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m1x4_t __riscv_vlseg4e8ff_v_u8m1x4_tum(vbool8_t mask,
                                             vuint8m1x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m1x5_t __riscv_vlseg5e8ff_v_u8m1x5_tum(vbool8_t mask,
                                             vuint8m1x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m1x6_t __riscv_vlseg6e8ff_v_u8m1x6_tum(vbool8_t mask,
                                             vuint8m1x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m1x7_t __riscv_vlseg7e8ff_v_u8m1x7_tum(vbool8_t mask,
                                             vuint8m1x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m1x8_t __riscv_vlseg8e8ff_v_u8m1x8_tum(vbool8_t mask,
                                             vuint8m1x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m2x2_t __riscv_vlseg2e8ff_v_u8m2x2_tum(vbool4_t mask,
                                             vuint8m2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m2x3_t __riscv_vlseg3e8ff_v_u8m2x3_tum(vbool4_t mask,
                                             vuint8m2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m2x4_t __riscv_vlseg4e8ff_v_u8m2x4_tum(vbool4_t mask,
                                             vuint8m2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint8m4x2_t __riscv_vlseg2e8ff_v_u8m4x2_tum(vbool2_t mask,
                                             vuint8m4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             size_t *new_vl, size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16ff_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16ff_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16ff_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16ff_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16ff_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16ff_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16ff_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16ff_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16ff_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16ff_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16ff_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16ff_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16ff_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16ff_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16m1x2_t __riscv_vlseg2e16ff_v_u16m1x2_tum(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16ff_v_u16m1x3_tum(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16ff_v_u16m1x4_tum(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16ff_v_u16m1x5_tum(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16ff_v_u16m1x6_tum(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16ff_v_u16m1x7_tum(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16ff_v_u16m1x8_tum(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16ff_v_u16m2x2_tum(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16ff_v_u16m2x3_tum(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16ff_v_u16m2x4_tum(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16ff_v_u16m4x2_tum(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                size_t *new_vl, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32ff_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32ff_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32ff_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32ff_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32ff_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32ff_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32ff_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32m1x2_t __riscv_vlseg2e32ff_v_u32m1x2_tum(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32ff_v_u32m1x3_tum(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32ff_v_u32m1x4_tum(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32ff_v_u32m1x5_tum(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32ff_v_u32m1x6_tum(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32ff_v_u32m1x7_tum(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32ff_v_u32m1x8_tum(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32ff_v_u32m2x2_tum(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32ff_v_u32m2x3_tum(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32ff_v_u32m2x4_tum(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32ff_v_u32m4x2_tum(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64ff_v_u64m1x2_tum(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64ff_v_u64m1x3_tum(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64ff_v_u64m1x4_tum(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64ff_v_u64m1x5_tum(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64ff_v_u64m1x6_tum(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64ff_v_u64m1x7_tum(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64ff_v_u64m1x8_tum(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64ff_v_u64m2x2_tum(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64ff_v_u64m2x3_tum(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64ff_v_u64m2x4_tum(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64ff_v_u64m4x2_tum(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                size_t *new_vl, size_t vl);
// masked functions
vfloat16mf4x2_t
__riscv_vlseg2e16_v_f16mf4x2_tumu(vbool64_t mask,
                                  vfloat16mf4x2_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf4x3_t
__riscv_vlseg3e16_v_f16mf4x3_tumu(vbool64_t mask,
                                  vfloat16mf4x3_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf4x4_t
__riscv_vlseg4e16_v_f16mf4x4_tumu(vbool64_t mask,
                                  vfloat16mf4x4_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf4x5_t
__riscv_vlseg5e16_v_f16mf4x5_tumu(vbool64_t mask,
                                  vfloat16mf4x5_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf4x6_t
__riscv_vlseg6e16_v_f16mf4x6_tumu(vbool64_t mask,
                                  vfloat16mf4x6_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf4x7_t
__riscv_vlseg7e16_v_f16mf4x7_tumu(vbool64_t mask,
                                  vfloat16mf4x7_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf4x8_t
__riscv_vlseg8e16_v_f16mf4x8_tumu(vbool64_t mask,
                                  vfloat16mf4x8_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf2x2_t
__riscv_vlseg2e16_v_f16mf2x2_tumu(vbool32_t mask,
                                  vfloat16mf2x2_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf2x3_t
__riscv_vlseg3e16_v_f16mf2x3_tumu(vbool32_t mask,
                                  vfloat16mf2x3_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf2x4_t
__riscv_vlseg4e16_v_f16mf2x4_tumu(vbool32_t mask,
                                  vfloat16mf2x4_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf2x5_t
__riscv_vlseg5e16_v_f16mf2x5_tumu(vbool32_t mask,
                                  vfloat16mf2x5_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf2x6_t
__riscv_vlseg6e16_v_f16mf2x6_tumu(vbool32_t mask,
                                  vfloat16mf2x6_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf2x7_t
__riscv_vlseg7e16_v_f16mf2x7_tumu(vbool32_t mask,
                                  vfloat16mf2x7_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16mf2x8_t
__riscv_vlseg8e16_v_f16mf2x8_tumu(vbool32_t mask,
                                  vfloat16mf2x8_t maskedoff_tuple,
                                  const float16_t *base, size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16_v_f16m1x2_tumu(vbool16_t mask,
                                                vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16_v_f16m1x3_tumu(vbool16_t mask,
                                                vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16_v_f16m1x4_tumu(vbool16_t mask,
                                                vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16_v_f16m1x5_tumu(vbool16_t mask,
                                                vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16_v_f16m1x6_tumu(vbool16_t mask,
                                                vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16_v_f16m1x7_tumu(vbool16_t mask,
                                                vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16_v_f16m1x8_tumu(vbool16_t mask,
                                                vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16_v_f16m2x2_tumu(vbool8_t mask,
                                                vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16_v_f16m2x3_tumu(vbool8_t mask,
                                                vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16_v_f16m2x4_tumu(vbool8_t mask,
                                                vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16_v_f16m4x2_tumu(vbool4_t mask,
                                                vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat32mf2x2_t
__riscv_vlseg2e32_v_f32mf2x2_tumu(vbool64_t mask,
                                  vfloat32mf2x2_t maskedoff_tuple,
                                  const float32_t *base, size_t vl);
vfloat32mf2x3_t
__riscv_vlseg3e32_v_f32mf2x3_tumu(vbool64_t mask,
                                  vfloat32mf2x3_t maskedoff_tuple,
                                  const float32_t *base, size_t vl);
vfloat32mf2x4_t
__riscv_vlseg4e32_v_f32mf2x4_tumu(vbool64_t mask,
                                  vfloat32mf2x4_t maskedoff_tuple,
                                  const float32_t *base, size_t vl);
vfloat32mf2x5_t
__riscv_vlseg5e32_v_f32mf2x5_tumu(vbool64_t mask,
                                  vfloat32mf2x5_t maskedoff_tuple,
                                  const float32_t *base, size_t vl);
vfloat32mf2x6_t
__riscv_vlseg6e32_v_f32mf2x6_tumu(vbool64_t mask,
                                  vfloat32mf2x6_t maskedoff_tuple,
                                  const float32_t *base, size_t vl);
vfloat32mf2x7_t
__riscv_vlseg7e32_v_f32mf2x7_tumu(vbool64_t mask,
                                  vfloat32mf2x7_t maskedoff_tuple,
                                  const float32_t *base, size_t vl);
vfloat32mf2x8_t
__riscv_vlseg8e32_v_f32mf2x8_tumu(vbool64_t mask,
                                  vfloat32mf2x8_t maskedoff_tuple,
                                  const float32_t *base, size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32_v_f32m1x2_tumu(vbool32_t mask,
                                                vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32_v_f32m1x3_tumu(vbool32_t mask,
                                                vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32_v_f32m1x4_tumu(vbool32_t mask,
                                                vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32_v_f32m1x5_tumu(vbool32_t mask,
                                                vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32_v_f32m1x6_tumu(vbool32_t mask,
                                                vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32_v_f32m1x7_tumu(vbool32_t mask,
                                                vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32_v_f32m1x8_tumu(vbool32_t mask,
                                                vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32_v_f32m2x2_tumu(vbool16_t mask,
                                                vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32_v_f32m2x3_tumu(vbool16_t mask,
                                                vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32_v_f32m2x4_tumu(vbool16_t mask,
                                                vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32_v_f32m4x2_tumu(vbool8_t mask,
                                                vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64_v_f64m1x2_tumu(vbool64_t mask,
                                                vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64_v_f64m1x3_tumu(vbool64_t mask,
                                                vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64_v_f64m1x4_tumu(vbool64_t mask,
                                                vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64_v_f64m1x5_tumu(vbool64_t mask,
                                                vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64_v_f64m1x6_tumu(vbool64_t mask,
                                                vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64_v_f64m1x7_tumu(vbool64_t mask,
                                                vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64_v_f64m1x8_tumu(vbool64_t mask,
                                                vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64_v_f64m2x2_tumu(vbool32_t mask,
                                                vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64_v_f64m2x3_tumu(vbool32_t mask,
                                                vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64_v_f64m2x4_tumu(vbool32_t mask,
                                                vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64_v_f64m4x2_tumu(vbool16_t mask,
                                                vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t vl);
vfloat16mf4x2_t __riscv_vlseg2e16ff_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x3_t __riscv_vlseg3e16ff_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x4_t __riscv_vlseg4e16ff_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x5_t __riscv_vlseg5e16ff_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x6_t __riscv_vlseg6e16ff_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x7_t __riscv_vlseg7e16ff_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x8_t __riscv_vlseg8e16ff_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x2_t __riscv_vlseg2e16ff_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x3_t __riscv_vlseg3e16ff_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x4_t __riscv_vlseg4e16ff_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x5_t __riscv_vlseg5e16ff_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x6_t __riscv_vlseg6e16ff_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x7_t __riscv_vlseg7e16ff_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x8_t __riscv_vlseg8e16ff_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16ff_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16ff_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16ff_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16ff_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16ff_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16ff_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16ff_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16ff_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16ff_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16ff_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16ff_v_f16m4x2_tumu(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x2_t __riscv_vlseg2e32ff_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x3_t __riscv_vlseg3e32ff_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x4_t __riscv_vlseg4e32ff_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x5_t __riscv_vlseg5e32ff_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x6_t __riscv_vlseg6e32ff_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x7_t __riscv_vlseg7e32ff_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x8_t __riscv_vlseg8e32ff_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32ff_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32ff_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32ff_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32ff_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32ff_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32ff_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32ff_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32ff_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32ff_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32ff_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32ff_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64ff_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64ff_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64ff_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64ff_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64ff_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64ff_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64ff_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64ff_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64ff_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64ff_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64ff_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    size_t *new_vl, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8_v_i8mf8x2_tumu(vbool64_t mask,
                                             vint8mf8x2_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf8x3_t __riscv_vlseg3e8_v_i8mf8x3_tumu(vbool64_t mask,
                                             vint8mf8x3_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf8x4_t __riscv_vlseg4e8_v_i8mf8x4_tumu(vbool64_t mask,
                                             vint8mf8x4_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf8x5_t __riscv_vlseg5e8_v_i8mf8x5_tumu(vbool64_t mask,
                                             vint8mf8x5_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf8x6_t __riscv_vlseg6e8_v_i8mf8x6_tumu(vbool64_t mask,
                                             vint8mf8x6_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf8x7_t __riscv_vlseg7e8_v_i8mf8x7_tumu(vbool64_t mask,
                                             vint8mf8x7_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf8x8_t __riscv_vlseg8e8_v_i8mf8x8_tumu(vbool64_t mask,
                                             vint8mf8x8_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf4x2_t __riscv_vlseg2e8_v_i8mf4x2_tumu(vbool32_t mask,
                                             vint8mf4x2_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf4x3_t __riscv_vlseg3e8_v_i8mf4x3_tumu(vbool32_t mask,
                                             vint8mf4x3_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf4x4_t __riscv_vlseg4e8_v_i8mf4x4_tumu(vbool32_t mask,
                                             vint8mf4x4_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf4x5_t __riscv_vlseg5e8_v_i8mf4x5_tumu(vbool32_t mask,
                                             vint8mf4x5_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf4x6_t __riscv_vlseg6e8_v_i8mf4x6_tumu(vbool32_t mask,
                                             vint8mf4x6_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf4x7_t __riscv_vlseg7e8_v_i8mf4x7_tumu(vbool32_t mask,
                                             vint8mf4x7_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf4x8_t __riscv_vlseg8e8_v_i8mf4x8_tumu(vbool32_t mask,
                                             vint8mf4x8_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf2x2_t __riscv_vlseg2e8_v_i8mf2x2_tumu(vbool16_t mask,
                                             vint8mf2x2_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf2x3_t __riscv_vlseg3e8_v_i8mf2x3_tumu(vbool16_t mask,
                                             vint8mf2x3_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf2x4_t __riscv_vlseg4e8_v_i8mf2x4_tumu(vbool16_t mask,
                                             vint8mf2x4_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf2x5_t __riscv_vlseg5e8_v_i8mf2x5_tumu(vbool16_t mask,
                                             vint8mf2x5_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf2x6_t __riscv_vlseg6e8_v_i8mf2x6_tumu(vbool16_t mask,
                                             vint8mf2x6_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf2x7_t __riscv_vlseg7e8_v_i8mf2x7_tumu(vbool16_t mask,
                                             vint8mf2x7_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8mf2x8_t __riscv_vlseg8e8_v_i8mf2x8_tumu(vbool16_t mask,
                                             vint8mf2x8_t maskedoff_tuple,
                                             const int8_t *base, size_t vl);
vint8m1x2_t __riscv_vlseg2e8_v_i8m1x2_tumu(vbool8_t mask,
                                           vint8m1x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x3_t __riscv_vlseg3e8_v_i8m1x3_tumu(vbool8_t mask,
                                           vint8m1x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x4_t __riscv_vlseg4e8_v_i8m1x4_tumu(vbool8_t mask,
                                           vint8m1x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x5_t __riscv_vlseg5e8_v_i8m1x5_tumu(vbool8_t mask,
                                           vint8m1x5_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x6_t __riscv_vlseg6e8_v_i8m1x6_tumu(vbool8_t mask,
                                           vint8m1x6_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x7_t __riscv_vlseg7e8_v_i8m1x7_tumu(vbool8_t mask,
                                           vint8m1x7_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x8_t __riscv_vlseg8e8_v_i8m1x8_tumu(vbool8_t mask,
                                           vint8m1x8_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m2x2_t __riscv_vlseg2e8_v_i8m2x2_tumu(vbool4_t mask,
                                           vint8m2x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m2x3_t __riscv_vlseg3e8_v_i8m2x3_tumu(vbool4_t mask,
                                           vint8m2x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m2x4_t __riscv_vlseg4e8_v_i8m2x4_tumu(vbool4_t mask,
                                           vint8m2x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m4x2_t __riscv_vlseg2e8_v_i8m4x2_tumu(vbool2_t mask,
                                           vint8m4x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint16mf4x2_t __riscv_vlseg2e16_v_i16mf4x2_tumu(vbool64_t mask,
                                                vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16_v_i16mf4x3_tumu(vbool64_t mask,
                                                vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16_v_i16mf4x4_tumu(vbool64_t mask,
                                                vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16_v_i16mf4x5_tumu(vbool64_t mask,
                                                vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16_v_i16mf4x6_tumu(vbool64_t mask,
                                                vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16_v_i16mf4x7_tumu(vbool64_t mask,
                                                vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16_v_i16mf4x8_tumu(vbool64_t mask,
                                                vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16_v_i16mf2x2_tumu(vbool32_t mask,
                                                vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16_v_i16mf2x3_tumu(vbool32_t mask,
                                                vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16_v_i16mf2x4_tumu(vbool32_t mask,
                                                vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16_v_i16mf2x5_tumu(vbool32_t mask,
                                                vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16_v_i16mf2x6_tumu(vbool32_t mask,
                                                vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16_v_i16mf2x7_tumu(vbool32_t mask,
                                                vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16_v_i16mf2x8_tumu(vbool32_t mask,
                                                vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base, size_t vl);
vint16m1x2_t __riscv_vlseg2e16_v_i16m1x2_tumu(vbool16_t mask,
                                              vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x3_t __riscv_vlseg3e16_v_i16m1x3_tumu(vbool16_t mask,
                                              vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x4_t __riscv_vlseg4e16_v_i16m1x4_tumu(vbool16_t mask,
                                              vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x5_t __riscv_vlseg5e16_v_i16m1x5_tumu(vbool16_t mask,
                                              vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x6_t __riscv_vlseg6e16_v_i16m1x6_tumu(vbool16_t mask,
                                              vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x7_t __riscv_vlseg7e16_v_i16m1x7_tumu(vbool16_t mask,
                                              vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x8_t __riscv_vlseg8e16_v_i16m1x8_tumu(vbool16_t mask,
                                              vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m2x2_t __riscv_vlseg2e16_v_i16m2x2_tumu(vbool8_t mask,
                                              vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m2x3_t __riscv_vlseg3e16_v_i16m2x3_tumu(vbool8_t mask,
                                              vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m2x4_t __riscv_vlseg4e16_v_i16m2x4_tumu(vbool8_t mask,
                                              vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m4x2_t __riscv_vlseg2e16_v_i16m4x2_tumu(vbool4_t mask,
                                              vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32_v_i32mf2x2_tumu(vbool64_t mask,
                                                vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32_v_i32mf2x3_tumu(vbool64_t mask,
                                                vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32_v_i32mf2x4_tumu(vbool64_t mask,
                                                vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32_v_i32mf2x5_tumu(vbool64_t mask,
                                                vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32_v_i32mf2x6_tumu(vbool64_t mask,
                                                vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32_v_i32mf2x7_tumu(vbool64_t mask,
                                                vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32_v_i32mf2x8_tumu(vbool64_t mask,
                                                vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base, size_t vl);
vint32m1x2_t __riscv_vlseg2e32_v_i32m1x2_tumu(vbool32_t mask,
                                              vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x3_t __riscv_vlseg3e32_v_i32m1x3_tumu(vbool32_t mask,
                                              vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x4_t __riscv_vlseg4e32_v_i32m1x4_tumu(vbool32_t mask,
                                              vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x5_t __riscv_vlseg5e32_v_i32m1x5_tumu(vbool32_t mask,
                                              vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x6_t __riscv_vlseg6e32_v_i32m1x6_tumu(vbool32_t mask,
                                              vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x7_t __riscv_vlseg7e32_v_i32m1x7_tumu(vbool32_t mask,
                                              vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x8_t __riscv_vlseg8e32_v_i32m1x8_tumu(vbool32_t mask,
                                              vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m2x2_t __riscv_vlseg2e32_v_i32m2x2_tumu(vbool16_t mask,
                                              vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m2x3_t __riscv_vlseg3e32_v_i32m2x3_tumu(vbool16_t mask,
                                              vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m2x4_t __riscv_vlseg4e32_v_i32m2x4_tumu(vbool16_t mask,
                                              vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m4x2_t __riscv_vlseg2e32_v_i32m4x2_tumu(vbool8_t mask,
                                              vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint64m1x2_t __riscv_vlseg2e64_v_i64m1x2_tumu(vbool64_t mask,
                                              vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m1x3_t __riscv_vlseg3e64_v_i64m1x3_tumu(vbool64_t mask,
                                              vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m1x4_t __riscv_vlseg4e64_v_i64m1x4_tumu(vbool64_t mask,
                                              vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m1x5_t __riscv_vlseg5e64_v_i64m1x5_tumu(vbool64_t mask,
                                              vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m1x6_t __riscv_vlseg6e64_v_i64m1x6_tumu(vbool64_t mask,
                                              vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m1x7_t __riscv_vlseg7e64_v_i64m1x7_tumu(vbool64_t mask,
                                              vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m1x8_t __riscv_vlseg8e64_v_i64m1x8_tumu(vbool64_t mask,
                                              vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m2x2_t __riscv_vlseg2e64_v_i64m2x2_tumu(vbool32_t mask,
                                              vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m2x3_t __riscv_vlseg3e64_v_i64m2x3_tumu(vbool32_t mask,
                                              vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m2x4_t __riscv_vlseg4e64_v_i64m2x4_tumu(vbool32_t mask,
                                              vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint64m4x2_t __riscv_vlseg2e64_v_i64m4x2_tumu(vbool16_t mask,
                                              vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8ff_v_i8mf8x2_tumu(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf8x3_t __riscv_vlseg3e8ff_v_i8mf8x3_tumu(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf8x4_t __riscv_vlseg4e8ff_v_i8mf8x4_tumu(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf8x5_t __riscv_vlseg5e8ff_v_i8mf8x5_tumu(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf8x6_t __riscv_vlseg6e8ff_v_i8mf8x6_tumu(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf8x7_t __riscv_vlseg7e8ff_v_i8mf8x7_tumu(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf8x8_t __riscv_vlseg8e8ff_v_i8mf8x8_tumu(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf4x2_t __riscv_vlseg2e8ff_v_i8mf4x2_tumu(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf4x3_t __riscv_vlseg3e8ff_v_i8mf4x3_tumu(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf4x4_t __riscv_vlseg4e8ff_v_i8mf4x4_tumu(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf4x5_t __riscv_vlseg5e8ff_v_i8mf4x5_tumu(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf4x6_t __riscv_vlseg6e8ff_v_i8mf4x6_tumu(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf4x7_t __riscv_vlseg7e8ff_v_i8mf4x7_tumu(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf4x8_t __riscv_vlseg8e8ff_v_i8mf4x8_tumu(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf2x2_t __riscv_vlseg2e8ff_v_i8mf2x2_tumu(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf2x3_t __riscv_vlseg3e8ff_v_i8mf2x3_tumu(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf2x4_t __riscv_vlseg4e8ff_v_i8mf2x4_tumu(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf2x5_t __riscv_vlseg5e8ff_v_i8mf2x5_tumu(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf2x6_t __riscv_vlseg6e8ff_v_i8mf2x6_tumu(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf2x7_t __riscv_vlseg7e8ff_v_i8mf2x7_tumu(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8mf2x8_t __riscv_vlseg8e8ff_v_i8mf2x8_tumu(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               size_t *new_vl, size_t vl);
vint8m1x2_t __riscv_vlseg2e8ff_v_i8m1x2_tumu(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x3_t __riscv_vlseg3e8ff_v_i8m1x3_tumu(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x4_t __riscv_vlseg4e8ff_v_i8m1x4_tumu(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x5_t __riscv_vlseg5e8ff_v_i8m1x5_tumu(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x6_t __riscv_vlseg6e8ff_v_i8m1x6_tumu(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x7_t __riscv_vlseg7e8ff_v_i8m1x7_tumu(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x8_t __riscv_vlseg8e8ff_v_i8m1x8_tumu(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m2x2_t __riscv_vlseg2e8ff_v_i8m2x2_tumu(vbool4_t mask,
                                             vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m2x3_t __riscv_vlseg3e8ff_v_i8m2x3_tumu(vbool4_t mask,
                                             vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m2x4_t __riscv_vlseg4e8ff_v_i8m2x4_tumu(vbool4_t mask,
                                             vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m4x2_t __riscv_vlseg2e8ff_v_i8m4x2_tumu(vbool2_t mask,
                                             vint8m4x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint16mf4x2_t __riscv_vlseg2e16ff_v_i16mf4x2_tumu(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16ff_v_i16mf4x3_tumu(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16ff_v_i16mf4x4_tumu(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16ff_v_i16mf4x5_tumu(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16ff_v_i16mf4x6_tumu(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16ff_v_i16mf4x7_tumu(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16ff_v_i16mf4x8_tumu(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16ff_v_i16mf2x2_tumu(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16ff_v_i16mf2x3_tumu(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16ff_v_i16mf2x4_tumu(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16ff_v_i16mf2x5_tumu(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16ff_v_i16mf2x6_tumu(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16ff_v_i16mf2x7_tumu(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16ff_v_i16mf2x8_tumu(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  size_t *new_vl, size_t vl);
vint16m1x2_t __riscv_vlseg2e16ff_v_i16m1x2_tumu(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x3_t __riscv_vlseg3e16ff_v_i16m1x3_tumu(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x4_t __riscv_vlseg4e16ff_v_i16m1x4_tumu(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x5_t __riscv_vlseg5e16ff_v_i16m1x5_tumu(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x6_t __riscv_vlseg6e16ff_v_i16m1x6_tumu(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x7_t __riscv_vlseg7e16ff_v_i16m1x7_tumu(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x8_t __riscv_vlseg8e16ff_v_i16m1x8_tumu(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m2x2_t __riscv_vlseg2e16ff_v_i16m2x2_tumu(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m2x3_t __riscv_vlseg3e16ff_v_i16m2x3_tumu(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m2x4_t __riscv_vlseg4e16ff_v_i16m2x4_tumu(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m4x2_t __riscv_vlseg2e16ff_v_i16m4x2_tumu(vbool4_t mask,
                                                vint16m4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32ff_v_i32mf2x2_tumu(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  size_t *new_vl, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32ff_v_i32mf2x3_tumu(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  size_t *new_vl, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32ff_v_i32mf2x4_tumu(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  size_t *new_vl, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32ff_v_i32mf2x5_tumu(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  size_t *new_vl, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32ff_v_i32mf2x6_tumu(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  size_t *new_vl, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32ff_v_i32mf2x7_tumu(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  size_t *new_vl, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32ff_v_i32mf2x8_tumu(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  size_t *new_vl, size_t vl);
vint32m1x2_t __riscv_vlseg2e32ff_v_i32m1x2_tumu(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x3_t __riscv_vlseg3e32ff_v_i32m1x3_tumu(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x4_t __riscv_vlseg4e32ff_v_i32m1x4_tumu(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x5_t __riscv_vlseg5e32ff_v_i32m1x5_tumu(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x6_t __riscv_vlseg6e32ff_v_i32m1x6_tumu(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x7_t __riscv_vlseg7e32ff_v_i32m1x7_tumu(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x8_t __riscv_vlseg8e32ff_v_i32m1x8_tumu(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m2x2_t __riscv_vlseg2e32ff_v_i32m2x2_tumu(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m2x3_t __riscv_vlseg3e32ff_v_i32m2x3_tumu(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m2x4_t __riscv_vlseg4e32ff_v_i32m2x4_tumu(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m4x2_t __riscv_vlseg2e32ff_v_i32m4x2_tumu(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint64m1x2_t __riscv_vlseg2e64ff_v_i64m1x2_tumu(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m1x3_t __riscv_vlseg3e64ff_v_i64m1x3_tumu(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m1x4_t __riscv_vlseg4e64ff_v_i64m1x4_tumu(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m1x5_t __riscv_vlseg5e64ff_v_i64m1x5_tumu(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m1x6_t __riscv_vlseg6e64ff_v_i64m1x6_tumu(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m1x7_t __riscv_vlseg7e64ff_v_i64m1x7_tumu(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m1x8_t __riscv_vlseg8e64ff_v_i64m1x8_tumu(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m2x2_t __riscv_vlseg2e64ff_v_i64m2x2_tumu(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m2x3_t __riscv_vlseg3e64ff_v_i64m2x3_tumu(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m2x4_t __riscv_vlseg4e64ff_v_i64m2x4_tumu(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vint64m4x2_t __riscv_vlseg2e64ff_v_i64m4x2_tumu(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8_v_u8mf8x2_tumu(vbool64_t mask,
                                              vuint8mf8x2_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8_v_u8mf8x3_tumu(vbool64_t mask,
                                              vuint8mf8x3_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8_v_u8mf8x4_tumu(vbool64_t mask,
                                              vuint8mf8x4_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8_v_u8mf8x5_tumu(vbool64_t mask,
                                              vuint8mf8x5_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8_v_u8mf8x6_tumu(vbool64_t mask,
                                              vuint8mf8x6_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8_v_u8mf8x7_tumu(vbool64_t mask,
                                              vuint8mf8x7_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8_v_u8mf8x8_tumu(vbool64_t mask,
                                              vuint8mf8x8_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8_v_u8mf4x2_tumu(vbool32_t mask,
                                              vuint8mf4x2_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8_v_u8mf4x3_tumu(vbool32_t mask,
                                              vuint8mf4x3_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8_v_u8mf4x4_tumu(vbool32_t mask,
                                              vuint8mf4x4_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8_v_u8mf4x5_tumu(vbool32_t mask,
                                              vuint8mf4x5_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8_v_u8mf4x6_tumu(vbool32_t mask,
                                              vuint8mf4x6_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8_v_u8mf4x7_tumu(vbool32_t mask,
                                              vuint8mf4x7_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8_v_u8mf4x8_tumu(vbool32_t mask,
                                              vuint8mf4x8_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8_v_u8mf2x2_tumu(vbool16_t mask,
                                              vuint8mf2x2_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8_v_u8mf2x3_tumu(vbool16_t mask,
                                              vuint8mf2x3_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8_v_u8mf2x4_tumu(vbool16_t mask,
                                              vuint8mf2x4_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8_v_u8mf2x5_tumu(vbool16_t mask,
                                              vuint8mf2x5_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8_v_u8mf2x6_tumu(vbool16_t mask,
                                              vuint8mf2x6_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8_v_u8mf2x7_tumu(vbool16_t mask,
                                              vuint8mf2x7_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8_v_u8mf2x8_tumu(vbool16_t mask,
                                              vuint8mf2x8_t maskedoff_tuple,
                                              const uint8_t *base, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8_v_u8m1x2_tumu(vbool8_t mask,
                                            vuint8m1x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x3_t __riscv_vlseg3e8_v_u8m1x3_tumu(vbool8_t mask,
                                            vuint8m1x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x4_t __riscv_vlseg4e8_v_u8m1x4_tumu(vbool8_t mask,
                                            vuint8m1x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x5_t __riscv_vlseg5e8_v_u8m1x5_tumu(vbool8_t mask,
                                            vuint8m1x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x6_t __riscv_vlseg6e8_v_u8m1x6_tumu(vbool8_t mask,
                                            vuint8m1x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x7_t __riscv_vlseg7e8_v_u8m1x7_tumu(vbool8_t mask,
                                            vuint8m1x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x8_t __riscv_vlseg8e8_v_u8m1x8_tumu(vbool8_t mask,
                                            vuint8m1x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m2x2_t __riscv_vlseg2e8_v_u8m2x2_tumu(vbool4_t mask,
                                            vuint8m2x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m2x3_t __riscv_vlseg3e8_v_u8m2x3_tumu(vbool4_t mask,
                                            vuint8m2x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m2x4_t __riscv_vlseg4e8_v_u8m2x4_tumu(vbool4_t mask,
                                            vuint8m2x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m4x2_t __riscv_vlseg2e8_v_u8m4x2_tumu(vbool2_t mask,
                                            vuint8m4x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16_v_u16mf4x2_tumu(vbool64_t mask,
                                                 vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16_v_u16mf4x3_tumu(vbool64_t mask,
                                                 vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16_v_u16mf4x4_tumu(vbool64_t mask,
                                                 vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16_v_u16mf4x5_tumu(vbool64_t mask,
                                                 vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16_v_u16mf4x6_tumu(vbool64_t mask,
                                                 vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16_v_u16mf4x7_tumu(vbool64_t mask,
                                                 vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16_v_u16mf4x8_tumu(vbool64_t mask,
                                                 vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16_v_u16mf2x2_tumu(vbool32_t mask,
                                                 vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16_v_u16mf2x3_tumu(vbool32_t mask,
                                                 vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16_v_u16mf2x4_tumu(vbool32_t mask,
                                                 vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16_v_u16mf2x5_tumu(vbool32_t mask,
                                                 vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16_v_u16mf2x6_tumu(vbool32_t mask,
                                                 vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16_v_u16mf2x7_tumu(vbool32_t mask,
                                                 vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16_v_u16mf2x8_tumu(vbool32_t mask,
                                                 vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t vl);
vuint16m1x2_t __riscv_vlseg2e16_v_u16m1x2_tumu(vbool16_t mask,
                                               vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16_v_u16m1x3_tumu(vbool16_t mask,
                                               vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16_v_u16m1x4_tumu(vbool16_t mask,
                                               vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16_v_u16m1x5_tumu(vbool16_t mask,
                                               vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16_v_u16m1x6_tumu(vbool16_t mask,
                                               vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16_v_u16m1x7_tumu(vbool16_t mask,
                                               vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16_v_u16m1x8_tumu(vbool16_t mask,
                                               vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16_v_u16m2x2_tumu(vbool8_t mask,
                                               vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16_v_u16m2x3_tumu(vbool8_t mask,
                                               vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16_v_u16m2x4_tumu(vbool8_t mask,
                                               vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16_v_u16m4x2_tumu(vbool4_t mask,
                                               vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32_v_u32mf2x2_tumu(vbool64_t mask,
                                                 vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32_v_u32mf2x3_tumu(vbool64_t mask,
                                                 vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32_v_u32mf2x4_tumu(vbool64_t mask,
                                                 vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32_v_u32mf2x5_tumu(vbool64_t mask,
                                                 vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32_v_u32mf2x6_tumu(vbool64_t mask,
                                                 vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32_v_u32mf2x7_tumu(vbool64_t mask,
                                                 vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32_v_u32mf2x8_tumu(vbool64_t mask,
                                                 vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t vl);
vuint32m1x2_t __riscv_vlseg2e32_v_u32m1x2_tumu(vbool32_t mask,
                                               vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32_v_u32m1x3_tumu(vbool32_t mask,
                                               vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32_v_u32m1x4_tumu(vbool32_t mask,
                                               vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32_v_u32m1x5_tumu(vbool32_t mask,
                                               vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32_v_u32m1x6_tumu(vbool32_t mask,
                                               vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32_v_u32m1x7_tumu(vbool32_t mask,
                                               vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32_v_u32m1x8_tumu(vbool32_t mask,
                                               vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32_v_u32m2x2_tumu(vbool16_t mask,
                                               vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32_v_u32m2x3_tumu(vbool16_t mask,
                                               vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32_v_u32m2x4_tumu(vbool16_t mask,
                                               vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32_v_u32m4x2_tumu(vbool8_t mask,
                                               vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64_v_u64m1x2_tumu(vbool64_t mask,
                                               vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64_v_u64m1x3_tumu(vbool64_t mask,
                                               vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64_v_u64m1x4_tumu(vbool64_t mask,
                                               vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64_v_u64m1x5_tumu(vbool64_t mask,
                                               vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64_v_u64m1x6_tumu(vbool64_t mask,
                                               vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64_v_u64m1x7_tumu(vbool64_t mask,
                                               vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64_v_u64m1x8_tumu(vbool64_t mask,
                                               vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64_v_u64m2x2_tumu(vbool32_t mask,
                                               vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64_v_u64m2x3_tumu(vbool32_t mask,
                                               vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64_v_u64m2x4_tumu(vbool32_t mask,
                                               vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64_v_u64m4x2_tumu(vbool16_t mask,
                                               vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8ff_v_u8mf8x2_tumu(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8ff_v_u8mf8x3_tumu(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8ff_v_u8mf8x4_tumu(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8ff_v_u8mf8x5_tumu(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8ff_v_u8mf8x6_tumu(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8ff_v_u8mf8x7_tumu(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8ff_v_u8mf8x8_tumu(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8ff_v_u8mf4x2_tumu(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8ff_v_u8mf4x3_tumu(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8ff_v_u8mf4x4_tumu(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8ff_v_u8mf4x5_tumu(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8ff_v_u8mf4x6_tumu(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8ff_v_u8mf4x7_tumu(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8ff_v_u8mf4x8_tumu(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8ff_v_u8mf2x2_tumu(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8ff_v_u8mf2x3_tumu(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8ff_v_u8mf2x4_tumu(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8ff_v_u8mf2x5_tumu(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8ff_v_u8mf2x6_tumu(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8ff_v_u8mf2x7_tumu(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8ff_v_u8mf2x8_tumu(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                size_t *new_vl, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8ff_v_u8m1x2_tumu(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x3_t __riscv_vlseg3e8ff_v_u8m1x3_tumu(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x4_t __riscv_vlseg4e8ff_v_u8m1x4_tumu(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x5_t __riscv_vlseg5e8ff_v_u8m1x5_tumu(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x6_t __riscv_vlseg6e8ff_v_u8m1x6_tumu(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x7_t __riscv_vlseg7e8ff_v_u8m1x7_tumu(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x8_t __riscv_vlseg8e8ff_v_u8m1x8_tumu(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m2x2_t __riscv_vlseg2e8ff_v_u8m2x2_tumu(vbool4_t mask,
                                              vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m2x3_t __riscv_vlseg3e8ff_v_u8m2x3_tumu(vbool4_t mask,
                                              vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m2x4_t __riscv_vlseg4e8ff_v_u8m2x4_tumu(vbool4_t mask,
                                              vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m4x2_t __riscv_vlseg2e8ff_v_u8m4x2_tumu(vbool2_t mask,
                                              vuint8m4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16ff_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16ff_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16ff_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16ff_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16ff_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16ff_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16ff_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16ff_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16ff_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16ff_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16ff_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16ff_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16ff_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16ff_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    size_t *new_vl, size_t vl);
vuint16m1x2_t __riscv_vlseg2e16ff_v_u16m1x2_tumu(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16ff_v_u16m1x3_tumu(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16ff_v_u16m1x4_tumu(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16ff_v_u16m1x5_tumu(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16ff_v_u16m1x6_tumu(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16ff_v_u16m1x7_tumu(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16ff_v_u16m1x8_tumu(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16ff_v_u16m2x2_tumu(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16ff_v_u16m2x3_tumu(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16ff_v_u16m2x4_tumu(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16ff_v_u16m4x2_tumu(vbool4_t mask,
                                                 vuint16m4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32ff_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32ff_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32ff_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32ff_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32ff_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32ff_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32ff_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    size_t *new_vl, size_t vl);
vuint32m1x2_t __riscv_vlseg2e32ff_v_u32m1x2_tumu(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32ff_v_u32m1x3_tumu(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32ff_v_u32m1x4_tumu(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32ff_v_u32m1x5_tumu(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32ff_v_u32m1x6_tumu(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32ff_v_u32m1x7_tumu(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32ff_v_u32m1x8_tumu(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32ff_v_u32m2x2_tumu(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32ff_v_u32m2x3_tumu(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32ff_v_u32m2x4_tumu(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32ff_v_u32m4x2_tumu(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64ff_v_u64m1x2_tumu(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64ff_v_u64m1x3_tumu(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64ff_v_u64m1x4_tumu(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64ff_v_u64m1x5_tumu(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64ff_v_u64m1x6_tumu(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64ff_v_u64m1x7_tumu(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64ff_v_u64m1x8_tumu(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64ff_v_u64m2x2_tumu(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64ff_v_u64m2x3_tumu(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64ff_v_u64m2x4_tumu(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64ff_v_u64m4x2_tumu(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 size_t *new_vl, size_t vl);
// masked functions
vfloat16mf4x2_t __riscv_vlseg2e16_v_f16mf4x2_mu(vbool64_t mask,
                                                vfloat16mf4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x3_t __riscv_vlseg3e16_v_f16mf4x3_mu(vbool64_t mask,
                                                vfloat16mf4x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x4_t __riscv_vlseg4e16_v_f16mf4x4_mu(vbool64_t mask,
                                                vfloat16mf4x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x5_t __riscv_vlseg5e16_v_f16mf4x5_mu(vbool64_t mask,
                                                vfloat16mf4x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x6_t __riscv_vlseg6e16_v_f16mf4x6_mu(vbool64_t mask,
                                                vfloat16mf4x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x7_t __riscv_vlseg7e16_v_f16mf4x7_mu(vbool64_t mask,
                                                vfloat16mf4x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf4x8_t __riscv_vlseg8e16_v_f16mf4x8_mu(vbool64_t mask,
                                                vfloat16mf4x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x2_t __riscv_vlseg2e16_v_f16mf2x2_mu(vbool32_t mask,
                                                vfloat16mf2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x3_t __riscv_vlseg3e16_v_f16mf2x3_mu(vbool32_t mask,
                                                vfloat16mf2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x4_t __riscv_vlseg4e16_v_f16mf2x4_mu(vbool32_t mask,
                                                vfloat16mf2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x5_t __riscv_vlseg5e16_v_f16mf2x5_mu(vbool32_t mask,
                                                vfloat16mf2x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x6_t __riscv_vlseg6e16_v_f16mf2x6_mu(vbool32_t mask,
                                                vfloat16mf2x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x7_t __riscv_vlseg7e16_v_f16mf2x7_mu(vbool32_t mask,
                                                vfloat16mf2x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16mf2x8_t __riscv_vlseg8e16_v_f16mf2x8_mu(vbool32_t mask,
                                                vfloat16mf2x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16_v_f16m1x2_mu(vbool16_t mask,
                                              vfloat16m1x2_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16_v_f16m1x3_mu(vbool16_t mask,
                                              vfloat16m1x3_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16_v_f16m1x4_mu(vbool16_t mask,
                                              vfloat16m1x4_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16_v_f16m1x5_mu(vbool16_t mask,
                                              vfloat16m1x5_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16_v_f16m1x6_mu(vbool16_t mask,
                                              vfloat16m1x6_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16_v_f16m1x7_mu(vbool16_t mask,
                                              vfloat16m1x7_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16_v_f16m1x8_mu(vbool16_t mask,
                                              vfloat16m1x8_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16_v_f16m2x2_mu(vbool8_t mask,
                                              vfloat16m2x2_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16_v_f16m2x3_mu(vbool8_t mask,
                                              vfloat16m2x3_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16_v_f16m2x4_mu(vbool8_t mask,
                                              vfloat16m2x4_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16_v_f16m4x2_mu(vbool4_t mask,
                                              vfloat16m4x2_t maskedoff_tuple,
                                              const float16_t *base, size_t vl);
vfloat32mf2x2_t __riscv_vlseg2e32_v_f32mf2x2_mu(vbool64_t mask,
                                                vfloat32mf2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x3_t __riscv_vlseg3e32_v_f32mf2x3_mu(vbool64_t mask,
                                                vfloat32mf2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x4_t __riscv_vlseg4e32_v_f32mf2x4_mu(vbool64_t mask,
                                                vfloat32mf2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x5_t __riscv_vlseg5e32_v_f32mf2x5_mu(vbool64_t mask,
                                                vfloat32mf2x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x6_t __riscv_vlseg6e32_v_f32mf2x6_mu(vbool64_t mask,
                                                vfloat32mf2x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x7_t __riscv_vlseg7e32_v_f32mf2x7_mu(vbool64_t mask,
                                                vfloat32mf2x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32mf2x8_t __riscv_vlseg8e32_v_f32mf2x8_mu(vbool64_t mask,
                                                vfloat32mf2x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32_v_f32m1x2_mu(vbool32_t mask,
                                              vfloat32m1x2_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32_v_f32m1x3_mu(vbool32_t mask,
                                              vfloat32m1x3_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32_v_f32m1x4_mu(vbool32_t mask,
                                              vfloat32m1x4_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32_v_f32m1x5_mu(vbool32_t mask,
                                              vfloat32m1x5_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32_v_f32m1x6_mu(vbool32_t mask,
                                              vfloat32m1x6_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32_v_f32m1x7_mu(vbool32_t mask,
                                              vfloat32m1x7_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32_v_f32m1x8_mu(vbool32_t mask,
                                              vfloat32m1x8_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32_v_f32m2x2_mu(vbool16_t mask,
                                              vfloat32m2x2_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32_v_f32m2x3_mu(vbool16_t mask,
                                              vfloat32m2x3_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32_v_f32m2x4_mu(vbool16_t mask,
                                              vfloat32m2x4_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32_v_f32m4x2_mu(vbool8_t mask,
                                              vfloat32m4x2_t maskedoff_tuple,
                                              const float32_t *base, size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64_v_f64m1x2_mu(vbool64_t mask,
                                              vfloat64m1x2_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64_v_f64m1x3_mu(vbool64_t mask,
                                              vfloat64m1x3_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64_v_f64m1x4_mu(vbool64_t mask,
                                              vfloat64m1x4_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64_v_f64m1x5_mu(vbool64_t mask,
                                              vfloat64m1x5_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64_v_f64m1x6_mu(vbool64_t mask,
                                              vfloat64m1x6_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64_v_f64m1x7_mu(vbool64_t mask,
                                              vfloat64m1x7_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64_v_f64m1x8_mu(vbool64_t mask,
                                              vfloat64m1x8_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64_v_f64m2x2_mu(vbool32_t mask,
                                              vfloat64m2x2_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64_v_f64m2x3_mu(vbool32_t mask,
                                              vfloat64m2x3_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64_v_f64m2x4_mu(vbool32_t mask,
                                              vfloat64m2x4_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64_v_f64m4x2_mu(vbool16_t mask,
                                              vfloat64m4x2_t maskedoff_tuple,
                                              const float64_t *base, size_t vl);
vfloat16mf4x2_t __riscv_vlseg2e16ff_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x3_t __riscv_vlseg3e16ff_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x4_t __riscv_vlseg4e16ff_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x5_t __riscv_vlseg5e16ff_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x6_t __riscv_vlseg6e16ff_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x7_t __riscv_vlseg7e16ff_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf4x8_t __riscv_vlseg8e16ff_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x2_t __riscv_vlseg2e16ff_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x3_t __riscv_vlseg3e16ff_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x4_t __riscv_vlseg4e16ff_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x5_t __riscv_vlseg5e16ff_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x6_t __riscv_vlseg6e16ff_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x7_t __riscv_vlseg7e16ff_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16mf2x8_t __riscv_vlseg8e16ff_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    size_t *new_vl, size_t vl);
vfloat16m1x2_t __riscv_vlseg2e16ff_v_f16m1x2_mu(vbool16_t mask,
                                                vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x3_t __riscv_vlseg3e16ff_v_f16m1x3_mu(vbool16_t mask,
                                                vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x4_t __riscv_vlseg4e16ff_v_f16m1x4_mu(vbool16_t mask,
                                                vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x5_t __riscv_vlseg5e16ff_v_f16m1x5_mu(vbool16_t mask,
                                                vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x6_t __riscv_vlseg6e16ff_v_f16m1x6_mu(vbool16_t mask,
                                                vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x7_t __riscv_vlseg7e16ff_v_f16m1x7_mu(vbool16_t mask,
                                                vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m1x8_t __riscv_vlseg8e16ff_v_f16m1x8_mu(vbool16_t mask,
                                                vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m2x2_t __riscv_vlseg2e16ff_v_f16m2x2_mu(vbool8_t mask,
                                                vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m2x3_t __riscv_vlseg3e16ff_v_f16m2x3_mu(vbool8_t mask,
                                                vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m2x4_t __riscv_vlseg4e16ff_v_f16m2x4_mu(vbool8_t mask,
                                                vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat16m4x2_t __riscv_vlseg2e16ff_v_f16m4x2_mu(vbool4_t mask,
                                                vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32mf2x2_t __riscv_vlseg2e32ff_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x3_t __riscv_vlseg3e32ff_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x4_t __riscv_vlseg4e32ff_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x5_t __riscv_vlseg5e32ff_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x6_t __riscv_vlseg6e32ff_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x7_t __riscv_vlseg7e32ff_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32mf2x8_t __riscv_vlseg8e32ff_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    size_t *new_vl, size_t vl);
vfloat32m1x2_t __riscv_vlseg2e32ff_v_f32m1x2_mu(vbool32_t mask,
                                                vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x3_t __riscv_vlseg3e32ff_v_f32m1x3_mu(vbool32_t mask,
                                                vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x4_t __riscv_vlseg4e32ff_v_f32m1x4_mu(vbool32_t mask,
                                                vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x5_t __riscv_vlseg5e32ff_v_f32m1x5_mu(vbool32_t mask,
                                                vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x6_t __riscv_vlseg6e32ff_v_f32m1x6_mu(vbool32_t mask,
                                                vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x7_t __riscv_vlseg7e32ff_v_f32m1x7_mu(vbool32_t mask,
                                                vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m1x8_t __riscv_vlseg8e32ff_v_f32m1x8_mu(vbool32_t mask,
                                                vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m2x2_t __riscv_vlseg2e32ff_v_f32m2x2_mu(vbool16_t mask,
                                                vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m2x3_t __riscv_vlseg3e32ff_v_f32m2x3_mu(vbool16_t mask,
                                                vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m2x4_t __riscv_vlseg4e32ff_v_f32m2x4_mu(vbool16_t mask,
                                                vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat32m4x2_t __riscv_vlseg2e32ff_v_f32m4x2_mu(vbool8_t mask,
                                                vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x2_t __riscv_vlseg2e64ff_v_f64m1x2_mu(vbool64_t mask,
                                                vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x3_t __riscv_vlseg3e64ff_v_f64m1x3_mu(vbool64_t mask,
                                                vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x4_t __riscv_vlseg4e64ff_v_f64m1x4_mu(vbool64_t mask,
                                                vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x5_t __riscv_vlseg5e64ff_v_f64m1x5_mu(vbool64_t mask,
                                                vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x6_t __riscv_vlseg6e64ff_v_f64m1x6_mu(vbool64_t mask,
                                                vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x7_t __riscv_vlseg7e64ff_v_f64m1x7_mu(vbool64_t mask,
                                                vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m1x8_t __riscv_vlseg8e64ff_v_f64m1x8_mu(vbool64_t mask,
                                                vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m2x2_t __riscv_vlseg2e64ff_v_f64m2x2_mu(vbool32_t mask,
                                                vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m2x3_t __riscv_vlseg3e64ff_v_f64m2x3_mu(vbool32_t mask,
                                                vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m2x4_t __riscv_vlseg4e64ff_v_f64m2x4_mu(vbool32_t mask,
                                                vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vfloat64m4x2_t __riscv_vlseg2e64ff_v_f64m4x2_mu(vbool16_t mask,
                                                vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                size_t *new_vl, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8_v_i8mf8x2_mu(vbool64_t mask,
                                           vint8mf8x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x3_t __riscv_vlseg3e8_v_i8mf8x3_mu(vbool64_t mask,
                                           vint8mf8x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x4_t __riscv_vlseg4e8_v_i8mf8x4_mu(vbool64_t mask,
                                           vint8mf8x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x5_t __riscv_vlseg5e8_v_i8mf8x5_mu(vbool64_t mask,
                                           vint8mf8x5_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x6_t __riscv_vlseg6e8_v_i8mf8x6_mu(vbool64_t mask,
                                           vint8mf8x6_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x7_t __riscv_vlseg7e8_v_i8mf8x7_mu(vbool64_t mask,
                                           vint8mf8x7_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf8x8_t __riscv_vlseg8e8_v_i8mf8x8_mu(vbool64_t mask,
                                           vint8mf8x8_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x2_t __riscv_vlseg2e8_v_i8mf4x2_mu(vbool32_t mask,
                                           vint8mf4x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x3_t __riscv_vlseg3e8_v_i8mf4x3_mu(vbool32_t mask,
                                           vint8mf4x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x4_t __riscv_vlseg4e8_v_i8mf4x4_mu(vbool32_t mask,
                                           vint8mf4x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x5_t __riscv_vlseg5e8_v_i8mf4x5_mu(vbool32_t mask,
                                           vint8mf4x5_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x6_t __riscv_vlseg6e8_v_i8mf4x6_mu(vbool32_t mask,
                                           vint8mf4x6_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x7_t __riscv_vlseg7e8_v_i8mf4x7_mu(vbool32_t mask,
                                           vint8mf4x7_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf4x8_t __riscv_vlseg8e8_v_i8mf4x8_mu(vbool32_t mask,
                                           vint8mf4x8_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x2_t __riscv_vlseg2e8_v_i8mf2x2_mu(vbool16_t mask,
                                           vint8mf2x2_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x3_t __riscv_vlseg3e8_v_i8mf2x3_mu(vbool16_t mask,
                                           vint8mf2x3_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x4_t __riscv_vlseg4e8_v_i8mf2x4_mu(vbool16_t mask,
                                           vint8mf2x4_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x5_t __riscv_vlseg5e8_v_i8mf2x5_mu(vbool16_t mask,
                                           vint8mf2x5_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x6_t __riscv_vlseg6e8_v_i8mf2x6_mu(vbool16_t mask,
                                           vint8mf2x6_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x7_t __riscv_vlseg7e8_v_i8mf2x7_mu(vbool16_t mask,
                                           vint8mf2x7_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8mf2x8_t __riscv_vlseg8e8_v_i8mf2x8_mu(vbool16_t mask,
                                           vint8mf2x8_t maskedoff_tuple,
                                           const int8_t *base, size_t vl);
vint8m1x2_t __riscv_vlseg2e8_v_i8m1x2_mu(vbool8_t mask,
                                         vint8m1x2_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x3_t __riscv_vlseg3e8_v_i8m1x3_mu(vbool8_t mask,
                                         vint8m1x3_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x4_t __riscv_vlseg4e8_v_i8m1x4_mu(vbool8_t mask,
                                         vint8m1x4_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x5_t __riscv_vlseg5e8_v_i8m1x5_mu(vbool8_t mask,
                                         vint8m1x5_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x6_t __riscv_vlseg6e8_v_i8m1x6_mu(vbool8_t mask,
                                         vint8m1x6_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x7_t __riscv_vlseg7e8_v_i8m1x7_mu(vbool8_t mask,
                                         vint8m1x7_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m1x8_t __riscv_vlseg8e8_v_i8m1x8_mu(vbool8_t mask,
                                         vint8m1x8_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m2x2_t __riscv_vlseg2e8_v_i8m2x2_mu(vbool4_t mask,
                                         vint8m2x2_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m2x3_t __riscv_vlseg3e8_v_i8m2x3_mu(vbool4_t mask,
                                         vint8m2x3_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m2x4_t __riscv_vlseg4e8_v_i8m2x4_mu(vbool4_t mask,
                                         vint8m2x4_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint8m4x2_t __riscv_vlseg2e8_v_i8m4x2_mu(vbool2_t mask,
                                         vint8m4x2_t maskedoff_tuple,
                                         const int8_t *base, size_t vl);
vint16mf4x2_t __riscv_vlseg2e16_v_i16mf4x2_mu(vbool64_t mask,
                                              vint16mf4x2_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16_v_i16mf4x3_mu(vbool64_t mask,
                                              vint16mf4x3_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16_v_i16mf4x4_mu(vbool64_t mask,
                                              vint16mf4x4_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16_v_i16mf4x5_mu(vbool64_t mask,
                                              vint16mf4x5_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16_v_i16mf4x6_mu(vbool64_t mask,
                                              vint16mf4x6_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16_v_i16mf4x7_mu(vbool64_t mask,
                                              vint16mf4x7_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16_v_i16mf4x8_mu(vbool64_t mask,
                                              vint16mf4x8_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16_v_i16mf2x2_mu(vbool32_t mask,
                                              vint16mf2x2_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16_v_i16mf2x3_mu(vbool32_t mask,
                                              vint16mf2x3_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16_v_i16mf2x4_mu(vbool32_t mask,
                                              vint16mf2x4_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16_v_i16mf2x5_mu(vbool32_t mask,
                                              vint16mf2x5_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16_v_i16mf2x6_mu(vbool32_t mask,
                                              vint16mf2x6_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16_v_i16mf2x7_mu(vbool32_t mask,
                                              vint16mf2x7_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16_v_i16mf2x8_mu(vbool32_t mask,
                                              vint16mf2x8_t maskedoff_tuple,
                                              const int16_t *base, size_t vl);
vint16m1x2_t __riscv_vlseg2e16_v_i16m1x2_mu(vbool16_t mask,
                                            vint16m1x2_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x3_t __riscv_vlseg3e16_v_i16m1x3_mu(vbool16_t mask,
                                            vint16m1x3_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x4_t __riscv_vlseg4e16_v_i16m1x4_mu(vbool16_t mask,
                                            vint16m1x4_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x5_t __riscv_vlseg5e16_v_i16m1x5_mu(vbool16_t mask,
                                            vint16m1x5_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x6_t __riscv_vlseg6e16_v_i16m1x6_mu(vbool16_t mask,
                                            vint16m1x6_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x7_t __riscv_vlseg7e16_v_i16m1x7_mu(vbool16_t mask,
                                            vint16m1x7_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m1x8_t __riscv_vlseg8e16_v_i16m1x8_mu(vbool16_t mask,
                                            vint16m1x8_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m2x2_t __riscv_vlseg2e16_v_i16m2x2_mu(vbool8_t mask,
                                            vint16m2x2_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m2x3_t __riscv_vlseg3e16_v_i16m2x3_mu(vbool8_t mask,
                                            vint16m2x3_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m2x4_t __riscv_vlseg4e16_v_i16m2x4_mu(vbool8_t mask,
                                            vint16m2x4_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint16m4x2_t __riscv_vlseg2e16_v_i16m4x2_mu(vbool4_t mask,
                                            vint16m4x2_t maskedoff_tuple,
                                            const int16_t *base, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32_v_i32mf2x2_mu(vbool64_t mask,
                                              vint32mf2x2_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32_v_i32mf2x3_mu(vbool64_t mask,
                                              vint32mf2x3_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32_v_i32mf2x4_mu(vbool64_t mask,
                                              vint32mf2x4_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32_v_i32mf2x5_mu(vbool64_t mask,
                                              vint32mf2x5_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32_v_i32mf2x6_mu(vbool64_t mask,
                                              vint32mf2x6_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32_v_i32mf2x7_mu(vbool64_t mask,
                                              vint32mf2x7_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32_v_i32mf2x8_mu(vbool64_t mask,
                                              vint32mf2x8_t maskedoff_tuple,
                                              const int32_t *base, size_t vl);
vint32m1x2_t __riscv_vlseg2e32_v_i32m1x2_mu(vbool32_t mask,
                                            vint32m1x2_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x3_t __riscv_vlseg3e32_v_i32m1x3_mu(vbool32_t mask,
                                            vint32m1x3_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x4_t __riscv_vlseg4e32_v_i32m1x4_mu(vbool32_t mask,
                                            vint32m1x4_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x5_t __riscv_vlseg5e32_v_i32m1x5_mu(vbool32_t mask,
                                            vint32m1x5_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x6_t __riscv_vlseg6e32_v_i32m1x6_mu(vbool32_t mask,
                                            vint32m1x6_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x7_t __riscv_vlseg7e32_v_i32m1x7_mu(vbool32_t mask,
                                            vint32m1x7_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m1x8_t __riscv_vlseg8e32_v_i32m1x8_mu(vbool32_t mask,
                                            vint32m1x8_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m2x2_t __riscv_vlseg2e32_v_i32m2x2_mu(vbool16_t mask,
                                            vint32m2x2_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m2x3_t __riscv_vlseg3e32_v_i32m2x3_mu(vbool16_t mask,
                                            vint32m2x3_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m2x4_t __riscv_vlseg4e32_v_i32m2x4_mu(vbool16_t mask,
                                            vint32m2x4_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint32m4x2_t __riscv_vlseg2e32_v_i32m4x2_mu(vbool8_t mask,
                                            vint32m4x2_t maskedoff_tuple,
                                            const int32_t *base, size_t vl);
vint64m1x2_t __riscv_vlseg2e64_v_i64m1x2_mu(vbool64_t mask,
                                            vint64m1x2_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x3_t __riscv_vlseg3e64_v_i64m1x3_mu(vbool64_t mask,
                                            vint64m1x3_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x4_t __riscv_vlseg4e64_v_i64m1x4_mu(vbool64_t mask,
                                            vint64m1x4_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x5_t __riscv_vlseg5e64_v_i64m1x5_mu(vbool64_t mask,
                                            vint64m1x5_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x6_t __riscv_vlseg6e64_v_i64m1x6_mu(vbool64_t mask,
                                            vint64m1x6_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x7_t __riscv_vlseg7e64_v_i64m1x7_mu(vbool64_t mask,
                                            vint64m1x7_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m1x8_t __riscv_vlseg8e64_v_i64m1x8_mu(vbool64_t mask,
                                            vint64m1x8_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m2x2_t __riscv_vlseg2e64_v_i64m2x2_mu(vbool32_t mask,
                                            vint64m2x2_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m2x3_t __riscv_vlseg3e64_v_i64m2x3_mu(vbool32_t mask,
                                            vint64m2x3_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m2x4_t __riscv_vlseg4e64_v_i64m2x4_mu(vbool32_t mask,
                                            vint64m2x4_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint64m4x2_t __riscv_vlseg2e64_v_i64m4x2_mu(vbool16_t mask,
                                            vint64m4x2_t maskedoff_tuple,
                                            const int64_t *base, size_t vl);
vint8mf8x2_t __riscv_vlseg2e8ff_v_i8mf8x2_mu(vbool64_t mask,
                                             vint8mf8x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x3_t __riscv_vlseg3e8ff_v_i8mf8x3_mu(vbool64_t mask,
                                             vint8mf8x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x4_t __riscv_vlseg4e8ff_v_i8mf8x4_mu(vbool64_t mask,
                                             vint8mf8x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x5_t __riscv_vlseg5e8ff_v_i8mf8x5_mu(vbool64_t mask,
                                             vint8mf8x5_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x6_t __riscv_vlseg6e8ff_v_i8mf8x6_mu(vbool64_t mask,
                                             vint8mf8x6_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x7_t __riscv_vlseg7e8ff_v_i8mf8x7_mu(vbool64_t mask,
                                             vint8mf8x7_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf8x8_t __riscv_vlseg8e8ff_v_i8mf8x8_mu(vbool64_t mask,
                                             vint8mf8x8_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x2_t __riscv_vlseg2e8ff_v_i8mf4x2_mu(vbool32_t mask,
                                             vint8mf4x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x3_t __riscv_vlseg3e8ff_v_i8mf4x3_mu(vbool32_t mask,
                                             vint8mf4x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x4_t __riscv_vlseg4e8ff_v_i8mf4x4_mu(vbool32_t mask,
                                             vint8mf4x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x5_t __riscv_vlseg5e8ff_v_i8mf4x5_mu(vbool32_t mask,
                                             vint8mf4x5_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x6_t __riscv_vlseg6e8ff_v_i8mf4x6_mu(vbool32_t mask,
                                             vint8mf4x6_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x7_t __riscv_vlseg7e8ff_v_i8mf4x7_mu(vbool32_t mask,
                                             vint8mf4x7_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf4x8_t __riscv_vlseg8e8ff_v_i8mf4x8_mu(vbool32_t mask,
                                             vint8mf4x8_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x2_t __riscv_vlseg2e8ff_v_i8mf2x2_mu(vbool16_t mask,
                                             vint8mf2x2_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x3_t __riscv_vlseg3e8ff_v_i8mf2x3_mu(vbool16_t mask,
                                             vint8mf2x3_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x4_t __riscv_vlseg4e8ff_v_i8mf2x4_mu(vbool16_t mask,
                                             vint8mf2x4_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x5_t __riscv_vlseg5e8ff_v_i8mf2x5_mu(vbool16_t mask,
                                             vint8mf2x5_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x6_t __riscv_vlseg6e8ff_v_i8mf2x6_mu(vbool16_t mask,
                                             vint8mf2x6_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x7_t __riscv_vlseg7e8ff_v_i8mf2x7_mu(vbool16_t mask,
                                             vint8mf2x7_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8mf2x8_t __riscv_vlseg8e8ff_v_i8mf2x8_mu(vbool16_t mask,
                                             vint8mf2x8_t maskedoff_tuple,
                                             const int8_t *base, size_t *new_vl,
                                             size_t vl);
vint8m1x2_t __riscv_vlseg2e8ff_v_i8m1x2_mu(vbool8_t mask,
                                           vint8m1x2_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x3_t __riscv_vlseg3e8ff_v_i8m1x3_mu(vbool8_t mask,
                                           vint8m1x3_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x4_t __riscv_vlseg4e8ff_v_i8m1x4_mu(vbool8_t mask,
                                           vint8m1x4_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x5_t __riscv_vlseg5e8ff_v_i8m1x5_mu(vbool8_t mask,
                                           vint8m1x5_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x6_t __riscv_vlseg6e8ff_v_i8m1x6_mu(vbool8_t mask,
                                           vint8m1x6_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x7_t __riscv_vlseg7e8ff_v_i8m1x7_mu(vbool8_t mask,
                                           vint8m1x7_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m1x8_t __riscv_vlseg8e8ff_v_i8m1x8_mu(vbool8_t mask,
                                           vint8m1x8_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m2x2_t __riscv_vlseg2e8ff_v_i8m2x2_mu(vbool4_t mask,
                                           vint8m2x2_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m2x3_t __riscv_vlseg3e8ff_v_i8m2x3_mu(vbool4_t mask,
                                           vint8m2x3_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m2x4_t __riscv_vlseg4e8ff_v_i8m2x4_mu(vbool4_t mask,
                                           vint8m2x4_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint8m4x2_t __riscv_vlseg2e8ff_v_i8m4x2_mu(vbool2_t mask,
                                           vint8m4x2_t maskedoff_tuple,
                                           const int8_t *base, size_t *new_vl,
                                           size_t vl);
vint16mf4x2_t __riscv_vlseg2e16ff_v_i16mf4x2_mu(vbool64_t mask,
                                                vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x3_t __riscv_vlseg3e16ff_v_i16mf4x3_mu(vbool64_t mask,
                                                vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x4_t __riscv_vlseg4e16ff_v_i16mf4x4_mu(vbool64_t mask,
                                                vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x5_t __riscv_vlseg5e16ff_v_i16mf4x5_mu(vbool64_t mask,
                                                vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x6_t __riscv_vlseg6e16ff_v_i16mf4x6_mu(vbool64_t mask,
                                                vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x7_t __riscv_vlseg7e16ff_v_i16mf4x7_mu(vbool64_t mask,
                                                vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf4x8_t __riscv_vlseg8e16ff_v_i16mf4x8_mu(vbool64_t mask,
                                                vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x2_t __riscv_vlseg2e16ff_v_i16mf2x2_mu(vbool32_t mask,
                                                vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x3_t __riscv_vlseg3e16ff_v_i16mf2x3_mu(vbool32_t mask,
                                                vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x4_t __riscv_vlseg4e16ff_v_i16mf2x4_mu(vbool32_t mask,
                                                vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x5_t __riscv_vlseg5e16ff_v_i16mf2x5_mu(vbool32_t mask,
                                                vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x6_t __riscv_vlseg6e16ff_v_i16mf2x6_mu(vbool32_t mask,
                                                vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x7_t __riscv_vlseg7e16ff_v_i16mf2x7_mu(vbool32_t mask,
                                                vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16mf2x8_t __riscv_vlseg8e16ff_v_i16mf2x8_mu(vbool32_t mask,
                                                vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                size_t *new_vl, size_t vl);
vint16m1x2_t __riscv_vlseg2e16ff_v_i16m1x2_mu(vbool16_t mask,
                                              vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x3_t __riscv_vlseg3e16ff_v_i16m1x3_mu(vbool16_t mask,
                                              vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x4_t __riscv_vlseg4e16ff_v_i16m1x4_mu(vbool16_t mask,
                                              vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x5_t __riscv_vlseg5e16ff_v_i16m1x5_mu(vbool16_t mask,
                                              vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x6_t __riscv_vlseg6e16ff_v_i16m1x6_mu(vbool16_t mask,
                                              vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x7_t __riscv_vlseg7e16ff_v_i16m1x7_mu(vbool16_t mask,
                                              vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m1x8_t __riscv_vlseg8e16ff_v_i16m1x8_mu(vbool16_t mask,
                                              vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m2x2_t __riscv_vlseg2e16ff_v_i16m2x2_mu(vbool8_t mask,
                                              vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m2x3_t __riscv_vlseg3e16ff_v_i16m2x3_mu(vbool8_t mask,
                                              vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m2x4_t __riscv_vlseg4e16ff_v_i16m2x4_mu(vbool8_t mask,
                                              vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint16m4x2_t __riscv_vlseg2e16ff_v_i16m4x2_mu(vbool4_t mask,
                                              vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              size_t *new_vl, size_t vl);
vint32mf2x2_t __riscv_vlseg2e32ff_v_i32mf2x2_mu(vbool64_t mask,
                                                vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x3_t __riscv_vlseg3e32ff_v_i32mf2x3_mu(vbool64_t mask,
                                                vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x4_t __riscv_vlseg4e32ff_v_i32mf2x4_mu(vbool64_t mask,
                                                vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x5_t __riscv_vlseg5e32ff_v_i32mf2x5_mu(vbool64_t mask,
                                                vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x6_t __riscv_vlseg6e32ff_v_i32mf2x6_mu(vbool64_t mask,
                                                vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x7_t __riscv_vlseg7e32ff_v_i32mf2x7_mu(vbool64_t mask,
                                                vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32mf2x8_t __riscv_vlseg8e32ff_v_i32mf2x8_mu(vbool64_t mask,
                                                vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                size_t *new_vl, size_t vl);
vint32m1x2_t __riscv_vlseg2e32ff_v_i32m1x2_mu(vbool32_t mask,
                                              vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x3_t __riscv_vlseg3e32ff_v_i32m1x3_mu(vbool32_t mask,
                                              vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x4_t __riscv_vlseg4e32ff_v_i32m1x4_mu(vbool32_t mask,
                                              vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x5_t __riscv_vlseg5e32ff_v_i32m1x5_mu(vbool32_t mask,
                                              vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x6_t __riscv_vlseg6e32ff_v_i32m1x6_mu(vbool32_t mask,
                                              vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x7_t __riscv_vlseg7e32ff_v_i32m1x7_mu(vbool32_t mask,
                                              vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m1x8_t __riscv_vlseg8e32ff_v_i32m1x8_mu(vbool32_t mask,
                                              vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m2x2_t __riscv_vlseg2e32ff_v_i32m2x2_mu(vbool16_t mask,
                                              vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m2x3_t __riscv_vlseg3e32ff_v_i32m2x3_mu(vbool16_t mask,
                                              vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m2x4_t __riscv_vlseg4e32ff_v_i32m2x4_mu(vbool16_t mask,
                                              vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint32m4x2_t __riscv_vlseg2e32ff_v_i32m4x2_mu(vbool8_t mask,
                                              vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x2_t __riscv_vlseg2e64ff_v_i64m1x2_mu(vbool64_t mask,
                                              vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x3_t __riscv_vlseg3e64ff_v_i64m1x3_mu(vbool64_t mask,
                                              vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x4_t __riscv_vlseg4e64ff_v_i64m1x4_mu(vbool64_t mask,
                                              vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x5_t __riscv_vlseg5e64ff_v_i64m1x5_mu(vbool64_t mask,
                                              vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x6_t __riscv_vlseg6e64ff_v_i64m1x6_mu(vbool64_t mask,
                                              vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x7_t __riscv_vlseg7e64ff_v_i64m1x7_mu(vbool64_t mask,
                                              vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m1x8_t __riscv_vlseg8e64ff_v_i64m1x8_mu(vbool64_t mask,
                                              vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m2x2_t __riscv_vlseg2e64ff_v_i64m2x2_mu(vbool32_t mask,
                                              vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m2x3_t __riscv_vlseg3e64ff_v_i64m2x3_mu(vbool32_t mask,
                                              vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m2x4_t __riscv_vlseg4e64ff_v_i64m2x4_mu(vbool32_t mask,
                                              vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vint64m4x2_t __riscv_vlseg2e64ff_v_i64m4x2_mu(vbool16_t mask,
                                              vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8_v_u8mf8x2_mu(vbool64_t mask,
                                            vuint8mf8x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8_v_u8mf8x3_mu(vbool64_t mask,
                                            vuint8mf8x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8_v_u8mf8x4_mu(vbool64_t mask,
                                            vuint8mf8x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8_v_u8mf8x5_mu(vbool64_t mask,
                                            vuint8mf8x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8_v_u8mf8x6_mu(vbool64_t mask,
                                            vuint8mf8x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8_v_u8mf8x7_mu(vbool64_t mask,
                                            vuint8mf8x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8_v_u8mf8x8_mu(vbool64_t mask,
                                            vuint8mf8x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8_v_u8mf4x2_mu(vbool32_t mask,
                                            vuint8mf4x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8_v_u8mf4x3_mu(vbool32_t mask,
                                            vuint8mf4x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8_v_u8mf4x4_mu(vbool32_t mask,
                                            vuint8mf4x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8_v_u8mf4x5_mu(vbool32_t mask,
                                            vuint8mf4x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8_v_u8mf4x6_mu(vbool32_t mask,
                                            vuint8mf4x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8_v_u8mf4x7_mu(vbool32_t mask,
                                            vuint8mf4x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8_v_u8mf4x8_mu(vbool32_t mask,
                                            vuint8mf4x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8_v_u8mf2x2_mu(vbool16_t mask,
                                            vuint8mf2x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8_v_u8mf2x3_mu(vbool16_t mask,
                                            vuint8mf2x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8_v_u8mf2x4_mu(vbool16_t mask,
                                            vuint8mf2x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8_v_u8mf2x5_mu(vbool16_t mask,
                                            vuint8mf2x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8_v_u8mf2x6_mu(vbool16_t mask,
                                            vuint8mf2x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8_v_u8mf2x7_mu(vbool16_t mask,
                                            vuint8mf2x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8_v_u8mf2x8_mu(vbool16_t mask,
                                            vuint8mf2x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8_v_u8m1x2_mu(vbool8_t mask,
                                          vuint8m1x2_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x3_t __riscv_vlseg3e8_v_u8m1x3_mu(vbool8_t mask,
                                          vuint8m1x3_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x4_t __riscv_vlseg4e8_v_u8m1x4_mu(vbool8_t mask,
                                          vuint8m1x4_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x5_t __riscv_vlseg5e8_v_u8m1x5_mu(vbool8_t mask,
                                          vuint8m1x5_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x6_t __riscv_vlseg6e8_v_u8m1x6_mu(vbool8_t mask,
                                          vuint8m1x6_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x7_t __riscv_vlseg7e8_v_u8m1x7_mu(vbool8_t mask,
                                          vuint8m1x7_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m1x8_t __riscv_vlseg8e8_v_u8m1x8_mu(vbool8_t mask,
                                          vuint8m1x8_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m2x2_t __riscv_vlseg2e8_v_u8m2x2_mu(vbool4_t mask,
                                          vuint8m2x2_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m2x3_t __riscv_vlseg3e8_v_u8m2x3_mu(vbool4_t mask,
                                          vuint8m2x3_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m2x4_t __riscv_vlseg4e8_v_u8m2x4_mu(vbool4_t mask,
                                          vuint8m2x4_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint8m4x2_t __riscv_vlseg2e8_v_u8m4x2_mu(vbool2_t mask,
                                          vuint8m4x2_t maskedoff_tuple,
                                          const uint8_t *base, size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16_v_u16mf4x2_mu(vbool64_t mask,
                                               vuint16mf4x2_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16_v_u16mf4x3_mu(vbool64_t mask,
                                               vuint16mf4x3_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16_v_u16mf4x4_mu(vbool64_t mask,
                                               vuint16mf4x4_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16_v_u16mf4x5_mu(vbool64_t mask,
                                               vuint16mf4x5_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16_v_u16mf4x6_mu(vbool64_t mask,
                                               vuint16mf4x6_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16_v_u16mf4x7_mu(vbool64_t mask,
                                               vuint16mf4x7_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16_v_u16mf4x8_mu(vbool64_t mask,
                                               vuint16mf4x8_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16_v_u16mf2x2_mu(vbool32_t mask,
                                               vuint16mf2x2_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16_v_u16mf2x3_mu(vbool32_t mask,
                                               vuint16mf2x3_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16_v_u16mf2x4_mu(vbool32_t mask,
                                               vuint16mf2x4_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16_v_u16mf2x5_mu(vbool32_t mask,
                                               vuint16mf2x5_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16_v_u16mf2x6_mu(vbool32_t mask,
                                               vuint16mf2x6_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16_v_u16mf2x7_mu(vbool32_t mask,
                                               vuint16mf2x7_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16_v_u16mf2x8_mu(vbool32_t mask,
                                               vuint16mf2x8_t maskedoff_tuple,
                                               const uint16_t *base, size_t vl);
vuint16m1x2_t __riscv_vlseg2e16_v_u16m1x2_mu(vbool16_t mask,
                                             vuint16m1x2_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16_v_u16m1x3_mu(vbool16_t mask,
                                             vuint16m1x3_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16_v_u16m1x4_mu(vbool16_t mask,
                                             vuint16m1x4_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16_v_u16m1x5_mu(vbool16_t mask,
                                             vuint16m1x5_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16_v_u16m1x6_mu(vbool16_t mask,
                                             vuint16m1x6_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16_v_u16m1x7_mu(vbool16_t mask,
                                             vuint16m1x7_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16_v_u16m1x8_mu(vbool16_t mask,
                                             vuint16m1x8_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16_v_u16m2x2_mu(vbool8_t mask,
                                             vuint16m2x2_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16_v_u16m2x3_mu(vbool8_t mask,
                                             vuint16m2x3_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16_v_u16m2x4_mu(vbool8_t mask,
                                             vuint16m2x4_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16_v_u16m4x2_mu(vbool4_t mask,
                                             vuint16m4x2_t maskedoff_tuple,
                                             const uint16_t *base, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32_v_u32mf2x2_mu(vbool64_t mask,
                                               vuint32mf2x2_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32_v_u32mf2x3_mu(vbool64_t mask,
                                               vuint32mf2x3_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32_v_u32mf2x4_mu(vbool64_t mask,
                                               vuint32mf2x4_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32_v_u32mf2x5_mu(vbool64_t mask,
                                               vuint32mf2x5_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32_v_u32mf2x6_mu(vbool64_t mask,
                                               vuint32mf2x6_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32_v_u32mf2x7_mu(vbool64_t mask,
                                               vuint32mf2x7_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32_v_u32mf2x8_mu(vbool64_t mask,
                                               vuint32mf2x8_t maskedoff_tuple,
                                               const uint32_t *base, size_t vl);
vuint32m1x2_t __riscv_vlseg2e32_v_u32m1x2_mu(vbool32_t mask,
                                             vuint32m1x2_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32_v_u32m1x3_mu(vbool32_t mask,
                                             vuint32m1x3_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32_v_u32m1x4_mu(vbool32_t mask,
                                             vuint32m1x4_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32_v_u32m1x5_mu(vbool32_t mask,
                                             vuint32m1x5_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32_v_u32m1x6_mu(vbool32_t mask,
                                             vuint32m1x6_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32_v_u32m1x7_mu(vbool32_t mask,
                                             vuint32m1x7_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32_v_u32m1x8_mu(vbool32_t mask,
                                             vuint32m1x8_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32_v_u32m2x2_mu(vbool16_t mask,
                                             vuint32m2x2_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32_v_u32m2x3_mu(vbool16_t mask,
                                             vuint32m2x3_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32_v_u32m2x4_mu(vbool16_t mask,
                                             vuint32m2x4_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32_v_u32m4x2_mu(vbool8_t mask,
                                             vuint32m4x2_t maskedoff_tuple,
                                             const uint32_t *base, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64_v_u64m1x2_mu(vbool64_t mask,
                                             vuint64m1x2_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64_v_u64m1x3_mu(vbool64_t mask,
                                             vuint64m1x3_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64_v_u64m1x4_mu(vbool64_t mask,
                                             vuint64m1x4_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64_v_u64m1x5_mu(vbool64_t mask,
                                             vuint64m1x5_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64_v_u64m1x6_mu(vbool64_t mask,
                                             vuint64m1x6_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64_v_u64m1x7_mu(vbool64_t mask,
                                             vuint64m1x7_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64_v_u64m1x8_mu(vbool64_t mask,
                                             vuint64m1x8_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64_v_u64m2x2_mu(vbool32_t mask,
                                             vuint64m2x2_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64_v_u64m2x3_mu(vbool32_t mask,
                                             vuint64m2x3_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64_v_u64m2x4_mu(vbool32_t mask,
                                             vuint64m2x4_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64_v_u64m4x2_mu(vbool16_t mask,
                                             vuint64m4x2_t maskedoff_tuple,
                                             const uint64_t *base, size_t vl);
vuint8mf8x2_t __riscv_vlseg2e8ff_v_u8mf8x2_mu(vbool64_t mask,
                                              vuint8mf8x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x3_t __riscv_vlseg3e8ff_v_u8mf8x3_mu(vbool64_t mask,
                                              vuint8mf8x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x4_t __riscv_vlseg4e8ff_v_u8mf8x4_mu(vbool64_t mask,
                                              vuint8mf8x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x5_t __riscv_vlseg5e8ff_v_u8mf8x5_mu(vbool64_t mask,
                                              vuint8mf8x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x6_t __riscv_vlseg6e8ff_v_u8mf8x6_mu(vbool64_t mask,
                                              vuint8mf8x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x7_t __riscv_vlseg7e8ff_v_u8mf8x7_mu(vbool64_t mask,
                                              vuint8mf8x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf8x8_t __riscv_vlseg8e8ff_v_u8mf8x8_mu(vbool64_t mask,
                                              vuint8mf8x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x2_t __riscv_vlseg2e8ff_v_u8mf4x2_mu(vbool32_t mask,
                                              vuint8mf4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x3_t __riscv_vlseg3e8ff_v_u8mf4x3_mu(vbool32_t mask,
                                              vuint8mf4x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x4_t __riscv_vlseg4e8ff_v_u8mf4x4_mu(vbool32_t mask,
                                              vuint8mf4x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x5_t __riscv_vlseg5e8ff_v_u8mf4x5_mu(vbool32_t mask,
                                              vuint8mf4x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x6_t __riscv_vlseg6e8ff_v_u8mf4x6_mu(vbool32_t mask,
                                              vuint8mf4x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x7_t __riscv_vlseg7e8ff_v_u8mf4x7_mu(vbool32_t mask,
                                              vuint8mf4x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf4x8_t __riscv_vlseg8e8ff_v_u8mf4x8_mu(vbool32_t mask,
                                              vuint8mf4x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x2_t __riscv_vlseg2e8ff_v_u8mf2x2_mu(vbool16_t mask,
                                              vuint8mf2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x3_t __riscv_vlseg3e8ff_v_u8mf2x3_mu(vbool16_t mask,
                                              vuint8mf2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x4_t __riscv_vlseg4e8ff_v_u8mf2x4_mu(vbool16_t mask,
                                              vuint8mf2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x5_t __riscv_vlseg5e8ff_v_u8mf2x5_mu(vbool16_t mask,
                                              vuint8mf2x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x6_t __riscv_vlseg6e8ff_v_u8mf2x6_mu(vbool16_t mask,
                                              vuint8mf2x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x7_t __riscv_vlseg7e8ff_v_u8mf2x7_mu(vbool16_t mask,
                                              vuint8mf2x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8mf2x8_t __riscv_vlseg8e8ff_v_u8mf2x8_mu(vbool16_t mask,
                                              vuint8mf2x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              size_t *new_vl, size_t vl);
vuint8m1x2_t __riscv_vlseg2e8ff_v_u8m1x2_mu(vbool8_t mask,
                                            vuint8m1x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x3_t __riscv_vlseg3e8ff_v_u8m1x3_mu(vbool8_t mask,
                                            vuint8m1x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x4_t __riscv_vlseg4e8ff_v_u8m1x4_mu(vbool8_t mask,
                                            vuint8m1x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x5_t __riscv_vlseg5e8ff_v_u8m1x5_mu(vbool8_t mask,
                                            vuint8m1x5_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x6_t __riscv_vlseg6e8ff_v_u8m1x6_mu(vbool8_t mask,
                                            vuint8m1x6_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x7_t __riscv_vlseg7e8ff_v_u8m1x7_mu(vbool8_t mask,
                                            vuint8m1x7_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m1x8_t __riscv_vlseg8e8ff_v_u8m1x8_mu(vbool8_t mask,
                                            vuint8m1x8_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m2x2_t __riscv_vlseg2e8ff_v_u8m2x2_mu(vbool4_t mask,
                                            vuint8m2x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m2x3_t __riscv_vlseg3e8ff_v_u8m2x3_mu(vbool4_t mask,
                                            vuint8m2x3_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m2x4_t __riscv_vlseg4e8ff_v_u8m2x4_mu(vbool4_t mask,
                                            vuint8m2x4_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint8m4x2_t __riscv_vlseg2e8ff_v_u8m4x2_mu(vbool2_t mask,
                                            vuint8m4x2_t maskedoff_tuple,
                                            const uint8_t *base, size_t *new_vl,
                                            size_t vl);
vuint16mf4x2_t __riscv_vlseg2e16ff_v_u16mf4x2_mu(vbool64_t mask,
                                                 vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x3_t __riscv_vlseg3e16ff_v_u16mf4x3_mu(vbool64_t mask,
                                                 vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x4_t __riscv_vlseg4e16ff_v_u16mf4x4_mu(vbool64_t mask,
                                                 vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x5_t __riscv_vlseg5e16ff_v_u16mf4x5_mu(vbool64_t mask,
                                                 vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x6_t __riscv_vlseg6e16ff_v_u16mf4x6_mu(vbool64_t mask,
                                                 vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x7_t __riscv_vlseg7e16ff_v_u16mf4x7_mu(vbool64_t mask,
                                                 vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf4x8_t __riscv_vlseg8e16ff_v_u16mf4x8_mu(vbool64_t mask,
                                                 vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x2_t __riscv_vlseg2e16ff_v_u16mf2x2_mu(vbool32_t mask,
                                                 vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x3_t __riscv_vlseg3e16ff_v_u16mf2x3_mu(vbool32_t mask,
                                                 vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x4_t __riscv_vlseg4e16ff_v_u16mf2x4_mu(vbool32_t mask,
                                                 vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x5_t __riscv_vlseg5e16ff_v_u16mf2x5_mu(vbool32_t mask,
                                                 vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x6_t __riscv_vlseg6e16ff_v_u16mf2x6_mu(vbool32_t mask,
                                                 vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x7_t __riscv_vlseg7e16ff_v_u16mf2x7_mu(vbool32_t mask,
                                                 vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16mf2x8_t __riscv_vlseg8e16ff_v_u16mf2x8_mu(vbool32_t mask,
                                                 vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 size_t *new_vl, size_t vl);
vuint16m1x2_t __riscv_vlseg2e16ff_v_u16m1x2_mu(vbool16_t mask,
                                               vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x3_t __riscv_vlseg3e16ff_v_u16m1x3_mu(vbool16_t mask,
                                               vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x4_t __riscv_vlseg4e16ff_v_u16m1x4_mu(vbool16_t mask,
                                               vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x5_t __riscv_vlseg5e16ff_v_u16m1x5_mu(vbool16_t mask,
                                               vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x6_t __riscv_vlseg6e16ff_v_u16m1x6_mu(vbool16_t mask,
                                               vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x7_t __riscv_vlseg7e16ff_v_u16m1x7_mu(vbool16_t mask,
                                               vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m1x8_t __riscv_vlseg8e16ff_v_u16m1x8_mu(vbool16_t mask,
                                               vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m2x2_t __riscv_vlseg2e16ff_v_u16m2x2_mu(vbool8_t mask,
                                               vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m2x3_t __riscv_vlseg3e16ff_v_u16m2x3_mu(vbool8_t mask,
                                               vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m2x4_t __riscv_vlseg4e16ff_v_u16m2x4_mu(vbool8_t mask,
                                               vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint16m4x2_t __riscv_vlseg2e16ff_v_u16m4x2_mu(vbool4_t mask,
                                               vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               size_t *new_vl, size_t vl);
vuint32mf2x2_t __riscv_vlseg2e32ff_v_u32mf2x2_mu(vbool64_t mask,
                                                 vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x3_t __riscv_vlseg3e32ff_v_u32mf2x3_mu(vbool64_t mask,
                                                 vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x4_t __riscv_vlseg4e32ff_v_u32mf2x4_mu(vbool64_t mask,
                                                 vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x5_t __riscv_vlseg5e32ff_v_u32mf2x5_mu(vbool64_t mask,
                                                 vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x6_t __riscv_vlseg6e32ff_v_u32mf2x6_mu(vbool64_t mask,
                                                 vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x7_t __riscv_vlseg7e32ff_v_u32mf2x7_mu(vbool64_t mask,
                                                 vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32mf2x8_t __riscv_vlseg8e32ff_v_u32mf2x8_mu(vbool64_t mask,
                                                 vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 size_t *new_vl, size_t vl);
vuint32m1x2_t __riscv_vlseg2e32ff_v_u32m1x2_mu(vbool32_t mask,
                                               vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x3_t __riscv_vlseg3e32ff_v_u32m1x3_mu(vbool32_t mask,
                                               vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x4_t __riscv_vlseg4e32ff_v_u32m1x4_mu(vbool32_t mask,
                                               vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x5_t __riscv_vlseg5e32ff_v_u32m1x5_mu(vbool32_t mask,
                                               vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x6_t __riscv_vlseg6e32ff_v_u32m1x6_mu(vbool32_t mask,
                                               vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x7_t __riscv_vlseg7e32ff_v_u32m1x7_mu(vbool32_t mask,
                                               vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m1x8_t __riscv_vlseg8e32ff_v_u32m1x8_mu(vbool32_t mask,
                                               vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m2x2_t __riscv_vlseg2e32ff_v_u32m2x2_mu(vbool16_t mask,
                                               vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m2x3_t __riscv_vlseg3e32ff_v_u32m2x3_mu(vbool16_t mask,
                                               vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m2x4_t __riscv_vlseg4e32ff_v_u32m2x4_mu(vbool16_t mask,
                                               vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint32m4x2_t __riscv_vlseg2e32ff_v_u32m4x2_mu(vbool8_t mask,
                                               vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x2_t __riscv_vlseg2e64ff_v_u64m1x2_mu(vbool64_t mask,
                                               vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x3_t __riscv_vlseg3e64ff_v_u64m1x3_mu(vbool64_t mask,
                                               vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x4_t __riscv_vlseg4e64ff_v_u64m1x4_mu(vbool64_t mask,
                                               vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x5_t __riscv_vlseg5e64ff_v_u64m1x5_mu(vbool64_t mask,
                                               vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x6_t __riscv_vlseg6e64ff_v_u64m1x6_mu(vbool64_t mask,
                                               vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x7_t __riscv_vlseg7e64ff_v_u64m1x7_mu(vbool64_t mask,
                                               vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m1x8_t __riscv_vlseg8e64ff_v_u64m1x8_mu(vbool64_t mask,
                                               vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m2x2_t __riscv_vlseg2e64ff_v_u64m2x2_mu(vbool32_t mask,
                                               vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m2x3_t __riscv_vlseg3e64ff_v_u64m2x3_mu(vbool32_t mask,
                                               vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m2x4_t __riscv_vlseg4e64ff_v_u64m2x4_mu(vbool32_t mask,
                                               vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
vuint64m4x2_t __riscv_vlseg2e64ff_v_u64m4x2_mu(vbool16_t mask,
                                               vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               size_t *new_vl, size_t vl);
----

[[policy-variant-vecrtor-unit-stride-segment-store]]
=== Vector Unit-Stride Segment Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-vector-strided-segment-load]]
=== Vector Strided Segment Load Intrinsics

[,c]
----
vfloat16mf4x2_t
__riscv_vlsseg2e16_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf4x3_t
__riscv_vlsseg3e16_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf4x4_t
__riscv_vlsseg4e16_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf4x5_t
__riscv_vlsseg5e16_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf4x6_t
__riscv_vlsseg6e16_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf4x7_t
__riscv_vlsseg7e16_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf4x8_t
__riscv_vlsseg8e16_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf2x2_t
__riscv_vlsseg2e16_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf2x3_t
__riscv_vlsseg3e16_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf2x4_t
__riscv_vlsseg4e16_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf2x5_t
__riscv_vlsseg5e16_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf2x6_t
__riscv_vlsseg6e16_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf2x7_t
__riscv_vlsseg7e16_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16mf2x8_t
__riscv_vlsseg8e16_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                 const float16_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat16m1x2_t __riscv_vlsseg2e16_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x3_t __riscv_vlsseg3e16_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x4_t __riscv_vlsseg4e16_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x5_t __riscv_vlsseg5e16_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x6_t __riscv_vlsseg6e16_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x7_t __riscv_vlsseg7e16_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x8_t __riscv_vlsseg8e16_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m2x2_t __riscv_vlsseg2e16_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m2x3_t __riscv_vlsseg3e16_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m2x4_t __riscv_vlsseg4e16_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m4x2_t __riscv_vlsseg2e16_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32mf2x2_t
__riscv_vlsseg2e32_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                 const float32_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat32mf2x3_t
__riscv_vlsseg3e32_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                 const float32_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat32mf2x4_t
__riscv_vlsseg4e32_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                 const float32_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat32mf2x5_t
__riscv_vlsseg5e32_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                 const float32_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat32mf2x6_t
__riscv_vlsseg6e32_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                 const float32_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat32mf2x7_t
__riscv_vlsseg7e32_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                 const float32_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat32mf2x8_t
__riscv_vlsseg8e32_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                 const float32_t *base, ptrdiff_t bstride,
                                 size_t vl);
vfloat32m1x2_t __riscv_vlsseg2e32_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x3_t __riscv_vlsseg3e32_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x4_t __riscv_vlsseg4e32_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x5_t __riscv_vlsseg5e32_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x6_t __riscv_vlsseg6e32_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x7_t __riscv_vlsseg7e32_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x8_t __riscv_vlsseg8e32_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m2x2_t __riscv_vlsseg2e32_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m2x3_t __riscv_vlsseg3e32_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m2x4_t __riscv_vlsseg4e32_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m4x2_t __riscv_vlsseg2e32_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x2_t __riscv_vlsseg2e64_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x3_t __riscv_vlsseg3e64_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x4_t __riscv_vlsseg4e64_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x5_t __riscv_vlsseg5e64_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x6_t __riscv_vlsseg6e64_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x7_t __riscv_vlsseg7e64_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x8_t __riscv_vlsseg8e64_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m2x2_t __riscv_vlsseg2e64_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m2x3_t __riscv_vlsseg3e64_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m2x4_t __riscv_vlsseg4e64_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m4x2_t __riscv_vlsseg2e64_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint8mf8x2_t __riscv_vlsseg2e8_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x3_t __riscv_vlsseg3e8_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x4_t __riscv_vlsseg4e8_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x5_t __riscv_vlsseg5e8_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x6_t __riscv_vlsseg6e8_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x7_t __riscv_vlsseg7e8_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x8_t __riscv_vlsseg8e8_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x2_t __riscv_vlsseg2e8_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x3_t __riscv_vlsseg3e8_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x4_t __riscv_vlsseg4e8_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x5_t __riscv_vlsseg5e8_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x6_t __riscv_vlsseg6e8_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x7_t __riscv_vlsseg7e8_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x8_t __riscv_vlsseg8e8_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x2_t __riscv_vlsseg2e8_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x3_t __riscv_vlsseg3e8_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x4_t __riscv_vlsseg4e8_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x5_t __riscv_vlsseg5e8_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x6_t __riscv_vlsseg6e8_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x7_t __riscv_vlsseg7e8_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x8_t __riscv_vlsseg8e8_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x2_t __riscv_vlsseg2e8_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x3_t __riscv_vlsseg3e8_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x4_t __riscv_vlsseg4e8_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x5_t __riscv_vlsseg5e8_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x6_t __riscv_vlsseg6e8_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x7_t __riscv_vlsseg7e8_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x8_t __riscv_vlsseg8e8_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m2x2_t __riscv_vlsseg2e8_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m2x3_t __riscv_vlsseg3e8_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m2x4_t __riscv_vlsseg4e8_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m4x2_t __riscv_vlsseg2e8_v_i8m4x2_tu(vint8m4x2_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint16mf4x2_t __riscv_vlsseg2e16_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x3_t __riscv_vlsseg3e16_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x4_t __riscv_vlsseg4e16_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x5_t __riscv_vlsseg5e16_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x6_t __riscv_vlsseg6e16_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x7_t __riscv_vlsseg7e16_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x8_t __riscv_vlsseg8e16_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x2_t __riscv_vlsseg2e16_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x3_t __riscv_vlsseg3e16_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x4_t __riscv_vlsseg4e16_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x5_t __riscv_vlsseg5e16_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x6_t __riscv_vlsseg6e16_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x7_t __riscv_vlsseg7e16_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x8_t __riscv_vlsseg8e16_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x2_t __riscv_vlsseg2e16_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x3_t __riscv_vlsseg3e16_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x4_t __riscv_vlsseg4e16_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x5_t __riscv_vlsseg5e16_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x6_t __riscv_vlsseg6e16_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x7_t __riscv_vlsseg7e16_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x8_t __riscv_vlsseg8e16_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m2x2_t __riscv_vlsseg2e16_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m2x3_t __riscv_vlsseg3e16_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m2x4_t __riscv_vlsseg4e16_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m4x2_t __riscv_vlsseg2e16_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32mf2x2_t __riscv_vlsseg2e32_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x3_t __riscv_vlsseg3e32_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x4_t __riscv_vlsseg4e32_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x5_t __riscv_vlsseg5e32_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x6_t __riscv_vlsseg6e32_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x7_t __riscv_vlsseg7e32_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x8_t __riscv_vlsseg8e32_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x2_t __riscv_vlsseg2e32_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x3_t __riscv_vlsseg3e32_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x4_t __riscv_vlsseg4e32_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x5_t __riscv_vlsseg5e32_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x6_t __riscv_vlsseg6e32_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x7_t __riscv_vlsseg7e32_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x8_t __riscv_vlsseg8e32_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m2x2_t __riscv_vlsseg2e32_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m2x3_t __riscv_vlsseg3e32_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m2x4_t __riscv_vlsseg4e32_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m4x2_t __riscv_vlsseg2e32_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x2_t __riscv_vlsseg2e64_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x3_t __riscv_vlsseg3e64_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x4_t __riscv_vlsseg4e64_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x5_t __riscv_vlsseg5e64_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x6_t __riscv_vlsseg6e64_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x7_t __riscv_vlsseg7e64_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x8_t __riscv_vlsseg8e64_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m2x2_t __riscv_vlsseg2e64_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m2x3_t __riscv_vlsseg3e64_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m2x4_t __riscv_vlsseg4e64_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m4x2_t __riscv_vlsseg2e64_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x2_t __riscv_vlsseg2e8_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x3_t __riscv_vlsseg3e8_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x4_t __riscv_vlsseg4e8_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x5_t __riscv_vlsseg5e8_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x6_t __riscv_vlsseg6e8_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x7_t __riscv_vlsseg7e8_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x8_t __riscv_vlsseg8e8_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x2_t __riscv_vlsseg2e8_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x3_t __riscv_vlsseg3e8_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x4_t __riscv_vlsseg4e8_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x5_t __riscv_vlsseg5e8_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x6_t __riscv_vlsseg6e8_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x7_t __riscv_vlsseg7e8_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x8_t __riscv_vlsseg8e8_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x2_t __riscv_vlsseg2e8_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x3_t __riscv_vlsseg3e8_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x4_t __riscv_vlsseg4e8_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x5_t __riscv_vlsseg5e8_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x6_t __riscv_vlsseg6e8_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x7_t __riscv_vlsseg7e8_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x8_t __riscv_vlsseg8e8_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x2_t __riscv_vlsseg2e8_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x3_t __riscv_vlsseg3e8_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x4_t __riscv_vlsseg4e8_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x5_t __riscv_vlsseg5e8_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x6_t __riscv_vlsseg6e8_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x7_t __riscv_vlsseg7e8_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x8_t __riscv_vlsseg8e8_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m2x2_t __riscv_vlsseg2e8_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m2x3_t __riscv_vlsseg3e8_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m2x4_t __riscv_vlsseg4e8_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m4x2_t __riscv_vlsseg2e8_v_u8m4x2_tu(vuint8m4x2_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint16mf4x2_t __riscv_vlsseg2e16_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x3_t __riscv_vlsseg3e16_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x4_t __riscv_vlsseg4e16_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x5_t __riscv_vlsseg5e16_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x6_t __riscv_vlsseg6e16_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x7_t __riscv_vlsseg7e16_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x8_t __riscv_vlsseg8e16_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x2_t __riscv_vlsseg2e16_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x3_t __riscv_vlsseg3e16_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x4_t __riscv_vlsseg4e16_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x5_t __riscv_vlsseg5e16_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x6_t __riscv_vlsseg6e16_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x7_t __riscv_vlsseg7e16_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x8_t __riscv_vlsseg8e16_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x2_t __riscv_vlsseg2e16_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x3_t __riscv_vlsseg3e16_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x4_t __riscv_vlsseg4e16_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x5_t __riscv_vlsseg5e16_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x6_t __riscv_vlsseg6e16_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x7_t __riscv_vlsseg7e16_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x8_t __riscv_vlsseg8e16_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m2x2_t __riscv_vlsseg2e16_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m2x3_t __riscv_vlsseg3e16_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m2x4_t __riscv_vlsseg4e16_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m4x2_t __riscv_vlsseg2e16_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32mf2x2_t __riscv_vlsseg2e32_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x3_t __riscv_vlsseg3e32_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x4_t __riscv_vlsseg4e32_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x5_t __riscv_vlsseg5e32_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x6_t __riscv_vlsseg6e32_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x7_t __riscv_vlsseg7e32_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x8_t __riscv_vlsseg8e32_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x2_t __riscv_vlsseg2e32_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x3_t __riscv_vlsseg3e32_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x4_t __riscv_vlsseg4e32_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x5_t __riscv_vlsseg5e32_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x6_t __riscv_vlsseg6e32_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x7_t __riscv_vlsseg7e32_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x8_t __riscv_vlsseg8e32_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m2x2_t __riscv_vlsseg2e32_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m2x3_t __riscv_vlsseg3e32_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m2x4_t __riscv_vlsseg4e32_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m4x2_t __riscv_vlsseg2e32_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x2_t __riscv_vlsseg2e64_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x3_t __riscv_vlsseg3e64_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x4_t __riscv_vlsseg4e64_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x5_t __riscv_vlsseg5e64_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x6_t __riscv_vlsseg6e64_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x7_t __riscv_vlsseg7e64_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x8_t __riscv_vlsseg8e64_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m2x2_t __riscv_vlsseg2e64_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m2x3_t __riscv_vlsseg3e64_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m2x4_t __riscv_vlsseg4e64_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m4x2_t __riscv_vlsseg2e64_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
// masked functions
vfloat16mf4x2_t __riscv_vlsseg2e16_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x3_t __riscv_vlsseg3e16_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x4_t __riscv_vlsseg4e16_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x5_t __riscv_vlsseg5e16_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x6_t __riscv_vlsseg6e16_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x7_t __riscv_vlsseg7e16_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x8_t __riscv_vlsseg8e16_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x2_t __riscv_vlsseg2e16_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x3_t __riscv_vlsseg3e16_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x4_t __riscv_vlsseg4e16_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x5_t __riscv_vlsseg5e16_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x6_t __riscv_vlsseg6e16_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x7_t __riscv_vlsseg7e16_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x8_t __riscv_vlsseg8e16_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16m1x2_t __riscv_vlsseg2e16_v_f16m1x2_tum(vbool16_t mask,
                                                vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m1x3_t __riscv_vlsseg3e16_v_f16m1x3_tum(vbool16_t mask,
                                                vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m1x4_t __riscv_vlsseg4e16_v_f16m1x4_tum(vbool16_t mask,
                                                vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m1x5_t __riscv_vlsseg5e16_v_f16m1x5_tum(vbool16_t mask,
                                                vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m1x6_t __riscv_vlsseg6e16_v_f16m1x6_tum(vbool16_t mask,
                                                vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m1x7_t __riscv_vlsseg7e16_v_f16m1x7_tum(vbool16_t mask,
                                                vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m1x8_t __riscv_vlsseg8e16_v_f16m1x8_tum(vbool16_t mask,
                                                vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m2x2_t __riscv_vlsseg2e16_v_f16m2x2_tum(vbool8_t mask,
                                                vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m2x3_t __riscv_vlsseg3e16_v_f16m2x3_tum(vbool8_t mask,
                                                vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m2x4_t __riscv_vlsseg4e16_v_f16m2x4_tum(vbool8_t mask,
                                                vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat16m4x2_t __riscv_vlsseg2e16_v_f16m4x2_tum(vbool4_t mask,
                                                vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32mf2x2_t __riscv_vlsseg2e32_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x3_t __riscv_vlsseg3e32_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x4_t __riscv_vlsseg4e32_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x5_t __riscv_vlsseg5e32_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x6_t __riscv_vlsseg6e32_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x7_t __riscv_vlsseg7e32_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x8_t __riscv_vlsseg8e32_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32m1x2_t __riscv_vlsseg2e32_v_f32m1x2_tum(vbool32_t mask,
                                                vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m1x3_t __riscv_vlsseg3e32_v_f32m1x3_tum(vbool32_t mask,
                                                vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m1x4_t __riscv_vlsseg4e32_v_f32m1x4_tum(vbool32_t mask,
                                                vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m1x5_t __riscv_vlsseg5e32_v_f32m1x5_tum(vbool32_t mask,
                                                vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m1x6_t __riscv_vlsseg6e32_v_f32m1x6_tum(vbool32_t mask,
                                                vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m1x7_t __riscv_vlsseg7e32_v_f32m1x7_tum(vbool32_t mask,
                                                vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m1x8_t __riscv_vlsseg8e32_v_f32m1x8_tum(vbool32_t mask,
                                                vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m2x2_t __riscv_vlsseg2e32_v_f32m2x2_tum(vbool16_t mask,
                                                vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m2x3_t __riscv_vlsseg3e32_v_f32m2x3_tum(vbool16_t mask,
                                                vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m2x4_t __riscv_vlsseg4e32_v_f32m2x4_tum(vbool16_t mask,
                                                vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat32m4x2_t __riscv_vlsseg2e32_v_f32m4x2_tum(vbool8_t mask,
                                                vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m1x2_t __riscv_vlsseg2e64_v_f64m1x2_tum(vbool64_t mask,
                                                vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m1x3_t __riscv_vlsseg3e64_v_f64m1x3_tum(vbool64_t mask,
                                                vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m1x4_t __riscv_vlsseg4e64_v_f64m1x4_tum(vbool64_t mask,
                                                vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m1x5_t __riscv_vlsseg5e64_v_f64m1x5_tum(vbool64_t mask,
                                                vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m1x6_t __riscv_vlsseg6e64_v_f64m1x6_tum(vbool64_t mask,
                                                vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m1x7_t __riscv_vlsseg7e64_v_f64m1x7_tum(vbool64_t mask,
                                                vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m1x8_t __riscv_vlsseg8e64_v_f64m1x8_tum(vbool64_t mask,
                                                vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m2x2_t __riscv_vlsseg2e64_v_f64m2x2_tum(vbool32_t mask,
                                                vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m2x3_t __riscv_vlsseg3e64_v_f64m2x3_tum(vbool32_t mask,
                                                vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m2x4_t __riscv_vlsseg4e64_v_f64m2x4_tum(vbool32_t mask,
                                                vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vfloat64m4x2_t __riscv_vlsseg2e64_v_f64m4x2_tum(vbool16_t mask,
                                                vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint8mf8x2_t __riscv_vlsseg2e8_v_i8mf8x2_tum(vbool64_t mask,
                                             vint8mf8x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf8x3_t __riscv_vlsseg3e8_v_i8mf8x3_tum(vbool64_t mask,
                                             vint8mf8x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf8x4_t __riscv_vlsseg4e8_v_i8mf8x4_tum(vbool64_t mask,
                                             vint8mf8x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf8x5_t __riscv_vlsseg5e8_v_i8mf8x5_tum(vbool64_t mask,
                                             vint8mf8x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf8x6_t __riscv_vlsseg6e8_v_i8mf8x6_tum(vbool64_t mask,
                                             vint8mf8x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf8x7_t __riscv_vlsseg7e8_v_i8mf8x7_tum(vbool64_t mask,
                                             vint8mf8x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf8x8_t __riscv_vlsseg8e8_v_i8mf8x8_tum(vbool64_t mask,
                                             vint8mf8x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf4x2_t __riscv_vlsseg2e8_v_i8mf4x2_tum(vbool32_t mask,
                                             vint8mf4x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf4x3_t __riscv_vlsseg3e8_v_i8mf4x3_tum(vbool32_t mask,
                                             vint8mf4x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf4x4_t __riscv_vlsseg4e8_v_i8mf4x4_tum(vbool32_t mask,
                                             vint8mf4x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf4x5_t __riscv_vlsseg5e8_v_i8mf4x5_tum(vbool32_t mask,
                                             vint8mf4x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf4x6_t __riscv_vlsseg6e8_v_i8mf4x6_tum(vbool32_t mask,
                                             vint8mf4x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf4x7_t __riscv_vlsseg7e8_v_i8mf4x7_tum(vbool32_t mask,
                                             vint8mf4x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf4x8_t __riscv_vlsseg8e8_v_i8mf4x8_tum(vbool32_t mask,
                                             vint8mf4x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf2x2_t __riscv_vlsseg2e8_v_i8mf2x2_tum(vbool16_t mask,
                                             vint8mf2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf2x3_t __riscv_vlsseg3e8_v_i8mf2x3_tum(vbool16_t mask,
                                             vint8mf2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf2x4_t __riscv_vlsseg4e8_v_i8mf2x4_tum(vbool16_t mask,
                                             vint8mf2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf2x5_t __riscv_vlsseg5e8_v_i8mf2x5_tum(vbool16_t mask,
                                             vint8mf2x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf2x6_t __riscv_vlsseg6e8_v_i8mf2x6_tum(vbool16_t mask,
                                             vint8mf2x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf2x7_t __riscv_vlsseg7e8_v_i8mf2x7_tum(vbool16_t mask,
                                             vint8mf2x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8mf2x8_t __riscv_vlsseg8e8_v_i8mf2x8_tum(vbool16_t mask,
                                             vint8mf2x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint8m1x2_t __riscv_vlsseg2e8_v_i8m1x2_tum(vbool8_t mask,
                                           vint8m1x2_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m1x3_t __riscv_vlsseg3e8_v_i8m1x3_tum(vbool8_t mask,
                                           vint8m1x3_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m1x4_t __riscv_vlsseg4e8_v_i8m1x4_tum(vbool8_t mask,
                                           vint8m1x4_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m1x5_t __riscv_vlsseg5e8_v_i8m1x5_tum(vbool8_t mask,
                                           vint8m1x5_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m1x6_t __riscv_vlsseg6e8_v_i8m1x6_tum(vbool8_t mask,
                                           vint8m1x6_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m1x7_t __riscv_vlsseg7e8_v_i8m1x7_tum(vbool8_t mask,
                                           vint8m1x7_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m1x8_t __riscv_vlsseg8e8_v_i8m1x8_tum(vbool8_t mask,
                                           vint8m1x8_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m2x2_t __riscv_vlsseg2e8_v_i8m2x2_tum(vbool4_t mask,
                                           vint8m2x2_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m2x3_t __riscv_vlsseg3e8_v_i8m2x3_tum(vbool4_t mask,
                                           vint8m2x3_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m2x4_t __riscv_vlsseg4e8_v_i8m2x4_tum(vbool4_t mask,
                                           vint8m2x4_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint8m4x2_t __riscv_vlsseg2e8_v_i8m4x2_tum(vbool2_t mask,
                                           vint8m4x2_t maskedoff_tuple,
                                           const int8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vint16mf4x2_t __riscv_vlsseg2e16_v_i16mf4x2_tum(vbool64_t mask,
                                                vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf4x3_t __riscv_vlsseg3e16_v_i16mf4x3_tum(vbool64_t mask,
                                                vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf4x4_t __riscv_vlsseg4e16_v_i16mf4x4_tum(vbool64_t mask,
                                                vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf4x5_t __riscv_vlsseg5e16_v_i16mf4x5_tum(vbool64_t mask,
                                                vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf4x6_t __riscv_vlsseg6e16_v_i16mf4x6_tum(vbool64_t mask,
                                                vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf4x7_t __riscv_vlsseg7e16_v_i16mf4x7_tum(vbool64_t mask,
                                                vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf4x8_t __riscv_vlsseg8e16_v_i16mf4x8_tum(vbool64_t mask,
                                                vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf2x2_t __riscv_vlsseg2e16_v_i16mf2x2_tum(vbool32_t mask,
                                                vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf2x3_t __riscv_vlsseg3e16_v_i16mf2x3_tum(vbool32_t mask,
                                                vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf2x4_t __riscv_vlsseg4e16_v_i16mf2x4_tum(vbool32_t mask,
                                                vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf2x5_t __riscv_vlsseg5e16_v_i16mf2x5_tum(vbool32_t mask,
                                                vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf2x6_t __riscv_vlsseg6e16_v_i16mf2x6_tum(vbool32_t mask,
                                                vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf2x7_t __riscv_vlsseg7e16_v_i16mf2x7_tum(vbool32_t mask,
                                                vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16mf2x8_t __riscv_vlsseg8e16_v_i16mf2x8_tum(vbool32_t mask,
                                                vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint16m1x2_t __riscv_vlsseg2e16_v_i16m1x2_tum(vbool16_t mask,
                                              vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m1x3_t __riscv_vlsseg3e16_v_i16m1x3_tum(vbool16_t mask,
                                              vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m1x4_t __riscv_vlsseg4e16_v_i16m1x4_tum(vbool16_t mask,
                                              vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m1x5_t __riscv_vlsseg5e16_v_i16m1x5_tum(vbool16_t mask,
                                              vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m1x6_t __riscv_vlsseg6e16_v_i16m1x6_tum(vbool16_t mask,
                                              vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m1x7_t __riscv_vlsseg7e16_v_i16m1x7_tum(vbool16_t mask,
                                              vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m1x8_t __riscv_vlsseg8e16_v_i16m1x8_tum(vbool16_t mask,
                                              vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m2x2_t __riscv_vlsseg2e16_v_i16m2x2_tum(vbool8_t mask,
                                              vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m2x3_t __riscv_vlsseg3e16_v_i16m2x3_tum(vbool8_t mask,
                                              vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m2x4_t __riscv_vlsseg4e16_v_i16m2x4_tum(vbool8_t mask,
                                              vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint16m4x2_t __riscv_vlsseg2e16_v_i16m4x2_tum(vbool4_t mask,
                                              vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32mf2x2_t __riscv_vlsseg2e32_v_i32mf2x2_tum(vbool64_t mask,
                                                vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint32mf2x3_t __riscv_vlsseg3e32_v_i32mf2x3_tum(vbool64_t mask,
                                                vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint32mf2x4_t __riscv_vlsseg4e32_v_i32mf2x4_tum(vbool64_t mask,
                                                vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint32mf2x5_t __riscv_vlsseg5e32_v_i32mf2x5_tum(vbool64_t mask,
                                                vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint32mf2x6_t __riscv_vlsseg6e32_v_i32mf2x6_tum(vbool64_t mask,
                                                vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint32mf2x7_t __riscv_vlsseg7e32_v_i32mf2x7_tum(vbool64_t mask,
                                                vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint32mf2x8_t __riscv_vlsseg8e32_v_i32mf2x8_tum(vbool64_t mask,
                                                vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vint32m1x2_t __riscv_vlsseg2e32_v_i32m1x2_tum(vbool32_t mask,
                                              vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m1x3_t __riscv_vlsseg3e32_v_i32m1x3_tum(vbool32_t mask,
                                              vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m1x4_t __riscv_vlsseg4e32_v_i32m1x4_tum(vbool32_t mask,
                                              vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m1x5_t __riscv_vlsseg5e32_v_i32m1x5_tum(vbool32_t mask,
                                              vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m1x6_t __riscv_vlsseg6e32_v_i32m1x6_tum(vbool32_t mask,
                                              vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m1x7_t __riscv_vlsseg7e32_v_i32m1x7_tum(vbool32_t mask,
                                              vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m1x8_t __riscv_vlsseg8e32_v_i32m1x8_tum(vbool32_t mask,
                                              vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m2x2_t __riscv_vlsseg2e32_v_i32m2x2_tum(vbool16_t mask,
                                              vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m2x3_t __riscv_vlsseg3e32_v_i32m2x3_tum(vbool16_t mask,
                                              vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m2x4_t __riscv_vlsseg4e32_v_i32m2x4_tum(vbool16_t mask,
                                              vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint32m4x2_t __riscv_vlsseg2e32_v_i32m4x2_tum(vbool8_t mask,
                                              vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m1x2_t __riscv_vlsseg2e64_v_i64m1x2_tum(vbool64_t mask,
                                              vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m1x3_t __riscv_vlsseg3e64_v_i64m1x3_tum(vbool64_t mask,
                                              vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m1x4_t __riscv_vlsseg4e64_v_i64m1x4_tum(vbool64_t mask,
                                              vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m1x5_t __riscv_vlsseg5e64_v_i64m1x5_tum(vbool64_t mask,
                                              vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m1x6_t __riscv_vlsseg6e64_v_i64m1x6_tum(vbool64_t mask,
                                              vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m1x7_t __riscv_vlsseg7e64_v_i64m1x7_tum(vbool64_t mask,
                                              vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m1x8_t __riscv_vlsseg8e64_v_i64m1x8_tum(vbool64_t mask,
                                              vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m2x2_t __riscv_vlsseg2e64_v_i64m2x2_tum(vbool32_t mask,
                                              vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m2x3_t __riscv_vlsseg3e64_v_i64m2x3_tum(vbool32_t mask,
                                              vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m2x4_t __riscv_vlsseg4e64_v_i64m2x4_tum(vbool32_t mask,
                                              vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint64m4x2_t __riscv_vlsseg2e64_v_i64m4x2_tum(vbool16_t mask,
                                              vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf8x2_t __riscv_vlsseg2e8_v_u8mf8x2_tum(vbool64_t mask,
                                              vuint8mf8x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf8x3_t __riscv_vlsseg3e8_v_u8mf8x3_tum(vbool64_t mask,
                                              vuint8mf8x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf8x4_t __riscv_vlsseg4e8_v_u8mf8x4_tum(vbool64_t mask,
                                              vuint8mf8x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf8x5_t __riscv_vlsseg5e8_v_u8mf8x5_tum(vbool64_t mask,
                                              vuint8mf8x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf8x6_t __riscv_vlsseg6e8_v_u8mf8x6_tum(vbool64_t mask,
                                              vuint8mf8x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf8x7_t __riscv_vlsseg7e8_v_u8mf8x7_tum(vbool64_t mask,
                                              vuint8mf8x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf8x8_t __riscv_vlsseg8e8_v_u8mf8x8_tum(vbool64_t mask,
                                              vuint8mf8x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf4x2_t __riscv_vlsseg2e8_v_u8mf4x2_tum(vbool32_t mask,
                                              vuint8mf4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf4x3_t __riscv_vlsseg3e8_v_u8mf4x3_tum(vbool32_t mask,
                                              vuint8mf4x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf4x4_t __riscv_vlsseg4e8_v_u8mf4x4_tum(vbool32_t mask,
                                              vuint8mf4x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf4x5_t __riscv_vlsseg5e8_v_u8mf4x5_tum(vbool32_t mask,
                                              vuint8mf4x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf4x6_t __riscv_vlsseg6e8_v_u8mf4x6_tum(vbool32_t mask,
                                              vuint8mf4x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf4x7_t __riscv_vlsseg7e8_v_u8mf4x7_tum(vbool32_t mask,
                                              vuint8mf4x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf4x8_t __riscv_vlsseg8e8_v_u8mf4x8_tum(vbool32_t mask,
                                              vuint8mf4x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf2x2_t __riscv_vlsseg2e8_v_u8mf2x2_tum(vbool16_t mask,
                                              vuint8mf2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf2x3_t __riscv_vlsseg3e8_v_u8mf2x3_tum(vbool16_t mask,
                                              vuint8mf2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf2x4_t __riscv_vlsseg4e8_v_u8mf2x4_tum(vbool16_t mask,
                                              vuint8mf2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf2x5_t __riscv_vlsseg5e8_v_u8mf2x5_tum(vbool16_t mask,
                                              vuint8mf2x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf2x6_t __riscv_vlsseg6e8_v_u8mf2x6_tum(vbool16_t mask,
                                              vuint8mf2x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf2x7_t __riscv_vlsseg7e8_v_u8mf2x7_tum(vbool16_t mask,
                                              vuint8mf2x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8mf2x8_t __riscv_vlsseg8e8_v_u8mf2x8_tum(vbool16_t mask,
                                              vuint8mf2x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint8m1x2_t __riscv_vlsseg2e8_v_u8m1x2_tum(vbool8_t mask,
                                            vuint8m1x2_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m1x3_t __riscv_vlsseg3e8_v_u8m1x3_tum(vbool8_t mask,
                                            vuint8m1x3_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m1x4_t __riscv_vlsseg4e8_v_u8m1x4_tum(vbool8_t mask,
                                            vuint8m1x4_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m1x5_t __riscv_vlsseg5e8_v_u8m1x5_tum(vbool8_t mask,
                                            vuint8m1x5_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m1x6_t __riscv_vlsseg6e8_v_u8m1x6_tum(vbool8_t mask,
                                            vuint8m1x6_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m1x7_t __riscv_vlsseg7e8_v_u8m1x7_tum(vbool8_t mask,
                                            vuint8m1x7_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m1x8_t __riscv_vlsseg8e8_v_u8m1x8_tum(vbool8_t mask,
                                            vuint8m1x8_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m2x2_t __riscv_vlsseg2e8_v_u8m2x2_tum(vbool4_t mask,
                                            vuint8m2x2_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m2x3_t __riscv_vlsseg3e8_v_u8m2x3_tum(vbool4_t mask,
                                            vuint8m2x3_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m2x4_t __riscv_vlsseg4e8_v_u8m2x4_tum(vbool4_t mask,
                                            vuint8m2x4_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint8m4x2_t __riscv_vlsseg2e8_v_u8m4x2_tum(vbool2_t mask,
                                            vuint8m4x2_t maskedoff_tuple,
                                            const uint8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vuint16mf4x2_t __riscv_vlsseg2e16_v_u16mf4x2_tum(vbool64_t mask,
                                                 vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf4x3_t __riscv_vlsseg3e16_v_u16mf4x3_tum(vbool64_t mask,
                                                 vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf4x4_t __riscv_vlsseg4e16_v_u16mf4x4_tum(vbool64_t mask,
                                                 vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf4x5_t __riscv_vlsseg5e16_v_u16mf4x5_tum(vbool64_t mask,
                                                 vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf4x6_t __riscv_vlsseg6e16_v_u16mf4x6_tum(vbool64_t mask,
                                                 vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf4x7_t __riscv_vlsseg7e16_v_u16mf4x7_tum(vbool64_t mask,
                                                 vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf4x8_t __riscv_vlsseg8e16_v_u16mf4x8_tum(vbool64_t mask,
                                                 vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf2x2_t __riscv_vlsseg2e16_v_u16mf2x2_tum(vbool32_t mask,
                                                 vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf2x3_t __riscv_vlsseg3e16_v_u16mf2x3_tum(vbool32_t mask,
                                                 vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf2x4_t __riscv_vlsseg4e16_v_u16mf2x4_tum(vbool32_t mask,
                                                 vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf2x5_t __riscv_vlsseg5e16_v_u16mf2x5_tum(vbool32_t mask,
                                                 vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf2x6_t __riscv_vlsseg6e16_v_u16mf2x6_tum(vbool32_t mask,
                                                 vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf2x7_t __riscv_vlsseg7e16_v_u16mf2x7_tum(vbool32_t mask,
                                                 vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16mf2x8_t __riscv_vlsseg8e16_v_u16mf2x8_tum(vbool32_t mask,
                                                 vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint16m1x2_t __riscv_vlsseg2e16_v_u16m1x2_tum(vbool16_t mask,
                                               vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m1x3_t __riscv_vlsseg3e16_v_u16m1x3_tum(vbool16_t mask,
                                               vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m1x4_t __riscv_vlsseg4e16_v_u16m1x4_tum(vbool16_t mask,
                                               vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m1x5_t __riscv_vlsseg5e16_v_u16m1x5_tum(vbool16_t mask,
                                               vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m1x6_t __riscv_vlsseg6e16_v_u16m1x6_tum(vbool16_t mask,
                                               vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m1x7_t __riscv_vlsseg7e16_v_u16m1x7_tum(vbool16_t mask,
                                               vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m1x8_t __riscv_vlsseg8e16_v_u16m1x8_tum(vbool16_t mask,
                                               vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m2x2_t __riscv_vlsseg2e16_v_u16m2x2_tum(vbool8_t mask,
                                               vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m2x3_t __riscv_vlsseg3e16_v_u16m2x3_tum(vbool8_t mask,
                                               vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m2x4_t __riscv_vlsseg4e16_v_u16m2x4_tum(vbool8_t mask,
                                               vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint16m4x2_t __riscv_vlsseg2e16_v_u16m4x2_tum(vbool4_t mask,
                                               vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32mf2x2_t __riscv_vlsseg2e32_v_u32mf2x2_tum(vbool64_t mask,
                                                 vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint32mf2x3_t __riscv_vlsseg3e32_v_u32mf2x3_tum(vbool64_t mask,
                                                 vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint32mf2x4_t __riscv_vlsseg4e32_v_u32mf2x4_tum(vbool64_t mask,
                                                 vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint32mf2x5_t __riscv_vlsseg5e32_v_u32mf2x5_tum(vbool64_t mask,
                                                 vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint32mf2x6_t __riscv_vlsseg6e32_v_u32mf2x6_tum(vbool64_t mask,
                                                 vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint32mf2x7_t __riscv_vlsseg7e32_v_u32mf2x7_tum(vbool64_t mask,
                                                 vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint32mf2x8_t __riscv_vlsseg8e32_v_u32mf2x8_tum(vbool64_t mask,
                                                 vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vuint32m1x2_t __riscv_vlsseg2e32_v_u32m1x2_tum(vbool32_t mask,
                                               vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m1x3_t __riscv_vlsseg3e32_v_u32m1x3_tum(vbool32_t mask,
                                               vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m1x4_t __riscv_vlsseg4e32_v_u32m1x4_tum(vbool32_t mask,
                                               vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m1x5_t __riscv_vlsseg5e32_v_u32m1x5_tum(vbool32_t mask,
                                               vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m1x6_t __riscv_vlsseg6e32_v_u32m1x6_tum(vbool32_t mask,
                                               vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m1x7_t __riscv_vlsseg7e32_v_u32m1x7_tum(vbool32_t mask,
                                               vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m1x8_t __riscv_vlsseg8e32_v_u32m1x8_tum(vbool32_t mask,
                                               vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m2x2_t __riscv_vlsseg2e32_v_u32m2x2_tum(vbool16_t mask,
                                               vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m2x3_t __riscv_vlsseg3e32_v_u32m2x3_tum(vbool16_t mask,
                                               vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m2x4_t __riscv_vlsseg4e32_v_u32m2x4_tum(vbool16_t mask,
                                               vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint32m4x2_t __riscv_vlsseg2e32_v_u32m4x2_tum(vbool8_t mask,
                                               vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m1x2_t __riscv_vlsseg2e64_v_u64m1x2_tum(vbool64_t mask,
                                               vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m1x3_t __riscv_vlsseg3e64_v_u64m1x3_tum(vbool64_t mask,
                                               vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m1x4_t __riscv_vlsseg4e64_v_u64m1x4_tum(vbool64_t mask,
                                               vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m1x5_t __riscv_vlsseg5e64_v_u64m1x5_tum(vbool64_t mask,
                                               vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m1x6_t __riscv_vlsseg6e64_v_u64m1x6_tum(vbool64_t mask,
                                               vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m1x7_t __riscv_vlsseg7e64_v_u64m1x7_tum(vbool64_t mask,
                                               vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m1x8_t __riscv_vlsseg8e64_v_u64m1x8_tum(vbool64_t mask,
                                               vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m2x2_t __riscv_vlsseg2e64_v_u64m2x2_tum(vbool32_t mask,
                                               vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m2x3_t __riscv_vlsseg3e64_v_u64m2x3_tum(vbool32_t mask,
                                               vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m2x4_t __riscv_vlsseg4e64_v_u64m2x4_tum(vbool32_t mask,
                                               vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint64m4x2_t __riscv_vlsseg2e64_v_u64m4x2_tum(vbool16_t mask,
                                               vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               ptrdiff_t bstride, size_t vl);
// masked functions
vfloat16mf4x2_t __riscv_vlsseg2e16_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x3_t __riscv_vlsseg3e16_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x4_t __riscv_vlsseg4e16_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x5_t __riscv_vlsseg5e16_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x6_t __riscv_vlsseg6e16_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x7_t __riscv_vlsseg7e16_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x8_t __riscv_vlsseg8e16_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x2_t __riscv_vlsseg2e16_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x3_t __riscv_vlsseg3e16_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x4_t __riscv_vlsseg4e16_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x5_t __riscv_vlsseg5e16_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x6_t __riscv_vlsseg6e16_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x7_t __riscv_vlsseg7e16_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x8_t __riscv_vlsseg8e16_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16m1x2_t __riscv_vlsseg2e16_v_f16m1x2_tumu(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m1x3_t __riscv_vlsseg3e16_v_f16m1x3_tumu(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m1x4_t __riscv_vlsseg4e16_v_f16m1x4_tumu(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m1x5_t __riscv_vlsseg5e16_v_f16m1x5_tumu(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m1x6_t __riscv_vlsseg6e16_v_f16m1x6_tumu(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m1x7_t __riscv_vlsseg7e16_v_f16m1x7_tumu(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m1x8_t __riscv_vlsseg8e16_v_f16m1x8_tumu(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m2x2_t __riscv_vlsseg2e16_v_f16m2x2_tumu(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m2x3_t __riscv_vlsseg3e16_v_f16m2x3_tumu(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m2x4_t __riscv_vlsseg4e16_v_f16m2x4_tumu(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat16m4x2_t __riscv_vlsseg2e16_v_f16m4x2_tumu(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32mf2x2_t __riscv_vlsseg2e32_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x3_t __riscv_vlsseg3e32_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x4_t __riscv_vlsseg4e32_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x5_t __riscv_vlsseg5e32_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x6_t __riscv_vlsseg6e32_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x7_t __riscv_vlsseg7e32_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x8_t __riscv_vlsseg8e32_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32m1x2_t __riscv_vlsseg2e32_v_f32m1x2_tumu(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m1x3_t __riscv_vlsseg3e32_v_f32m1x3_tumu(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m1x4_t __riscv_vlsseg4e32_v_f32m1x4_tumu(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m1x5_t __riscv_vlsseg5e32_v_f32m1x5_tumu(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m1x6_t __riscv_vlsseg6e32_v_f32m1x6_tumu(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m1x7_t __riscv_vlsseg7e32_v_f32m1x7_tumu(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m1x8_t __riscv_vlsseg8e32_v_f32m1x8_tumu(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m2x2_t __riscv_vlsseg2e32_v_f32m2x2_tumu(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m2x3_t __riscv_vlsseg3e32_v_f32m2x3_tumu(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m2x4_t __riscv_vlsseg4e32_v_f32m2x4_tumu(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat32m4x2_t __riscv_vlsseg2e32_v_f32m4x2_tumu(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m1x2_t __riscv_vlsseg2e64_v_f64m1x2_tumu(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m1x3_t __riscv_vlsseg3e64_v_f64m1x3_tumu(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m1x4_t __riscv_vlsseg4e64_v_f64m1x4_tumu(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m1x5_t __riscv_vlsseg5e64_v_f64m1x5_tumu(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m1x6_t __riscv_vlsseg6e64_v_f64m1x6_tumu(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m1x7_t __riscv_vlsseg7e64_v_f64m1x7_tumu(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m1x8_t __riscv_vlsseg8e64_v_f64m1x8_tumu(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m2x2_t __riscv_vlsseg2e64_v_f64m2x2_tumu(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m2x3_t __riscv_vlsseg3e64_v_f64m2x3_tumu(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m2x4_t __riscv_vlsseg4e64_v_f64m2x4_tumu(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vfloat64m4x2_t __riscv_vlsseg2e64_v_f64m4x2_tumu(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint8mf8x2_t __riscv_vlsseg2e8_v_i8mf8x2_tumu(vbool64_t mask,
                                              vint8mf8x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf8x3_t __riscv_vlsseg3e8_v_i8mf8x3_tumu(vbool64_t mask,
                                              vint8mf8x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf8x4_t __riscv_vlsseg4e8_v_i8mf8x4_tumu(vbool64_t mask,
                                              vint8mf8x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf8x5_t __riscv_vlsseg5e8_v_i8mf8x5_tumu(vbool64_t mask,
                                              vint8mf8x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf8x6_t __riscv_vlsseg6e8_v_i8mf8x6_tumu(vbool64_t mask,
                                              vint8mf8x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf8x7_t __riscv_vlsseg7e8_v_i8mf8x7_tumu(vbool64_t mask,
                                              vint8mf8x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf8x8_t __riscv_vlsseg8e8_v_i8mf8x8_tumu(vbool64_t mask,
                                              vint8mf8x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf4x2_t __riscv_vlsseg2e8_v_i8mf4x2_tumu(vbool32_t mask,
                                              vint8mf4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf4x3_t __riscv_vlsseg3e8_v_i8mf4x3_tumu(vbool32_t mask,
                                              vint8mf4x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf4x4_t __riscv_vlsseg4e8_v_i8mf4x4_tumu(vbool32_t mask,
                                              vint8mf4x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf4x5_t __riscv_vlsseg5e8_v_i8mf4x5_tumu(vbool32_t mask,
                                              vint8mf4x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf4x6_t __riscv_vlsseg6e8_v_i8mf4x6_tumu(vbool32_t mask,
                                              vint8mf4x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf4x7_t __riscv_vlsseg7e8_v_i8mf4x7_tumu(vbool32_t mask,
                                              vint8mf4x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf4x8_t __riscv_vlsseg8e8_v_i8mf4x8_tumu(vbool32_t mask,
                                              vint8mf4x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf2x2_t __riscv_vlsseg2e8_v_i8mf2x2_tumu(vbool16_t mask,
                                              vint8mf2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf2x3_t __riscv_vlsseg3e8_v_i8mf2x3_tumu(vbool16_t mask,
                                              vint8mf2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf2x4_t __riscv_vlsseg4e8_v_i8mf2x4_tumu(vbool16_t mask,
                                              vint8mf2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf2x5_t __riscv_vlsseg5e8_v_i8mf2x5_tumu(vbool16_t mask,
                                              vint8mf2x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf2x6_t __riscv_vlsseg6e8_v_i8mf2x6_tumu(vbool16_t mask,
                                              vint8mf2x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf2x7_t __riscv_vlsseg7e8_v_i8mf2x7_tumu(vbool16_t mask,
                                              vint8mf2x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8mf2x8_t __riscv_vlsseg8e8_v_i8mf2x8_tumu(vbool16_t mask,
                                              vint8mf2x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              ptrdiff_t bstride, size_t vl);
vint8m1x2_t __riscv_vlsseg2e8_v_i8m1x2_tumu(vbool8_t mask,
                                            vint8m1x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x3_t __riscv_vlsseg3e8_v_i8m1x3_tumu(vbool8_t mask,
                                            vint8m1x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x4_t __riscv_vlsseg4e8_v_i8m1x4_tumu(vbool8_t mask,
                                            vint8m1x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x5_t __riscv_vlsseg5e8_v_i8m1x5_tumu(vbool8_t mask,
                                            vint8m1x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x6_t __riscv_vlsseg6e8_v_i8m1x6_tumu(vbool8_t mask,
                                            vint8m1x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x7_t __riscv_vlsseg7e8_v_i8m1x7_tumu(vbool8_t mask,
                                            vint8m1x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x8_t __riscv_vlsseg8e8_v_i8m1x8_tumu(vbool8_t mask,
                                            vint8m1x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m2x2_t __riscv_vlsseg2e8_v_i8m2x2_tumu(vbool4_t mask,
                                            vint8m2x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m2x3_t __riscv_vlsseg3e8_v_i8m2x3_tumu(vbool4_t mask,
                                            vint8m2x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m2x4_t __riscv_vlsseg4e8_v_i8m2x4_tumu(vbool4_t mask,
                                            vint8m2x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m4x2_t __riscv_vlsseg2e8_v_i8m4x2_tumu(vbool2_t mask,
                                            vint8m4x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint16mf4x2_t __riscv_vlsseg2e16_v_i16mf4x2_tumu(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf4x3_t __riscv_vlsseg3e16_v_i16mf4x3_tumu(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf4x4_t __riscv_vlsseg4e16_v_i16mf4x4_tumu(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf4x5_t __riscv_vlsseg5e16_v_i16mf4x5_tumu(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf4x6_t __riscv_vlsseg6e16_v_i16mf4x6_tumu(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf4x7_t __riscv_vlsseg7e16_v_i16mf4x7_tumu(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf4x8_t __riscv_vlsseg8e16_v_i16mf4x8_tumu(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf2x2_t __riscv_vlsseg2e16_v_i16mf2x2_tumu(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf2x3_t __riscv_vlsseg3e16_v_i16mf2x3_tumu(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf2x4_t __riscv_vlsseg4e16_v_i16mf2x4_tumu(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf2x5_t __riscv_vlsseg5e16_v_i16mf2x5_tumu(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf2x6_t __riscv_vlsseg6e16_v_i16mf2x6_tumu(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf2x7_t __riscv_vlsseg7e16_v_i16mf2x7_tumu(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16mf2x8_t __riscv_vlsseg8e16_v_i16mf2x8_tumu(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint16m1x2_t __riscv_vlsseg2e16_v_i16m1x2_tumu(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x3_t __riscv_vlsseg3e16_v_i16m1x3_tumu(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x4_t __riscv_vlsseg4e16_v_i16m1x4_tumu(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x5_t __riscv_vlsseg5e16_v_i16m1x5_tumu(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x6_t __riscv_vlsseg6e16_v_i16m1x6_tumu(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x7_t __riscv_vlsseg7e16_v_i16m1x7_tumu(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x8_t __riscv_vlsseg8e16_v_i16m1x8_tumu(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m2x2_t __riscv_vlsseg2e16_v_i16m2x2_tumu(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m2x3_t __riscv_vlsseg3e16_v_i16m2x3_tumu(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m2x4_t __riscv_vlsseg4e16_v_i16m2x4_tumu(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m4x2_t __riscv_vlsseg2e16_v_i16m4x2_tumu(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x2_t __riscv_vlsseg2e32_v_i32mf2x2_tumu(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint32mf2x3_t __riscv_vlsseg3e32_v_i32mf2x3_tumu(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint32mf2x4_t __riscv_vlsseg4e32_v_i32mf2x4_tumu(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint32mf2x5_t __riscv_vlsseg5e32_v_i32mf2x5_tumu(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint32mf2x6_t __riscv_vlsseg6e32_v_i32mf2x6_tumu(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint32mf2x7_t __riscv_vlsseg7e32_v_i32mf2x7_tumu(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint32mf2x8_t __riscv_vlsseg8e32_v_i32mf2x8_tumu(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 ptrdiff_t bstride, size_t vl);
vint32m1x2_t __riscv_vlsseg2e32_v_i32m1x2_tumu(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x3_t __riscv_vlsseg3e32_v_i32m1x3_tumu(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x4_t __riscv_vlsseg4e32_v_i32m1x4_tumu(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x5_t __riscv_vlsseg5e32_v_i32m1x5_tumu(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x6_t __riscv_vlsseg6e32_v_i32m1x6_tumu(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x7_t __riscv_vlsseg7e32_v_i32m1x7_tumu(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x8_t __riscv_vlsseg8e32_v_i32m1x8_tumu(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m2x2_t __riscv_vlsseg2e32_v_i32m2x2_tumu(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m2x3_t __riscv_vlsseg3e32_v_i32m2x3_tumu(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m2x4_t __riscv_vlsseg4e32_v_i32m2x4_tumu(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m4x2_t __riscv_vlsseg2e32_v_i32m4x2_tumu(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m1x2_t __riscv_vlsseg2e64_v_i64m1x2_tumu(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m1x3_t __riscv_vlsseg3e64_v_i64m1x3_tumu(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m1x4_t __riscv_vlsseg4e64_v_i64m1x4_tumu(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m1x5_t __riscv_vlsseg5e64_v_i64m1x5_tumu(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m1x6_t __riscv_vlsseg6e64_v_i64m1x6_tumu(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m1x7_t __riscv_vlsseg7e64_v_i64m1x7_tumu(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m1x8_t __riscv_vlsseg8e64_v_i64m1x8_tumu(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m2x2_t __riscv_vlsseg2e64_v_i64m2x2_tumu(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m2x3_t __riscv_vlsseg3e64_v_i64m2x3_tumu(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m2x4_t __riscv_vlsseg4e64_v_i64m2x4_tumu(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint64m4x2_t __riscv_vlsseg2e64_v_i64m4x2_tumu(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf8x2_t __riscv_vlsseg2e8_v_u8mf8x2_tumu(vbool64_t mask,
                                               vuint8mf8x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf8x3_t __riscv_vlsseg3e8_v_u8mf8x3_tumu(vbool64_t mask,
                                               vuint8mf8x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf8x4_t __riscv_vlsseg4e8_v_u8mf8x4_tumu(vbool64_t mask,
                                               vuint8mf8x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf8x5_t __riscv_vlsseg5e8_v_u8mf8x5_tumu(vbool64_t mask,
                                               vuint8mf8x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf8x6_t __riscv_vlsseg6e8_v_u8mf8x6_tumu(vbool64_t mask,
                                               vuint8mf8x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf8x7_t __riscv_vlsseg7e8_v_u8mf8x7_tumu(vbool64_t mask,
                                               vuint8mf8x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf8x8_t __riscv_vlsseg8e8_v_u8mf8x8_tumu(vbool64_t mask,
                                               vuint8mf8x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf4x2_t __riscv_vlsseg2e8_v_u8mf4x2_tumu(vbool32_t mask,
                                               vuint8mf4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf4x3_t __riscv_vlsseg3e8_v_u8mf4x3_tumu(vbool32_t mask,
                                               vuint8mf4x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf4x4_t __riscv_vlsseg4e8_v_u8mf4x4_tumu(vbool32_t mask,
                                               vuint8mf4x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf4x5_t __riscv_vlsseg5e8_v_u8mf4x5_tumu(vbool32_t mask,
                                               vuint8mf4x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf4x6_t __riscv_vlsseg6e8_v_u8mf4x6_tumu(vbool32_t mask,
                                               vuint8mf4x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf4x7_t __riscv_vlsseg7e8_v_u8mf4x7_tumu(vbool32_t mask,
                                               vuint8mf4x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf4x8_t __riscv_vlsseg8e8_v_u8mf4x8_tumu(vbool32_t mask,
                                               vuint8mf4x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf2x2_t __riscv_vlsseg2e8_v_u8mf2x2_tumu(vbool16_t mask,
                                               vuint8mf2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf2x3_t __riscv_vlsseg3e8_v_u8mf2x3_tumu(vbool16_t mask,
                                               vuint8mf2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf2x4_t __riscv_vlsseg4e8_v_u8mf2x4_tumu(vbool16_t mask,
                                               vuint8mf2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf2x5_t __riscv_vlsseg5e8_v_u8mf2x5_tumu(vbool16_t mask,
                                               vuint8mf2x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf2x6_t __riscv_vlsseg6e8_v_u8mf2x6_tumu(vbool16_t mask,
                                               vuint8mf2x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf2x7_t __riscv_vlsseg7e8_v_u8mf2x7_tumu(vbool16_t mask,
                                               vuint8mf2x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8mf2x8_t __riscv_vlsseg8e8_v_u8mf2x8_tumu(vbool16_t mask,
                                               vuint8mf2x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               ptrdiff_t bstride, size_t vl);
vuint8m1x2_t __riscv_vlsseg2e8_v_u8m1x2_tumu(vbool8_t mask,
                                             vuint8m1x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x3_t __riscv_vlsseg3e8_v_u8m1x3_tumu(vbool8_t mask,
                                             vuint8m1x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x4_t __riscv_vlsseg4e8_v_u8m1x4_tumu(vbool8_t mask,
                                             vuint8m1x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x5_t __riscv_vlsseg5e8_v_u8m1x5_tumu(vbool8_t mask,
                                             vuint8m1x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x6_t __riscv_vlsseg6e8_v_u8m1x6_tumu(vbool8_t mask,
                                             vuint8m1x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x7_t __riscv_vlsseg7e8_v_u8m1x7_tumu(vbool8_t mask,
                                             vuint8m1x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x8_t __riscv_vlsseg8e8_v_u8m1x8_tumu(vbool8_t mask,
                                             vuint8m1x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m2x2_t __riscv_vlsseg2e8_v_u8m2x2_tumu(vbool4_t mask,
                                             vuint8m2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m2x3_t __riscv_vlsseg3e8_v_u8m2x3_tumu(vbool4_t mask,
                                             vuint8m2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m2x4_t __riscv_vlsseg4e8_v_u8m2x4_tumu(vbool4_t mask,
                                             vuint8m2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m4x2_t __riscv_vlsseg2e8_v_u8m4x2_tumu(vbool2_t mask,
                                             vuint8m4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint16mf4x2_t __riscv_vlsseg2e16_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf4x3_t __riscv_vlsseg3e16_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf4x4_t __riscv_vlsseg4e16_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf4x5_t __riscv_vlsseg5e16_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf4x6_t __riscv_vlsseg6e16_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf4x7_t __riscv_vlsseg7e16_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf4x8_t __riscv_vlsseg8e16_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf2x2_t __riscv_vlsseg2e16_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf2x3_t __riscv_vlsseg3e16_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf2x4_t __riscv_vlsseg4e16_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf2x5_t __riscv_vlsseg5e16_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf2x6_t __riscv_vlsseg6e16_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf2x7_t __riscv_vlsseg7e16_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16mf2x8_t __riscv_vlsseg8e16_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    ptrdiff_t bstride, size_t vl);
vuint16m1x2_t __riscv_vlsseg2e16_v_u16m1x2_tumu(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x3_t __riscv_vlsseg3e16_v_u16m1x3_tumu(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x4_t __riscv_vlsseg4e16_v_u16m1x4_tumu(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x5_t __riscv_vlsseg5e16_v_u16m1x5_tumu(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x6_t __riscv_vlsseg6e16_v_u16m1x6_tumu(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x7_t __riscv_vlsseg7e16_v_u16m1x7_tumu(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x8_t __riscv_vlsseg8e16_v_u16m1x8_tumu(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m2x2_t __riscv_vlsseg2e16_v_u16m2x2_tumu(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m2x3_t __riscv_vlsseg3e16_v_u16m2x3_tumu(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m2x4_t __riscv_vlsseg4e16_v_u16m2x4_tumu(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m4x2_t __riscv_vlsseg2e16_v_u16m4x2_tumu(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x2_t __riscv_vlsseg2e32_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    ptrdiff_t bstride, size_t vl);
vuint32mf2x3_t __riscv_vlsseg3e32_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    ptrdiff_t bstride, size_t vl);
vuint32mf2x4_t __riscv_vlsseg4e32_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    ptrdiff_t bstride, size_t vl);
vuint32mf2x5_t __riscv_vlsseg5e32_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    ptrdiff_t bstride, size_t vl);
vuint32mf2x6_t __riscv_vlsseg6e32_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    ptrdiff_t bstride, size_t vl);
vuint32mf2x7_t __riscv_vlsseg7e32_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    ptrdiff_t bstride, size_t vl);
vuint32mf2x8_t __riscv_vlsseg8e32_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    ptrdiff_t bstride, size_t vl);
vuint32m1x2_t __riscv_vlsseg2e32_v_u32m1x2_tumu(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x3_t __riscv_vlsseg3e32_v_u32m1x3_tumu(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x4_t __riscv_vlsseg4e32_v_u32m1x4_tumu(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x5_t __riscv_vlsseg5e32_v_u32m1x5_tumu(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x6_t __riscv_vlsseg6e32_v_u32m1x6_tumu(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x7_t __riscv_vlsseg7e32_v_u32m1x7_tumu(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x8_t __riscv_vlsseg8e32_v_u32m1x8_tumu(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m2x2_t __riscv_vlsseg2e32_v_u32m2x2_tumu(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m2x3_t __riscv_vlsseg3e32_v_u32m2x3_tumu(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m2x4_t __riscv_vlsseg4e32_v_u32m2x4_tumu(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m4x2_t __riscv_vlsseg2e32_v_u32m4x2_tumu(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m1x2_t __riscv_vlsseg2e64_v_u64m1x2_tumu(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m1x3_t __riscv_vlsseg3e64_v_u64m1x3_tumu(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m1x4_t __riscv_vlsseg4e64_v_u64m1x4_tumu(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m1x5_t __riscv_vlsseg5e64_v_u64m1x5_tumu(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m1x6_t __riscv_vlsseg6e64_v_u64m1x6_tumu(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m1x7_t __riscv_vlsseg7e64_v_u64m1x7_tumu(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m1x8_t __riscv_vlsseg8e64_v_u64m1x8_tumu(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m2x2_t __riscv_vlsseg2e64_v_u64m2x2_tumu(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m2x3_t __riscv_vlsseg3e64_v_u64m2x3_tumu(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m2x4_t __riscv_vlsseg4e64_v_u64m2x4_tumu(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint64m4x2_t __riscv_vlsseg2e64_v_u64m4x2_tumu(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                ptrdiff_t bstride, size_t vl);
// masked functions
vfloat16mf4x2_t __riscv_vlsseg2e16_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x3_t __riscv_vlsseg3e16_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x4_t __riscv_vlsseg4e16_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x5_t __riscv_vlsseg5e16_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x6_t __riscv_vlsseg6e16_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x7_t __riscv_vlsseg7e16_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf4x8_t __riscv_vlsseg8e16_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x2_t __riscv_vlsseg2e16_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x3_t __riscv_vlsseg3e16_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x4_t __riscv_vlsseg4e16_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x5_t __riscv_vlsseg5e16_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x6_t __riscv_vlsseg6e16_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x7_t __riscv_vlsseg7e16_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16mf2x8_t __riscv_vlsseg8e16_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat16m1x2_t __riscv_vlsseg2e16_v_f16m1x2_mu(vbool16_t mask,
                                               vfloat16m1x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x3_t __riscv_vlsseg3e16_v_f16m1x3_mu(vbool16_t mask,
                                               vfloat16m1x3_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x4_t __riscv_vlsseg4e16_v_f16m1x4_mu(vbool16_t mask,
                                               vfloat16m1x4_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x5_t __riscv_vlsseg5e16_v_f16m1x5_mu(vbool16_t mask,
                                               vfloat16m1x5_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x6_t __riscv_vlsseg6e16_v_f16m1x6_mu(vbool16_t mask,
                                               vfloat16m1x6_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x7_t __riscv_vlsseg7e16_v_f16m1x7_mu(vbool16_t mask,
                                               vfloat16m1x7_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m1x8_t __riscv_vlsseg8e16_v_f16m1x8_mu(vbool16_t mask,
                                               vfloat16m1x8_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m2x2_t __riscv_vlsseg2e16_v_f16m2x2_mu(vbool8_t mask,
                                               vfloat16m2x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m2x3_t __riscv_vlsseg3e16_v_f16m2x3_mu(vbool8_t mask,
                                               vfloat16m2x3_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m2x4_t __riscv_vlsseg4e16_v_f16m2x4_mu(vbool8_t mask,
                                               vfloat16m2x4_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat16m4x2_t __riscv_vlsseg2e16_v_f16m4x2_mu(vbool4_t mask,
                                               vfloat16m4x2_t maskedoff_tuple,
                                               const float16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32mf2x2_t __riscv_vlsseg2e32_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x3_t __riscv_vlsseg3e32_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x4_t __riscv_vlsseg4e32_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x5_t __riscv_vlsseg5e32_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x6_t __riscv_vlsseg6e32_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x7_t __riscv_vlsseg7e32_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32mf2x8_t __riscv_vlsseg8e32_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    ptrdiff_t bstride, size_t vl);
vfloat32m1x2_t __riscv_vlsseg2e32_v_f32m1x2_mu(vbool32_t mask,
                                               vfloat32m1x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x3_t __riscv_vlsseg3e32_v_f32m1x3_mu(vbool32_t mask,
                                               vfloat32m1x3_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x4_t __riscv_vlsseg4e32_v_f32m1x4_mu(vbool32_t mask,
                                               vfloat32m1x4_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x5_t __riscv_vlsseg5e32_v_f32m1x5_mu(vbool32_t mask,
                                               vfloat32m1x5_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x6_t __riscv_vlsseg6e32_v_f32m1x6_mu(vbool32_t mask,
                                               vfloat32m1x6_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x7_t __riscv_vlsseg7e32_v_f32m1x7_mu(vbool32_t mask,
                                               vfloat32m1x7_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m1x8_t __riscv_vlsseg8e32_v_f32m1x8_mu(vbool32_t mask,
                                               vfloat32m1x8_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m2x2_t __riscv_vlsseg2e32_v_f32m2x2_mu(vbool16_t mask,
                                               vfloat32m2x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m2x3_t __riscv_vlsseg3e32_v_f32m2x3_mu(vbool16_t mask,
                                               vfloat32m2x3_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m2x4_t __riscv_vlsseg4e32_v_f32m2x4_mu(vbool16_t mask,
                                               vfloat32m2x4_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat32m4x2_t __riscv_vlsseg2e32_v_f32m4x2_mu(vbool8_t mask,
                                               vfloat32m4x2_t maskedoff_tuple,
                                               const float32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x2_t __riscv_vlsseg2e64_v_f64m1x2_mu(vbool64_t mask,
                                               vfloat64m1x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x3_t __riscv_vlsseg3e64_v_f64m1x3_mu(vbool64_t mask,
                                               vfloat64m1x3_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x4_t __riscv_vlsseg4e64_v_f64m1x4_mu(vbool64_t mask,
                                               vfloat64m1x4_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x5_t __riscv_vlsseg5e64_v_f64m1x5_mu(vbool64_t mask,
                                               vfloat64m1x5_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x6_t __riscv_vlsseg6e64_v_f64m1x6_mu(vbool64_t mask,
                                               vfloat64m1x6_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x7_t __riscv_vlsseg7e64_v_f64m1x7_mu(vbool64_t mask,
                                               vfloat64m1x7_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m1x8_t __riscv_vlsseg8e64_v_f64m1x8_mu(vbool64_t mask,
                                               vfloat64m1x8_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m2x2_t __riscv_vlsseg2e64_v_f64m2x2_mu(vbool32_t mask,
                                               vfloat64m2x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m2x3_t __riscv_vlsseg3e64_v_f64m2x3_mu(vbool32_t mask,
                                               vfloat64m2x3_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m2x4_t __riscv_vlsseg4e64_v_f64m2x4_mu(vbool32_t mask,
                                               vfloat64m2x4_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vfloat64m4x2_t __riscv_vlsseg2e64_v_f64m4x2_mu(vbool16_t mask,
                                               vfloat64m4x2_t maskedoff_tuple,
                                               const float64_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint8mf8x2_t __riscv_vlsseg2e8_v_i8mf8x2_mu(vbool64_t mask,
                                            vint8mf8x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x3_t __riscv_vlsseg3e8_v_i8mf8x3_mu(vbool64_t mask,
                                            vint8mf8x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x4_t __riscv_vlsseg4e8_v_i8mf8x4_mu(vbool64_t mask,
                                            vint8mf8x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x5_t __riscv_vlsseg5e8_v_i8mf8x5_mu(vbool64_t mask,
                                            vint8mf8x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x6_t __riscv_vlsseg6e8_v_i8mf8x6_mu(vbool64_t mask,
                                            vint8mf8x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x7_t __riscv_vlsseg7e8_v_i8mf8x7_mu(vbool64_t mask,
                                            vint8mf8x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf8x8_t __riscv_vlsseg8e8_v_i8mf8x8_mu(vbool64_t mask,
                                            vint8mf8x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x2_t __riscv_vlsseg2e8_v_i8mf4x2_mu(vbool32_t mask,
                                            vint8mf4x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x3_t __riscv_vlsseg3e8_v_i8mf4x3_mu(vbool32_t mask,
                                            vint8mf4x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x4_t __riscv_vlsseg4e8_v_i8mf4x4_mu(vbool32_t mask,
                                            vint8mf4x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x5_t __riscv_vlsseg5e8_v_i8mf4x5_mu(vbool32_t mask,
                                            vint8mf4x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x6_t __riscv_vlsseg6e8_v_i8mf4x6_mu(vbool32_t mask,
                                            vint8mf4x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x7_t __riscv_vlsseg7e8_v_i8mf4x7_mu(vbool32_t mask,
                                            vint8mf4x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf4x8_t __riscv_vlsseg8e8_v_i8mf4x8_mu(vbool32_t mask,
                                            vint8mf4x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x2_t __riscv_vlsseg2e8_v_i8mf2x2_mu(vbool16_t mask,
                                            vint8mf2x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x3_t __riscv_vlsseg3e8_v_i8mf2x3_mu(vbool16_t mask,
                                            vint8mf2x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x4_t __riscv_vlsseg4e8_v_i8mf2x4_mu(vbool16_t mask,
                                            vint8mf2x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x5_t __riscv_vlsseg5e8_v_i8mf2x5_mu(vbool16_t mask,
                                            vint8mf2x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x6_t __riscv_vlsseg6e8_v_i8mf2x6_mu(vbool16_t mask,
                                            vint8mf2x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x7_t __riscv_vlsseg7e8_v_i8mf2x7_mu(vbool16_t mask,
                                            vint8mf2x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8mf2x8_t __riscv_vlsseg8e8_v_i8mf2x8_mu(vbool16_t mask,
                                            vint8mf2x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            ptrdiff_t bstride, size_t vl);
vint8m1x2_t __riscv_vlsseg2e8_v_i8m1x2_mu(vbool8_t mask,
                                          vint8m1x2_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x3_t __riscv_vlsseg3e8_v_i8m1x3_mu(vbool8_t mask,
                                          vint8m1x3_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x4_t __riscv_vlsseg4e8_v_i8m1x4_mu(vbool8_t mask,
                                          vint8m1x4_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x5_t __riscv_vlsseg5e8_v_i8m1x5_mu(vbool8_t mask,
                                          vint8m1x5_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x6_t __riscv_vlsseg6e8_v_i8m1x6_mu(vbool8_t mask,
                                          vint8m1x6_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x7_t __riscv_vlsseg7e8_v_i8m1x7_mu(vbool8_t mask,
                                          vint8m1x7_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m1x8_t __riscv_vlsseg8e8_v_i8m1x8_mu(vbool8_t mask,
                                          vint8m1x8_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m2x2_t __riscv_vlsseg2e8_v_i8m2x2_mu(vbool4_t mask,
                                          vint8m2x2_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m2x3_t __riscv_vlsseg3e8_v_i8m2x3_mu(vbool4_t mask,
                                          vint8m2x3_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m2x4_t __riscv_vlsseg4e8_v_i8m2x4_mu(vbool4_t mask,
                                          vint8m2x4_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint8m4x2_t __riscv_vlsseg2e8_v_i8m4x2_mu(vbool2_t mask,
                                          vint8m4x2_t maskedoff_tuple,
                                          const int8_t *base, ptrdiff_t bstride,
                                          size_t vl);
vint16mf4x2_t __riscv_vlsseg2e16_v_i16mf4x2_mu(vbool64_t mask,
                                               vint16mf4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x3_t __riscv_vlsseg3e16_v_i16mf4x3_mu(vbool64_t mask,
                                               vint16mf4x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x4_t __riscv_vlsseg4e16_v_i16mf4x4_mu(vbool64_t mask,
                                               vint16mf4x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x5_t __riscv_vlsseg5e16_v_i16mf4x5_mu(vbool64_t mask,
                                               vint16mf4x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x6_t __riscv_vlsseg6e16_v_i16mf4x6_mu(vbool64_t mask,
                                               vint16mf4x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x7_t __riscv_vlsseg7e16_v_i16mf4x7_mu(vbool64_t mask,
                                               vint16mf4x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf4x8_t __riscv_vlsseg8e16_v_i16mf4x8_mu(vbool64_t mask,
                                               vint16mf4x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x2_t __riscv_vlsseg2e16_v_i16mf2x2_mu(vbool32_t mask,
                                               vint16mf2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x3_t __riscv_vlsseg3e16_v_i16mf2x3_mu(vbool32_t mask,
                                               vint16mf2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x4_t __riscv_vlsseg4e16_v_i16mf2x4_mu(vbool32_t mask,
                                               vint16mf2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x5_t __riscv_vlsseg5e16_v_i16mf2x5_mu(vbool32_t mask,
                                               vint16mf2x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x6_t __riscv_vlsseg6e16_v_i16mf2x6_mu(vbool32_t mask,
                                               vint16mf2x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x7_t __riscv_vlsseg7e16_v_i16mf2x7_mu(vbool32_t mask,
                                               vint16mf2x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16mf2x8_t __riscv_vlsseg8e16_v_i16mf2x8_mu(vbool32_t mask,
                                               vint16mf2x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint16m1x2_t __riscv_vlsseg2e16_v_i16m1x2_mu(vbool16_t mask,
                                             vint16m1x2_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x3_t __riscv_vlsseg3e16_v_i16m1x3_mu(vbool16_t mask,
                                             vint16m1x3_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x4_t __riscv_vlsseg4e16_v_i16m1x4_mu(vbool16_t mask,
                                             vint16m1x4_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x5_t __riscv_vlsseg5e16_v_i16m1x5_mu(vbool16_t mask,
                                             vint16m1x5_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x6_t __riscv_vlsseg6e16_v_i16m1x6_mu(vbool16_t mask,
                                             vint16m1x6_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x7_t __riscv_vlsseg7e16_v_i16m1x7_mu(vbool16_t mask,
                                             vint16m1x7_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m1x8_t __riscv_vlsseg8e16_v_i16m1x8_mu(vbool16_t mask,
                                             vint16m1x8_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m2x2_t __riscv_vlsseg2e16_v_i16m2x2_mu(vbool8_t mask,
                                             vint16m2x2_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m2x3_t __riscv_vlsseg3e16_v_i16m2x3_mu(vbool8_t mask,
                                             vint16m2x3_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m2x4_t __riscv_vlsseg4e16_v_i16m2x4_mu(vbool8_t mask,
                                             vint16m2x4_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint16m4x2_t __riscv_vlsseg2e16_v_i16m4x2_mu(vbool4_t mask,
                                             vint16m4x2_t maskedoff_tuple,
                                             const int16_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32mf2x2_t __riscv_vlsseg2e32_v_i32mf2x2_mu(vbool64_t mask,
                                               vint32mf2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x3_t __riscv_vlsseg3e32_v_i32mf2x3_mu(vbool64_t mask,
                                               vint32mf2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x4_t __riscv_vlsseg4e32_v_i32mf2x4_mu(vbool64_t mask,
                                               vint32mf2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x5_t __riscv_vlsseg5e32_v_i32mf2x5_mu(vbool64_t mask,
                                               vint32mf2x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x6_t __riscv_vlsseg6e32_v_i32mf2x6_mu(vbool64_t mask,
                                               vint32mf2x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x7_t __riscv_vlsseg7e32_v_i32mf2x7_mu(vbool64_t mask,
                                               vint32mf2x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32mf2x8_t __riscv_vlsseg8e32_v_i32mf2x8_mu(vbool64_t mask,
                                               vint32mf2x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               ptrdiff_t bstride, size_t vl);
vint32m1x2_t __riscv_vlsseg2e32_v_i32m1x2_mu(vbool32_t mask,
                                             vint32m1x2_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x3_t __riscv_vlsseg3e32_v_i32m1x3_mu(vbool32_t mask,
                                             vint32m1x3_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x4_t __riscv_vlsseg4e32_v_i32m1x4_mu(vbool32_t mask,
                                             vint32m1x4_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x5_t __riscv_vlsseg5e32_v_i32m1x5_mu(vbool32_t mask,
                                             vint32m1x5_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x6_t __riscv_vlsseg6e32_v_i32m1x6_mu(vbool32_t mask,
                                             vint32m1x6_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x7_t __riscv_vlsseg7e32_v_i32m1x7_mu(vbool32_t mask,
                                             vint32m1x7_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m1x8_t __riscv_vlsseg8e32_v_i32m1x8_mu(vbool32_t mask,
                                             vint32m1x8_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m2x2_t __riscv_vlsseg2e32_v_i32m2x2_mu(vbool16_t mask,
                                             vint32m2x2_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m2x3_t __riscv_vlsseg3e32_v_i32m2x3_mu(vbool16_t mask,
                                             vint32m2x3_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m2x4_t __riscv_vlsseg4e32_v_i32m2x4_mu(vbool16_t mask,
                                             vint32m2x4_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint32m4x2_t __riscv_vlsseg2e32_v_i32m4x2_mu(vbool8_t mask,
                                             vint32m4x2_t maskedoff_tuple,
                                             const int32_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x2_t __riscv_vlsseg2e64_v_i64m1x2_mu(vbool64_t mask,
                                             vint64m1x2_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x3_t __riscv_vlsseg3e64_v_i64m1x3_mu(vbool64_t mask,
                                             vint64m1x3_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x4_t __riscv_vlsseg4e64_v_i64m1x4_mu(vbool64_t mask,
                                             vint64m1x4_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x5_t __riscv_vlsseg5e64_v_i64m1x5_mu(vbool64_t mask,
                                             vint64m1x5_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x6_t __riscv_vlsseg6e64_v_i64m1x6_mu(vbool64_t mask,
                                             vint64m1x6_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x7_t __riscv_vlsseg7e64_v_i64m1x7_mu(vbool64_t mask,
                                             vint64m1x7_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m1x8_t __riscv_vlsseg8e64_v_i64m1x8_mu(vbool64_t mask,
                                             vint64m1x8_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m2x2_t __riscv_vlsseg2e64_v_i64m2x2_mu(vbool32_t mask,
                                             vint64m2x2_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m2x3_t __riscv_vlsseg3e64_v_i64m2x3_mu(vbool32_t mask,
                                             vint64m2x3_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m2x4_t __riscv_vlsseg4e64_v_i64m2x4_mu(vbool32_t mask,
                                             vint64m2x4_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vint64m4x2_t __riscv_vlsseg2e64_v_i64m4x2_mu(vbool16_t mask,
                                             vint64m4x2_t maskedoff_tuple,
                                             const int64_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x2_t __riscv_vlsseg2e8_v_u8mf8x2_mu(vbool64_t mask,
                                             vuint8mf8x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x3_t __riscv_vlsseg3e8_v_u8mf8x3_mu(vbool64_t mask,
                                             vuint8mf8x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x4_t __riscv_vlsseg4e8_v_u8mf8x4_mu(vbool64_t mask,
                                             vuint8mf8x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x5_t __riscv_vlsseg5e8_v_u8mf8x5_mu(vbool64_t mask,
                                             vuint8mf8x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x6_t __riscv_vlsseg6e8_v_u8mf8x6_mu(vbool64_t mask,
                                             vuint8mf8x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x7_t __riscv_vlsseg7e8_v_u8mf8x7_mu(vbool64_t mask,
                                             vuint8mf8x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf8x8_t __riscv_vlsseg8e8_v_u8mf8x8_mu(vbool64_t mask,
                                             vuint8mf8x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x2_t __riscv_vlsseg2e8_v_u8mf4x2_mu(vbool32_t mask,
                                             vuint8mf4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x3_t __riscv_vlsseg3e8_v_u8mf4x3_mu(vbool32_t mask,
                                             vuint8mf4x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x4_t __riscv_vlsseg4e8_v_u8mf4x4_mu(vbool32_t mask,
                                             vuint8mf4x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x5_t __riscv_vlsseg5e8_v_u8mf4x5_mu(vbool32_t mask,
                                             vuint8mf4x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x6_t __riscv_vlsseg6e8_v_u8mf4x6_mu(vbool32_t mask,
                                             vuint8mf4x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x7_t __riscv_vlsseg7e8_v_u8mf4x7_mu(vbool32_t mask,
                                             vuint8mf4x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf4x8_t __riscv_vlsseg8e8_v_u8mf4x8_mu(vbool32_t mask,
                                             vuint8mf4x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x2_t __riscv_vlsseg2e8_v_u8mf2x2_mu(vbool16_t mask,
                                             vuint8mf2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x3_t __riscv_vlsseg3e8_v_u8mf2x3_mu(vbool16_t mask,
                                             vuint8mf2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x4_t __riscv_vlsseg4e8_v_u8mf2x4_mu(vbool16_t mask,
                                             vuint8mf2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x5_t __riscv_vlsseg5e8_v_u8mf2x5_mu(vbool16_t mask,
                                             vuint8mf2x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x6_t __riscv_vlsseg6e8_v_u8mf2x6_mu(vbool16_t mask,
                                             vuint8mf2x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x7_t __riscv_vlsseg7e8_v_u8mf2x7_mu(vbool16_t mask,
                                             vuint8mf2x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8mf2x8_t __riscv_vlsseg8e8_v_u8mf2x8_mu(vbool16_t mask,
                                             vuint8mf2x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             ptrdiff_t bstride, size_t vl);
vuint8m1x2_t __riscv_vlsseg2e8_v_u8m1x2_mu(vbool8_t mask,
                                           vuint8m1x2_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x3_t __riscv_vlsseg3e8_v_u8m1x3_mu(vbool8_t mask,
                                           vuint8m1x3_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x4_t __riscv_vlsseg4e8_v_u8m1x4_mu(vbool8_t mask,
                                           vuint8m1x4_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x5_t __riscv_vlsseg5e8_v_u8m1x5_mu(vbool8_t mask,
                                           vuint8m1x5_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x6_t __riscv_vlsseg6e8_v_u8m1x6_mu(vbool8_t mask,
                                           vuint8m1x6_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x7_t __riscv_vlsseg7e8_v_u8m1x7_mu(vbool8_t mask,
                                           vuint8m1x7_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m1x8_t __riscv_vlsseg8e8_v_u8m1x8_mu(vbool8_t mask,
                                           vuint8m1x8_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m2x2_t __riscv_vlsseg2e8_v_u8m2x2_mu(vbool4_t mask,
                                           vuint8m2x2_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m2x3_t __riscv_vlsseg3e8_v_u8m2x3_mu(vbool4_t mask,
                                           vuint8m2x3_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m2x4_t __riscv_vlsseg4e8_v_u8m2x4_mu(vbool4_t mask,
                                           vuint8m2x4_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint8m4x2_t __riscv_vlsseg2e8_v_u8m4x2_mu(vbool2_t mask,
                                           vuint8m4x2_t maskedoff_tuple,
                                           const uint8_t *base,
                                           ptrdiff_t bstride, size_t vl);
vuint16mf4x2_t __riscv_vlsseg2e16_v_u16mf4x2_mu(vbool64_t mask,
                                                vuint16mf4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x3_t __riscv_vlsseg3e16_v_u16mf4x3_mu(vbool64_t mask,
                                                vuint16mf4x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x4_t __riscv_vlsseg4e16_v_u16mf4x4_mu(vbool64_t mask,
                                                vuint16mf4x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x5_t __riscv_vlsseg5e16_v_u16mf4x5_mu(vbool64_t mask,
                                                vuint16mf4x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x6_t __riscv_vlsseg6e16_v_u16mf4x6_mu(vbool64_t mask,
                                                vuint16mf4x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x7_t __riscv_vlsseg7e16_v_u16mf4x7_mu(vbool64_t mask,
                                                vuint16mf4x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf4x8_t __riscv_vlsseg8e16_v_u16mf4x8_mu(vbool64_t mask,
                                                vuint16mf4x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x2_t __riscv_vlsseg2e16_v_u16mf2x2_mu(vbool32_t mask,
                                                vuint16mf2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x3_t __riscv_vlsseg3e16_v_u16mf2x3_mu(vbool32_t mask,
                                                vuint16mf2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x4_t __riscv_vlsseg4e16_v_u16mf2x4_mu(vbool32_t mask,
                                                vuint16mf2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x5_t __riscv_vlsseg5e16_v_u16mf2x5_mu(vbool32_t mask,
                                                vuint16mf2x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x6_t __riscv_vlsseg6e16_v_u16mf2x6_mu(vbool32_t mask,
                                                vuint16mf2x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x7_t __riscv_vlsseg7e16_v_u16mf2x7_mu(vbool32_t mask,
                                                vuint16mf2x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16mf2x8_t __riscv_vlsseg8e16_v_u16mf2x8_mu(vbool32_t mask,
                                                vuint16mf2x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint16m1x2_t __riscv_vlsseg2e16_v_u16m1x2_mu(vbool16_t mask,
                                              vuint16m1x2_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x3_t __riscv_vlsseg3e16_v_u16m1x3_mu(vbool16_t mask,
                                              vuint16m1x3_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x4_t __riscv_vlsseg4e16_v_u16m1x4_mu(vbool16_t mask,
                                              vuint16m1x4_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x5_t __riscv_vlsseg5e16_v_u16m1x5_mu(vbool16_t mask,
                                              vuint16m1x5_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x6_t __riscv_vlsseg6e16_v_u16m1x6_mu(vbool16_t mask,
                                              vuint16m1x6_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x7_t __riscv_vlsseg7e16_v_u16m1x7_mu(vbool16_t mask,
                                              vuint16m1x7_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m1x8_t __riscv_vlsseg8e16_v_u16m1x8_mu(vbool16_t mask,
                                              vuint16m1x8_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m2x2_t __riscv_vlsseg2e16_v_u16m2x2_mu(vbool8_t mask,
                                              vuint16m2x2_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m2x3_t __riscv_vlsseg3e16_v_u16m2x3_mu(vbool8_t mask,
                                              vuint16m2x3_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m2x4_t __riscv_vlsseg4e16_v_u16m2x4_mu(vbool8_t mask,
                                              vuint16m2x4_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint16m4x2_t __riscv_vlsseg2e16_v_u16m4x2_mu(vbool4_t mask,
                                              vuint16m4x2_t maskedoff_tuple,
                                              const uint16_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32mf2x2_t __riscv_vlsseg2e32_v_u32mf2x2_mu(vbool64_t mask,
                                                vuint32mf2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x3_t __riscv_vlsseg3e32_v_u32mf2x3_mu(vbool64_t mask,
                                                vuint32mf2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x4_t __riscv_vlsseg4e32_v_u32mf2x4_mu(vbool64_t mask,
                                                vuint32mf2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x5_t __riscv_vlsseg5e32_v_u32mf2x5_mu(vbool64_t mask,
                                                vuint32mf2x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x6_t __riscv_vlsseg6e32_v_u32mf2x6_mu(vbool64_t mask,
                                                vuint32mf2x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x7_t __riscv_vlsseg7e32_v_u32mf2x7_mu(vbool64_t mask,
                                                vuint32mf2x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32mf2x8_t __riscv_vlsseg8e32_v_u32mf2x8_mu(vbool64_t mask,
                                                vuint32mf2x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                ptrdiff_t bstride, size_t vl);
vuint32m1x2_t __riscv_vlsseg2e32_v_u32m1x2_mu(vbool32_t mask,
                                              vuint32m1x2_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x3_t __riscv_vlsseg3e32_v_u32m1x3_mu(vbool32_t mask,
                                              vuint32m1x3_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x4_t __riscv_vlsseg4e32_v_u32m1x4_mu(vbool32_t mask,
                                              vuint32m1x4_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x5_t __riscv_vlsseg5e32_v_u32m1x5_mu(vbool32_t mask,
                                              vuint32m1x5_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x6_t __riscv_vlsseg6e32_v_u32m1x6_mu(vbool32_t mask,
                                              vuint32m1x6_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x7_t __riscv_vlsseg7e32_v_u32m1x7_mu(vbool32_t mask,
                                              vuint32m1x7_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m1x8_t __riscv_vlsseg8e32_v_u32m1x8_mu(vbool32_t mask,
                                              vuint32m1x8_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m2x2_t __riscv_vlsseg2e32_v_u32m2x2_mu(vbool16_t mask,
                                              vuint32m2x2_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m2x3_t __riscv_vlsseg3e32_v_u32m2x3_mu(vbool16_t mask,
                                              vuint32m2x3_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m2x4_t __riscv_vlsseg4e32_v_u32m2x4_mu(vbool16_t mask,
                                              vuint32m2x4_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint32m4x2_t __riscv_vlsseg2e32_v_u32m4x2_mu(vbool8_t mask,
                                              vuint32m4x2_t maskedoff_tuple,
                                              const uint32_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x2_t __riscv_vlsseg2e64_v_u64m1x2_mu(vbool64_t mask,
                                              vuint64m1x2_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x3_t __riscv_vlsseg3e64_v_u64m1x3_mu(vbool64_t mask,
                                              vuint64m1x3_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x4_t __riscv_vlsseg4e64_v_u64m1x4_mu(vbool64_t mask,
                                              vuint64m1x4_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x5_t __riscv_vlsseg5e64_v_u64m1x5_mu(vbool64_t mask,
                                              vuint64m1x5_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x6_t __riscv_vlsseg6e64_v_u64m1x6_mu(vbool64_t mask,
                                              vuint64m1x6_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x7_t __riscv_vlsseg7e64_v_u64m1x7_mu(vbool64_t mask,
                                              vuint64m1x7_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m1x8_t __riscv_vlsseg8e64_v_u64m1x8_mu(vbool64_t mask,
                                              vuint64m1x8_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m2x2_t __riscv_vlsseg2e64_v_u64m2x2_mu(vbool32_t mask,
                                              vuint64m2x2_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m2x3_t __riscv_vlsseg3e64_v_u64m2x3_mu(vbool32_t mask,
                                              vuint64m2x3_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m2x4_t __riscv_vlsseg4e64_v_u64m2x4_mu(vbool32_t mask,
                                              vuint64m2x4_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
vuint64m4x2_t __riscv_vlsseg2e64_v_u64m4x2_mu(vbool16_t mask,
                                              vuint64m4x2_t maskedoff_tuple,
                                              const uint64_t *base,
                                              ptrdiff_t bstride, size_t vl);
----

[[policy-variant-vector-strided-segment-store]]
=== Vector Strided Segment Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-vector-indexed-segment-load]]
=== Vector Indexed Segment Load Intrinsics

[,c]
----
vfloat16mf4x2_t
__riscv_vloxseg2ei8_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x3_t
__riscv_vloxseg3ei8_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x4_t
__riscv_vloxseg4ei8_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x5_t
__riscv_vloxseg5ei8_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x6_t
__riscv_vloxseg6ei8_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x7_t
__riscv_vloxseg7ei8_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x8_t
__riscv_vloxseg8ei8_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf2x2_t
__riscv_vloxseg2ei8_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x3_t
__riscv_vloxseg3ei8_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x4_t
__riscv_vloxseg4ei8_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x5_t
__riscv_vloxseg5ei8_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x6_t
__riscv_vloxseg6ei8_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x7_t
__riscv_vloxseg7ei8_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x8_t
__riscv_vloxseg8ei8_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei8_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei8_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei8_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei8_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei8_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei8_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei8_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei8_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei8_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei8_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei8_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t
__riscv_vloxseg2ei16_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x3_t
__riscv_vloxseg3ei16_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x4_t
__riscv_vloxseg4ei16_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x5_t
__riscv_vloxseg5ei16_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x6_t
__riscv_vloxseg6ei16_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x7_t
__riscv_vloxseg7ei16_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x8_t
__riscv_vloxseg8ei16_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf2x2_t
__riscv_vloxseg2ei16_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x3_t
__riscv_vloxseg3ei16_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x4_t
__riscv_vloxseg4ei16_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x5_t
__riscv_vloxseg5ei16_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x6_t
__riscv_vloxseg6ei16_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x7_t
__riscv_vloxseg7ei16_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x8_t
__riscv_vloxseg8ei16_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei16_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei16_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei16_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei16_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei16_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei16_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei16_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei16_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei16_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei16_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei16_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t
__riscv_vloxseg2ei32_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x3_t
__riscv_vloxseg3ei32_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x4_t
__riscv_vloxseg4ei32_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x5_t
__riscv_vloxseg5ei32_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x6_t
__riscv_vloxseg6ei32_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x7_t
__riscv_vloxseg7ei32_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x8_t
__riscv_vloxseg8ei32_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf2x2_t
__riscv_vloxseg2ei32_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x3_t
__riscv_vloxseg3ei32_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x4_t
__riscv_vloxseg4ei32_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x5_t
__riscv_vloxseg5ei32_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x6_t
__riscv_vloxseg6ei32_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x7_t
__riscv_vloxseg7ei32_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x8_t
__riscv_vloxseg8ei32_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei32_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei32_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei32_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei32_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei32_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei32_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei32_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei32_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei32_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei32_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei32_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t
__riscv_vloxseg2ei64_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x3_t
__riscv_vloxseg3ei64_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x4_t
__riscv_vloxseg4ei64_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x5_t
__riscv_vloxseg5ei64_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x6_t
__riscv_vloxseg6ei64_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x7_t
__riscv_vloxseg7ei64_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x8_t
__riscv_vloxseg8ei64_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf2x2_t
__riscv_vloxseg2ei64_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x3_t
__riscv_vloxseg3ei64_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x4_t
__riscv_vloxseg4ei64_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x5_t
__riscv_vloxseg5ei64_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x6_t
__riscv_vloxseg6ei64_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x7_t
__riscv_vloxseg7ei64_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x8_t
__riscv_vloxseg8ei64_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei64_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei64_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei64_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei64_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei64_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei64_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei64_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei64_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei64_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei64_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vloxseg2ei8_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x3_t
__riscv_vloxseg3ei8_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x4_t
__riscv_vloxseg4ei8_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x5_t
__riscv_vloxseg5ei8_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x6_t
__riscv_vloxseg6ei8_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x7_t
__riscv_vloxseg7ei8_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x8_t
__riscv_vloxseg8ei8_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei8_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei8_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei8_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei8_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei8_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei8_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei8_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei8_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei8_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei8_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei8_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vloxseg2ei16_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x3_t
__riscv_vloxseg3ei16_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x4_t
__riscv_vloxseg4ei16_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x5_t
__riscv_vloxseg5ei16_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x6_t
__riscv_vloxseg6ei16_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x7_t
__riscv_vloxseg7ei16_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x8_t
__riscv_vloxseg8ei16_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei16_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei16_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei16_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei16_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei16_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei16_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei16_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei16_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei16_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei16_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei16_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vloxseg2ei32_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x3_t
__riscv_vloxseg3ei32_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x4_t
__riscv_vloxseg4ei32_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x5_t
__riscv_vloxseg5ei32_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x6_t
__riscv_vloxseg6ei32_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x7_t
__riscv_vloxseg7ei32_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x8_t
__riscv_vloxseg8ei32_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei32_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei32_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei32_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei32_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei32_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei32_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei32_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei32_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei32_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei32_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei32_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vloxseg2ei64_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x3_t
__riscv_vloxseg3ei64_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x4_t
__riscv_vloxseg4ei64_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x5_t
__riscv_vloxseg5ei64_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x6_t
__riscv_vloxseg6ei64_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x7_t
__riscv_vloxseg7ei64_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x8_t
__riscv_vloxseg8ei64_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei64_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei64_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei64_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei64_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei64_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei64_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei64_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei64_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei64_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei64_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei64_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei8_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei8_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei8_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei8_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei8_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei8_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei8_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei8_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei8_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei8_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei8_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei16_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei16_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei16_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei16_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei16_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei16_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei16_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei16_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei16_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei16_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei16_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei32_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei32_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei32_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei32_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei32_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei32_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei32_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei32_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei32_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei32_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei32_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei64_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei64_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei64_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei64_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei64_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei64_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei64_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei64_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei64_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei64_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei64_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16mf4x2_t
__riscv_vluxseg2ei8_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x3_t
__riscv_vluxseg3ei8_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x4_t
__riscv_vluxseg4ei8_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x5_t
__riscv_vluxseg5ei8_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x6_t
__riscv_vluxseg6ei8_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x7_t
__riscv_vluxseg7ei8_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf4x8_t
__riscv_vluxseg8ei8_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat16mf2x2_t
__riscv_vluxseg2ei8_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x3_t
__riscv_vluxseg3ei8_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x4_t
__riscv_vluxseg4ei8_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x5_t
__riscv_vluxseg5ei8_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x6_t
__riscv_vluxseg6ei8_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x7_t
__riscv_vluxseg7ei8_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16mf2x8_t
__riscv_vluxseg8ei8_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                  const float16_t *base, vuint8mf4_t bindex,
                                  size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei8_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei8_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei8_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei8_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei8_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei8_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei8_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei8_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei8_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei8_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei8_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t
__riscv_vluxseg2ei16_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x3_t
__riscv_vluxseg3ei16_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x4_t
__riscv_vluxseg4ei16_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x5_t
__riscv_vluxseg5ei16_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x6_t
__riscv_vluxseg6ei16_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x7_t
__riscv_vluxseg7ei16_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf4x8_t
__riscv_vluxseg8ei16_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat16mf2x2_t
__riscv_vluxseg2ei16_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x3_t
__riscv_vluxseg3ei16_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x4_t
__riscv_vluxseg4ei16_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x5_t
__riscv_vluxseg5ei16_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x6_t
__riscv_vluxseg6ei16_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x7_t
__riscv_vluxseg7ei16_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16mf2x8_t
__riscv_vluxseg8ei16_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                   const float16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei16_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei16_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei16_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei16_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei16_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei16_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei16_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei16_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei16_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei16_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei16_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t
__riscv_vluxseg2ei32_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x3_t
__riscv_vluxseg3ei32_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x4_t
__riscv_vluxseg4ei32_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x5_t
__riscv_vluxseg5ei32_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x6_t
__riscv_vluxseg6ei32_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x7_t
__riscv_vluxseg7ei32_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf4x8_t
__riscv_vluxseg8ei32_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                   const float16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat16mf2x2_t
__riscv_vluxseg2ei32_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x3_t
__riscv_vluxseg3ei32_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x4_t
__riscv_vluxseg4ei32_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x5_t
__riscv_vluxseg5ei32_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x6_t
__riscv_vluxseg6ei32_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x7_t
__riscv_vluxseg7ei32_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16mf2x8_t
__riscv_vluxseg8ei32_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                   const float16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei32_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei32_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei32_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei32_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei32_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei32_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei32_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei32_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei32_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei32_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei32_v_f16m4x2_tu(vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t
__riscv_vluxseg2ei64_v_f16mf4x2_tu(vfloat16mf4x2_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x3_t
__riscv_vluxseg3ei64_v_f16mf4x3_tu(vfloat16mf4x3_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x4_t
__riscv_vluxseg4ei64_v_f16mf4x4_tu(vfloat16mf4x4_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x5_t
__riscv_vluxseg5ei64_v_f16mf4x5_tu(vfloat16mf4x5_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x6_t
__riscv_vluxseg6ei64_v_f16mf4x6_tu(vfloat16mf4x6_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x7_t
__riscv_vluxseg7ei64_v_f16mf4x7_tu(vfloat16mf4x7_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf4x8_t
__riscv_vluxseg8ei64_v_f16mf4x8_tu(vfloat16mf4x8_t maskedoff_tuple,
                                   const float16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat16mf2x2_t
__riscv_vluxseg2ei64_v_f16mf2x2_tu(vfloat16mf2x2_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x3_t
__riscv_vluxseg3ei64_v_f16mf2x3_tu(vfloat16mf2x3_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x4_t
__riscv_vluxseg4ei64_v_f16mf2x4_tu(vfloat16mf2x4_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x5_t
__riscv_vluxseg5ei64_v_f16mf2x5_tu(vfloat16mf2x5_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x6_t
__riscv_vluxseg6ei64_v_f16mf2x6_tu(vfloat16mf2x6_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x7_t
__riscv_vluxseg7ei64_v_f16mf2x7_tu(vfloat16mf2x7_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16mf2x8_t
__riscv_vluxseg8ei64_v_f16mf2x8_tu(vfloat16mf2x8_t maskedoff_tuple,
                                   const float16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei64_v_f16m1x2_tu(vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei64_v_f16m1x3_tu(vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei64_v_f16m1x4_tu(vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei64_v_f16m1x5_tu(vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei64_v_f16m1x6_tu(vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei64_v_f16m1x7_tu(vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei64_v_f16m1x8_tu(vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei64_v_f16m2x2_tu(vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei64_v_f16m2x3_tu(vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei64_v_f16m2x4_tu(vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vluxseg2ei8_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x3_t
__riscv_vluxseg3ei8_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x4_t
__riscv_vluxseg4ei8_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x5_t
__riscv_vluxseg5ei8_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x6_t
__riscv_vluxseg6ei8_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x7_t
__riscv_vluxseg7ei8_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32mf2x8_t
__riscv_vluxseg8ei8_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                  const float32_t *base, vuint8mf8_t bindex,
                                  size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei8_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei8_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei8_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei8_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei8_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei8_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei8_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei8_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei8_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei8_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei8_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vluxseg2ei16_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x3_t
__riscv_vluxseg3ei16_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x4_t
__riscv_vluxseg4ei16_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x5_t
__riscv_vluxseg5ei16_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x6_t
__riscv_vluxseg6ei16_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x7_t
__riscv_vluxseg7ei16_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32mf2x8_t
__riscv_vluxseg8ei16_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                   const float32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei16_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei16_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei16_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei16_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei16_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei16_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei16_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei16_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei16_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei16_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei16_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vluxseg2ei32_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x3_t
__riscv_vluxseg3ei32_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x4_t
__riscv_vluxseg4ei32_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x5_t
__riscv_vluxseg5ei32_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x6_t
__riscv_vluxseg6ei32_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x7_t
__riscv_vluxseg7ei32_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32mf2x8_t
__riscv_vluxseg8ei32_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                   const float32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei32_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei32_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei32_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei32_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei32_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei32_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei32_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei32_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei32_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei32_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei32_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t
__riscv_vluxseg2ei64_v_f32mf2x2_tu(vfloat32mf2x2_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x3_t
__riscv_vluxseg3ei64_v_f32mf2x3_tu(vfloat32mf2x3_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x4_t
__riscv_vluxseg4ei64_v_f32mf2x4_tu(vfloat32mf2x4_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x5_t
__riscv_vluxseg5ei64_v_f32mf2x5_tu(vfloat32mf2x5_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x6_t
__riscv_vluxseg6ei64_v_f32mf2x6_tu(vfloat32mf2x6_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x7_t
__riscv_vluxseg7ei64_v_f32mf2x7_tu(vfloat32mf2x7_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32mf2x8_t
__riscv_vluxseg8ei64_v_f32mf2x8_tu(vfloat32mf2x8_t maskedoff_tuple,
                                   const float32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei64_v_f32m1x2_tu(vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei64_v_f32m1x3_tu(vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei64_v_f32m1x4_tu(vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei64_v_f32m1x5_tu(vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei64_v_f32m1x6_tu(vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei64_v_f32m1x7_tu(vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei64_v_f32m1x8_tu(vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei64_v_f32m2x2_tu(vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei64_v_f32m2x3_tu(vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei64_v_f32m2x4_tu(vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei64_v_f32m4x2_tu(vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei8_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei8_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei8_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei8_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei8_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei8_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei8_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei8_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei8_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei8_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei8_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei16_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei16_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei16_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei16_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei16_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei16_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei16_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei16_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei16_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei16_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei16_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei32_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei32_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei32_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei32_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei32_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei32_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei32_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei32_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei32_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei32_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei32_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei64_v_f64m1x2_tu(vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei64_v_f64m1x3_tu(vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei64_v_f64m1x4_tu(vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei64_v_f64m1x5_tu(vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei64_v_f64m1x6_tu(vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei64_v_f64m1x7_tu(vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei64_v_f64m1x8_tu(vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei64_v_f64m2x2_tu(vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei64_v_f64m2x3_tu(vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei64_v_f64m2x4_tu(vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei64_v_f64m4x2_tu(vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei8_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei8_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei8_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei8_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei8_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei8_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei8_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei8_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei8_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei8_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei8_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei8_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei8_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei8_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei8_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei8_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei8_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei8_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei8_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei8_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei8_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei8_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei8_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei8_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei8_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei8_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei8_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei8_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei8_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei8_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei8_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei8_v_i8m4x2_tu(vint8m4x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei16_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei16_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei16_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei16_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei16_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei16_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei16_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei16_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei16_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei16_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei16_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei16_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei16_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei16_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei16_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei16_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei16_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei16_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei16_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei16_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei16_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei16_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei16_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei16_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei16_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei16_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei16_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei16_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei16_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei16_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei16_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei16_v_i8m4x2_tu(vint8m4x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei32_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei32_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei32_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei32_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei32_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei32_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei32_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei32_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei32_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei32_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei32_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei32_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei32_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei32_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei32_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei32_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei32_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei32_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei32_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei32_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei32_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei32_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei32_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei32_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei32_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei32_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei32_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei32_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei32_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei32_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei32_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei64_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei64_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei64_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei64_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei64_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei64_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei64_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei64_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei64_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei64_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei64_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei64_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei64_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei64_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei64_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei64_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei64_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei64_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei64_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei64_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei64_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei64_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei64_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei64_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei64_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei64_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei64_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei64_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei8_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei8_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei8_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei8_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei8_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei8_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei8_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei8_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei8_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei8_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei8_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei8_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei8_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei8_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei8_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei8_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei8_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei8_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei8_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei8_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei8_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei8_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei8_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei8_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei8_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei16_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei16_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei16_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei16_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei16_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei16_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei16_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei16_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei16_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei16_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei16_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei16_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei16_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei16_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16m1x2_t __riscv_vloxseg2ei16_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei16_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei16_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei16_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei16_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei16_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei16_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei16_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei16_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei16_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei16_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei32_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei32_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei32_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei32_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei32_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei32_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei32_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei32_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei32_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei32_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei32_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei32_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei32_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei32_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei32_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei32_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei32_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei32_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei32_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei32_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei32_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei32_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei32_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei32_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei32_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei64_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei64_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei64_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei64_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei64_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei64_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei64_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei64_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei64_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei64_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei64_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei64_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei64_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei64_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei64_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei64_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei64_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei64_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei64_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei64_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei64_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei64_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei64_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei64_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei8_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei8_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei8_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei8_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei8_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei8_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei8_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei8_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei8_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei8_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei8_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei8_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei8_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei8_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei8_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei8_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei8_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei8_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei16_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei16_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei16_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei16_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei16_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei16_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei16_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vloxseg2ei16_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei16_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei16_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei16_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei16_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei16_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei16_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei16_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei16_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei16_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei16_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei32_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei32_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei32_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei32_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei32_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei32_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei32_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vloxseg2ei32_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei32_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei32_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei32_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei32_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei32_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei32_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei32_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei32_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei32_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei32_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei64_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei64_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei64_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei64_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei64_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei64_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei64_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei64_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei64_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei64_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei64_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei64_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei64_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei64_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei64_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei64_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei64_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei64_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei8_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei8_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei8_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei8_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei8_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei8_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei8_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei8_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei8_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei8_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei8_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei16_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei16_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei16_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei16_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei16_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei16_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei16_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei16_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei16_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei16_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei16_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei32_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei32_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei32_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei32_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei32_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei32_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei32_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei32_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei32_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei32_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei32_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei64_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei64_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei64_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei64_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei64_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei64_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei64_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei64_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei64_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei64_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei64_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei8_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei8_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei8_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei8_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei8_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei8_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei8_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei8_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei8_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei8_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei8_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei8_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei8_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei8_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei8_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei8_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei8_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei8_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei8_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei8_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei8_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei8_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei8_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei8_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei8_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei8_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei8_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei8_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei8_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei8_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei8_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei8_v_i8m4x2_tu(vint8m4x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei16_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei16_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei16_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei16_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei16_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei16_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei16_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei16_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei16_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei16_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei16_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei16_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei16_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei16_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei16_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei16_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei16_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei16_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei16_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei16_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei16_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei16_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei16_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei16_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei16_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei16_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei16_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei16_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei16_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei16_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei16_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei16_v_i8m4x2_tu(vint8m4x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei32_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei32_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei32_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei32_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei32_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei32_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei32_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei32_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei32_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei32_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei32_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei32_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei32_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei32_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei32_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei32_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei32_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei32_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei32_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei32_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei32_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei32_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei32_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei32_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei32_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei32_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei32_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei32_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei32_v_i8m2x2_tu(vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei32_v_i8m2x3_tu(vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei32_v_i8m2x4_tu(vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei64_v_i8mf8x2_tu(vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei64_v_i8mf8x3_tu(vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei64_v_i8mf8x4_tu(vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei64_v_i8mf8x5_tu(vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei64_v_i8mf8x6_tu(vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei64_v_i8mf8x7_tu(vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei64_v_i8mf8x8_tu(vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei64_v_i8mf4x2_tu(vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei64_v_i8mf4x3_tu(vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei64_v_i8mf4x4_tu(vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei64_v_i8mf4x5_tu(vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei64_v_i8mf4x6_tu(vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei64_v_i8mf4x7_tu(vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei64_v_i8mf4x8_tu(vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei64_v_i8mf2x2_tu(vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei64_v_i8mf2x3_tu(vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei64_v_i8mf2x4_tu(vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei64_v_i8mf2x5_tu(vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei64_v_i8mf2x6_tu(vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei64_v_i8mf2x7_tu(vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei64_v_i8mf2x8_tu(vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei64_v_i8m1x2_tu(vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei64_v_i8m1x3_tu(vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei64_v_i8m1x4_tu(vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei64_v_i8m1x5_tu(vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei64_v_i8m1x6_tu(vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei64_v_i8m1x7_tu(vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei64_v_i8m1x8_tu(vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei8_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei8_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei8_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei8_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei8_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei8_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei8_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei8_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei8_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei8_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei8_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei8_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei8_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei8_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei8_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei8_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei8_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei8_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei8_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei8_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei8_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei8_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei8_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei8_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei8_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei16_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei16_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei16_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei16_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei16_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei16_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei16_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei16_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei16_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei16_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei16_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei16_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei16_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei16_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16m1x2_t __riscv_vluxseg2ei16_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei16_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei16_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei16_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei16_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei16_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei16_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei16_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei16_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei16_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei16_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei32_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei32_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei32_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei32_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei32_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei32_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei32_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei32_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei32_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei32_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei32_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei32_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei32_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei32_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei32_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei32_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei32_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei32_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei32_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei32_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei32_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei32_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei32_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei32_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei32_v_i16m4x2_tu(vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei64_v_i16mf4x2_tu(vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei64_v_i16mf4x3_tu(vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei64_v_i16mf4x4_tu(vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei64_v_i16mf4x5_tu(vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei64_v_i16mf4x6_tu(vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei64_v_i16mf4x7_tu(vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei64_v_i16mf4x8_tu(vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei64_v_i16mf2x2_tu(vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei64_v_i16mf2x3_tu(vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei64_v_i16mf2x4_tu(vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei64_v_i16mf2x5_tu(vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei64_v_i16mf2x6_tu(vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei64_v_i16mf2x7_tu(vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei64_v_i16mf2x8_tu(vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei64_v_i16m1x2_tu(vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei64_v_i16m1x3_tu(vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei64_v_i16m1x4_tu(vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei64_v_i16m1x5_tu(vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei64_v_i16m1x6_tu(vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei64_v_i16m1x7_tu(vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei64_v_i16m1x8_tu(vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei64_v_i16m2x2_tu(vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei64_v_i16m2x3_tu(vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei64_v_i16m2x4_tu(vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei8_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei8_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei8_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei8_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei8_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei8_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei8_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei8_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei8_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei8_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei8_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei8_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei8_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei8_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei8_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei8_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei8_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei8_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei16_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei16_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei16_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei16_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei16_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei16_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei16_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vluxseg2ei16_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei16_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei16_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei16_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei16_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei16_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei16_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei16_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei16_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei16_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei16_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei32_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei32_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei32_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei32_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei32_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei32_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei32_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vluxseg2ei32_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei32_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei32_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei32_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei32_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei32_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei32_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei32_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei32_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei32_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei32_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei64_v_i32mf2x2_tu(vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei64_v_i32mf2x3_tu(vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei64_v_i32mf2x4_tu(vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei64_v_i32mf2x5_tu(vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei64_v_i32mf2x6_tu(vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei64_v_i32mf2x7_tu(vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei64_v_i32mf2x8_tu(vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei64_v_i32m1x2_tu(vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei64_v_i32m1x3_tu(vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei64_v_i32m1x4_tu(vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei64_v_i32m1x5_tu(vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei64_v_i32m1x6_tu(vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei64_v_i32m1x7_tu(vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei64_v_i32m1x8_tu(vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei64_v_i32m2x2_tu(vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei64_v_i32m2x3_tu(vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei64_v_i32m2x4_tu(vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei64_v_i32m4x2_tu(vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei8_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei8_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei8_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei8_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei8_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei8_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei8_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei8_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei8_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei8_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei8_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei16_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei16_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei16_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei16_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei16_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei16_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei16_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei16_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei16_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei16_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei16_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei32_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei32_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei32_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei32_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei32_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei32_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei32_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei32_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei32_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei32_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei32_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei64_v_i64m1x2_tu(vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei64_v_i64m1x3_tu(vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei64_v_i64m1x4_tu(vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei64_v_i64m1x5_tu(vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei64_v_i64m1x6_tu(vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei64_v_i64m1x7_tu(vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei64_v_i64m1x8_tu(vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei64_v_i64m2x2_tu(vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei64_v_i64m2x3_tu(vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei64_v_i64m2x4_tu(vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei64_v_i64m4x2_tu(vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei8_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei8_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei8_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei8_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei8_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei8_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei8_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei8_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei8_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei8_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei8_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei8_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei8_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei8_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei8_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei8_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei8_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei8_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei8_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei8_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei8_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei8_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei8_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei8_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei8_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei8_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei8_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei8_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei8_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei8_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei8_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei8_v_u8m4x2_tu(vuint8m4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei16_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei16_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei16_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei16_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei16_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei16_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei16_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei16_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei16_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei16_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei16_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei16_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei16_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei16_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei16_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei16_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei16_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei16_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei16_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei16_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei16_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei16_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei16_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei16_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei16_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei16_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei16_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei16_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei16_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei16_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei16_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei16_v_u8m4x2_tu(vuint8m4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei32_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei32_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei32_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei32_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei32_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei32_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei32_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei32_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei32_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei32_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei32_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei32_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei32_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei32_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei32_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei32_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei32_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei32_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei32_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei32_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei32_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei32_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei32_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei32_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei32_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei32_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei32_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei32_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei32_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei32_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei32_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei64_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei64_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei64_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei64_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei64_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei64_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei64_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei64_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei64_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei64_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei64_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei64_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei64_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei64_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei64_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei64_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei64_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei64_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei64_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei64_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei64_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei64_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei64_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei64_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei64_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei64_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei64_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei64_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei8_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei8_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei8_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei8_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei8_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei8_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei8_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei8_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei8_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei8_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei8_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei8_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei8_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei8_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei8_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei8_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei8_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei8_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei8_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei8_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei8_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei8_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei8_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei8_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei8_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t
__riscv_vloxseg2ei16_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x3_t
__riscv_vloxseg3ei16_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x4_t
__riscv_vloxseg4ei16_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x5_t
__riscv_vloxseg5ei16_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x6_t
__riscv_vloxseg6ei16_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x7_t
__riscv_vloxseg7ei16_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x8_t
__riscv_vloxseg8ei16_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf2x2_t
__riscv_vloxseg2ei16_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x3_t
__riscv_vloxseg3ei16_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x4_t
__riscv_vloxseg4ei16_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x5_t
__riscv_vloxseg5ei16_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x6_t
__riscv_vloxseg6ei16_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x7_t
__riscv_vloxseg7ei16_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x8_t
__riscv_vloxseg8ei16_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei16_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei16_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei16_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei16_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei16_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei16_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei16_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei16_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei16_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei16_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei16_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint16mf4x2_t
__riscv_vloxseg2ei32_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x3_t
__riscv_vloxseg3ei32_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x4_t
__riscv_vloxseg4ei32_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x5_t
__riscv_vloxseg5ei32_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x6_t
__riscv_vloxseg6ei32_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x7_t
__riscv_vloxseg7ei32_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x8_t
__riscv_vloxseg8ei32_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf2x2_t
__riscv_vloxseg2ei32_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x3_t
__riscv_vloxseg3ei32_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x4_t
__riscv_vloxseg4ei32_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x5_t
__riscv_vloxseg5ei32_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x6_t
__riscv_vloxseg6ei32_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x7_t
__riscv_vloxseg7ei32_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x8_t
__riscv_vloxseg8ei32_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei32_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei32_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei32_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei32_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei32_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei32_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei32_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei32_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei32_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei32_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei32_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint16mf4x2_t
__riscv_vloxseg2ei64_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x3_t
__riscv_vloxseg3ei64_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x4_t
__riscv_vloxseg4ei64_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x5_t
__riscv_vloxseg5ei64_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x6_t
__riscv_vloxseg6ei64_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x7_t
__riscv_vloxseg7ei64_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x8_t
__riscv_vloxseg8ei64_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf2x2_t
__riscv_vloxseg2ei64_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x3_t
__riscv_vloxseg3ei64_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x4_t
__riscv_vloxseg4ei64_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x5_t
__riscv_vloxseg5ei64_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x6_t
__riscv_vloxseg6ei64_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x7_t
__riscv_vloxseg7ei64_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x8_t
__riscv_vloxseg8ei64_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei64_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei64_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei64_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei64_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei64_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei64_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei64_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei64_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei64_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei64_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei8_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei8_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei8_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei8_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei8_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei8_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei8_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei8_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei8_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei8_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei8_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei8_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei8_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei8_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei8_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei8_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei8_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei8_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t
__riscv_vloxseg2ei16_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x3_t
__riscv_vloxseg3ei16_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x4_t
__riscv_vloxseg4ei16_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x5_t
__riscv_vloxseg5ei16_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x6_t
__riscv_vloxseg6ei16_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x7_t
__riscv_vloxseg7ei16_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x8_t
__riscv_vloxseg8ei16_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei16_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei16_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei16_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei16_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei16_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei16_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei16_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei16_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei16_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei16_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei16_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint32mf2x2_t
__riscv_vloxseg2ei32_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x3_t
__riscv_vloxseg3ei32_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x4_t
__riscv_vloxseg4ei32_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x5_t
__riscv_vloxseg5ei32_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x6_t
__riscv_vloxseg6ei32_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x7_t
__riscv_vloxseg7ei32_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x8_t
__riscv_vloxseg8ei32_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei32_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei32_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei32_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei32_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei32_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei32_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei32_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei32_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei32_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei32_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei32_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint32mf2x2_t
__riscv_vloxseg2ei64_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x3_t
__riscv_vloxseg3ei64_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x4_t
__riscv_vloxseg4ei64_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x5_t
__riscv_vloxseg5ei64_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x6_t
__riscv_vloxseg6ei64_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x7_t
__riscv_vloxseg7ei64_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x8_t
__riscv_vloxseg8ei64_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei64_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei64_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei64_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei64_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei64_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei64_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei64_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei64_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei64_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei64_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei64_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei8_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei8_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei8_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei8_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei8_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei8_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei8_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei8_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei8_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei8_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei8_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei16_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei16_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei16_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei16_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei16_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei16_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei16_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei16_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei16_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei16_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei16_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei32_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei32_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei32_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei32_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei32_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei32_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei32_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei32_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei32_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei32_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei32_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei64_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei64_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei64_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei64_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei64_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei64_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei64_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei64_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei64_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei64_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei64_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei8_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei8_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei8_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei8_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei8_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei8_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei8_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei8_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei8_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei8_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei8_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei8_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei8_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei8_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei8_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei8_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei8_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei8_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei8_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei8_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei8_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei8_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei8_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei8_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei8_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei8_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei8_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei8_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei8_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei8_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei8_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei8_v_u8m4x2_tu(vuint8m4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei16_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei16_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei16_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei16_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei16_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei16_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei16_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei16_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei16_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei16_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei16_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei16_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei16_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei16_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei16_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei16_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei16_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei16_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei16_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei16_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei16_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei16_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei16_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei16_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei16_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei16_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei16_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei16_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei16_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei16_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei16_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei16_v_u8m4x2_tu(vuint8m4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei32_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei32_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei32_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei32_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei32_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei32_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei32_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei32_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei32_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei32_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei32_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei32_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei32_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei32_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei32_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei32_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei32_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei32_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei32_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei32_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei32_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei32_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei32_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei32_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei32_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei32_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei32_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei32_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei32_v_u8m2x2_tu(vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei32_v_u8m2x3_tu(vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei32_v_u8m2x4_tu(vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei64_v_u8mf8x2_tu(vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei64_v_u8mf8x3_tu(vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei64_v_u8mf8x4_tu(vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei64_v_u8mf8x5_tu(vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei64_v_u8mf8x6_tu(vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei64_v_u8mf8x7_tu(vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei64_v_u8mf8x8_tu(vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei64_v_u8mf4x2_tu(vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei64_v_u8mf4x3_tu(vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei64_v_u8mf4x4_tu(vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei64_v_u8mf4x5_tu(vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei64_v_u8mf4x6_tu(vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei64_v_u8mf4x7_tu(vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei64_v_u8mf4x8_tu(vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei64_v_u8mf2x2_tu(vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei64_v_u8mf2x3_tu(vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei64_v_u8mf2x4_tu(vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei64_v_u8mf2x5_tu(vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei64_v_u8mf2x6_tu(vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei64_v_u8mf2x7_tu(vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei64_v_u8mf2x8_tu(vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei64_v_u8m1x2_tu(vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei64_v_u8m1x3_tu(vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei64_v_u8m1x4_tu(vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei64_v_u8m1x5_tu(vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei64_v_u8m1x6_tu(vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei64_v_u8m1x7_tu(vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei64_v_u8m1x8_tu(vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei8_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei8_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei8_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei8_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei8_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei8_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei8_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei8_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei8_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei8_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei8_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei8_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei8_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei8_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei8_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei8_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei8_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei8_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei8_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei8_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei8_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei8_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei8_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei8_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei8_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t
__riscv_vluxseg2ei16_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x3_t
__riscv_vluxseg3ei16_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x4_t
__riscv_vluxseg4ei16_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x5_t
__riscv_vluxseg5ei16_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x6_t
__riscv_vluxseg6ei16_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x7_t
__riscv_vluxseg7ei16_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf4x8_t
__riscv_vluxseg8ei16_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint16mf2x2_t
__riscv_vluxseg2ei16_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x3_t
__riscv_vluxseg3ei16_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x4_t
__riscv_vluxseg4ei16_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x5_t
__riscv_vluxseg5ei16_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x6_t
__riscv_vluxseg6ei16_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x7_t
__riscv_vluxseg7ei16_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16mf2x8_t
__riscv_vluxseg8ei16_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint16mf2_t bindex,
                                   size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei16_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei16_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei16_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei16_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei16_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei16_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei16_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei16_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei16_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei16_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei16_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint16mf4x2_t
__riscv_vluxseg2ei32_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x3_t
__riscv_vluxseg3ei32_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x4_t
__riscv_vluxseg4ei32_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x5_t
__riscv_vluxseg5ei32_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x6_t
__riscv_vluxseg6ei32_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x7_t
__riscv_vluxseg7ei32_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf4x8_t
__riscv_vluxseg8ei32_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint16mf2x2_t
__riscv_vluxseg2ei32_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x3_t
__riscv_vluxseg3ei32_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x4_t
__riscv_vluxseg4ei32_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x5_t
__riscv_vluxseg5ei32_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x6_t
__riscv_vluxseg6ei32_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x7_t
__riscv_vluxseg7ei32_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16mf2x8_t
__riscv_vluxseg8ei32_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint32m1_t bindex,
                                   size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei32_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei32_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei32_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei32_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei32_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei32_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei32_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei32_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei32_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei32_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei32_v_u16m4x2_tu(vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint16mf4x2_t
__riscv_vluxseg2ei64_v_u16mf4x2_tu(vuint16mf4x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x3_t
__riscv_vluxseg3ei64_v_u16mf4x3_tu(vuint16mf4x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x4_t
__riscv_vluxseg4ei64_v_u16mf4x4_tu(vuint16mf4x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x5_t
__riscv_vluxseg5ei64_v_u16mf4x5_tu(vuint16mf4x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x6_t
__riscv_vluxseg6ei64_v_u16mf4x6_tu(vuint16mf4x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x7_t
__riscv_vluxseg7ei64_v_u16mf4x7_tu(vuint16mf4x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf4x8_t
__riscv_vluxseg8ei64_v_u16mf4x8_tu(vuint16mf4x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint16mf2x2_t
__riscv_vluxseg2ei64_v_u16mf2x2_tu(vuint16mf2x2_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x3_t
__riscv_vluxseg3ei64_v_u16mf2x3_tu(vuint16mf2x3_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x4_t
__riscv_vluxseg4ei64_v_u16mf2x4_tu(vuint16mf2x4_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x5_t
__riscv_vluxseg5ei64_v_u16mf2x5_tu(vuint16mf2x5_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x6_t
__riscv_vluxseg6ei64_v_u16mf2x6_tu(vuint16mf2x6_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x7_t
__riscv_vluxseg7ei64_v_u16mf2x7_tu(vuint16mf2x7_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16mf2x8_t
__riscv_vluxseg8ei64_v_u16mf2x8_tu(vuint16mf2x8_t maskedoff_tuple,
                                   const uint16_t *base, vuint64m2_t bindex,
                                   size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei64_v_u16m1x2_tu(vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei64_v_u16m1x3_tu(vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei64_v_u16m1x4_tu(vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei64_v_u16m1x5_tu(vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei64_v_u16m1x6_tu(vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei64_v_u16m1x7_tu(vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei64_v_u16m1x8_tu(vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei64_v_u16m2x2_tu(vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei64_v_u16m2x3_tu(vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei64_v_u16m2x4_tu(vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei8_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei8_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei8_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei8_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei8_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei8_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei8_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei8_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei8_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei8_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei8_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei8_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei8_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei8_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei8_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei8_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei8_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei8_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t
__riscv_vluxseg2ei16_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x3_t
__riscv_vluxseg3ei16_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x4_t
__riscv_vluxseg4ei16_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x5_t
__riscv_vluxseg5ei16_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x6_t
__riscv_vluxseg6ei16_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x7_t
__riscv_vluxseg7ei16_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32mf2x8_t
__riscv_vluxseg8ei16_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                   const uint32_t *base, vuint16mf4_t bindex,
                                   size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei16_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei16_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei16_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei16_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei16_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei16_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei16_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei16_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei16_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei16_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei16_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint32mf2x2_t
__riscv_vluxseg2ei32_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x3_t
__riscv_vluxseg3ei32_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x4_t
__riscv_vluxseg4ei32_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x5_t
__riscv_vluxseg5ei32_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x6_t
__riscv_vluxseg6ei32_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x7_t
__riscv_vluxseg7ei32_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32mf2x8_t
__riscv_vluxseg8ei32_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                   const uint32_t *base, vuint32mf2_t bindex,
                                   size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei32_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei32_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei32_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei32_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei32_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei32_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei32_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei32_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei32_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei32_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei32_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint32mf2x2_t
__riscv_vluxseg2ei64_v_u32mf2x2_tu(vuint32mf2x2_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x3_t
__riscv_vluxseg3ei64_v_u32mf2x3_tu(vuint32mf2x3_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x4_t
__riscv_vluxseg4ei64_v_u32mf2x4_tu(vuint32mf2x4_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x5_t
__riscv_vluxseg5ei64_v_u32mf2x5_tu(vuint32mf2x5_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x6_t
__riscv_vluxseg6ei64_v_u32mf2x6_tu(vuint32mf2x6_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x7_t
__riscv_vluxseg7ei64_v_u32mf2x7_tu(vuint32mf2x7_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32mf2x8_t
__riscv_vluxseg8ei64_v_u32mf2x8_tu(vuint32mf2x8_t maskedoff_tuple,
                                   const uint32_t *base, vuint64m1_t bindex,
                                   size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei64_v_u32m1x2_tu(vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei64_v_u32m1x3_tu(vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei64_v_u32m1x4_tu(vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei64_v_u32m1x5_tu(vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei64_v_u32m1x6_tu(vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei64_v_u32m1x7_tu(vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei64_v_u32m1x8_tu(vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei64_v_u32m2x2_tu(vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei64_v_u32m2x3_tu(vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei64_v_u32m2x4_tu(vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei64_v_u32m4x2_tu(vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei8_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei8_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei8_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei8_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei8_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei8_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei8_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei8_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei8_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei8_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei8_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei16_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei16_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei16_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei16_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei16_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei16_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei16_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei16_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei16_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei16_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei16_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei32_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei32_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei32_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei32_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei32_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei32_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei32_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei32_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei32_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei32_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei32_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei64_v_u64m1x2_tu(vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei64_v_u64m1x3_tu(vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei64_v_u64m1x4_tu(vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei64_v_u64m1x5_tu(vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei64_v_u64m1x6_tu(vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei64_v_u64m1x7_tu(vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei64_v_u64m1x8_tu(vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei64_v_u64m2x2_tu(vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei64_v_u64m2x3_tu(vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei64_v_u64m2x4_tu(vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei64_v_u64m4x2_tu(vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m4_t bindex, size_t vl);
// masked functions
vfloat16mf4x2_t __riscv_vloxseg2ei8_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei8_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei8_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei8_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei8_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei8_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei8_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei8_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei8_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei8_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei8_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei8_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei8_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei8_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei8_v_f16m1x2_tum(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei8_v_f16m1x3_tum(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei8_v_f16m1x4_tum(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei8_v_f16m1x5_tum(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei8_v_f16m1x6_tum(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei8_v_f16m1x7_tum(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei8_v_f16m1x8_tum(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei8_v_f16m2x2_tum(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei8_v_f16m2x3_tum(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei8_v_f16m2x4_tum(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei8_v_f16m4x2_tum(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei16_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei16_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei16_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei16_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei16_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei16_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei16_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei16_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei16_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei16_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei16_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei16_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei16_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei16_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei16_v_f16m1x2_tum(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei16_v_f16m1x3_tum(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei16_v_f16m1x4_tum(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei16_v_f16m1x5_tum(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei16_v_f16m1x6_tum(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei16_v_f16m1x7_tum(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei16_v_f16m1x8_tum(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei16_v_f16m2x2_tum(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei16_v_f16m2x3_tum(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei16_v_f16m2x4_tum(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei16_v_f16m4x2_tum(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei32_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei32_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei32_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei32_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei32_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei32_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei32_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei32_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei32_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei32_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei32_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei32_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei32_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei32_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei32_v_f16m1x2_tum(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei32_v_f16m1x3_tum(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei32_v_f16m1x4_tum(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei32_v_f16m1x5_tum(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei32_v_f16m1x6_tum(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei32_v_f16m1x7_tum(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei32_v_f16m1x8_tum(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei32_v_f16m2x2_tum(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei32_v_f16m2x3_tum(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei32_v_f16m2x4_tum(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei32_v_f16m4x2_tum(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei64_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei64_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei64_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei64_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei64_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei64_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei64_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei64_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei64_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei64_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei64_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei64_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei64_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei64_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei64_v_f16m1x2_tum(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei64_v_f16m1x3_tum(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei64_v_f16m1x4_tum(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei64_v_f16m1x5_tum(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei64_v_f16m1x6_tum(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei64_v_f16m1x7_tum(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei64_v_f16m1x8_tum(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei64_v_f16m2x2_tum(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei64_v_f16m2x3_tum(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei64_v_f16m2x4_tum(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei8_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei8_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei8_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei8_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei8_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei8_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei8_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei8_v_f32m1x2_tum(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei8_v_f32m1x3_tum(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei8_v_f32m1x4_tum(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei8_v_f32m1x5_tum(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei8_v_f32m1x6_tum(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei8_v_f32m1x7_tum(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei8_v_f32m1x8_tum(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei8_v_f32m2x2_tum(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei8_v_f32m2x3_tum(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei8_v_f32m2x4_tum(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei8_v_f32m4x2_tum(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei16_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei16_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei16_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei16_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei16_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei16_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei16_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei16_v_f32m1x2_tum(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei16_v_f32m1x3_tum(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei16_v_f32m1x4_tum(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei16_v_f32m1x5_tum(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei16_v_f32m1x6_tum(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei16_v_f32m1x7_tum(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei16_v_f32m1x8_tum(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei16_v_f32m2x2_tum(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei16_v_f32m2x3_tum(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei16_v_f32m2x4_tum(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei16_v_f32m4x2_tum(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei32_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei32_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei32_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei32_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei32_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei32_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei32_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei32_v_f32m1x2_tum(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei32_v_f32m1x3_tum(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei32_v_f32m1x4_tum(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei32_v_f32m1x5_tum(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei32_v_f32m1x6_tum(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei32_v_f32m1x7_tum(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei32_v_f32m1x8_tum(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei32_v_f32m2x2_tum(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei32_v_f32m2x3_tum(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei32_v_f32m2x4_tum(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei32_v_f32m4x2_tum(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei64_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei64_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei64_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei64_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei64_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei64_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei64_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei64_v_f32m1x2_tum(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei64_v_f32m1x3_tum(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei64_v_f32m1x4_tum(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei64_v_f32m1x5_tum(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei64_v_f32m1x6_tum(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei64_v_f32m1x7_tum(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei64_v_f32m1x8_tum(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei64_v_f32m2x2_tum(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei64_v_f32m2x3_tum(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei64_v_f32m2x4_tum(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei64_v_f32m4x2_tum(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei8_v_f64m1x2_tum(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei8_v_f64m1x3_tum(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei8_v_f64m1x4_tum(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei8_v_f64m1x5_tum(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei8_v_f64m1x6_tum(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei8_v_f64m1x7_tum(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei8_v_f64m1x8_tum(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei8_v_f64m2x2_tum(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei8_v_f64m2x3_tum(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei8_v_f64m2x4_tum(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei8_v_f64m4x2_tum(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei16_v_f64m1x2_tum(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei16_v_f64m1x3_tum(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei16_v_f64m1x4_tum(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei16_v_f64m1x5_tum(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei16_v_f64m1x6_tum(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei16_v_f64m1x7_tum(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei16_v_f64m1x8_tum(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei16_v_f64m2x2_tum(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei16_v_f64m2x3_tum(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei16_v_f64m2x4_tum(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei16_v_f64m4x2_tum(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei32_v_f64m1x2_tum(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei32_v_f64m1x3_tum(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei32_v_f64m1x4_tum(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei32_v_f64m1x5_tum(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei32_v_f64m1x6_tum(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei32_v_f64m1x7_tum(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei32_v_f64m1x8_tum(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei32_v_f64m2x2_tum(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei32_v_f64m2x3_tum(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei32_v_f64m2x4_tum(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei32_v_f64m4x2_tum(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei64_v_f64m1x2_tum(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei64_v_f64m1x3_tum(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei64_v_f64m1x4_tum(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei64_v_f64m1x5_tum(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei64_v_f64m1x6_tum(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei64_v_f64m1x7_tum(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei64_v_f64m1x8_tum(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei64_v_f64m2x2_tum(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei64_v_f64m2x3_tum(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei64_v_f64m2x4_tum(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei64_v_f64m4x2_tum(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei8_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei8_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei8_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei8_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei8_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei8_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei8_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei8_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei8_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei8_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei8_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei8_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei8_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei8_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei8_v_f16m1x2_tum(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei8_v_f16m1x3_tum(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei8_v_f16m1x4_tum(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei8_v_f16m1x5_tum(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei8_v_f16m1x6_tum(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei8_v_f16m1x7_tum(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei8_v_f16m1x8_tum(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei8_v_f16m2x2_tum(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei8_v_f16m2x3_tum(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei8_v_f16m2x4_tum(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei8_v_f16m4x2_tum(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei16_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei16_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei16_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei16_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei16_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei16_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei16_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei16_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei16_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei16_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei16_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei16_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei16_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei16_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei16_v_f16m1x2_tum(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei16_v_f16m1x3_tum(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei16_v_f16m1x4_tum(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei16_v_f16m1x5_tum(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei16_v_f16m1x6_tum(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei16_v_f16m1x7_tum(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei16_v_f16m1x8_tum(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei16_v_f16m2x2_tum(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei16_v_f16m2x3_tum(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei16_v_f16m2x4_tum(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei16_v_f16m4x2_tum(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei32_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei32_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei32_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei32_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei32_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei32_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei32_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei32_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei32_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei32_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei32_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei32_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei32_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei32_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei32_v_f16m1x2_tum(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei32_v_f16m1x3_tum(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei32_v_f16m1x4_tum(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei32_v_f16m1x5_tum(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei32_v_f16m1x6_tum(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei32_v_f16m1x7_tum(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei32_v_f16m1x8_tum(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei32_v_f16m2x2_tum(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei32_v_f16m2x3_tum(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei32_v_f16m2x4_tum(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei32_v_f16m4x2_tum(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei64_v_f16mf4x2_tum(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei64_v_f16mf4x3_tum(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei64_v_f16mf4x4_tum(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei64_v_f16mf4x5_tum(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei64_v_f16mf4x6_tum(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei64_v_f16mf4x7_tum(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei64_v_f16mf4x8_tum(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei64_v_f16mf2x2_tum(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei64_v_f16mf2x3_tum(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei64_v_f16mf2x4_tum(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei64_v_f16mf2x5_tum(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei64_v_f16mf2x6_tum(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei64_v_f16mf2x7_tum(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei64_v_f16mf2x8_tum(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei64_v_f16m1x2_tum(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei64_v_f16m1x3_tum(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei64_v_f16m1x4_tum(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei64_v_f16m1x5_tum(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei64_v_f16m1x6_tum(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei64_v_f16m1x7_tum(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei64_v_f16m1x8_tum(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei64_v_f16m2x2_tum(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei64_v_f16m2x3_tum(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei64_v_f16m2x4_tum(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei8_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei8_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei8_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei8_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei8_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei8_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei8_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei8_v_f32m1x2_tum(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei8_v_f32m1x3_tum(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei8_v_f32m1x4_tum(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei8_v_f32m1x5_tum(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei8_v_f32m1x6_tum(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei8_v_f32m1x7_tum(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei8_v_f32m1x8_tum(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei8_v_f32m2x2_tum(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei8_v_f32m2x3_tum(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei8_v_f32m2x4_tum(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei8_v_f32m4x2_tum(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei16_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei16_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei16_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei16_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei16_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei16_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei16_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei16_v_f32m1x2_tum(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei16_v_f32m1x3_tum(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei16_v_f32m1x4_tum(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei16_v_f32m1x5_tum(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei16_v_f32m1x6_tum(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei16_v_f32m1x7_tum(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei16_v_f32m1x8_tum(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei16_v_f32m2x2_tum(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei16_v_f32m2x3_tum(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei16_v_f32m2x4_tum(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei16_v_f32m4x2_tum(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei32_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei32_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei32_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei32_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei32_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei32_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei32_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei32_v_f32m1x2_tum(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei32_v_f32m1x3_tum(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei32_v_f32m1x4_tum(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei32_v_f32m1x5_tum(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei32_v_f32m1x6_tum(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei32_v_f32m1x7_tum(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei32_v_f32m1x8_tum(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei32_v_f32m2x2_tum(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei32_v_f32m2x3_tum(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei32_v_f32m2x4_tum(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei32_v_f32m4x2_tum(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei64_v_f32mf2x2_tum(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei64_v_f32mf2x3_tum(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei64_v_f32mf2x4_tum(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei64_v_f32mf2x5_tum(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei64_v_f32mf2x6_tum(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei64_v_f32mf2x7_tum(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei64_v_f32mf2x8_tum(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei64_v_f32m1x2_tum(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei64_v_f32m1x3_tum(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei64_v_f32m1x4_tum(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei64_v_f32m1x5_tum(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei64_v_f32m1x6_tum(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei64_v_f32m1x7_tum(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei64_v_f32m1x8_tum(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei64_v_f32m2x2_tum(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei64_v_f32m2x3_tum(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei64_v_f32m2x4_tum(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei64_v_f32m4x2_tum(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei8_v_f64m1x2_tum(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei8_v_f64m1x3_tum(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei8_v_f64m1x4_tum(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei8_v_f64m1x5_tum(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei8_v_f64m1x6_tum(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei8_v_f64m1x7_tum(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei8_v_f64m1x8_tum(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei8_v_f64m2x2_tum(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei8_v_f64m2x3_tum(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei8_v_f64m2x4_tum(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei8_v_f64m4x2_tum(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei16_v_f64m1x2_tum(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei16_v_f64m1x3_tum(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei16_v_f64m1x4_tum(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei16_v_f64m1x5_tum(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei16_v_f64m1x6_tum(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei16_v_f64m1x7_tum(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei16_v_f64m1x8_tum(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei16_v_f64m2x2_tum(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei16_v_f64m2x3_tum(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei16_v_f64m2x4_tum(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei16_v_f64m4x2_tum(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei32_v_f64m1x2_tum(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei32_v_f64m1x3_tum(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei32_v_f64m1x4_tum(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei32_v_f64m1x5_tum(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei32_v_f64m1x6_tum(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei32_v_f64m1x7_tum(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei32_v_f64m1x8_tum(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei32_v_f64m2x2_tum(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei32_v_f64m2x3_tum(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei32_v_f64m2x4_tum(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei32_v_f64m4x2_tum(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei64_v_f64m1x2_tum(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei64_v_f64m1x3_tum(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei64_v_f64m1x4_tum(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei64_v_f64m1x5_tum(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei64_v_f64m1x6_tum(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei64_v_f64m1x7_tum(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei64_v_f64m1x8_tum(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei64_v_f64m2x2_tum(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei64_v_f64m2x3_tum(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei64_v_f64m2x4_tum(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei64_v_f64m4x2_tum(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei8_v_i8mf8x2_tum(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei8_v_i8mf8x3_tum(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei8_v_i8mf8x4_tum(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei8_v_i8mf8x5_tum(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei8_v_i8mf8x6_tum(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei8_v_i8mf8x7_tum(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei8_v_i8mf8x8_tum(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei8_v_i8mf4x2_tum(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei8_v_i8mf4x3_tum(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei8_v_i8mf4x4_tum(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei8_v_i8mf4x5_tum(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei8_v_i8mf4x6_tum(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei8_v_i8mf4x7_tum(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei8_v_i8mf4x8_tum(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei8_v_i8mf2x2_tum(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei8_v_i8mf2x3_tum(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei8_v_i8mf2x4_tum(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei8_v_i8mf2x5_tum(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei8_v_i8mf2x6_tum(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei8_v_i8mf2x7_tum(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei8_v_i8mf2x8_tum(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei8_v_i8m1x2_tum(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei8_v_i8m1x3_tum(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei8_v_i8m1x4_tum(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei8_v_i8m1x5_tum(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei8_v_i8m1x6_tum(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei8_v_i8m1x7_tum(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei8_v_i8m1x8_tum(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei8_v_i8m2x2_tum(vbool4_t mask,
                                             vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei8_v_i8m2x3_tum(vbool4_t mask,
                                             vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei8_v_i8m2x4_tum(vbool4_t mask,
                                             vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei8_v_i8m4x2_tum(vbool2_t mask,
                                             vint8m4x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei16_v_i8mf8x2_tum(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei16_v_i8mf8x3_tum(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei16_v_i8mf8x4_tum(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei16_v_i8mf8x5_tum(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei16_v_i8mf8x6_tum(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei16_v_i8mf8x7_tum(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei16_v_i8mf8x8_tum(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei16_v_i8mf4x2_tum(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei16_v_i8mf4x3_tum(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei16_v_i8mf4x4_tum(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei16_v_i8mf4x5_tum(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei16_v_i8mf4x6_tum(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei16_v_i8mf4x7_tum(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei16_v_i8mf4x8_tum(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei16_v_i8mf2x2_tum(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei16_v_i8mf2x3_tum(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei16_v_i8mf2x4_tum(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei16_v_i8mf2x5_tum(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei16_v_i8mf2x6_tum(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei16_v_i8mf2x7_tum(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei16_v_i8mf2x8_tum(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei16_v_i8m1x2_tum(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei16_v_i8m1x3_tum(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei16_v_i8m1x4_tum(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei16_v_i8m1x5_tum(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei16_v_i8m1x6_tum(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei16_v_i8m1x7_tum(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei16_v_i8m1x8_tum(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei16_v_i8m2x2_tum(vbool4_t mask,
                                              vint8m2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei16_v_i8m2x3_tum(vbool4_t mask,
                                              vint8m2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei16_v_i8m2x4_tum(vbool4_t mask,
                                              vint8m2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei16_v_i8m4x2_tum(vbool2_t mask,
                                              vint8m4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei32_v_i8mf8x2_tum(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei32_v_i8mf8x3_tum(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei32_v_i8mf8x4_tum(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei32_v_i8mf8x5_tum(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei32_v_i8mf8x6_tum(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei32_v_i8mf8x7_tum(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei32_v_i8mf8x8_tum(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei32_v_i8mf4x2_tum(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei32_v_i8mf4x3_tum(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei32_v_i8mf4x4_tum(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei32_v_i8mf4x5_tum(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei32_v_i8mf4x6_tum(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei32_v_i8mf4x7_tum(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei32_v_i8mf4x8_tum(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei32_v_i8mf2x2_tum(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei32_v_i8mf2x3_tum(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei32_v_i8mf2x4_tum(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei32_v_i8mf2x5_tum(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei32_v_i8mf2x6_tum(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei32_v_i8mf2x7_tum(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei32_v_i8mf2x8_tum(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei32_v_i8m1x2_tum(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei32_v_i8m1x3_tum(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei32_v_i8m1x4_tum(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei32_v_i8m1x5_tum(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei32_v_i8m1x6_tum(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei32_v_i8m1x7_tum(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei32_v_i8m1x8_tum(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei32_v_i8m2x2_tum(vbool4_t mask,
                                              vint8m2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei32_v_i8m2x3_tum(vbool4_t mask,
                                              vint8m2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei32_v_i8m2x4_tum(vbool4_t mask,
                                              vint8m2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei64_v_i8mf8x2_tum(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei64_v_i8mf8x3_tum(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei64_v_i8mf8x4_tum(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei64_v_i8mf8x5_tum(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei64_v_i8mf8x6_tum(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei64_v_i8mf8x7_tum(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei64_v_i8mf8x8_tum(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei64_v_i8mf4x2_tum(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei64_v_i8mf4x3_tum(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei64_v_i8mf4x4_tum(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei64_v_i8mf4x5_tum(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei64_v_i8mf4x6_tum(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei64_v_i8mf4x7_tum(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei64_v_i8mf4x8_tum(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei64_v_i8mf2x2_tum(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei64_v_i8mf2x3_tum(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei64_v_i8mf2x4_tum(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei64_v_i8mf2x5_tum(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei64_v_i8mf2x6_tum(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei64_v_i8mf2x7_tum(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei64_v_i8mf2x8_tum(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei64_v_i8m1x2_tum(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei64_v_i8m1x3_tum(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei64_v_i8m1x4_tum(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei64_v_i8m1x5_tum(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei64_v_i8m1x6_tum(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei64_v_i8m1x7_tum(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei64_v_i8m1x8_tum(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei8_v_i16mf4x2_tum(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei8_v_i16mf4x3_tum(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei8_v_i16mf4x4_tum(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei8_v_i16mf4x5_tum(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei8_v_i16mf4x6_tum(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei8_v_i16mf4x7_tum(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei8_v_i16mf4x8_tum(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei8_v_i16mf2x2_tum(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei8_v_i16mf2x3_tum(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei8_v_i16mf2x4_tum(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei8_v_i16mf2x5_tum(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei8_v_i16mf2x6_tum(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei8_v_i16mf2x7_tum(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei8_v_i16mf2x8_tum(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei8_v_i16m1x2_tum(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei8_v_i16m1x3_tum(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei8_v_i16m1x4_tum(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei8_v_i16m1x5_tum(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei8_v_i16m1x6_tum(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei8_v_i16m1x7_tum(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei8_v_i16m1x8_tum(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei8_v_i16m2x2_tum(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei8_v_i16m2x3_tum(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei8_v_i16m2x4_tum(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei8_v_i16m4x2_tum(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei16_v_i16mf4x2_tum(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei16_v_i16mf4x3_tum(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei16_v_i16mf4x4_tum(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei16_v_i16mf4x5_tum(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei16_v_i16mf4x6_tum(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei16_v_i16mf4x7_tum(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei16_v_i16mf4x8_tum(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei16_v_i16mf2x2_tum(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei16_v_i16mf2x3_tum(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei16_v_i16mf2x4_tum(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei16_v_i16mf2x5_tum(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei16_v_i16mf2x6_tum(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei16_v_i16mf2x7_tum(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei16_v_i16mf2x8_tum(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vloxseg2ei16_v_i16m1x2_tum(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei16_v_i16m1x3_tum(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei16_v_i16m1x4_tum(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei16_v_i16m1x5_tum(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei16_v_i16m1x6_tum(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei16_v_i16m1x7_tum(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei16_v_i16m1x8_tum(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei16_v_i16m2x2_tum(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei16_v_i16m2x3_tum(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei16_v_i16m2x4_tum(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei16_v_i16m4x2_tum(vbool4_t mask,
                                                vint16m4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei32_v_i16mf4x2_tum(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei32_v_i16mf4x3_tum(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei32_v_i16mf4x4_tum(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei32_v_i16mf4x5_tum(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei32_v_i16mf4x6_tum(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei32_v_i16mf4x7_tum(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei32_v_i16mf4x8_tum(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei32_v_i16mf2x2_tum(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei32_v_i16mf2x3_tum(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei32_v_i16mf2x4_tum(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei32_v_i16mf2x5_tum(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei32_v_i16mf2x6_tum(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei32_v_i16mf2x7_tum(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei32_v_i16mf2x8_tum(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vloxseg2ei32_v_i16m1x2_tum(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei32_v_i16m1x3_tum(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei32_v_i16m1x4_tum(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei32_v_i16m1x5_tum(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei32_v_i16m1x6_tum(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei32_v_i16m1x7_tum(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei32_v_i16m1x8_tum(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei32_v_i16m2x2_tum(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei32_v_i16m2x3_tum(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei32_v_i16m2x4_tum(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei32_v_i16m4x2_tum(vbool4_t mask,
                                                vint16m4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei64_v_i16mf4x2_tum(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei64_v_i16mf4x3_tum(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei64_v_i16mf4x4_tum(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei64_v_i16mf4x5_tum(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei64_v_i16mf4x6_tum(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei64_v_i16mf4x7_tum(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei64_v_i16mf4x8_tum(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei64_v_i16mf2x2_tum(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei64_v_i16mf2x3_tum(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei64_v_i16mf2x4_tum(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei64_v_i16mf2x5_tum(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei64_v_i16mf2x6_tum(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei64_v_i16mf2x7_tum(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei64_v_i16mf2x8_tum(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vloxseg2ei64_v_i16m1x2_tum(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei64_v_i16m1x3_tum(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei64_v_i16m1x4_tum(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei64_v_i16m1x5_tum(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei64_v_i16m1x6_tum(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei64_v_i16m1x7_tum(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei64_v_i16m1x8_tum(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei64_v_i16m2x2_tum(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei64_v_i16m2x3_tum(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei64_v_i16m2x4_tum(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei8_v_i32mf2x2_tum(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei8_v_i32mf2x3_tum(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei8_v_i32mf2x4_tum(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei8_v_i32mf2x5_tum(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei8_v_i32mf2x6_tum(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei8_v_i32mf2x7_tum(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei8_v_i32mf2x8_tum(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei8_v_i32m1x2_tum(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei8_v_i32m1x3_tum(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei8_v_i32m1x4_tum(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei8_v_i32m1x5_tum(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei8_v_i32m1x6_tum(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei8_v_i32m1x7_tum(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei8_v_i32m1x8_tum(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei8_v_i32m2x2_tum(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei8_v_i32m2x3_tum(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei8_v_i32m2x4_tum(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei8_v_i32m4x2_tum(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei16_v_i32mf2x2_tum(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei16_v_i32mf2x3_tum(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei16_v_i32mf2x4_tum(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei16_v_i32mf2x5_tum(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei16_v_i32mf2x6_tum(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei16_v_i32mf2x7_tum(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei16_v_i32mf2x8_tum(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vloxseg2ei16_v_i32m1x2_tum(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei16_v_i32m1x3_tum(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei16_v_i32m1x4_tum(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei16_v_i32m1x5_tum(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei16_v_i32m1x6_tum(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei16_v_i32m1x7_tum(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei16_v_i32m1x8_tum(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei16_v_i32m2x2_tum(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei16_v_i32m2x3_tum(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei16_v_i32m2x4_tum(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei16_v_i32m4x2_tum(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei32_v_i32mf2x2_tum(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei32_v_i32mf2x3_tum(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei32_v_i32mf2x4_tum(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei32_v_i32mf2x5_tum(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei32_v_i32mf2x6_tum(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei32_v_i32mf2x7_tum(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei32_v_i32mf2x8_tum(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vloxseg2ei32_v_i32m1x2_tum(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei32_v_i32m1x3_tum(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei32_v_i32m1x4_tum(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei32_v_i32m1x5_tum(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei32_v_i32m1x6_tum(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei32_v_i32m1x7_tum(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei32_v_i32m1x8_tum(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei32_v_i32m2x2_tum(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei32_v_i32m2x3_tum(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei32_v_i32m2x4_tum(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei32_v_i32m4x2_tum(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei64_v_i32mf2x2_tum(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei64_v_i32mf2x3_tum(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei64_v_i32mf2x4_tum(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei64_v_i32mf2x5_tum(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei64_v_i32mf2x6_tum(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei64_v_i32mf2x7_tum(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei64_v_i32mf2x8_tum(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vloxseg2ei64_v_i32m1x2_tum(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei64_v_i32m1x3_tum(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei64_v_i32m1x4_tum(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei64_v_i32m1x5_tum(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei64_v_i32m1x6_tum(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei64_v_i32m1x7_tum(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei64_v_i32m1x8_tum(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei64_v_i32m2x2_tum(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei64_v_i32m2x3_tum(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei64_v_i32m2x4_tum(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei64_v_i32m4x2_tum(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei8_v_i64m1x2_tum(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei8_v_i64m1x3_tum(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei8_v_i64m1x4_tum(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei8_v_i64m1x5_tum(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei8_v_i64m1x6_tum(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei8_v_i64m1x7_tum(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei8_v_i64m1x8_tum(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei8_v_i64m2x2_tum(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei8_v_i64m2x3_tum(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei8_v_i64m2x4_tum(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei8_v_i64m4x2_tum(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei16_v_i64m1x2_tum(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei16_v_i64m1x3_tum(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei16_v_i64m1x4_tum(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei16_v_i64m1x5_tum(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei16_v_i64m1x6_tum(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei16_v_i64m1x7_tum(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei16_v_i64m1x8_tum(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei16_v_i64m2x2_tum(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei16_v_i64m2x3_tum(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei16_v_i64m2x4_tum(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei16_v_i64m4x2_tum(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei32_v_i64m1x2_tum(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei32_v_i64m1x3_tum(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei32_v_i64m1x4_tum(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei32_v_i64m1x5_tum(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei32_v_i64m1x6_tum(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei32_v_i64m1x7_tum(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei32_v_i64m1x8_tum(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei32_v_i64m2x2_tum(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei32_v_i64m2x3_tum(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei32_v_i64m2x4_tum(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei32_v_i64m4x2_tum(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei64_v_i64m1x2_tum(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei64_v_i64m1x3_tum(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei64_v_i64m1x4_tum(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei64_v_i64m1x5_tum(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei64_v_i64m1x6_tum(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei64_v_i64m1x7_tum(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei64_v_i64m1x8_tum(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei64_v_i64m2x2_tum(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei64_v_i64m2x3_tum(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei64_v_i64m2x4_tum(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei64_v_i64m4x2_tum(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei8_v_i8mf8x2_tum(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei8_v_i8mf8x3_tum(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei8_v_i8mf8x4_tum(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei8_v_i8mf8x5_tum(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei8_v_i8mf8x6_tum(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei8_v_i8mf8x7_tum(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei8_v_i8mf8x8_tum(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei8_v_i8mf4x2_tum(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei8_v_i8mf4x3_tum(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei8_v_i8mf4x4_tum(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei8_v_i8mf4x5_tum(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei8_v_i8mf4x6_tum(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei8_v_i8mf4x7_tum(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei8_v_i8mf4x8_tum(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei8_v_i8mf2x2_tum(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei8_v_i8mf2x3_tum(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei8_v_i8mf2x4_tum(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei8_v_i8mf2x5_tum(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei8_v_i8mf2x6_tum(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei8_v_i8mf2x7_tum(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei8_v_i8mf2x8_tum(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei8_v_i8m1x2_tum(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei8_v_i8m1x3_tum(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei8_v_i8m1x4_tum(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei8_v_i8m1x5_tum(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei8_v_i8m1x6_tum(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei8_v_i8m1x7_tum(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei8_v_i8m1x8_tum(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei8_v_i8m2x2_tum(vbool4_t mask,
                                             vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei8_v_i8m2x3_tum(vbool4_t mask,
                                             vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei8_v_i8m2x4_tum(vbool4_t mask,
                                             vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei8_v_i8m4x2_tum(vbool2_t mask,
                                             vint8m4x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei16_v_i8mf8x2_tum(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei16_v_i8mf8x3_tum(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei16_v_i8mf8x4_tum(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei16_v_i8mf8x5_tum(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei16_v_i8mf8x6_tum(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei16_v_i8mf8x7_tum(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei16_v_i8mf8x8_tum(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei16_v_i8mf4x2_tum(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei16_v_i8mf4x3_tum(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei16_v_i8mf4x4_tum(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei16_v_i8mf4x5_tum(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei16_v_i8mf4x6_tum(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei16_v_i8mf4x7_tum(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei16_v_i8mf4x8_tum(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei16_v_i8mf2x2_tum(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei16_v_i8mf2x3_tum(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei16_v_i8mf2x4_tum(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei16_v_i8mf2x5_tum(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei16_v_i8mf2x6_tum(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei16_v_i8mf2x7_tum(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei16_v_i8mf2x8_tum(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei16_v_i8m1x2_tum(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei16_v_i8m1x3_tum(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei16_v_i8m1x4_tum(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei16_v_i8m1x5_tum(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei16_v_i8m1x6_tum(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei16_v_i8m1x7_tum(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei16_v_i8m1x8_tum(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei16_v_i8m2x2_tum(vbool4_t mask,
                                              vint8m2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei16_v_i8m2x3_tum(vbool4_t mask,
                                              vint8m2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei16_v_i8m2x4_tum(vbool4_t mask,
                                              vint8m2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei16_v_i8m4x2_tum(vbool2_t mask,
                                              vint8m4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei32_v_i8mf8x2_tum(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei32_v_i8mf8x3_tum(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei32_v_i8mf8x4_tum(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei32_v_i8mf8x5_tum(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei32_v_i8mf8x6_tum(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei32_v_i8mf8x7_tum(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei32_v_i8mf8x8_tum(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei32_v_i8mf4x2_tum(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei32_v_i8mf4x3_tum(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei32_v_i8mf4x4_tum(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei32_v_i8mf4x5_tum(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei32_v_i8mf4x6_tum(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei32_v_i8mf4x7_tum(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei32_v_i8mf4x8_tum(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei32_v_i8mf2x2_tum(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei32_v_i8mf2x3_tum(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei32_v_i8mf2x4_tum(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei32_v_i8mf2x5_tum(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei32_v_i8mf2x6_tum(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei32_v_i8mf2x7_tum(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei32_v_i8mf2x8_tum(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei32_v_i8m1x2_tum(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei32_v_i8m1x3_tum(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei32_v_i8m1x4_tum(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei32_v_i8m1x5_tum(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei32_v_i8m1x6_tum(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei32_v_i8m1x7_tum(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei32_v_i8m1x8_tum(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei32_v_i8m2x2_tum(vbool4_t mask,
                                              vint8m2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei32_v_i8m2x3_tum(vbool4_t mask,
                                              vint8m2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei32_v_i8m2x4_tum(vbool4_t mask,
                                              vint8m2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei64_v_i8mf8x2_tum(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei64_v_i8mf8x3_tum(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei64_v_i8mf8x4_tum(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei64_v_i8mf8x5_tum(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei64_v_i8mf8x6_tum(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei64_v_i8mf8x7_tum(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei64_v_i8mf8x8_tum(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei64_v_i8mf4x2_tum(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei64_v_i8mf4x3_tum(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei64_v_i8mf4x4_tum(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei64_v_i8mf4x5_tum(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei64_v_i8mf4x6_tum(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei64_v_i8mf4x7_tum(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei64_v_i8mf4x8_tum(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei64_v_i8mf2x2_tum(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei64_v_i8mf2x3_tum(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei64_v_i8mf2x4_tum(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei64_v_i8mf2x5_tum(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei64_v_i8mf2x6_tum(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei64_v_i8mf2x7_tum(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei64_v_i8mf2x8_tum(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei64_v_i8m1x2_tum(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei64_v_i8m1x3_tum(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei64_v_i8m1x4_tum(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei64_v_i8m1x5_tum(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei64_v_i8m1x6_tum(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei64_v_i8m1x7_tum(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei64_v_i8m1x8_tum(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei8_v_i16mf4x2_tum(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei8_v_i16mf4x3_tum(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei8_v_i16mf4x4_tum(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei8_v_i16mf4x5_tum(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei8_v_i16mf4x6_tum(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei8_v_i16mf4x7_tum(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei8_v_i16mf4x8_tum(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei8_v_i16mf2x2_tum(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei8_v_i16mf2x3_tum(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei8_v_i16mf2x4_tum(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei8_v_i16mf2x5_tum(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei8_v_i16mf2x6_tum(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei8_v_i16mf2x7_tum(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei8_v_i16mf2x8_tum(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei8_v_i16m1x2_tum(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei8_v_i16m1x3_tum(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei8_v_i16m1x4_tum(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei8_v_i16m1x5_tum(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei8_v_i16m1x6_tum(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei8_v_i16m1x7_tum(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei8_v_i16m1x8_tum(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei8_v_i16m2x2_tum(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei8_v_i16m2x3_tum(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei8_v_i16m2x4_tum(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei8_v_i16m4x2_tum(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei16_v_i16mf4x2_tum(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei16_v_i16mf4x3_tum(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei16_v_i16mf4x4_tum(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei16_v_i16mf4x5_tum(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei16_v_i16mf4x6_tum(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei16_v_i16mf4x7_tum(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei16_v_i16mf4x8_tum(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei16_v_i16mf2x2_tum(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei16_v_i16mf2x3_tum(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei16_v_i16mf2x4_tum(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei16_v_i16mf2x5_tum(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei16_v_i16mf2x6_tum(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei16_v_i16mf2x7_tum(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei16_v_i16mf2x8_tum(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vluxseg2ei16_v_i16m1x2_tum(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei16_v_i16m1x3_tum(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei16_v_i16m1x4_tum(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei16_v_i16m1x5_tum(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei16_v_i16m1x6_tum(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei16_v_i16m1x7_tum(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei16_v_i16m1x8_tum(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei16_v_i16m2x2_tum(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei16_v_i16m2x3_tum(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei16_v_i16m2x4_tum(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei16_v_i16m4x2_tum(vbool4_t mask,
                                                vint16m4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei32_v_i16mf4x2_tum(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei32_v_i16mf4x3_tum(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei32_v_i16mf4x4_tum(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei32_v_i16mf4x5_tum(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei32_v_i16mf4x6_tum(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei32_v_i16mf4x7_tum(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei32_v_i16mf4x8_tum(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei32_v_i16mf2x2_tum(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei32_v_i16mf2x3_tum(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei32_v_i16mf2x4_tum(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei32_v_i16mf2x5_tum(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei32_v_i16mf2x6_tum(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei32_v_i16mf2x7_tum(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei32_v_i16mf2x8_tum(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vluxseg2ei32_v_i16m1x2_tum(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei32_v_i16m1x3_tum(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei32_v_i16m1x4_tum(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei32_v_i16m1x5_tum(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei32_v_i16m1x6_tum(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei32_v_i16m1x7_tum(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei32_v_i16m1x8_tum(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei32_v_i16m2x2_tum(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei32_v_i16m2x3_tum(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei32_v_i16m2x4_tum(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei32_v_i16m4x2_tum(vbool4_t mask,
                                                vint16m4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei64_v_i16mf4x2_tum(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei64_v_i16mf4x3_tum(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei64_v_i16mf4x4_tum(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei64_v_i16mf4x5_tum(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei64_v_i16mf4x6_tum(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei64_v_i16mf4x7_tum(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei64_v_i16mf4x8_tum(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei64_v_i16mf2x2_tum(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei64_v_i16mf2x3_tum(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei64_v_i16mf2x4_tum(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei64_v_i16mf2x5_tum(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei64_v_i16mf2x6_tum(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei64_v_i16mf2x7_tum(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei64_v_i16mf2x8_tum(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vluxseg2ei64_v_i16m1x2_tum(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei64_v_i16m1x3_tum(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei64_v_i16m1x4_tum(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei64_v_i16m1x5_tum(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei64_v_i16m1x6_tum(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei64_v_i16m1x7_tum(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei64_v_i16m1x8_tum(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei64_v_i16m2x2_tum(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei64_v_i16m2x3_tum(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei64_v_i16m2x4_tum(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei8_v_i32mf2x2_tum(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei8_v_i32mf2x3_tum(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei8_v_i32mf2x4_tum(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei8_v_i32mf2x5_tum(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei8_v_i32mf2x6_tum(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei8_v_i32mf2x7_tum(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei8_v_i32mf2x8_tum(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei8_v_i32m1x2_tum(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei8_v_i32m1x3_tum(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei8_v_i32m1x4_tum(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei8_v_i32m1x5_tum(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei8_v_i32m1x6_tum(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei8_v_i32m1x7_tum(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei8_v_i32m1x8_tum(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei8_v_i32m2x2_tum(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei8_v_i32m2x3_tum(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei8_v_i32m2x4_tum(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei8_v_i32m4x2_tum(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei16_v_i32mf2x2_tum(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei16_v_i32mf2x3_tum(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei16_v_i32mf2x4_tum(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei16_v_i32mf2x5_tum(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei16_v_i32mf2x6_tum(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei16_v_i32mf2x7_tum(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei16_v_i32mf2x8_tum(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vluxseg2ei16_v_i32m1x2_tum(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei16_v_i32m1x3_tum(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei16_v_i32m1x4_tum(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei16_v_i32m1x5_tum(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei16_v_i32m1x6_tum(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei16_v_i32m1x7_tum(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei16_v_i32m1x8_tum(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei16_v_i32m2x2_tum(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei16_v_i32m2x3_tum(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei16_v_i32m2x4_tum(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei16_v_i32m4x2_tum(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei32_v_i32mf2x2_tum(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei32_v_i32mf2x3_tum(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei32_v_i32mf2x4_tum(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei32_v_i32mf2x5_tum(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei32_v_i32mf2x6_tum(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei32_v_i32mf2x7_tum(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei32_v_i32mf2x8_tum(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vluxseg2ei32_v_i32m1x2_tum(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei32_v_i32m1x3_tum(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei32_v_i32m1x4_tum(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei32_v_i32m1x5_tum(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei32_v_i32m1x6_tum(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei32_v_i32m1x7_tum(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei32_v_i32m1x8_tum(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei32_v_i32m2x2_tum(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei32_v_i32m2x3_tum(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei32_v_i32m2x4_tum(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei32_v_i32m4x2_tum(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei64_v_i32mf2x2_tum(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei64_v_i32mf2x3_tum(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei64_v_i32mf2x4_tum(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei64_v_i32mf2x5_tum(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei64_v_i32mf2x6_tum(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei64_v_i32mf2x7_tum(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei64_v_i32mf2x8_tum(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vluxseg2ei64_v_i32m1x2_tum(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei64_v_i32m1x3_tum(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei64_v_i32m1x4_tum(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei64_v_i32m1x5_tum(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei64_v_i32m1x6_tum(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei64_v_i32m1x7_tum(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei64_v_i32m1x8_tum(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei64_v_i32m2x2_tum(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei64_v_i32m2x3_tum(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei64_v_i32m2x4_tum(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei64_v_i32m4x2_tum(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei8_v_i64m1x2_tum(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei8_v_i64m1x3_tum(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei8_v_i64m1x4_tum(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei8_v_i64m1x5_tum(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei8_v_i64m1x6_tum(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei8_v_i64m1x7_tum(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei8_v_i64m1x8_tum(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei8_v_i64m2x2_tum(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei8_v_i64m2x3_tum(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei8_v_i64m2x4_tum(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei8_v_i64m4x2_tum(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei16_v_i64m1x2_tum(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei16_v_i64m1x3_tum(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei16_v_i64m1x4_tum(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei16_v_i64m1x5_tum(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei16_v_i64m1x6_tum(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei16_v_i64m1x7_tum(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei16_v_i64m1x8_tum(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei16_v_i64m2x2_tum(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei16_v_i64m2x3_tum(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei16_v_i64m2x4_tum(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei16_v_i64m4x2_tum(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei32_v_i64m1x2_tum(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei32_v_i64m1x3_tum(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei32_v_i64m1x4_tum(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei32_v_i64m1x5_tum(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei32_v_i64m1x6_tum(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei32_v_i64m1x7_tum(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei32_v_i64m1x8_tum(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei32_v_i64m2x2_tum(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei32_v_i64m2x3_tum(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei32_v_i64m2x4_tum(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei32_v_i64m4x2_tum(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei64_v_i64m1x2_tum(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei64_v_i64m1x3_tum(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei64_v_i64m1x4_tum(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei64_v_i64m1x5_tum(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei64_v_i64m1x6_tum(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei64_v_i64m1x7_tum(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei64_v_i64m1x8_tum(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei64_v_i64m2x2_tum(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei64_v_i64m2x3_tum(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei64_v_i64m2x4_tum(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei64_v_i64m4x2_tum(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei8_v_u8mf8x2_tum(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei8_v_u8mf8x3_tum(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei8_v_u8mf8x4_tum(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei8_v_u8mf8x5_tum(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei8_v_u8mf8x6_tum(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei8_v_u8mf8x7_tum(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei8_v_u8mf8x8_tum(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei8_v_u8mf4x2_tum(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei8_v_u8mf4x3_tum(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei8_v_u8mf4x4_tum(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei8_v_u8mf4x5_tum(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei8_v_u8mf4x6_tum(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei8_v_u8mf4x7_tum(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei8_v_u8mf4x8_tum(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei8_v_u8mf2x2_tum(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei8_v_u8mf2x3_tum(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei8_v_u8mf2x4_tum(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei8_v_u8mf2x5_tum(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei8_v_u8mf2x6_tum(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei8_v_u8mf2x7_tum(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei8_v_u8mf2x8_tum(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei8_v_u8m1x2_tum(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei8_v_u8m1x3_tum(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei8_v_u8m1x4_tum(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei8_v_u8m1x5_tum(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei8_v_u8m1x6_tum(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei8_v_u8m1x7_tum(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei8_v_u8m1x8_tum(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei8_v_u8m2x2_tum(vbool4_t mask,
                                              vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei8_v_u8m2x3_tum(vbool4_t mask,
                                              vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei8_v_u8m2x4_tum(vbool4_t mask,
                                              vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei8_v_u8m4x2_tum(vbool2_t mask,
                                              vuint8m4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei16_v_u8mf8x2_tum(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei16_v_u8mf8x3_tum(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei16_v_u8mf8x4_tum(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei16_v_u8mf8x5_tum(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei16_v_u8mf8x6_tum(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei16_v_u8mf8x7_tum(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei16_v_u8mf8x8_tum(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei16_v_u8mf4x2_tum(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei16_v_u8mf4x3_tum(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei16_v_u8mf4x4_tum(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei16_v_u8mf4x5_tum(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei16_v_u8mf4x6_tum(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei16_v_u8mf4x7_tum(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei16_v_u8mf4x8_tum(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei16_v_u8mf2x2_tum(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei16_v_u8mf2x3_tum(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei16_v_u8mf2x4_tum(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei16_v_u8mf2x5_tum(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei16_v_u8mf2x6_tum(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei16_v_u8mf2x7_tum(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei16_v_u8mf2x8_tum(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei16_v_u8m1x2_tum(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei16_v_u8m1x3_tum(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei16_v_u8m1x4_tum(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei16_v_u8m1x5_tum(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei16_v_u8m1x6_tum(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei16_v_u8m1x7_tum(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei16_v_u8m1x8_tum(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei16_v_u8m2x2_tum(vbool4_t mask,
                                               vuint8m2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei16_v_u8m2x3_tum(vbool4_t mask,
                                               vuint8m2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei16_v_u8m2x4_tum(vbool4_t mask,
                                               vuint8m2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei16_v_u8m4x2_tum(vbool2_t mask,
                                               vuint8m4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei32_v_u8mf8x2_tum(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei32_v_u8mf8x3_tum(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei32_v_u8mf8x4_tum(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei32_v_u8mf8x5_tum(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei32_v_u8mf8x6_tum(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei32_v_u8mf8x7_tum(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei32_v_u8mf8x8_tum(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei32_v_u8mf4x2_tum(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei32_v_u8mf4x3_tum(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei32_v_u8mf4x4_tum(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei32_v_u8mf4x5_tum(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei32_v_u8mf4x6_tum(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei32_v_u8mf4x7_tum(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei32_v_u8mf4x8_tum(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei32_v_u8mf2x2_tum(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei32_v_u8mf2x3_tum(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei32_v_u8mf2x4_tum(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei32_v_u8mf2x5_tum(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei32_v_u8mf2x6_tum(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei32_v_u8mf2x7_tum(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei32_v_u8mf2x8_tum(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei32_v_u8m1x2_tum(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei32_v_u8m1x3_tum(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei32_v_u8m1x4_tum(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei32_v_u8m1x5_tum(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei32_v_u8m1x6_tum(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei32_v_u8m1x7_tum(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei32_v_u8m1x8_tum(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei32_v_u8m2x2_tum(vbool4_t mask,
                                               vuint8m2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei32_v_u8m2x3_tum(vbool4_t mask,
                                               vuint8m2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei32_v_u8m2x4_tum(vbool4_t mask,
                                               vuint8m2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei64_v_u8mf8x2_tum(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei64_v_u8mf8x3_tum(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei64_v_u8mf8x4_tum(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei64_v_u8mf8x5_tum(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei64_v_u8mf8x6_tum(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei64_v_u8mf8x7_tum(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei64_v_u8mf8x8_tum(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei64_v_u8mf4x2_tum(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei64_v_u8mf4x3_tum(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei64_v_u8mf4x4_tum(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei64_v_u8mf4x5_tum(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei64_v_u8mf4x6_tum(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei64_v_u8mf4x7_tum(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei64_v_u8mf4x8_tum(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei64_v_u8mf2x2_tum(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei64_v_u8mf2x3_tum(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei64_v_u8mf2x4_tum(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei64_v_u8mf2x5_tum(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei64_v_u8mf2x6_tum(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei64_v_u8mf2x7_tum(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei64_v_u8mf2x8_tum(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei64_v_u8m1x2_tum(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei64_v_u8m1x3_tum(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei64_v_u8m1x4_tum(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei64_v_u8m1x5_tum(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei64_v_u8m1x6_tum(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei64_v_u8m1x7_tum(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei64_v_u8m1x8_tum(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei8_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei8_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei8_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei8_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei8_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei8_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei8_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei8_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei8_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei8_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei8_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei8_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei8_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei8_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei8_v_u16m1x2_tum(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei8_v_u16m1x3_tum(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei8_v_u16m1x4_tum(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei8_v_u16m1x5_tum(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei8_v_u16m1x6_tum(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei8_v_u16m1x7_tum(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei8_v_u16m1x8_tum(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei8_v_u16m2x2_tum(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei8_v_u16m2x3_tum(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei8_v_u16m2x4_tum(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei8_v_u16m4x2_tum(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei16_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei16_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei16_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei16_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei16_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei16_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei16_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei16_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei16_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei16_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei16_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei16_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei16_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei16_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei16_v_u16m1x2_tum(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei16_v_u16m1x3_tum(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei16_v_u16m1x4_tum(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei16_v_u16m1x5_tum(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei16_v_u16m1x6_tum(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei16_v_u16m1x7_tum(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei16_v_u16m1x8_tum(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei16_v_u16m2x2_tum(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei16_v_u16m2x3_tum(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei16_v_u16m2x4_tum(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei16_v_u16m4x2_tum(vbool4_t mask,
                                                 vuint16m4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei32_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei32_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei32_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei32_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei32_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei32_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei32_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei32_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei32_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei32_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei32_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei32_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei32_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei32_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei32_v_u16m1x2_tum(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei32_v_u16m1x3_tum(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei32_v_u16m1x4_tum(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei32_v_u16m1x5_tum(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei32_v_u16m1x6_tum(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei32_v_u16m1x7_tum(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei32_v_u16m1x8_tum(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei32_v_u16m2x2_tum(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei32_v_u16m2x3_tum(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei32_v_u16m2x4_tum(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei32_v_u16m4x2_tum(vbool4_t mask,
                                                 vuint16m4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei64_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei64_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei64_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei64_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei64_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei64_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei64_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei64_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei64_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei64_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei64_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei64_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei64_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei64_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei64_v_u16m1x2_tum(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei64_v_u16m1x3_tum(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei64_v_u16m1x4_tum(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei64_v_u16m1x5_tum(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei64_v_u16m1x6_tum(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei64_v_u16m1x7_tum(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei64_v_u16m1x8_tum(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei64_v_u16m2x2_tum(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei64_v_u16m2x3_tum(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei64_v_u16m2x4_tum(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei8_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei8_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei8_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei8_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei8_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei8_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei8_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei8_v_u32m1x2_tum(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei8_v_u32m1x3_tum(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei8_v_u32m1x4_tum(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei8_v_u32m1x5_tum(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei8_v_u32m1x6_tum(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei8_v_u32m1x7_tum(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei8_v_u32m1x8_tum(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei8_v_u32m2x2_tum(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei8_v_u32m2x3_tum(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei8_v_u32m2x4_tum(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei8_v_u32m4x2_tum(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei16_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei16_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei16_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei16_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei16_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei16_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei16_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei16_v_u32m1x2_tum(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei16_v_u32m1x3_tum(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei16_v_u32m1x4_tum(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei16_v_u32m1x5_tum(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei16_v_u32m1x6_tum(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei16_v_u32m1x7_tum(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei16_v_u32m1x8_tum(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei16_v_u32m2x2_tum(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei16_v_u32m2x3_tum(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei16_v_u32m2x4_tum(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei16_v_u32m4x2_tum(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei32_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei32_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei32_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei32_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei32_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei32_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei32_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei32_v_u32m1x2_tum(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei32_v_u32m1x3_tum(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei32_v_u32m1x4_tum(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei32_v_u32m1x5_tum(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei32_v_u32m1x6_tum(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei32_v_u32m1x7_tum(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei32_v_u32m1x8_tum(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei32_v_u32m2x2_tum(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei32_v_u32m2x3_tum(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei32_v_u32m2x4_tum(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei32_v_u32m4x2_tum(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei64_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei64_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei64_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei64_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei64_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei64_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei64_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei64_v_u32m1x2_tum(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei64_v_u32m1x3_tum(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei64_v_u32m1x4_tum(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei64_v_u32m1x5_tum(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei64_v_u32m1x6_tum(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei64_v_u32m1x7_tum(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei64_v_u32m1x8_tum(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei64_v_u32m2x2_tum(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei64_v_u32m2x3_tum(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei64_v_u32m2x4_tum(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei64_v_u32m4x2_tum(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei8_v_u64m1x2_tum(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei8_v_u64m1x3_tum(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei8_v_u64m1x4_tum(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei8_v_u64m1x5_tum(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei8_v_u64m1x6_tum(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei8_v_u64m1x7_tum(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei8_v_u64m1x8_tum(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei8_v_u64m2x2_tum(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei8_v_u64m2x3_tum(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei8_v_u64m2x4_tum(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei8_v_u64m4x2_tum(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei16_v_u64m1x2_tum(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei16_v_u64m1x3_tum(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei16_v_u64m1x4_tum(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei16_v_u64m1x5_tum(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei16_v_u64m1x6_tum(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei16_v_u64m1x7_tum(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei16_v_u64m1x8_tum(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei16_v_u64m2x2_tum(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei16_v_u64m2x3_tum(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei16_v_u64m2x4_tum(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei16_v_u64m4x2_tum(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei32_v_u64m1x2_tum(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei32_v_u64m1x3_tum(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei32_v_u64m1x4_tum(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei32_v_u64m1x5_tum(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei32_v_u64m1x6_tum(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei32_v_u64m1x7_tum(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei32_v_u64m1x8_tum(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei32_v_u64m2x2_tum(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei32_v_u64m2x3_tum(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei32_v_u64m2x4_tum(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei32_v_u64m4x2_tum(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei64_v_u64m1x2_tum(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei64_v_u64m1x3_tum(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei64_v_u64m1x4_tum(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei64_v_u64m1x5_tum(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei64_v_u64m1x6_tum(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei64_v_u64m1x7_tum(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei64_v_u64m1x8_tum(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei64_v_u64m2x2_tum(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei64_v_u64m2x3_tum(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei64_v_u64m2x4_tum(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei64_v_u64m4x2_tum(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei8_v_u8mf8x2_tum(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei8_v_u8mf8x3_tum(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei8_v_u8mf8x4_tum(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei8_v_u8mf8x5_tum(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei8_v_u8mf8x6_tum(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei8_v_u8mf8x7_tum(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei8_v_u8mf8x8_tum(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei8_v_u8mf4x2_tum(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei8_v_u8mf4x3_tum(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei8_v_u8mf4x4_tum(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei8_v_u8mf4x5_tum(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei8_v_u8mf4x6_tum(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei8_v_u8mf4x7_tum(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei8_v_u8mf4x8_tum(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei8_v_u8mf2x2_tum(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei8_v_u8mf2x3_tum(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei8_v_u8mf2x4_tum(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei8_v_u8mf2x5_tum(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei8_v_u8mf2x6_tum(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei8_v_u8mf2x7_tum(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei8_v_u8mf2x8_tum(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei8_v_u8m1x2_tum(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei8_v_u8m1x3_tum(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei8_v_u8m1x4_tum(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei8_v_u8m1x5_tum(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei8_v_u8m1x6_tum(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei8_v_u8m1x7_tum(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei8_v_u8m1x8_tum(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei8_v_u8m2x2_tum(vbool4_t mask,
                                              vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei8_v_u8m2x3_tum(vbool4_t mask,
                                              vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei8_v_u8m2x4_tum(vbool4_t mask,
                                              vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei8_v_u8m4x2_tum(vbool2_t mask,
                                              vuint8m4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei16_v_u8mf8x2_tum(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei16_v_u8mf8x3_tum(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei16_v_u8mf8x4_tum(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei16_v_u8mf8x5_tum(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei16_v_u8mf8x6_tum(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei16_v_u8mf8x7_tum(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei16_v_u8mf8x8_tum(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei16_v_u8mf4x2_tum(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei16_v_u8mf4x3_tum(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei16_v_u8mf4x4_tum(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei16_v_u8mf4x5_tum(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei16_v_u8mf4x6_tum(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei16_v_u8mf4x7_tum(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei16_v_u8mf4x8_tum(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei16_v_u8mf2x2_tum(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei16_v_u8mf2x3_tum(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei16_v_u8mf2x4_tum(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei16_v_u8mf2x5_tum(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei16_v_u8mf2x6_tum(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei16_v_u8mf2x7_tum(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei16_v_u8mf2x8_tum(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei16_v_u8m1x2_tum(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei16_v_u8m1x3_tum(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei16_v_u8m1x4_tum(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei16_v_u8m1x5_tum(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei16_v_u8m1x6_tum(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei16_v_u8m1x7_tum(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei16_v_u8m1x8_tum(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei16_v_u8m2x2_tum(vbool4_t mask,
                                               vuint8m2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei16_v_u8m2x3_tum(vbool4_t mask,
                                               vuint8m2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei16_v_u8m2x4_tum(vbool4_t mask,
                                               vuint8m2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei16_v_u8m4x2_tum(vbool2_t mask,
                                               vuint8m4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei32_v_u8mf8x2_tum(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei32_v_u8mf8x3_tum(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei32_v_u8mf8x4_tum(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei32_v_u8mf8x5_tum(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei32_v_u8mf8x6_tum(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei32_v_u8mf8x7_tum(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei32_v_u8mf8x8_tum(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei32_v_u8mf4x2_tum(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei32_v_u8mf4x3_tum(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei32_v_u8mf4x4_tum(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei32_v_u8mf4x5_tum(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei32_v_u8mf4x6_tum(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei32_v_u8mf4x7_tum(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei32_v_u8mf4x8_tum(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei32_v_u8mf2x2_tum(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei32_v_u8mf2x3_tum(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei32_v_u8mf2x4_tum(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei32_v_u8mf2x5_tum(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei32_v_u8mf2x6_tum(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei32_v_u8mf2x7_tum(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei32_v_u8mf2x8_tum(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei32_v_u8m1x2_tum(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei32_v_u8m1x3_tum(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei32_v_u8m1x4_tum(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei32_v_u8m1x5_tum(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei32_v_u8m1x6_tum(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei32_v_u8m1x7_tum(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei32_v_u8m1x8_tum(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei32_v_u8m2x2_tum(vbool4_t mask,
                                               vuint8m2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei32_v_u8m2x3_tum(vbool4_t mask,
                                               vuint8m2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei32_v_u8m2x4_tum(vbool4_t mask,
                                               vuint8m2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei64_v_u8mf8x2_tum(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei64_v_u8mf8x3_tum(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei64_v_u8mf8x4_tum(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei64_v_u8mf8x5_tum(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei64_v_u8mf8x6_tum(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei64_v_u8mf8x7_tum(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei64_v_u8mf8x8_tum(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei64_v_u8mf4x2_tum(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei64_v_u8mf4x3_tum(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei64_v_u8mf4x4_tum(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei64_v_u8mf4x5_tum(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei64_v_u8mf4x6_tum(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei64_v_u8mf4x7_tum(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei64_v_u8mf4x8_tum(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei64_v_u8mf2x2_tum(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei64_v_u8mf2x3_tum(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei64_v_u8mf2x4_tum(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei64_v_u8mf2x5_tum(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei64_v_u8mf2x6_tum(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei64_v_u8mf2x7_tum(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei64_v_u8mf2x8_tum(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei64_v_u8m1x2_tum(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei64_v_u8m1x3_tum(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei64_v_u8m1x4_tum(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei64_v_u8m1x5_tum(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei64_v_u8m1x6_tum(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei64_v_u8m1x7_tum(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei64_v_u8m1x8_tum(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei8_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei8_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei8_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei8_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei8_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei8_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei8_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei8_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei8_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei8_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei8_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei8_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei8_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei8_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei8_v_u16m1x2_tum(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei8_v_u16m1x3_tum(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei8_v_u16m1x4_tum(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei8_v_u16m1x5_tum(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei8_v_u16m1x6_tum(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei8_v_u16m1x7_tum(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei8_v_u16m1x8_tum(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei8_v_u16m2x2_tum(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei8_v_u16m2x3_tum(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei8_v_u16m2x4_tum(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei8_v_u16m4x2_tum(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei16_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei16_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei16_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei16_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei16_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei16_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei16_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei16_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei16_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei16_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei16_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei16_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei16_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei16_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei16_v_u16m1x2_tum(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei16_v_u16m1x3_tum(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei16_v_u16m1x4_tum(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei16_v_u16m1x5_tum(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei16_v_u16m1x6_tum(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei16_v_u16m1x7_tum(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei16_v_u16m1x8_tum(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei16_v_u16m2x2_tum(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei16_v_u16m2x3_tum(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei16_v_u16m2x4_tum(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei16_v_u16m4x2_tum(vbool4_t mask,
                                                 vuint16m4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei32_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei32_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei32_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei32_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei32_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei32_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei32_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei32_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei32_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei32_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei32_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei32_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei32_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei32_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei32_v_u16m1x2_tum(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei32_v_u16m1x3_tum(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei32_v_u16m1x4_tum(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei32_v_u16m1x5_tum(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei32_v_u16m1x6_tum(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei32_v_u16m1x7_tum(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei32_v_u16m1x8_tum(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei32_v_u16m2x2_tum(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei32_v_u16m2x3_tum(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei32_v_u16m2x4_tum(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei32_v_u16m4x2_tum(vbool4_t mask,
                                                 vuint16m4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei64_v_u16mf4x2_tum(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei64_v_u16mf4x3_tum(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei64_v_u16mf4x4_tum(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei64_v_u16mf4x5_tum(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei64_v_u16mf4x6_tum(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei64_v_u16mf4x7_tum(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei64_v_u16mf4x8_tum(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei64_v_u16mf2x2_tum(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei64_v_u16mf2x3_tum(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei64_v_u16mf2x4_tum(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei64_v_u16mf2x5_tum(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei64_v_u16mf2x6_tum(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei64_v_u16mf2x7_tum(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei64_v_u16mf2x8_tum(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei64_v_u16m1x2_tum(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei64_v_u16m1x3_tum(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei64_v_u16m1x4_tum(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei64_v_u16m1x5_tum(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei64_v_u16m1x6_tum(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei64_v_u16m1x7_tum(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei64_v_u16m1x8_tum(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei64_v_u16m2x2_tum(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei64_v_u16m2x3_tum(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei64_v_u16m2x4_tum(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei8_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei8_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei8_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei8_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei8_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei8_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei8_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei8_v_u32m1x2_tum(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei8_v_u32m1x3_tum(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei8_v_u32m1x4_tum(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei8_v_u32m1x5_tum(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei8_v_u32m1x6_tum(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei8_v_u32m1x7_tum(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei8_v_u32m1x8_tum(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei8_v_u32m2x2_tum(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei8_v_u32m2x3_tum(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei8_v_u32m2x4_tum(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei8_v_u32m4x2_tum(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei16_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei16_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei16_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei16_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei16_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei16_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei16_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei16_v_u32m1x2_tum(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei16_v_u32m1x3_tum(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei16_v_u32m1x4_tum(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei16_v_u32m1x5_tum(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei16_v_u32m1x6_tum(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei16_v_u32m1x7_tum(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei16_v_u32m1x8_tum(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei16_v_u32m2x2_tum(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei16_v_u32m2x3_tum(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei16_v_u32m2x4_tum(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei16_v_u32m4x2_tum(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei32_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei32_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei32_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei32_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei32_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei32_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei32_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei32_v_u32m1x2_tum(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei32_v_u32m1x3_tum(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei32_v_u32m1x4_tum(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei32_v_u32m1x5_tum(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei32_v_u32m1x6_tum(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei32_v_u32m1x7_tum(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei32_v_u32m1x8_tum(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei32_v_u32m2x2_tum(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei32_v_u32m2x3_tum(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei32_v_u32m2x4_tum(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei32_v_u32m4x2_tum(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei64_v_u32mf2x2_tum(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei64_v_u32mf2x3_tum(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei64_v_u32mf2x4_tum(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei64_v_u32mf2x5_tum(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei64_v_u32mf2x6_tum(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei64_v_u32mf2x7_tum(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei64_v_u32mf2x8_tum(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei64_v_u32m1x2_tum(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei64_v_u32m1x3_tum(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei64_v_u32m1x4_tum(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei64_v_u32m1x5_tum(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei64_v_u32m1x6_tum(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei64_v_u32m1x7_tum(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei64_v_u32m1x8_tum(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei64_v_u32m2x2_tum(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei64_v_u32m2x3_tum(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei64_v_u32m2x4_tum(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei64_v_u32m4x2_tum(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei8_v_u64m1x2_tum(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei8_v_u64m1x3_tum(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei8_v_u64m1x4_tum(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei8_v_u64m1x5_tum(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei8_v_u64m1x6_tum(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei8_v_u64m1x7_tum(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei8_v_u64m1x8_tum(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei8_v_u64m2x2_tum(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei8_v_u64m2x3_tum(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei8_v_u64m2x4_tum(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei8_v_u64m4x2_tum(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei16_v_u64m1x2_tum(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei16_v_u64m1x3_tum(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei16_v_u64m1x4_tum(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei16_v_u64m1x5_tum(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei16_v_u64m1x6_tum(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei16_v_u64m1x7_tum(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei16_v_u64m1x8_tum(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei16_v_u64m2x2_tum(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei16_v_u64m2x3_tum(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei16_v_u64m2x4_tum(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei16_v_u64m4x2_tum(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei32_v_u64m1x2_tum(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei32_v_u64m1x3_tum(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei32_v_u64m1x4_tum(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei32_v_u64m1x5_tum(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei32_v_u64m1x6_tum(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei32_v_u64m1x7_tum(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei32_v_u64m1x8_tum(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei32_v_u64m2x2_tum(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei32_v_u64m2x3_tum(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei32_v_u64m2x4_tum(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei32_v_u64m4x2_tum(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei64_v_u64m1x2_tum(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei64_v_u64m1x3_tum(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei64_v_u64m1x4_tum(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei64_v_u64m1x5_tum(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei64_v_u64m1x6_tum(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei64_v_u64m1x7_tum(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei64_v_u64m1x8_tum(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei64_v_u64m2x2_tum(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei64_v_u64m2x3_tum(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei64_v_u64m2x4_tum(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei64_v_u64m4x2_tum(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
// masked functions
vfloat16mf4x2_t __riscv_vloxseg2ei8_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei8_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei8_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei8_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei8_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei8_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei8_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei8_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei8_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei8_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei8_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei8_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei8_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei8_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei8_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei8_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei8_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei8_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei8_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei8_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei8_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei8_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei8_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei8_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei8_v_f16m4x2_tumu(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei16_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei16_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei16_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei16_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei16_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei16_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei16_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei16_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei16_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei16_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei16_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei16_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei16_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei16_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei16_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei16_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei16_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei16_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei16_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei16_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei16_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei16_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei16_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei16_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei16_v_f16m4x2_tumu(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei32_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei32_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei32_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei32_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei32_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei32_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei32_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei32_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei32_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei32_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei32_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei32_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei32_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei32_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei32_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei32_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei32_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei32_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei32_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei32_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei32_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei32_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei32_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei32_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei32_v_f16m4x2_tumu(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei64_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei64_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei64_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei64_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei64_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei64_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei64_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei64_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei64_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei64_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei64_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei64_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei64_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei64_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei64_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei64_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei64_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei64_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei64_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei64_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei64_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei64_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei64_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei64_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei8_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei8_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei8_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei8_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei8_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei8_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei8_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei8_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei8_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei8_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei8_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei8_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei8_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei8_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei8_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei8_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei8_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei8_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei16_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei16_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei16_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei16_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei16_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei16_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei16_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei16_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei16_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei16_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei16_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei16_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei16_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei16_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei16_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei16_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei16_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei16_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei32_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei32_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei32_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei32_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei32_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei32_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei32_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei32_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei32_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei32_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei32_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei32_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei32_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei32_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei32_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei32_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei32_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei32_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei64_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei64_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei64_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei64_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei64_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei64_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei64_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei64_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei64_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei64_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei64_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei64_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei64_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei64_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei64_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei64_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei64_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei64_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei8_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei8_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei8_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei8_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei8_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei8_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei8_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei8_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei8_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei8_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei8_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei16_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei16_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei16_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei16_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei16_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei16_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei16_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei16_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei16_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei16_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei16_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei32_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei32_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei32_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei32_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei32_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei32_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei32_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei32_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei32_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei32_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei32_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei64_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei64_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei64_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei64_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei64_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei64_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei64_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei64_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei64_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei64_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei64_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei8_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei8_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei8_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei8_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei8_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei8_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei8_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei8_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei8_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei8_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei8_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei8_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei8_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei8_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei8_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei8_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei8_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei8_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei8_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei8_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei8_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei8_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei8_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei8_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei8_v_f16m4x2_tumu(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei16_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei16_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei16_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei16_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei16_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei16_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei16_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei16_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei16_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei16_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei16_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei16_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei16_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei16_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei16_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei16_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei16_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei16_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei16_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei16_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei16_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei16_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei16_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei16_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei16_v_f16m4x2_tumu(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei32_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei32_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei32_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei32_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei32_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei32_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei32_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei32_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei32_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei32_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei32_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei32_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei32_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei32_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei32_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei32_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei32_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei32_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei32_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei32_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei32_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei32_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei32_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei32_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei32_v_f16m4x2_tumu(
    vbool4_t mask, vfloat16m4x2_t maskedoff_tuple, const float16_t *base,
    vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei64_v_f16mf4x2_tumu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei64_v_f16mf4x3_tumu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei64_v_f16mf4x4_tumu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei64_v_f16mf4x5_tumu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei64_v_f16mf4x6_tumu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei64_v_f16mf4x7_tumu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei64_v_f16mf4x8_tumu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei64_v_f16mf2x2_tumu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei64_v_f16mf2x3_tumu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei64_v_f16mf2x4_tumu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei64_v_f16mf2x5_tumu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei64_v_f16mf2x6_tumu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei64_v_f16mf2x7_tumu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei64_v_f16mf2x8_tumu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei64_v_f16m1x2_tumu(
    vbool16_t mask, vfloat16m1x2_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei64_v_f16m1x3_tumu(
    vbool16_t mask, vfloat16m1x3_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei64_v_f16m1x4_tumu(
    vbool16_t mask, vfloat16m1x4_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei64_v_f16m1x5_tumu(
    vbool16_t mask, vfloat16m1x5_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei64_v_f16m1x6_tumu(
    vbool16_t mask, vfloat16m1x6_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei64_v_f16m1x7_tumu(
    vbool16_t mask, vfloat16m1x7_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei64_v_f16m1x8_tumu(
    vbool16_t mask, vfloat16m1x8_t maskedoff_tuple, const float16_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei64_v_f16m2x2_tumu(
    vbool8_t mask, vfloat16m2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei64_v_f16m2x3_tumu(
    vbool8_t mask, vfloat16m2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei64_v_f16m2x4_tumu(
    vbool8_t mask, vfloat16m2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei8_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei8_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei8_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei8_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei8_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei8_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei8_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei8_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei8_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei8_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei8_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei8_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei8_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei8_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei8_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei8_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei8_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei8_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei16_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei16_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei16_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei16_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei16_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei16_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei16_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei16_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei16_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei16_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei16_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei16_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei16_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei16_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei16_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei16_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei16_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei16_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei32_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei32_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei32_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei32_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei32_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei32_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei32_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei32_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei32_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei32_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei32_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei32_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei32_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei32_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei32_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei32_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei32_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei32_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei64_v_f32mf2x2_tumu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei64_v_f32mf2x3_tumu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei64_v_f32mf2x4_tumu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei64_v_f32mf2x5_tumu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei64_v_f32mf2x6_tumu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei64_v_f32mf2x7_tumu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei64_v_f32mf2x8_tumu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei64_v_f32m1x2_tumu(
    vbool32_t mask, vfloat32m1x2_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei64_v_f32m1x3_tumu(
    vbool32_t mask, vfloat32m1x3_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei64_v_f32m1x4_tumu(
    vbool32_t mask, vfloat32m1x4_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei64_v_f32m1x5_tumu(
    vbool32_t mask, vfloat32m1x5_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei64_v_f32m1x6_tumu(
    vbool32_t mask, vfloat32m1x6_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei64_v_f32m1x7_tumu(
    vbool32_t mask, vfloat32m1x7_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei64_v_f32m1x8_tumu(
    vbool32_t mask, vfloat32m1x8_t maskedoff_tuple, const float32_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei64_v_f32m2x2_tumu(
    vbool16_t mask, vfloat32m2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei64_v_f32m2x3_tumu(
    vbool16_t mask, vfloat32m2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei64_v_f32m2x4_tumu(
    vbool16_t mask, vfloat32m2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei64_v_f32m4x2_tumu(
    vbool8_t mask, vfloat32m4x2_t maskedoff_tuple, const float32_t *base,
    vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei8_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei8_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei8_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei8_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei8_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei8_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei8_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei8_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei8_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei8_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei8_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei16_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei16_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei16_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei16_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei16_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei16_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei16_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei16_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei16_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei16_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei16_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei32_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei32_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei32_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei32_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei32_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei32_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei32_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei32_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei32_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei32_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei32_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei64_v_f64m1x2_tumu(
    vbool64_t mask, vfloat64m1x2_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei64_v_f64m1x3_tumu(
    vbool64_t mask, vfloat64m1x3_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei64_v_f64m1x4_tumu(
    vbool64_t mask, vfloat64m1x4_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei64_v_f64m1x5_tumu(
    vbool64_t mask, vfloat64m1x5_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei64_v_f64m1x6_tumu(
    vbool64_t mask, vfloat64m1x6_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei64_v_f64m1x7_tumu(
    vbool64_t mask, vfloat64m1x7_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei64_v_f64m1x8_tumu(
    vbool64_t mask, vfloat64m1x8_t maskedoff_tuple, const float64_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei64_v_f64m2x2_tumu(
    vbool32_t mask, vfloat64m2x2_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei64_v_f64m2x3_tumu(
    vbool32_t mask, vfloat64m2x3_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei64_v_f64m2x4_tumu(
    vbool32_t mask, vfloat64m2x4_t maskedoff_tuple, const float64_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei64_v_f64m4x2_tumu(
    vbool16_t mask, vfloat64m4x2_t maskedoff_tuple, const float64_t *base,
    vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei8_v_i8mf8x2_tumu(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei8_v_i8mf8x3_tumu(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei8_v_i8mf8x4_tumu(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei8_v_i8mf8x5_tumu(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei8_v_i8mf8x6_tumu(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei8_v_i8mf8x7_tumu(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei8_v_i8mf8x8_tumu(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei8_v_i8mf4x2_tumu(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei8_v_i8mf4x3_tumu(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei8_v_i8mf4x4_tumu(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei8_v_i8mf4x5_tumu(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei8_v_i8mf4x6_tumu(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei8_v_i8mf4x7_tumu(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei8_v_i8mf4x8_tumu(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei8_v_i8mf2x2_tumu(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei8_v_i8mf2x3_tumu(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei8_v_i8mf2x4_tumu(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei8_v_i8mf2x5_tumu(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei8_v_i8mf2x6_tumu(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei8_v_i8mf2x7_tumu(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei8_v_i8mf2x8_tumu(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei8_v_i8m1x2_tumu(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei8_v_i8m1x3_tumu(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei8_v_i8m1x4_tumu(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei8_v_i8m1x5_tumu(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei8_v_i8m1x6_tumu(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei8_v_i8m1x7_tumu(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei8_v_i8m1x8_tumu(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei8_v_i8m2x2_tumu(vbool4_t mask,
                                              vint8m2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei8_v_i8m2x3_tumu(vbool4_t mask,
                                              vint8m2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei8_v_i8m2x4_tumu(vbool4_t mask,
                                              vint8m2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei8_v_i8m4x2_tumu(vbool2_t mask,
                                              vint8m4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei16_v_i8mf8x2_tumu(vbool64_t mask,
                                                 vint8mf8x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei16_v_i8mf8x3_tumu(vbool64_t mask,
                                                 vint8mf8x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei16_v_i8mf8x4_tumu(vbool64_t mask,
                                                 vint8mf8x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei16_v_i8mf8x5_tumu(vbool64_t mask,
                                                 vint8mf8x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei16_v_i8mf8x6_tumu(vbool64_t mask,
                                                 vint8mf8x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei16_v_i8mf8x7_tumu(vbool64_t mask,
                                                 vint8mf8x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei16_v_i8mf8x8_tumu(vbool64_t mask,
                                                 vint8mf8x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei16_v_i8mf4x2_tumu(vbool32_t mask,
                                                 vint8mf4x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei16_v_i8mf4x3_tumu(vbool32_t mask,
                                                 vint8mf4x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei16_v_i8mf4x4_tumu(vbool32_t mask,
                                                 vint8mf4x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei16_v_i8mf4x5_tumu(vbool32_t mask,
                                                 vint8mf4x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei16_v_i8mf4x6_tumu(vbool32_t mask,
                                                 vint8mf4x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei16_v_i8mf4x7_tumu(vbool32_t mask,
                                                 vint8mf4x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei16_v_i8mf4x8_tumu(vbool32_t mask,
                                                 vint8mf4x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei16_v_i8mf2x2_tumu(vbool16_t mask,
                                                 vint8mf2x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei16_v_i8mf2x3_tumu(vbool16_t mask,
                                                 vint8mf2x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei16_v_i8mf2x4_tumu(vbool16_t mask,
                                                 vint8mf2x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei16_v_i8mf2x5_tumu(vbool16_t mask,
                                                 vint8mf2x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei16_v_i8mf2x6_tumu(vbool16_t mask,
                                                 vint8mf2x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei16_v_i8mf2x7_tumu(vbool16_t mask,
                                                 vint8mf2x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei16_v_i8mf2x8_tumu(vbool16_t mask,
                                                 vint8mf2x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei16_v_i8m1x2_tumu(vbool8_t mask,
                                               vint8m1x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei16_v_i8m1x3_tumu(vbool8_t mask,
                                               vint8m1x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei16_v_i8m1x4_tumu(vbool8_t mask,
                                               vint8m1x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei16_v_i8m1x5_tumu(vbool8_t mask,
                                               vint8m1x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei16_v_i8m1x6_tumu(vbool8_t mask,
                                               vint8m1x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei16_v_i8m1x7_tumu(vbool8_t mask,
                                               vint8m1x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei16_v_i8m1x8_tumu(vbool8_t mask,
                                               vint8m1x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei16_v_i8m2x2_tumu(vbool4_t mask,
                                               vint8m2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei16_v_i8m2x3_tumu(vbool4_t mask,
                                               vint8m2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei16_v_i8m2x4_tumu(vbool4_t mask,
                                               vint8m2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei16_v_i8m4x2_tumu(vbool2_t mask,
                                               vint8m4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei32_v_i8mf8x2_tumu(vbool64_t mask,
                                                 vint8mf8x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei32_v_i8mf8x3_tumu(vbool64_t mask,
                                                 vint8mf8x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei32_v_i8mf8x4_tumu(vbool64_t mask,
                                                 vint8mf8x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei32_v_i8mf8x5_tumu(vbool64_t mask,
                                                 vint8mf8x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei32_v_i8mf8x6_tumu(vbool64_t mask,
                                                 vint8mf8x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei32_v_i8mf8x7_tumu(vbool64_t mask,
                                                 vint8mf8x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei32_v_i8mf8x8_tumu(vbool64_t mask,
                                                 vint8mf8x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei32_v_i8mf4x2_tumu(vbool32_t mask,
                                                 vint8mf4x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei32_v_i8mf4x3_tumu(vbool32_t mask,
                                                 vint8mf4x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei32_v_i8mf4x4_tumu(vbool32_t mask,
                                                 vint8mf4x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei32_v_i8mf4x5_tumu(vbool32_t mask,
                                                 vint8mf4x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei32_v_i8mf4x6_tumu(vbool32_t mask,
                                                 vint8mf4x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei32_v_i8mf4x7_tumu(vbool32_t mask,
                                                 vint8mf4x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei32_v_i8mf4x8_tumu(vbool32_t mask,
                                                 vint8mf4x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei32_v_i8mf2x2_tumu(vbool16_t mask,
                                                 vint8mf2x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei32_v_i8mf2x3_tumu(vbool16_t mask,
                                                 vint8mf2x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei32_v_i8mf2x4_tumu(vbool16_t mask,
                                                 vint8mf2x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei32_v_i8mf2x5_tumu(vbool16_t mask,
                                                 vint8mf2x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei32_v_i8mf2x6_tumu(vbool16_t mask,
                                                 vint8mf2x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei32_v_i8mf2x7_tumu(vbool16_t mask,
                                                 vint8mf2x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei32_v_i8mf2x8_tumu(vbool16_t mask,
                                                 vint8mf2x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei32_v_i8m1x2_tumu(vbool8_t mask,
                                               vint8m1x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei32_v_i8m1x3_tumu(vbool8_t mask,
                                               vint8m1x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei32_v_i8m1x4_tumu(vbool8_t mask,
                                               vint8m1x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei32_v_i8m1x5_tumu(vbool8_t mask,
                                               vint8m1x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei32_v_i8m1x6_tumu(vbool8_t mask,
                                               vint8m1x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei32_v_i8m1x7_tumu(vbool8_t mask,
                                               vint8m1x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei32_v_i8m1x8_tumu(vbool8_t mask,
                                               vint8m1x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei32_v_i8m2x2_tumu(vbool4_t mask,
                                               vint8m2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei32_v_i8m2x3_tumu(vbool4_t mask,
                                               vint8m2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei32_v_i8m2x4_tumu(vbool4_t mask,
                                               vint8m2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei64_v_i8mf8x2_tumu(vbool64_t mask,
                                                 vint8mf8x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei64_v_i8mf8x3_tumu(vbool64_t mask,
                                                 vint8mf8x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei64_v_i8mf8x4_tumu(vbool64_t mask,
                                                 vint8mf8x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei64_v_i8mf8x5_tumu(vbool64_t mask,
                                                 vint8mf8x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei64_v_i8mf8x6_tumu(vbool64_t mask,
                                                 vint8mf8x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei64_v_i8mf8x7_tumu(vbool64_t mask,
                                                 vint8mf8x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei64_v_i8mf8x8_tumu(vbool64_t mask,
                                                 vint8mf8x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei64_v_i8mf4x2_tumu(vbool32_t mask,
                                                 vint8mf4x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei64_v_i8mf4x3_tumu(vbool32_t mask,
                                                 vint8mf4x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei64_v_i8mf4x4_tumu(vbool32_t mask,
                                                 vint8mf4x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei64_v_i8mf4x5_tumu(vbool32_t mask,
                                                 vint8mf4x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei64_v_i8mf4x6_tumu(vbool32_t mask,
                                                 vint8mf4x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei64_v_i8mf4x7_tumu(vbool32_t mask,
                                                 vint8mf4x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei64_v_i8mf4x8_tumu(vbool32_t mask,
                                                 vint8mf4x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei64_v_i8mf2x2_tumu(vbool16_t mask,
                                                 vint8mf2x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei64_v_i8mf2x3_tumu(vbool16_t mask,
                                                 vint8mf2x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei64_v_i8mf2x4_tumu(vbool16_t mask,
                                                 vint8mf2x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei64_v_i8mf2x5_tumu(vbool16_t mask,
                                                 vint8mf2x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei64_v_i8mf2x6_tumu(vbool16_t mask,
                                                 vint8mf2x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei64_v_i8mf2x7_tumu(vbool16_t mask,
                                                 vint8mf2x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei64_v_i8mf2x8_tumu(vbool16_t mask,
                                                 vint8mf2x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei64_v_i8m1x2_tumu(vbool8_t mask,
                                               vint8m1x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei64_v_i8m1x3_tumu(vbool8_t mask,
                                               vint8m1x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei64_v_i8m1x4_tumu(vbool8_t mask,
                                               vint8m1x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei64_v_i8m1x5_tumu(vbool8_t mask,
                                               vint8m1x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei64_v_i8m1x6_tumu(vbool8_t mask,
                                               vint8m1x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei64_v_i8m1x7_tumu(vbool8_t mask,
                                               vint8m1x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei64_v_i8m1x8_tumu(vbool8_t mask,
                                               vint8m1x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei8_v_i16mf4x2_tumu(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei8_v_i16mf4x3_tumu(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei8_v_i16mf4x4_tumu(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei8_v_i16mf4x5_tumu(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei8_v_i16mf4x6_tumu(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei8_v_i16mf4x7_tumu(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei8_v_i16mf4x8_tumu(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei8_v_i16mf2x2_tumu(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei8_v_i16mf2x3_tumu(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei8_v_i16mf2x4_tumu(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei8_v_i16mf2x5_tumu(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei8_v_i16mf2x6_tumu(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei8_v_i16mf2x7_tumu(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei8_v_i16mf2x8_tumu(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vloxseg2ei8_v_i16m1x2_tumu(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei8_v_i16m1x3_tumu(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei8_v_i16m1x4_tumu(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei8_v_i16m1x5_tumu(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei8_v_i16m1x6_tumu(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei8_v_i16m1x7_tumu(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei8_v_i16m1x8_tumu(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei8_v_i16m2x2_tumu(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei8_v_i16m2x3_tumu(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei8_v_i16m2x4_tumu(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei8_v_i16m4x2_tumu(vbool4_t mask,
                                                vint16m4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei16_v_i16mf4x2_tumu(
    vbool64_t mask, vint16mf4x2_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei16_v_i16mf4x3_tumu(
    vbool64_t mask, vint16mf4x3_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei16_v_i16mf4x4_tumu(
    vbool64_t mask, vint16mf4x4_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei16_v_i16mf4x5_tumu(
    vbool64_t mask, vint16mf4x5_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei16_v_i16mf4x6_tumu(
    vbool64_t mask, vint16mf4x6_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei16_v_i16mf4x7_tumu(
    vbool64_t mask, vint16mf4x7_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei16_v_i16mf4x8_tumu(
    vbool64_t mask, vint16mf4x8_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei16_v_i16mf2x2_tumu(
    vbool32_t mask, vint16mf2x2_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei16_v_i16mf2x3_tumu(
    vbool32_t mask, vint16mf2x3_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei16_v_i16mf2x4_tumu(
    vbool32_t mask, vint16mf2x4_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei16_v_i16mf2x5_tumu(
    vbool32_t mask, vint16mf2x5_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei16_v_i16mf2x6_tumu(
    vbool32_t mask, vint16mf2x6_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei16_v_i16mf2x7_tumu(
    vbool32_t mask, vint16mf2x7_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei16_v_i16mf2x8_tumu(
    vbool32_t mask, vint16mf2x8_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei16_v_i16m1x2_tumu(vbool16_t mask,
                                                 vint16m1x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei16_v_i16m1x3_tumu(vbool16_t mask,
                                                 vint16m1x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei16_v_i16m1x4_tumu(vbool16_t mask,
                                                 vint16m1x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei16_v_i16m1x5_tumu(vbool16_t mask,
                                                 vint16m1x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei16_v_i16m1x6_tumu(vbool16_t mask,
                                                 vint16m1x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei16_v_i16m1x7_tumu(vbool16_t mask,
                                                 vint16m1x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei16_v_i16m1x8_tumu(vbool16_t mask,
                                                 vint16m1x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei16_v_i16m2x2_tumu(vbool8_t mask,
                                                 vint16m2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei16_v_i16m2x3_tumu(vbool8_t mask,
                                                 vint16m2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei16_v_i16m2x4_tumu(vbool8_t mask,
                                                 vint16m2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei16_v_i16m4x2_tumu(vbool4_t mask,
                                                 vint16m4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei32_v_i16mf4x2_tumu(
    vbool64_t mask, vint16mf4x2_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei32_v_i16mf4x3_tumu(
    vbool64_t mask, vint16mf4x3_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei32_v_i16mf4x4_tumu(
    vbool64_t mask, vint16mf4x4_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei32_v_i16mf4x5_tumu(
    vbool64_t mask, vint16mf4x5_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei32_v_i16mf4x6_tumu(
    vbool64_t mask, vint16mf4x6_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei32_v_i16mf4x7_tumu(
    vbool64_t mask, vint16mf4x7_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei32_v_i16mf4x8_tumu(
    vbool64_t mask, vint16mf4x8_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei32_v_i16mf2x2_tumu(
    vbool32_t mask, vint16mf2x2_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei32_v_i16mf2x3_tumu(
    vbool32_t mask, vint16mf2x3_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei32_v_i16mf2x4_tumu(
    vbool32_t mask, vint16mf2x4_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei32_v_i16mf2x5_tumu(
    vbool32_t mask, vint16mf2x5_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei32_v_i16mf2x6_tumu(
    vbool32_t mask, vint16mf2x6_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei32_v_i16mf2x7_tumu(
    vbool32_t mask, vint16mf2x7_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei32_v_i16mf2x8_tumu(
    vbool32_t mask, vint16mf2x8_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei32_v_i16m1x2_tumu(vbool16_t mask,
                                                 vint16m1x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei32_v_i16m1x3_tumu(vbool16_t mask,
                                                 vint16m1x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei32_v_i16m1x4_tumu(vbool16_t mask,
                                                 vint16m1x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei32_v_i16m1x5_tumu(vbool16_t mask,
                                                 vint16m1x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei32_v_i16m1x6_tumu(vbool16_t mask,
                                                 vint16m1x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei32_v_i16m1x7_tumu(vbool16_t mask,
                                                 vint16m1x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei32_v_i16m1x8_tumu(vbool16_t mask,
                                                 vint16m1x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei32_v_i16m2x2_tumu(vbool8_t mask,
                                                 vint16m2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei32_v_i16m2x3_tumu(vbool8_t mask,
                                                 vint16m2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei32_v_i16m2x4_tumu(vbool8_t mask,
                                                 vint16m2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei32_v_i16m4x2_tumu(vbool4_t mask,
                                                 vint16m4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei64_v_i16mf4x2_tumu(
    vbool64_t mask, vint16mf4x2_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei64_v_i16mf4x3_tumu(
    vbool64_t mask, vint16mf4x3_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei64_v_i16mf4x4_tumu(
    vbool64_t mask, vint16mf4x4_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei64_v_i16mf4x5_tumu(
    vbool64_t mask, vint16mf4x5_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei64_v_i16mf4x6_tumu(
    vbool64_t mask, vint16mf4x6_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei64_v_i16mf4x7_tumu(
    vbool64_t mask, vint16mf4x7_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei64_v_i16mf4x8_tumu(
    vbool64_t mask, vint16mf4x8_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei64_v_i16mf2x2_tumu(
    vbool32_t mask, vint16mf2x2_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei64_v_i16mf2x3_tumu(
    vbool32_t mask, vint16mf2x3_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei64_v_i16mf2x4_tumu(
    vbool32_t mask, vint16mf2x4_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei64_v_i16mf2x5_tumu(
    vbool32_t mask, vint16mf2x5_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei64_v_i16mf2x6_tumu(
    vbool32_t mask, vint16mf2x6_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei64_v_i16mf2x7_tumu(
    vbool32_t mask, vint16mf2x7_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei64_v_i16mf2x8_tumu(
    vbool32_t mask, vint16mf2x8_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei64_v_i16m1x2_tumu(vbool16_t mask,
                                                 vint16m1x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei64_v_i16m1x3_tumu(vbool16_t mask,
                                                 vint16m1x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei64_v_i16m1x4_tumu(vbool16_t mask,
                                                 vint16m1x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei64_v_i16m1x5_tumu(vbool16_t mask,
                                                 vint16m1x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei64_v_i16m1x6_tumu(vbool16_t mask,
                                                 vint16m1x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei64_v_i16m1x7_tumu(vbool16_t mask,
                                                 vint16m1x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei64_v_i16m1x8_tumu(vbool16_t mask,
                                                 vint16m1x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei64_v_i16m2x2_tumu(vbool8_t mask,
                                                 vint16m2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei64_v_i16m2x3_tumu(vbool8_t mask,
                                                 vint16m2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei64_v_i16m2x4_tumu(vbool8_t mask,
                                                 vint16m2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei8_v_i32mf2x2_tumu(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei8_v_i32mf2x3_tumu(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei8_v_i32mf2x4_tumu(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei8_v_i32mf2x5_tumu(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei8_v_i32mf2x6_tumu(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei8_v_i32mf2x7_tumu(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei8_v_i32mf2x8_tumu(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vloxseg2ei8_v_i32m1x2_tumu(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei8_v_i32m1x3_tumu(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei8_v_i32m1x4_tumu(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei8_v_i32m1x5_tumu(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei8_v_i32m1x6_tumu(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei8_v_i32m1x7_tumu(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei8_v_i32m1x8_tumu(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei8_v_i32m2x2_tumu(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei8_v_i32m2x3_tumu(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei8_v_i32m2x4_tumu(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei8_v_i32m4x2_tumu(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei16_v_i32mf2x2_tumu(
    vbool64_t mask, vint32mf2x2_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei16_v_i32mf2x3_tumu(
    vbool64_t mask, vint32mf2x3_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei16_v_i32mf2x4_tumu(
    vbool64_t mask, vint32mf2x4_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei16_v_i32mf2x5_tumu(
    vbool64_t mask, vint32mf2x5_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei16_v_i32mf2x6_tumu(
    vbool64_t mask, vint32mf2x6_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei16_v_i32mf2x7_tumu(
    vbool64_t mask, vint32mf2x7_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei16_v_i32mf2x8_tumu(
    vbool64_t mask, vint32mf2x8_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei16_v_i32m1x2_tumu(vbool32_t mask,
                                                 vint32m1x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x3_t __riscv_vloxseg3ei16_v_i32m1x3_tumu(vbool32_t mask,
                                                 vint32m1x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x4_t __riscv_vloxseg4ei16_v_i32m1x4_tumu(vbool32_t mask,
                                                 vint32m1x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x5_t __riscv_vloxseg5ei16_v_i32m1x5_tumu(vbool32_t mask,
                                                 vint32m1x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x6_t __riscv_vloxseg6ei16_v_i32m1x6_tumu(vbool32_t mask,
                                                 vint32m1x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x7_t __riscv_vloxseg7ei16_v_i32m1x7_tumu(vbool32_t mask,
                                                 vint32m1x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x8_t __riscv_vloxseg8ei16_v_i32m1x8_tumu(vbool32_t mask,
                                                 vint32m1x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m2x2_t __riscv_vloxseg2ei16_v_i32m2x2_tumu(vbool16_t mask,
                                                 vint32m2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei16_v_i32m2x3_tumu(vbool16_t mask,
                                                 vint32m2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei16_v_i32m2x4_tumu(vbool16_t mask,
                                                 vint32m2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei16_v_i32m4x2_tumu(vbool8_t mask,
                                                 vint32m4x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei32_v_i32mf2x2_tumu(
    vbool64_t mask, vint32mf2x2_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei32_v_i32mf2x3_tumu(
    vbool64_t mask, vint32mf2x3_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei32_v_i32mf2x4_tumu(
    vbool64_t mask, vint32mf2x4_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei32_v_i32mf2x5_tumu(
    vbool64_t mask, vint32mf2x5_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei32_v_i32mf2x6_tumu(
    vbool64_t mask, vint32mf2x6_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei32_v_i32mf2x7_tumu(
    vbool64_t mask, vint32mf2x7_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei32_v_i32mf2x8_tumu(
    vbool64_t mask, vint32mf2x8_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei32_v_i32m1x2_tumu(vbool32_t mask,
                                                 vint32m1x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei32_v_i32m1x3_tumu(vbool32_t mask,
                                                 vint32m1x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei32_v_i32m1x4_tumu(vbool32_t mask,
                                                 vint32m1x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei32_v_i32m1x5_tumu(vbool32_t mask,
                                                 vint32m1x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei32_v_i32m1x6_tumu(vbool32_t mask,
                                                 vint32m1x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei32_v_i32m1x7_tumu(vbool32_t mask,
                                                 vint32m1x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei32_v_i32m1x8_tumu(vbool32_t mask,
                                                 vint32m1x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei32_v_i32m2x2_tumu(vbool16_t mask,
                                                 vint32m2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei32_v_i32m2x3_tumu(vbool16_t mask,
                                                 vint32m2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei32_v_i32m2x4_tumu(vbool16_t mask,
                                                 vint32m2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei32_v_i32m4x2_tumu(vbool8_t mask,
                                                 vint32m4x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei64_v_i32mf2x2_tumu(
    vbool64_t mask, vint32mf2x2_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei64_v_i32mf2x3_tumu(
    vbool64_t mask, vint32mf2x3_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei64_v_i32mf2x4_tumu(
    vbool64_t mask, vint32mf2x4_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei64_v_i32mf2x5_tumu(
    vbool64_t mask, vint32mf2x5_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei64_v_i32mf2x6_tumu(
    vbool64_t mask, vint32mf2x6_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei64_v_i32mf2x7_tumu(
    vbool64_t mask, vint32mf2x7_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei64_v_i32mf2x8_tumu(
    vbool64_t mask, vint32mf2x8_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei64_v_i32m1x2_tumu(vbool32_t mask,
                                                 vint32m1x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei64_v_i32m1x3_tumu(vbool32_t mask,
                                                 vint32m1x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei64_v_i32m1x4_tumu(vbool32_t mask,
                                                 vint32m1x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei64_v_i32m1x5_tumu(vbool32_t mask,
                                                 vint32m1x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei64_v_i32m1x6_tumu(vbool32_t mask,
                                                 vint32m1x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei64_v_i32m1x7_tumu(vbool32_t mask,
                                                 vint32m1x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei64_v_i32m1x8_tumu(vbool32_t mask,
                                                 vint32m1x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei64_v_i32m2x2_tumu(vbool16_t mask,
                                                 vint32m2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei64_v_i32m2x3_tumu(vbool16_t mask,
                                                 vint32m2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei64_v_i32m2x4_tumu(vbool16_t mask,
                                                 vint32m2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei64_v_i32m4x2_tumu(vbool8_t mask,
                                                 vint32m4x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei8_v_i64m1x2_tumu(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei8_v_i64m1x3_tumu(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei8_v_i64m1x4_tumu(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei8_v_i64m1x5_tumu(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei8_v_i64m1x6_tumu(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei8_v_i64m1x7_tumu(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei8_v_i64m1x8_tumu(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei8_v_i64m2x2_tumu(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei8_v_i64m2x3_tumu(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei8_v_i64m2x4_tumu(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei8_v_i64m4x2_tumu(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei16_v_i64m1x2_tumu(vbool64_t mask,
                                                 vint64m1x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x3_t __riscv_vloxseg3ei16_v_i64m1x3_tumu(vbool64_t mask,
                                                 vint64m1x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x4_t __riscv_vloxseg4ei16_v_i64m1x4_tumu(vbool64_t mask,
                                                 vint64m1x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x5_t __riscv_vloxseg5ei16_v_i64m1x5_tumu(vbool64_t mask,
                                                 vint64m1x5_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x6_t __riscv_vloxseg6ei16_v_i64m1x6_tumu(vbool64_t mask,
                                                 vint64m1x6_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x7_t __riscv_vloxseg7ei16_v_i64m1x7_tumu(vbool64_t mask,
                                                 vint64m1x7_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x8_t __riscv_vloxseg8ei16_v_i64m1x8_tumu(vbool64_t mask,
                                                 vint64m1x8_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m2x2_t __riscv_vloxseg2ei16_v_i64m2x2_tumu(vbool32_t mask,
                                                 vint64m2x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint64m2x3_t __riscv_vloxseg3ei16_v_i64m2x3_tumu(vbool32_t mask,
                                                 vint64m2x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint64m2x4_t __riscv_vloxseg4ei16_v_i64m2x4_tumu(vbool32_t mask,
                                                 vint64m2x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint64m4x2_t __riscv_vloxseg2ei16_v_i64m4x2_tumu(vbool16_t mask,
                                                 vint64m4x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei32_v_i64m1x2_tumu(vbool64_t mask,
                                                 vint64m1x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x3_t __riscv_vloxseg3ei32_v_i64m1x3_tumu(vbool64_t mask,
                                                 vint64m1x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x4_t __riscv_vloxseg4ei32_v_i64m1x4_tumu(vbool64_t mask,
                                                 vint64m1x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x5_t __riscv_vloxseg5ei32_v_i64m1x5_tumu(vbool64_t mask,
                                                 vint64m1x5_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x6_t __riscv_vloxseg6ei32_v_i64m1x6_tumu(vbool64_t mask,
                                                 vint64m1x6_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x7_t __riscv_vloxseg7ei32_v_i64m1x7_tumu(vbool64_t mask,
                                                 vint64m1x7_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x8_t __riscv_vloxseg8ei32_v_i64m1x8_tumu(vbool64_t mask,
                                                 vint64m1x8_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m2x2_t __riscv_vloxseg2ei32_v_i64m2x2_tumu(vbool32_t mask,
                                                 vint64m2x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei32_v_i64m2x3_tumu(vbool32_t mask,
                                                 vint64m2x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei32_v_i64m2x4_tumu(vbool32_t mask,
                                                 vint64m2x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei32_v_i64m4x2_tumu(vbool16_t mask,
                                                 vint64m4x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei64_v_i64m1x2_tumu(vbool64_t mask,
                                                 vint64m1x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei64_v_i64m1x3_tumu(vbool64_t mask,
                                                 vint64m1x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei64_v_i64m1x4_tumu(vbool64_t mask,
                                                 vint64m1x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei64_v_i64m1x5_tumu(vbool64_t mask,
                                                 vint64m1x5_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei64_v_i64m1x6_tumu(vbool64_t mask,
                                                 vint64m1x6_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei64_v_i64m1x7_tumu(vbool64_t mask,
                                                 vint64m1x7_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei64_v_i64m1x8_tumu(vbool64_t mask,
                                                 vint64m1x8_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei64_v_i64m2x2_tumu(vbool32_t mask,
                                                 vint64m2x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei64_v_i64m2x3_tumu(vbool32_t mask,
                                                 vint64m2x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei64_v_i64m2x4_tumu(vbool32_t mask,
                                                 vint64m2x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei64_v_i64m4x2_tumu(vbool16_t mask,
                                                 vint64m4x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei8_v_i8mf8x2_tumu(vbool64_t mask,
                                                vint8mf8x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei8_v_i8mf8x3_tumu(vbool64_t mask,
                                                vint8mf8x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei8_v_i8mf8x4_tumu(vbool64_t mask,
                                                vint8mf8x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei8_v_i8mf8x5_tumu(vbool64_t mask,
                                                vint8mf8x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei8_v_i8mf8x6_tumu(vbool64_t mask,
                                                vint8mf8x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei8_v_i8mf8x7_tumu(vbool64_t mask,
                                                vint8mf8x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei8_v_i8mf8x8_tumu(vbool64_t mask,
                                                vint8mf8x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei8_v_i8mf4x2_tumu(vbool32_t mask,
                                                vint8mf4x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei8_v_i8mf4x3_tumu(vbool32_t mask,
                                                vint8mf4x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei8_v_i8mf4x4_tumu(vbool32_t mask,
                                                vint8mf4x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei8_v_i8mf4x5_tumu(vbool32_t mask,
                                                vint8mf4x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei8_v_i8mf4x6_tumu(vbool32_t mask,
                                                vint8mf4x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei8_v_i8mf4x7_tumu(vbool32_t mask,
                                                vint8mf4x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei8_v_i8mf4x8_tumu(vbool32_t mask,
                                                vint8mf4x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei8_v_i8mf2x2_tumu(vbool16_t mask,
                                                vint8mf2x2_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei8_v_i8mf2x3_tumu(vbool16_t mask,
                                                vint8mf2x3_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei8_v_i8mf2x4_tumu(vbool16_t mask,
                                                vint8mf2x4_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei8_v_i8mf2x5_tumu(vbool16_t mask,
                                                vint8mf2x5_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei8_v_i8mf2x6_tumu(vbool16_t mask,
                                                vint8mf2x6_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei8_v_i8mf2x7_tumu(vbool16_t mask,
                                                vint8mf2x7_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei8_v_i8mf2x8_tumu(vbool16_t mask,
                                                vint8mf2x8_t maskedoff_tuple,
                                                const int8_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei8_v_i8m1x2_tumu(vbool8_t mask,
                                              vint8m1x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei8_v_i8m1x3_tumu(vbool8_t mask,
                                              vint8m1x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei8_v_i8m1x4_tumu(vbool8_t mask,
                                              vint8m1x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei8_v_i8m1x5_tumu(vbool8_t mask,
                                              vint8m1x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei8_v_i8m1x6_tumu(vbool8_t mask,
                                              vint8m1x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei8_v_i8m1x7_tumu(vbool8_t mask,
                                              vint8m1x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei8_v_i8m1x8_tumu(vbool8_t mask,
                                              vint8m1x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei8_v_i8m2x2_tumu(vbool4_t mask,
                                              vint8m2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei8_v_i8m2x3_tumu(vbool4_t mask,
                                              vint8m2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei8_v_i8m2x4_tumu(vbool4_t mask,
                                              vint8m2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei8_v_i8m4x2_tumu(vbool2_t mask,
                                              vint8m4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei16_v_i8mf8x2_tumu(vbool64_t mask,
                                                 vint8mf8x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei16_v_i8mf8x3_tumu(vbool64_t mask,
                                                 vint8mf8x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei16_v_i8mf8x4_tumu(vbool64_t mask,
                                                 vint8mf8x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei16_v_i8mf8x5_tumu(vbool64_t mask,
                                                 vint8mf8x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei16_v_i8mf8x6_tumu(vbool64_t mask,
                                                 vint8mf8x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei16_v_i8mf8x7_tumu(vbool64_t mask,
                                                 vint8mf8x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei16_v_i8mf8x8_tumu(vbool64_t mask,
                                                 vint8mf8x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei16_v_i8mf4x2_tumu(vbool32_t mask,
                                                 vint8mf4x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei16_v_i8mf4x3_tumu(vbool32_t mask,
                                                 vint8mf4x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei16_v_i8mf4x4_tumu(vbool32_t mask,
                                                 vint8mf4x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei16_v_i8mf4x5_tumu(vbool32_t mask,
                                                 vint8mf4x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei16_v_i8mf4x6_tumu(vbool32_t mask,
                                                 vint8mf4x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei16_v_i8mf4x7_tumu(vbool32_t mask,
                                                 vint8mf4x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei16_v_i8mf4x8_tumu(vbool32_t mask,
                                                 vint8mf4x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei16_v_i8mf2x2_tumu(vbool16_t mask,
                                                 vint8mf2x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei16_v_i8mf2x3_tumu(vbool16_t mask,
                                                 vint8mf2x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei16_v_i8mf2x4_tumu(vbool16_t mask,
                                                 vint8mf2x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei16_v_i8mf2x5_tumu(vbool16_t mask,
                                                 vint8mf2x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei16_v_i8mf2x6_tumu(vbool16_t mask,
                                                 vint8mf2x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei16_v_i8mf2x7_tumu(vbool16_t mask,
                                                 vint8mf2x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei16_v_i8mf2x8_tumu(vbool16_t mask,
                                                 vint8mf2x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei16_v_i8m1x2_tumu(vbool8_t mask,
                                               vint8m1x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei16_v_i8m1x3_tumu(vbool8_t mask,
                                               vint8m1x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei16_v_i8m1x4_tumu(vbool8_t mask,
                                               vint8m1x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei16_v_i8m1x5_tumu(vbool8_t mask,
                                               vint8m1x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei16_v_i8m1x6_tumu(vbool8_t mask,
                                               vint8m1x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei16_v_i8m1x7_tumu(vbool8_t mask,
                                               vint8m1x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei16_v_i8m1x8_tumu(vbool8_t mask,
                                               vint8m1x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei16_v_i8m2x2_tumu(vbool4_t mask,
                                               vint8m2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei16_v_i8m2x3_tumu(vbool4_t mask,
                                               vint8m2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei16_v_i8m2x4_tumu(vbool4_t mask,
                                               vint8m2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei16_v_i8m4x2_tumu(vbool2_t mask,
                                               vint8m4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei32_v_i8mf8x2_tumu(vbool64_t mask,
                                                 vint8mf8x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei32_v_i8mf8x3_tumu(vbool64_t mask,
                                                 vint8mf8x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei32_v_i8mf8x4_tumu(vbool64_t mask,
                                                 vint8mf8x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei32_v_i8mf8x5_tumu(vbool64_t mask,
                                                 vint8mf8x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei32_v_i8mf8x6_tumu(vbool64_t mask,
                                                 vint8mf8x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei32_v_i8mf8x7_tumu(vbool64_t mask,
                                                 vint8mf8x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei32_v_i8mf8x8_tumu(vbool64_t mask,
                                                 vint8mf8x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei32_v_i8mf4x2_tumu(vbool32_t mask,
                                                 vint8mf4x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei32_v_i8mf4x3_tumu(vbool32_t mask,
                                                 vint8mf4x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei32_v_i8mf4x4_tumu(vbool32_t mask,
                                                 vint8mf4x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei32_v_i8mf4x5_tumu(vbool32_t mask,
                                                 vint8mf4x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei32_v_i8mf4x6_tumu(vbool32_t mask,
                                                 vint8mf4x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei32_v_i8mf4x7_tumu(vbool32_t mask,
                                                 vint8mf4x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei32_v_i8mf4x8_tumu(vbool32_t mask,
                                                 vint8mf4x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei32_v_i8mf2x2_tumu(vbool16_t mask,
                                                 vint8mf2x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei32_v_i8mf2x3_tumu(vbool16_t mask,
                                                 vint8mf2x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei32_v_i8mf2x4_tumu(vbool16_t mask,
                                                 vint8mf2x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei32_v_i8mf2x5_tumu(vbool16_t mask,
                                                 vint8mf2x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei32_v_i8mf2x6_tumu(vbool16_t mask,
                                                 vint8mf2x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei32_v_i8mf2x7_tumu(vbool16_t mask,
                                                 vint8mf2x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei32_v_i8mf2x8_tumu(vbool16_t mask,
                                                 vint8mf2x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei32_v_i8m1x2_tumu(vbool8_t mask,
                                               vint8m1x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei32_v_i8m1x3_tumu(vbool8_t mask,
                                               vint8m1x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei32_v_i8m1x4_tumu(vbool8_t mask,
                                               vint8m1x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei32_v_i8m1x5_tumu(vbool8_t mask,
                                               vint8m1x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei32_v_i8m1x6_tumu(vbool8_t mask,
                                               vint8m1x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei32_v_i8m1x7_tumu(vbool8_t mask,
                                               vint8m1x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei32_v_i8m1x8_tumu(vbool8_t mask,
                                               vint8m1x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei32_v_i8m2x2_tumu(vbool4_t mask,
                                               vint8m2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei32_v_i8m2x3_tumu(vbool4_t mask,
                                               vint8m2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei32_v_i8m2x4_tumu(vbool4_t mask,
                                               vint8m2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei64_v_i8mf8x2_tumu(vbool64_t mask,
                                                 vint8mf8x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei64_v_i8mf8x3_tumu(vbool64_t mask,
                                                 vint8mf8x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei64_v_i8mf8x4_tumu(vbool64_t mask,
                                                 vint8mf8x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei64_v_i8mf8x5_tumu(vbool64_t mask,
                                                 vint8mf8x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei64_v_i8mf8x6_tumu(vbool64_t mask,
                                                 vint8mf8x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei64_v_i8mf8x7_tumu(vbool64_t mask,
                                                 vint8mf8x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei64_v_i8mf8x8_tumu(vbool64_t mask,
                                                 vint8mf8x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei64_v_i8mf4x2_tumu(vbool32_t mask,
                                                 vint8mf4x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei64_v_i8mf4x3_tumu(vbool32_t mask,
                                                 vint8mf4x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei64_v_i8mf4x4_tumu(vbool32_t mask,
                                                 vint8mf4x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei64_v_i8mf4x5_tumu(vbool32_t mask,
                                                 vint8mf4x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei64_v_i8mf4x6_tumu(vbool32_t mask,
                                                 vint8mf4x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei64_v_i8mf4x7_tumu(vbool32_t mask,
                                                 vint8mf4x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei64_v_i8mf4x8_tumu(vbool32_t mask,
                                                 vint8mf4x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei64_v_i8mf2x2_tumu(vbool16_t mask,
                                                 vint8mf2x2_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei64_v_i8mf2x3_tumu(vbool16_t mask,
                                                 vint8mf2x3_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei64_v_i8mf2x4_tumu(vbool16_t mask,
                                                 vint8mf2x4_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei64_v_i8mf2x5_tumu(vbool16_t mask,
                                                 vint8mf2x5_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei64_v_i8mf2x6_tumu(vbool16_t mask,
                                                 vint8mf2x6_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei64_v_i8mf2x7_tumu(vbool16_t mask,
                                                 vint8mf2x7_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei64_v_i8mf2x8_tumu(vbool16_t mask,
                                                 vint8mf2x8_t maskedoff_tuple,
                                                 const int8_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei64_v_i8m1x2_tumu(vbool8_t mask,
                                               vint8m1x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei64_v_i8m1x3_tumu(vbool8_t mask,
                                               vint8m1x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei64_v_i8m1x4_tumu(vbool8_t mask,
                                               vint8m1x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei64_v_i8m1x5_tumu(vbool8_t mask,
                                               vint8m1x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei64_v_i8m1x6_tumu(vbool8_t mask,
                                               vint8m1x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei64_v_i8m1x7_tumu(vbool8_t mask,
                                               vint8m1x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei64_v_i8m1x8_tumu(vbool8_t mask,
                                               vint8m1x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei8_v_i16mf4x2_tumu(vbool64_t mask,
                                                  vint16mf4x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei8_v_i16mf4x3_tumu(vbool64_t mask,
                                                  vint16mf4x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei8_v_i16mf4x4_tumu(vbool64_t mask,
                                                  vint16mf4x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei8_v_i16mf4x5_tumu(vbool64_t mask,
                                                  vint16mf4x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei8_v_i16mf4x6_tumu(vbool64_t mask,
                                                  vint16mf4x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei8_v_i16mf4x7_tumu(vbool64_t mask,
                                                  vint16mf4x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei8_v_i16mf4x8_tumu(vbool64_t mask,
                                                  vint16mf4x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei8_v_i16mf2x2_tumu(vbool32_t mask,
                                                  vint16mf2x2_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei8_v_i16mf2x3_tumu(vbool32_t mask,
                                                  vint16mf2x3_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei8_v_i16mf2x4_tumu(vbool32_t mask,
                                                  vint16mf2x4_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei8_v_i16mf2x5_tumu(vbool32_t mask,
                                                  vint16mf2x5_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei8_v_i16mf2x6_tumu(vbool32_t mask,
                                                  vint16mf2x6_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei8_v_i16mf2x7_tumu(vbool32_t mask,
                                                  vint16mf2x7_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei8_v_i16mf2x8_tumu(vbool32_t mask,
                                                  vint16mf2x8_t maskedoff_tuple,
                                                  const int16_t *base,
                                                  vuint8mf4_t bindex,
                                                  size_t vl);
vint16m1x2_t __riscv_vluxseg2ei8_v_i16m1x2_tumu(vbool16_t mask,
                                                vint16m1x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei8_v_i16m1x3_tumu(vbool16_t mask,
                                                vint16m1x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei8_v_i16m1x4_tumu(vbool16_t mask,
                                                vint16m1x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei8_v_i16m1x5_tumu(vbool16_t mask,
                                                vint16m1x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei8_v_i16m1x6_tumu(vbool16_t mask,
                                                vint16m1x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei8_v_i16m1x7_tumu(vbool16_t mask,
                                                vint16m1x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei8_v_i16m1x8_tumu(vbool16_t mask,
                                                vint16m1x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei8_v_i16m2x2_tumu(vbool8_t mask,
                                                vint16m2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei8_v_i16m2x3_tumu(vbool8_t mask,
                                                vint16m2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei8_v_i16m2x4_tumu(vbool8_t mask,
                                                vint16m2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei8_v_i16m4x2_tumu(vbool4_t mask,
                                                vint16m4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei16_v_i16mf4x2_tumu(
    vbool64_t mask, vint16mf4x2_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei16_v_i16mf4x3_tumu(
    vbool64_t mask, vint16mf4x3_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei16_v_i16mf4x4_tumu(
    vbool64_t mask, vint16mf4x4_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei16_v_i16mf4x5_tumu(
    vbool64_t mask, vint16mf4x5_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei16_v_i16mf4x6_tumu(
    vbool64_t mask, vint16mf4x6_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei16_v_i16mf4x7_tumu(
    vbool64_t mask, vint16mf4x7_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei16_v_i16mf4x8_tumu(
    vbool64_t mask, vint16mf4x8_t maskedoff_tuple, const int16_t *base,
    vuint16mf4_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei16_v_i16mf2x2_tumu(
    vbool32_t mask, vint16mf2x2_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei16_v_i16mf2x3_tumu(
    vbool32_t mask, vint16mf2x3_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei16_v_i16mf2x4_tumu(
    vbool32_t mask, vint16mf2x4_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei16_v_i16mf2x5_tumu(
    vbool32_t mask, vint16mf2x5_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei16_v_i16mf2x6_tumu(
    vbool32_t mask, vint16mf2x6_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei16_v_i16mf2x7_tumu(
    vbool32_t mask, vint16mf2x7_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei16_v_i16mf2x8_tumu(
    vbool32_t mask, vint16mf2x8_t maskedoff_tuple, const int16_t *base,
    vuint16mf2_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei16_v_i16m1x2_tumu(vbool16_t mask,
                                                 vint16m1x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei16_v_i16m1x3_tumu(vbool16_t mask,
                                                 vint16m1x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei16_v_i16m1x4_tumu(vbool16_t mask,
                                                 vint16m1x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei16_v_i16m1x5_tumu(vbool16_t mask,
                                                 vint16m1x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei16_v_i16m1x6_tumu(vbool16_t mask,
                                                 vint16m1x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei16_v_i16m1x7_tumu(vbool16_t mask,
                                                 vint16m1x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei16_v_i16m1x8_tumu(vbool16_t mask,
                                                 vint16m1x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei16_v_i16m2x2_tumu(vbool8_t mask,
                                                 vint16m2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei16_v_i16m2x3_tumu(vbool8_t mask,
                                                 vint16m2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei16_v_i16m2x4_tumu(vbool8_t mask,
                                                 vint16m2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei16_v_i16m4x2_tumu(vbool4_t mask,
                                                 vint16m4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei32_v_i16mf4x2_tumu(
    vbool64_t mask, vint16mf4x2_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei32_v_i16mf4x3_tumu(
    vbool64_t mask, vint16mf4x3_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei32_v_i16mf4x4_tumu(
    vbool64_t mask, vint16mf4x4_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei32_v_i16mf4x5_tumu(
    vbool64_t mask, vint16mf4x5_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei32_v_i16mf4x6_tumu(
    vbool64_t mask, vint16mf4x6_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei32_v_i16mf4x7_tumu(
    vbool64_t mask, vint16mf4x7_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei32_v_i16mf4x8_tumu(
    vbool64_t mask, vint16mf4x8_t maskedoff_tuple, const int16_t *base,
    vuint32mf2_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei32_v_i16mf2x2_tumu(
    vbool32_t mask, vint16mf2x2_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei32_v_i16mf2x3_tumu(
    vbool32_t mask, vint16mf2x3_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei32_v_i16mf2x4_tumu(
    vbool32_t mask, vint16mf2x4_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei32_v_i16mf2x5_tumu(
    vbool32_t mask, vint16mf2x5_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei32_v_i16mf2x6_tumu(
    vbool32_t mask, vint16mf2x6_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei32_v_i16mf2x7_tumu(
    vbool32_t mask, vint16mf2x7_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei32_v_i16mf2x8_tumu(
    vbool32_t mask, vint16mf2x8_t maskedoff_tuple, const int16_t *base,
    vuint32m1_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei32_v_i16m1x2_tumu(vbool16_t mask,
                                                 vint16m1x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei32_v_i16m1x3_tumu(vbool16_t mask,
                                                 vint16m1x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei32_v_i16m1x4_tumu(vbool16_t mask,
                                                 vint16m1x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei32_v_i16m1x5_tumu(vbool16_t mask,
                                                 vint16m1x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei32_v_i16m1x6_tumu(vbool16_t mask,
                                                 vint16m1x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei32_v_i16m1x7_tumu(vbool16_t mask,
                                                 vint16m1x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei32_v_i16m1x8_tumu(vbool16_t mask,
                                                 vint16m1x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei32_v_i16m2x2_tumu(vbool8_t mask,
                                                 vint16m2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei32_v_i16m2x3_tumu(vbool8_t mask,
                                                 vint16m2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei32_v_i16m2x4_tumu(vbool8_t mask,
                                                 vint16m2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei32_v_i16m4x2_tumu(vbool4_t mask,
                                                 vint16m4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei64_v_i16mf4x2_tumu(
    vbool64_t mask, vint16mf4x2_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei64_v_i16mf4x3_tumu(
    vbool64_t mask, vint16mf4x3_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei64_v_i16mf4x4_tumu(
    vbool64_t mask, vint16mf4x4_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei64_v_i16mf4x5_tumu(
    vbool64_t mask, vint16mf4x5_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei64_v_i16mf4x6_tumu(
    vbool64_t mask, vint16mf4x6_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei64_v_i16mf4x7_tumu(
    vbool64_t mask, vint16mf4x7_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei64_v_i16mf4x8_tumu(
    vbool64_t mask, vint16mf4x8_t maskedoff_tuple, const int16_t *base,
    vuint64m1_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei64_v_i16mf2x2_tumu(
    vbool32_t mask, vint16mf2x2_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei64_v_i16mf2x3_tumu(
    vbool32_t mask, vint16mf2x3_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei64_v_i16mf2x4_tumu(
    vbool32_t mask, vint16mf2x4_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei64_v_i16mf2x5_tumu(
    vbool32_t mask, vint16mf2x5_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei64_v_i16mf2x6_tumu(
    vbool32_t mask, vint16mf2x6_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei64_v_i16mf2x7_tumu(
    vbool32_t mask, vint16mf2x7_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei64_v_i16mf2x8_tumu(
    vbool32_t mask, vint16mf2x8_t maskedoff_tuple, const int16_t *base,
    vuint64m2_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei64_v_i16m1x2_tumu(vbool16_t mask,
                                                 vint16m1x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei64_v_i16m1x3_tumu(vbool16_t mask,
                                                 vint16m1x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei64_v_i16m1x4_tumu(vbool16_t mask,
                                                 vint16m1x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei64_v_i16m1x5_tumu(vbool16_t mask,
                                                 vint16m1x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei64_v_i16m1x6_tumu(vbool16_t mask,
                                                 vint16m1x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei64_v_i16m1x7_tumu(vbool16_t mask,
                                                 vint16m1x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei64_v_i16m1x8_tumu(vbool16_t mask,
                                                 vint16m1x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei64_v_i16m2x2_tumu(vbool8_t mask,
                                                 vint16m2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei64_v_i16m2x3_tumu(vbool8_t mask,
                                                 vint16m2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei64_v_i16m2x4_tumu(vbool8_t mask,
                                                 vint16m2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei8_v_i32mf2x2_tumu(vbool64_t mask,
                                                  vint32mf2x2_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei8_v_i32mf2x3_tumu(vbool64_t mask,
                                                  vint32mf2x3_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei8_v_i32mf2x4_tumu(vbool64_t mask,
                                                  vint32mf2x4_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei8_v_i32mf2x5_tumu(vbool64_t mask,
                                                  vint32mf2x5_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei8_v_i32mf2x6_tumu(vbool64_t mask,
                                                  vint32mf2x6_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei8_v_i32mf2x7_tumu(vbool64_t mask,
                                                  vint32mf2x7_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei8_v_i32mf2x8_tumu(vbool64_t mask,
                                                  vint32mf2x8_t maskedoff_tuple,
                                                  const int32_t *base,
                                                  vuint8mf8_t bindex,
                                                  size_t vl);
vint32m1x2_t __riscv_vluxseg2ei8_v_i32m1x2_tumu(vbool32_t mask,
                                                vint32m1x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei8_v_i32m1x3_tumu(vbool32_t mask,
                                                vint32m1x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei8_v_i32m1x4_tumu(vbool32_t mask,
                                                vint32m1x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei8_v_i32m1x5_tumu(vbool32_t mask,
                                                vint32m1x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei8_v_i32m1x6_tumu(vbool32_t mask,
                                                vint32m1x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei8_v_i32m1x7_tumu(vbool32_t mask,
                                                vint32m1x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei8_v_i32m1x8_tumu(vbool32_t mask,
                                                vint32m1x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei8_v_i32m2x2_tumu(vbool16_t mask,
                                                vint32m2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei8_v_i32m2x3_tumu(vbool16_t mask,
                                                vint32m2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei8_v_i32m2x4_tumu(vbool16_t mask,
                                                vint32m2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei8_v_i32m4x2_tumu(vbool8_t mask,
                                                vint32m4x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei16_v_i32mf2x2_tumu(
    vbool64_t mask, vint32mf2x2_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei16_v_i32mf2x3_tumu(
    vbool64_t mask, vint32mf2x3_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei16_v_i32mf2x4_tumu(
    vbool64_t mask, vint32mf2x4_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei16_v_i32mf2x5_tumu(
    vbool64_t mask, vint32mf2x5_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei16_v_i32mf2x6_tumu(
    vbool64_t mask, vint32mf2x6_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei16_v_i32mf2x7_tumu(
    vbool64_t mask, vint32mf2x7_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei16_v_i32mf2x8_tumu(
    vbool64_t mask, vint32mf2x8_t maskedoff_tuple, const int32_t *base,
    vuint16mf4_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei16_v_i32m1x2_tumu(vbool32_t mask,
                                                 vint32m1x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x3_t __riscv_vluxseg3ei16_v_i32m1x3_tumu(vbool32_t mask,
                                                 vint32m1x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x4_t __riscv_vluxseg4ei16_v_i32m1x4_tumu(vbool32_t mask,
                                                 vint32m1x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x5_t __riscv_vluxseg5ei16_v_i32m1x5_tumu(vbool32_t mask,
                                                 vint32m1x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x6_t __riscv_vluxseg6ei16_v_i32m1x6_tumu(vbool32_t mask,
                                                 vint32m1x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x7_t __riscv_vluxseg7ei16_v_i32m1x7_tumu(vbool32_t mask,
                                                 vint32m1x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m1x8_t __riscv_vluxseg8ei16_v_i32m1x8_tumu(vbool32_t mask,
                                                 vint32m1x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint32m2x2_t __riscv_vluxseg2ei16_v_i32m2x2_tumu(vbool16_t mask,
                                                 vint32m2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei16_v_i32m2x3_tumu(vbool16_t mask,
                                                 vint32m2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei16_v_i32m2x4_tumu(vbool16_t mask,
                                                 vint32m2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei16_v_i32m4x2_tumu(vbool8_t mask,
                                                 vint32m4x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei32_v_i32mf2x2_tumu(
    vbool64_t mask, vint32mf2x2_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei32_v_i32mf2x3_tumu(
    vbool64_t mask, vint32mf2x3_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei32_v_i32mf2x4_tumu(
    vbool64_t mask, vint32mf2x4_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei32_v_i32mf2x5_tumu(
    vbool64_t mask, vint32mf2x5_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei32_v_i32mf2x6_tumu(
    vbool64_t mask, vint32mf2x6_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei32_v_i32mf2x7_tumu(
    vbool64_t mask, vint32mf2x7_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei32_v_i32mf2x8_tumu(
    vbool64_t mask, vint32mf2x8_t maskedoff_tuple, const int32_t *base,
    vuint32mf2_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei32_v_i32m1x2_tumu(vbool32_t mask,
                                                 vint32m1x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei32_v_i32m1x3_tumu(vbool32_t mask,
                                                 vint32m1x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei32_v_i32m1x4_tumu(vbool32_t mask,
                                                 vint32m1x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei32_v_i32m1x5_tumu(vbool32_t mask,
                                                 vint32m1x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei32_v_i32m1x6_tumu(vbool32_t mask,
                                                 vint32m1x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei32_v_i32m1x7_tumu(vbool32_t mask,
                                                 vint32m1x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei32_v_i32m1x8_tumu(vbool32_t mask,
                                                 vint32m1x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei32_v_i32m2x2_tumu(vbool16_t mask,
                                                 vint32m2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei32_v_i32m2x3_tumu(vbool16_t mask,
                                                 vint32m2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei32_v_i32m2x4_tumu(vbool16_t mask,
                                                 vint32m2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei32_v_i32m4x2_tumu(vbool8_t mask,
                                                 vint32m4x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei64_v_i32mf2x2_tumu(
    vbool64_t mask, vint32mf2x2_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei64_v_i32mf2x3_tumu(
    vbool64_t mask, vint32mf2x3_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei64_v_i32mf2x4_tumu(
    vbool64_t mask, vint32mf2x4_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei64_v_i32mf2x5_tumu(
    vbool64_t mask, vint32mf2x5_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei64_v_i32mf2x6_tumu(
    vbool64_t mask, vint32mf2x6_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei64_v_i32mf2x7_tumu(
    vbool64_t mask, vint32mf2x7_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei64_v_i32mf2x8_tumu(
    vbool64_t mask, vint32mf2x8_t maskedoff_tuple, const int32_t *base,
    vuint64m1_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei64_v_i32m1x2_tumu(vbool32_t mask,
                                                 vint32m1x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei64_v_i32m1x3_tumu(vbool32_t mask,
                                                 vint32m1x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei64_v_i32m1x4_tumu(vbool32_t mask,
                                                 vint32m1x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei64_v_i32m1x5_tumu(vbool32_t mask,
                                                 vint32m1x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei64_v_i32m1x6_tumu(vbool32_t mask,
                                                 vint32m1x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei64_v_i32m1x7_tumu(vbool32_t mask,
                                                 vint32m1x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei64_v_i32m1x8_tumu(vbool32_t mask,
                                                 vint32m1x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei64_v_i32m2x2_tumu(vbool16_t mask,
                                                 vint32m2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei64_v_i32m2x3_tumu(vbool16_t mask,
                                                 vint32m2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei64_v_i32m2x4_tumu(vbool16_t mask,
                                                 vint32m2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei64_v_i32m4x2_tumu(vbool8_t mask,
                                                 vint32m4x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei8_v_i64m1x2_tumu(vbool64_t mask,
                                                vint64m1x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei8_v_i64m1x3_tumu(vbool64_t mask,
                                                vint64m1x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei8_v_i64m1x4_tumu(vbool64_t mask,
                                                vint64m1x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei8_v_i64m1x5_tumu(vbool64_t mask,
                                                vint64m1x5_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei8_v_i64m1x6_tumu(vbool64_t mask,
                                                vint64m1x6_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei8_v_i64m1x7_tumu(vbool64_t mask,
                                                vint64m1x7_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei8_v_i64m1x8_tumu(vbool64_t mask,
                                                vint64m1x8_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei8_v_i64m2x2_tumu(vbool32_t mask,
                                                vint64m2x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei8_v_i64m2x3_tumu(vbool32_t mask,
                                                vint64m2x3_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei8_v_i64m2x4_tumu(vbool32_t mask,
                                                vint64m2x4_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei8_v_i64m4x2_tumu(vbool16_t mask,
                                                vint64m4x2_t maskedoff_tuple,
                                                const int64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei16_v_i64m1x2_tumu(vbool64_t mask,
                                                 vint64m1x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x3_t __riscv_vluxseg3ei16_v_i64m1x3_tumu(vbool64_t mask,
                                                 vint64m1x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x4_t __riscv_vluxseg4ei16_v_i64m1x4_tumu(vbool64_t mask,
                                                 vint64m1x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x5_t __riscv_vluxseg5ei16_v_i64m1x5_tumu(vbool64_t mask,
                                                 vint64m1x5_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x6_t __riscv_vluxseg6ei16_v_i64m1x6_tumu(vbool64_t mask,
                                                 vint64m1x6_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x7_t __riscv_vluxseg7ei16_v_i64m1x7_tumu(vbool64_t mask,
                                                 vint64m1x7_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m1x8_t __riscv_vluxseg8ei16_v_i64m1x8_tumu(vbool64_t mask,
                                                 vint64m1x8_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint64m2x2_t __riscv_vluxseg2ei16_v_i64m2x2_tumu(vbool32_t mask,
                                                 vint64m2x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint64m2x3_t __riscv_vluxseg3ei16_v_i64m2x3_tumu(vbool32_t mask,
                                                 vint64m2x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint64m2x4_t __riscv_vluxseg4ei16_v_i64m2x4_tumu(vbool32_t mask,
                                                 vint64m2x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint64m4x2_t __riscv_vluxseg2ei16_v_i64m4x2_tumu(vbool16_t mask,
                                                 vint64m4x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei32_v_i64m1x2_tumu(vbool64_t mask,
                                                 vint64m1x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x3_t __riscv_vluxseg3ei32_v_i64m1x3_tumu(vbool64_t mask,
                                                 vint64m1x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x4_t __riscv_vluxseg4ei32_v_i64m1x4_tumu(vbool64_t mask,
                                                 vint64m1x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x5_t __riscv_vluxseg5ei32_v_i64m1x5_tumu(vbool64_t mask,
                                                 vint64m1x5_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x6_t __riscv_vluxseg6ei32_v_i64m1x6_tumu(vbool64_t mask,
                                                 vint64m1x6_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x7_t __riscv_vluxseg7ei32_v_i64m1x7_tumu(vbool64_t mask,
                                                 vint64m1x7_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m1x8_t __riscv_vluxseg8ei32_v_i64m1x8_tumu(vbool64_t mask,
                                                 vint64m1x8_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint64m2x2_t __riscv_vluxseg2ei32_v_i64m2x2_tumu(vbool32_t mask,
                                                 vint64m2x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei32_v_i64m2x3_tumu(vbool32_t mask,
                                                 vint64m2x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei32_v_i64m2x4_tumu(vbool32_t mask,
                                                 vint64m2x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei32_v_i64m4x2_tumu(vbool16_t mask,
                                                 vint64m4x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei64_v_i64m1x2_tumu(vbool64_t mask,
                                                 vint64m1x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei64_v_i64m1x3_tumu(vbool64_t mask,
                                                 vint64m1x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei64_v_i64m1x4_tumu(vbool64_t mask,
                                                 vint64m1x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei64_v_i64m1x5_tumu(vbool64_t mask,
                                                 vint64m1x5_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei64_v_i64m1x6_tumu(vbool64_t mask,
                                                 vint64m1x6_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei64_v_i64m1x7_tumu(vbool64_t mask,
                                                 vint64m1x7_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei64_v_i64m1x8_tumu(vbool64_t mask,
                                                 vint64m1x8_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei64_v_i64m2x2_tumu(vbool32_t mask,
                                                 vint64m2x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei64_v_i64m2x3_tumu(vbool32_t mask,
                                                 vint64m2x3_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei64_v_i64m2x4_tumu(vbool32_t mask,
                                                 vint64m2x4_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei64_v_i64m4x2_tumu(vbool16_t mask,
                                                 vint64m4x2_t maskedoff_tuple,
                                                 const int64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei8_v_u8mf8x2_tumu(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei8_v_u8mf8x3_tumu(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei8_v_u8mf8x4_tumu(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei8_v_u8mf8x5_tumu(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei8_v_u8mf8x6_tumu(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei8_v_u8mf8x7_tumu(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei8_v_u8mf8x8_tumu(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei8_v_u8mf4x2_tumu(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei8_v_u8mf4x3_tumu(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei8_v_u8mf4x4_tumu(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei8_v_u8mf4x5_tumu(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei8_v_u8mf4x6_tumu(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei8_v_u8mf4x7_tumu(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei8_v_u8mf4x8_tumu(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei8_v_u8mf2x2_tumu(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei8_v_u8mf2x3_tumu(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei8_v_u8mf2x4_tumu(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei8_v_u8mf2x5_tumu(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei8_v_u8mf2x6_tumu(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei8_v_u8mf2x7_tumu(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei8_v_u8mf2x8_tumu(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei8_v_u8m1x2_tumu(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei8_v_u8m1x3_tumu(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei8_v_u8m1x4_tumu(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei8_v_u8m1x5_tumu(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei8_v_u8m1x6_tumu(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei8_v_u8m1x7_tumu(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei8_v_u8m1x8_tumu(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei8_v_u8m2x2_tumu(vbool4_t mask,
                                               vuint8m2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei8_v_u8m2x3_tumu(vbool4_t mask,
                                               vuint8m2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei8_v_u8m2x4_tumu(vbool4_t mask,
                                               vuint8m2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei8_v_u8m4x2_tumu(vbool2_t mask,
                                               vuint8m4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei16_v_u8mf8x2_tumu(vbool64_t mask,
                                                  vuint8mf8x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei16_v_u8mf8x3_tumu(vbool64_t mask,
                                                  vuint8mf8x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei16_v_u8mf8x4_tumu(vbool64_t mask,
                                                  vuint8mf8x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei16_v_u8mf8x5_tumu(vbool64_t mask,
                                                  vuint8mf8x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei16_v_u8mf8x6_tumu(vbool64_t mask,
                                                  vuint8mf8x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei16_v_u8mf8x7_tumu(vbool64_t mask,
                                                  vuint8mf8x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei16_v_u8mf8x8_tumu(vbool64_t mask,
                                                  vuint8mf8x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei16_v_u8mf4x2_tumu(vbool32_t mask,
                                                  vuint8mf4x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei16_v_u8mf4x3_tumu(vbool32_t mask,
                                                  vuint8mf4x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei16_v_u8mf4x4_tumu(vbool32_t mask,
                                                  vuint8mf4x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei16_v_u8mf4x5_tumu(vbool32_t mask,
                                                  vuint8mf4x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei16_v_u8mf4x6_tumu(vbool32_t mask,
                                                  vuint8mf4x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei16_v_u8mf4x7_tumu(vbool32_t mask,
                                                  vuint8mf4x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei16_v_u8mf4x8_tumu(vbool32_t mask,
                                                  vuint8mf4x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei16_v_u8mf2x2_tumu(vbool16_t mask,
                                                  vuint8mf2x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei16_v_u8mf2x3_tumu(vbool16_t mask,
                                                  vuint8mf2x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei16_v_u8mf2x4_tumu(vbool16_t mask,
                                                  vuint8mf2x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei16_v_u8mf2x5_tumu(vbool16_t mask,
                                                  vuint8mf2x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei16_v_u8mf2x6_tumu(vbool16_t mask,
                                                  vuint8mf2x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei16_v_u8mf2x7_tumu(vbool16_t mask,
                                                  vuint8mf2x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei16_v_u8mf2x8_tumu(vbool16_t mask,
                                                  vuint8mf2x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei16_v_u8m1x2_tumu(vbool8_t mask,
                                                vuint8m1x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei16_v_u8m1x3_tumu(vbool8_t mask,
                                                vuint8m1x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei16_v_u8m1x4_tumu(vbool8_t mask,
                                                vuint8m1x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei16_v_u8m1x5_tumu(vbool8_t mask,
                                                vuint8m1x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei16_v_u8m1x6_tumu(vbool8_t mask,
                                                vuint8m1x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei16_v_u8m1x7_tumu(vbool8_t mask,
                                                vuint8m1x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei16_v_u8m1x8_tumu(vbool8_t mask,
                                                vuint8m1x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei16_v_u8m2x2_tumu(vbool4_t mask,
                                                vuint8m2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei16_v_u8m2x3_tumu(vbool4_t mask,
                                                vuint8m2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei16_v_u8m2x4_tumu(vbool4_t mask,
                                                vuint8m2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei16_v_u8m4x2_tumu(vbool2_t mask,
                                                vuint8m4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei32_v_u8mf8x2_tumu(vbool64_t mask,
                                                  vuint8mf8x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei32_v_u8mf8x3_tumu(vbool64_t mask,
                                                  vuint8mf8x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei32_v_u8mf8x4_tumu(vbool64_t mask,
                                                  vuint8mf8x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei32_v_u8mf8x5_tumu(vbool64_t mask,
                                                  vuint8mf8x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei32_v_u8mf8x6_tumu(vbool64_t mask,
                                                  vuint8mf8x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei32_v_u8mf8x7_tumu(vbool64_t mask,
                                                  vuint8mf8x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei32_v_u8mf8x8_tumu(vbool64_t mask,
                                                  vuint8mf8x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei32_v_u8mf4x2_tumu(vbool32_t mask,
                                                  vuint8mf4x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei32_v_u8mf4x3_tumu(vbool32_t mask,
                                                  vuint8mf4x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei32_v_u8mf4x4_tumu(vbool32_t mask,
                                                  vuint8mf4x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei32_v_u8mf4x5_tumu(vbool32_t mask,
                                                  vuint8mf4x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei32_v_u8mf4x6_tumu(vbool32_t mask,
                                                  vuint8mf4x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei32_v_u8mf4x7_tumu(vbool32_t mask,
                                                  vuint8mf4x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei32_v_u8mf4x8_tumu(vbool32_t mask,
                                                  vuint8mf4x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei32_v_u8mf2x2_tumu(vbool16_t mask,
                                                  vuint8mf2x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei32_v_u8mf2x3_tumu(vbool16_t mask,
                                                  vuint8mf2x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei32_v_u8mf2x4_tumu(vbool16_t mask,
                                                  vuint8mf2x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei32_v_u8mf2x5_tumu(vbool16_t mask,
                                                  vuint8mf2x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei32_v_u8mf2x6_tumu(vbool16_t mask,
                                                  vuint8mf2x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei32_v_u8mf2x7_tumu(vbool16_t mask,
                                                  vuint8mf2x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei32_v_u8mf2x8_tumu(vbool16_t mask,
                                                  vuint8mf2x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei32_v_u8m1x2_tumu(vbool8_t mask,
                                                vuint8m1x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei32_v_u8m1x3_tumu(vbool8_t mask,
                                                vuint8m1x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei32_v_u8m1x4_tumu(vbool8_t mask,
                                                vuint8m1x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei32_v_u8m1x5_tumu(vbool8_t mask,
                                                vuint8m1x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei32_v_u8m1x6_tumu(vbool8_t mask,
                                                vuint8m1x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei32_v_u8m1x7_tumu(vbool8_t mask,
                                                vuint8m1x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei32_v_u8m1x8_tumu(vbool8_t mask,
                                                vuint8m1x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei32_v_u8m2x2_tumu(vbool4_t mask,
                                                vuint8m2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei32_v_u8m2x3_tumu(vbool4_t mask,
                                                vuint8m2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei32_v_u8m2x4_tumu(vbool4_t mask,
                                                vuint8m2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei64_v_u8mf8x2_tumu(vbool64_t mask,
                                                  vuint8mf8x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei64_v_u8mf8x3_tumu(vbool64_t mask,
                                                  vuint8mf8x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei64_v_u8mf8x4_tumu(vbool64_t mask,
                                                  vuint8mf8x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei64_v_u8mf8x5_tumu(vbool64_t mask,
                                                  vuint8mf8x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei64_v_u8mf8x6_tumu(vbool64_t mask,
                                                  vuint8mf8x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei64_v_u8mf8x7_tumu(vbool64_t mask,
                                                  vuint8mf8x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei64_v_u8mf8x8_tumu(vbool64_t mask,
                                                  vuint8mf8x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei64_v_u8mf4x2_tumu(vbool32_t mask,
                                                  vuint8mf4x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei64_v_u8mf4x3_tumu(vbool32_t mask,
                                                  vuint8mf4x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei64_v_u8mf4x4_tumu(vbool32_t mask,
                                                  vuint8mf4x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei64_v_u8mf4x5_tumu(vbool32_t mask,
                                                  vuint8mf4x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei64_v_u8mf4x6_tumu(vbool32_t mask,
                                                  vuint8mf4x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei64_v_u8mf4x7_tumu(vbool32_t mask,
                                                  vuint8mf4x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei64_v_u8mf4x8_tumu(vbool32_t mask,
                                                  vuint8mf4x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei64_v_u8mf2x2_tumu(vbool16_t mask,
                                                  vuint8mf2x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei64_v_u8mf2x3_tumu(vbool16_t mask,
                                                  vuint8mf2x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei64_v_u8mf2x4_tumu(vbool16_t mask,
                                                  vuint8mf2x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei64_v_u8mf2x5_tumu(vbool16_t mask,
                                                  vuint8mf2x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei64_v_u8mf2x6_tumu(vbool16_t mask,
                                                  vuint8mf2x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei64_v_u8mf2x7_tumu(vbool16_t mask,
                                                  vuint8mf2x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei64_v_u8mf2x8_tumu(vbool16_t mask,
                                                  vuint8mf2x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei64_v_u8m1x2_tumu(vbool8_t mask,
                                                vuint8m1x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei64_v_u8m1x3_tumu(vbool8_t mask,
                                                vuint8m1x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei64_v_u8m1x4_tumu(vbool8_t mask,
                                                vuint8m1x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei64_v_u8m1x5_tumu(vbool8_t mask,
                                                vuint8m1x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei64_v_u8m1x6_tumu(vbool8_t mask,
                                                vuint8m1x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei64_v_u8m1x7_tumu(vbool8_t mask,
                                                vuint8m1x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei64_v_u8m1x8_tumu(vbool8_t mask,
                                                vuint8m1x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei8_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei8_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei8_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei8_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei8_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei8_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei8_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei8_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei8_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei8_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei8_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei8_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei8_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei8_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei8_v_u16m1x2_tumu(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei8_v_u16m1x3_tumu(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei8_v_u16m1x4_tumu(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei8_v_u16m1x5_tumu(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei8_v_u16m1x6_tumu(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei8_v_u16m1x7_tumu(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei8_v_u16m1x8_tumu(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei8_v_u16m2x2_tumu(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei8_v_u16m2x3_tumu(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei8_v_u16m2x4_tumu(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei8_v_u16m4x2_tumu(vbool4_t mask,
                                                 vuint16m4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei16_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei16_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei16_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei16_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei16_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei16_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei16_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei16_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei16_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei16_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei16_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei16_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei16_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei16_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei16_v_u16m1x2_tumu(vbool16_t mask,
                                                  vuint16m1x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei16_v_u16m1x3_tumu(vbool16_t mask,
                                                  vuint16m1x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei16_v_u16m1x4_tumu(vbool16_t mask,
                                                  vuint16m1x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei16_v_u16m1x5_tumu(vbool16_t mask,
                                                  vuint16m1x5_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei16_v_u16m1x6_tumu(vbool16_t mask,
                                                  vuint16m1x6_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei16_v_u16m1x7_tumu(vbool16_t mask,
                                                  vuint16m1x7_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei16_v_u16m1x8_tumu(vbool16_t mask,
                                                  vuint16m1x8_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei16_v_u16m2x2_tumu(vbool8_t mask,
                                                  vuint16m2x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei16_v_u16m2x3_tumu(vbool8_t mask,
                                                  vuint16m2x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei16_v_u16m2x4_tumu(vbool8_t mask,
                                                  vuint16m2x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei16_v_u16m4x2_tumu(vbool4_t mask,
                                                  vuint16m4x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m4_t bindex,
                                                  size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei32_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei32_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei32_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei32_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei32_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei32_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei32_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei32_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei32_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei32_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei32_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei32_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei32_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei32_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei32_v_u16m1x2_tumu(vbool16_t mask,
                                                  vuint16m1x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei32_v_u16m1x3_tumu(vbool16_t mask,
                                                  vuint16m1x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei32_v_u16m1x4_tumu(vbool16_t mask,
                                                  vuint16m1x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei32_v_u16m1x5_tumu(vbool16_t mask,
                                                  vuint16m1x5_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei32_v_u16m1x6_tumu(vbool16_t mask,
                                                  vuint16m1x6_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei32_v_u16m1x7_tumu(vbool16_t mask,
                                                  vuint16m1x7_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei32_v_u16m1x8_tumu(vbool16_t mask,
                                                  vuint16m1x8_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei32_v_u16m2x2_tumu(vbool8_t mask,
                                                  vuint16m2x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei32_v_u16m2x3_tumu(vbool8_t mask,
                                                  vuint16m2x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei32_v_u16m2x4_tumu(vbool8_t mask,
                                                  vuint16m2x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei32_v_u16m4x2_tumu(vbool4_t mask,
                                                  vuint16m4x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m8_t bindex,
                                                  size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei64_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei64_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei64_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei64_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei64_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei64_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei64_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei64_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei64_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei64_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei64_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei64_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei64_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei64_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei64_v_u16m1x2_tumu(vbool16_t mask,
                                                  vuint16m1x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei64_v_u16m1x3_tumu(vbool16_t mask,
                                                  vuint16m1x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei64_v_u16m1x4_tumu(vbool16_t mask,
                                                  vuint16m1x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei64_v_u16m1x5_tumu(vbool16_t mask,
                                                  vuint16m1x5_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei64_v_u16m1x6_tumu(vbool16_t mask,
                                                  vuint16m1x6_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei64_v_u16m1x7_tumu(vbool16_t mask,
                                                  vuint16m1x7_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei64_v_u16m1x8_tumu(vbool16_t mask,
                                                  vuint16m1x8_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei64_v_u16m2x2_tumu(vbool8_t mask,
                                                  vuint16m2x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei64_v_u16m2x3_tumu(vbool8_t mask,
                                                  vuint16m2x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei64_v_u16m2x4_tumu(vbool8_t mask,
                                                  vuint16m2x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei8_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei8_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei8_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei8_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei8_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei8_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei8_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei8_v_u32m1x2_tumu(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei8_v_u32m1x3_tumu(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei8_v_u32m1x4_tumu(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei8_v_u32m1x5_tumu(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei8_v_u32m1x6_tumu(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei8_v_u32m1x7_tumu(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei8_v_u32m1x8_tumu(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei8_v_u32m2x2_tumu(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei8_v_u32m2x3_tumu(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei8_v_u32m2x4_tumu(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei8_v_u32m4x2_tumu(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei16_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei16_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei16_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei16_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei16_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei16_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei16_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei16_v_u32m1x2_tumu(vbool32_t mask,
                                                  vuint32m1x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei16_v_u32m1x3_tumu(vbool32_t mask,
                                                  vuint32m1x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei16_v_u32m1x4_tumu(vbool32_t mask,
                                                  vuint32m1x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei16_v_u32m1x5_tumu(vbool32_t mask,
                                                  vuint32m1x5_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei16_v_u32m1x6_tumu(vbool32_t mask,
                                                  vuint32m1x6_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei16_v_u32m1x7_tumu(vbool32_t mask,
                                                  vuint32m1x7_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei16_v_u32m1x8_tumu(vbool32_t mask,
                                                  vuint32m1x8_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei16_v_u32m2x2_tumu(vbool16_t mask,
                                                  vuint32m2x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei16_v_u32m2x3_tumu(vbool16_t mask,
                                                  vuint32m2x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei16_v_u32m2x4_tumu(vbool16_t mask,
                                                  vuint32m2x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei16_v_u32m4x2_tumu(vbool8_t mask,
                                                  vuint32m4x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei32_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei32_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei32_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei32_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei32_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei32_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei32_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei32_v_u32m1x2_tumu(vbool32_t mask,
                                                  vuint32m1x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei32_v_u32m1x3_tumu(vbool32_t mask,
                                                  vuint32m1x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei32_v_u32m1x4_tumu(vbool32_t mask,
                                                  vuint32m1x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei32_v_u32m1x5_tumu(vbool32_t mask,
                                                  vuint32m1x5_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei32_v_u32m1x6_tumu(vbool32_t mask,
                                                  vuint32m1x6_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei32_v_u32m1x7_tumu(vbool32_t mask,
                                                  vuint32m1x7_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei32_v_u32m1x8_tumu(vbool32_t mask,
                                                  vuint32m1x8_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei32_v_u32m2x2_tumu(vbool16_t mask,
                                                  vuint32m2x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei32_v_u32m2x3_tumu(vbool16_t mask,
                                                  vuint32m2x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei32_v_u32m2x4_tumu(vbool16_t mask,
                                                  vuint32m2x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei32_v_u32m4x2_tumu(vbool8_t mask,
                                                  vuint32m4x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei64_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei64_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei64_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei64_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei64_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei64_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei64_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei64_v_u32m1x2_tumu(vbool32_t mask,
                                                  vuint32m1x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei64_v_u32m1x3_tumu(vbool32_t mask,
                                                  vuint32m1x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei64_v_u32m1x4_tumu(vbool32_t mask,
                                                  vuint32m1x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei64_v_u32m1x5_tumu(vbool32_t mask,
                                                  vuint32m1x5_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei64_v_u32m1x6_tumu(vbool32_t mask,
                                                  vuint32m1x6_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei64_v_u32m1x7_tumu(vbool32_t mask,
                                                  vuint32m1x7_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei64_v_u32m1x8_tumu(vbool32_t mask,
                                                  vuint32m1x8_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei64_v_u32m2x2_tumu(vbool16_t mask,
                                                  vuint32m2x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei64_v_u32m2x3_tumu(vbool16_t mask,
                                                  vuint32m2x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei64_v_u32m2x4_tumu(vbool16_t mask,
                                                  vuint32m2x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei64_v_u32m4x2_tumu(vbool8_t mask,
                                                  vuint32m4x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei8_v_u64m1x2_tumu(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei8_v_u64m1x3_tumu(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei8_v_u64m1x4_tumu(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei8_v_u64m1x5_tumu(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei8_v_u64m1x6_tumu(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei8_v_u64m1x7_tumu(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei8_v_u64m1x8_tumu(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei8_v_u64m2x2_tumu(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei8_v_u64m2x3_tumu(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei8_v_u64m2x4_tumu(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei8_v_u64m4x2_tumu(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei16_v_u64m1x2_tumu(vbool64_t mask,
                                                  vuint64m1x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei16_v_u64m1x3_tumu(vbool64_t mask,
                                                  vuint64m1x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei16_v_u64m1x4_tumu(vbool64_t mask,
                                                  vuint64m1x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei16_v_u64m1x5_tumu(vbool64_t mask,
                                                  vuint64m1x5_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei16_v_u64m1x6_tumu(vbool64_t mask,
                                                  vuint64m1x6_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei16_v_u64m1x7_tumu(vbool64_t mask,
                                                  vuint64m1x7_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei16_v_u64m1x8_tumu(vbool64_t mask,
                                                  vuint64m1x8_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei16_v_u64m2x2_tumu(vbool32_t mask,
                                                  vuint64m2x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei16_v_u64m2x3_tumu(vbool32_t mask,
                                                  vuint64m2x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei16_v_u64m2x4_tumu(vbool32_t mask,
                                                  vuint64m2x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei16_v_u64m4x2_tumu(vbool16_t mask,
                                                  vuint64m4x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei32_v_u64m1x2_tumu(vbool64_t mask,
                                                  vuint64m1x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei32_v_u64m1x3_tumu(vbool64_t mask,
                                                  vuint64m1x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei32_v_u64m1x4_tumu(vbool64_t mask,
                                                  vuint64m1x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei32_v_u64m1x5_tumu(vbool64_t mask,
                                                  vuint64m1x5_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei32_v_u64m1x6_tumu(vbool64_t mask,
                                                  vuint64m1x6_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei32_v_u64m1x7_tumu(vbool64_t mask,
                                                  vuint64m1x7_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei32_v_u64m1x8_tumu(vbool64_t mask,
                                                  vuint64m1x8_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei32_v_u64m2x2_tumu(vbool32_t mask,
                                                  vuint64m2x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei32_v_u64m2x3_tumu(vbool32_t mask,
                                                  vuint64m2x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei32_v_u64m2x4_tumu(vbool32_t mask,
                                                  vuint64m2x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei32_v_u64m4x2_tumu(vbool16_t mask,
                                                  vuint64m4x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei64_v_u64m1x2_tumu(vbool64_t mask,
                                                  vuint64m1x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei64_v_u64m1x3_tumu(vbool64_t mask,
                                                  vuint64m1x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei64_v_u64m1x4_tumu(vbool64_t mask,
                                                  vuint64m1x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei64_v_u64m1x5_tumu(vbool64_t mask,
                                                  vuint64m1x5_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei64_v_u64m1x6_tumu(vbool64_t mask,
                                                  vuint64m1x6_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei64_v_u64m1x7_tumu(vbool64_t mask,
                                                  vuint64m1x7_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei64_v_u64m1x8_tumu(vbool64_t mask,
                                                  vuint64m1x8_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei64_v_u64m2x2_tumu(vbool32_t mask,
                                                  vuint64m2x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei64_v_u64m2x3_tumu(vbool32_t mask,
                                                  vuint64m2x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei64_v_u64m2x4_tumu(vbool32_t mask,
                                                  vuint64m2x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei64_v_u64m4x2_tumu(vbool16_t mask,
                                                  vuint64m4x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei8_v_u8mf8x2_tumu(vbool64_t mask,
                                                 vuint8mf8x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei8_v_u8mf8x3_tumu(vbool64_t mask,
                                                 vuint8mf8x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei8_v_u8mf8x4_tumu(vbool64_t mask,
                                                 vuint8mf8x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei8_v_u8mf8x5_tumu(vbool64_t mask,
                                                 vuint8mf8x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei8_v_u8mf8x6_tumu(vbool64_t mask,
                                                 vuint8mf8x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei8_v_u8mf8x7_tumu(vbool64_t mask,
                                                 vuint8mf8x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei8_v_u8mf8x8_tumu(vbool64_t mask,
                                                 vuint8mf8x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei8_v_u8mf4x2_tumu(vbool32_t mask,
                                                 vuint8mf4x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei8_v_u8mf4x3_tumu(vbool32_t mask,
                                                 vuint8mf4x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei8_v_u8mf4x4_tumu(vbool32_t mask,
                                                 vuint8mf4x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei8_v_u8mf4x5_tumu(vbool32_t mask,
                                                 vuint8mf4x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei8_v_u8mf4x6_tumu(vbool32_t mask,
                                                 vuint8mf4x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei8_v_u8mf4x7_tumu(vbool32_t mask,
                                                 vuint8mf4x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei8_v_u8mf4x8_tumu(vbool32_t mask,
                                                 vuint8mf4x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei8_v_u8mf2x2_tumu(vbool16_t mask,
                                                 vuint8mf2x2_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei8_v_u8mf2x3_tumu(vbool16_t mask,
                                                 vuint8mf2x3_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei8_v_u8mf2x4_tumu(vbool16_t mask,
                                                 vuint8mf2x4_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei8_v_u8mf2x5_tumu(vbool16_t mask,
                                                 vuint8mf2x5_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei8_v_u8mf2x6_tumu(vbool16_t mask,
                                                 vuint8mf2x6_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei8_v_u8mf2x7_tumu(vbool16_t mask,
                                                 vuint8mf2x7_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei8_v_u8mf2x8_tumu(vbool16_t mask,
                                                 vuint8mf2x8_t maskedoff_tuple,
                                                 const uint8_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei8_v_u8m1x2_tumu(vbool8_t mask,
                                               vuint8m1x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei8_v_u8m1x3_tumu(vbool8_t mask,
                                               vuint8m1x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei8_v_u8m1x4_tumu(vbool8_t mask,
                                               vuint8m1x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei8_v_u8m1x5_tumu(vbool8_t mask,
                                               vuint8m1x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei8_v_u8m1x6_tumu(vbool8_t mask,
                                               vuint8m1x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei8_v_u8m1x7_tumu(vbool8_t mask,
                                               vuint8m1x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei8_v_u8m1x8_tumu(vbool8_t mask,
                                               vuint8m1x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei8_v_u8m2x2_tumu(vbool4_t mask,
                                               vuint8m2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei8_v_u8m2x3_tumu(vbool4_t mask,
                                               vuint8m2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei8_v_u8m2x4_tumu(vbool4_t mask,
                                               vuint8m2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei8_v_u8m4x2_tumu(vbool2_t mask,
                                               vuint8m4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei16_v_u8mf8x2_tumu(vbool64_t mask,
                                                  vuint8mf8x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei16_v_u8mf8x3_tumu(vbool64_t mask,
                                                  vuint8mf8x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei16_v_u8mf8x4_tumu(vbool64_t mask,
                                                  vuint8mf8x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei16_v_u8mf8x5_tumu(vbool64_t mask,
                                                  vuint8mf8x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei16_v_u8mf8x6_tumu(vbool64_t mask,
                                                  vuint8mf8x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei16_v_u8mf8x7_tumu(vbool64_t mask,
                                                  vuint8mf8x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei16_v_u8mf8x8_tumu(vbool64_t mask,
                                                  vuint8mf8x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei16_v_u8mf4x2_tumu(vbool32_t mask,
                                                  vuint8mf4x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei16_v_u8mf4x3_tumu(vbool32_t mask,
                                                  vuint8mf4x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei16_v_u8mf4x4_tumu(vbool32_t mask,
                                                  vuint8mf4x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei16_v_u8mf4x5_tumu(vbool32_t mask,
                                                  vuint8mf4x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei16_v_u8mf4x6_tumu(vbool32_t mask,
                                                  vuint8mf4x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei16_v_u8mf4x7_tumu(vbool32_t mask,
                                                  vuint8mf4x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei16_v_u8mf4x8_tumu(vbool32_t mask,
                                                  vuint8mf4x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei16_v_u8mf2x2_tumu(vbool16_t mask,
                                                  vuint8mf2x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei16_v_u8mf2x3_tumu(vbool16_t mask,
                                                  vuint8mf2x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei16_v_u8mf2x4_tumu(vbool16_t mask,
                                                  vuint8mf2x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei16_v_u8mf2x5_tumu(vbool16_t mask,
                                                  vuint8mf2x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei16_v_u8mf2x6_tumu(vbool16_t mask,
                                                  vuint8mf2x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei16_v_u8mf2x7_tumu(vbool16_t mask,
                                                  vuint8mf2x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei16_v_u8mf2x8_tumu(vbool16_t mask,
                                                  vuint8mf2x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei16_v_u8m1x2_tumu(vbool8_t mask,
                                                vuint8m1x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei16_v_u8m1x3_tumu(vbool8_t mask,
                                                vuint8m1x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei16_v_u8m1x4_tumu(vbool8_t mask,
                                                vuint8m1x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei16_v_u8m1x5_tumu(vbool8_t mask,
                                                vuint8m1x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei16_v_u8m1x6_tumu(vbool8_t mask,
                                                vuint8m1x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei16_v_u8m1x7_tumu(vbool8_t mask,
                                                vuint8m1x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei16_v_u8m1x8_tumu(vbool8_t mask,
                                                vuint8m1x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei16_v_u8m2x2_tumu(vbool4_t mask,
                                                vuint8m2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei16_v_u8m2x3_tumu(vbool4_t mask,
                                                vuint8m2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei16_v_u8m2x4_tumu(vbool4_t mask,
                                                vuint8m2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei16_v_u8m4x2_tumu(vbool2_t mask,
                                                vuint8m4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei32_v_u8mf8x2_tumu(vbool64_t mask,
                                                  vuint8mf8x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei32_v_u8mf8x3_tumu(vbool64_t mask,
                                                  vuint8mf8x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei32_v_u8mf8x4_tumu(vbool64_t mask,
                                                  vuint8mf8x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei32_v_u8mf8x5_tumu(vbool64_t mask,
                                                  vuint8mf8x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei32_v_u8mf8x6_tumu(vbool64_t mask,
                                                  vuint8mf8x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei32_v_u8mf8x7_tumu(vbool64_t mask,
                                                  vuint8mf8x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei32_v_u8mf8x8_tumu(vbool64_t mask,
                                                  vuint8mf8x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei32_v_u8mf4x2_tumu(vbool32_t mask,
                                                  vuint8mf4x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei32_v_u8mf4x3_tumu(vbool32_t mask,
                                                  vuint8mf4x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei32_v_u8mf4x4_tumu(vbool32_t mask,
                                                  vuint8mf4x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei32_v_u8mf4x5_tumu(vbool32_t mask,
                                                  vuint8mf4x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei32_v_u8mf4x6_tumu(vbool32_t mask,
                                                  vuint8mf4x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei32_v_u8mf4x7_tumu(vbool32_t mask,
                                                  vuint8mf4x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei32_v_u8mf4x8_tumu(vbool32_t mask,
                                                  vuint8mf4x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei32_v_u8mf2x2_tumu(vbool16_t mask,
                                                  vuint8mf2x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei32_v_u8mf2x3_tumu(vbool16_t mask,
                                                  vuint8mf2x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei32_v_u8mf2x4_tumu(vbool16_t mask,
                                                  vuint8mf2x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei32_v_u8mf2x5_tumu(vbool16_t mask,
                                                  vuint8mf2x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei32_v_u8mf2x6_tumu(vbool16_t mask,
                                                  vuint8mf2x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei32_v_u8mf2x7_tumu(vbool16_t mask,
                                                  vuint8mf2x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei32_v_u8mf2x8_tumu(vbool16_t mask,
                                                  vuint8mf2x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei32_v_u8m1x2_tumu(vbool8_t mask,
                                                vuint8m1x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei32_v_u8m1x3_tumu(vbool8_t mask,
                                                vuint8m1x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei32_v_u8m1x4_tumu(vbool8_t mask,
                                                vuint8m1x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei32_v_u8m1x5_tumu(vbool8_t mask,
                                                vuint8m1x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei32_v_u8m1x6_tumu(vbool8_t mask,
                                                vuint8m1x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei32_v_u8m1x7_tumu(vbool8_t mask,
                                                vuint8m1x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei32_v_u8m1x8_tumu(vbool8_t mask,
                                                vuint8m1x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei32_v_u8m2x2_tumu(vbool4_t mask,
                                                vuint8m2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei32_v_u8m2x3_tumu(vbool4_t mask,
                                                vuint8m2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei32_v_u8m2x4_tumu(vbool4_t mask,
                                                vuint8m2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei64_v_u8mf8x2_tumu(vbool64_t mask,
                                                  vuint8mf8x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei64_v_u8mf8x3_tumu(vbool64_t mask,
                                                  vuint8mf8x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei64_v_u8mf8x4_tumu(vbool64_t mask,
                                                  vuint8mf8x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei64_v_u8mf8x5_tumu(vbool64_t mask,
                                                  vuint8mf8x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei64_v_u8mf8x6_tumu(vbool64_t mask,
                                                  vuint8mf8x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei64_v_u8mf8x7_tumu(vbool64_t mask,
                                                  vuint8mf8x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei64_v_u8mf8x8_tumu(vbool64_t mask,
                                                  vuint8mf8x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei64_v_u8mf4x2_tumu(vbool32_t mask,
                                                  vuint8mf4x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei64_v_u8mf4x3_tumu(vbool32_t mask,
                                                  vuint8mf4x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei64_v_u8mf4x4_tumu(vbool32_t mask,
                                                  vuint8mf4x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei64_v_u8mf4x5_tumu(vbool32_t mask,
                                                  vuint8mf4x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei64_v_u8mf4x6_tumu(vbool32_t mask,
                                                  vuint8mf4x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei64_v_u8mf4x7_tumu(vbool32_t mask,
                                                  vuint8mf4x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei64_v_u8mf4x8_tumu(vbool32_t mask,
                                                  vuint8mf4x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei64_v_u8mf2x2_tumu(vbool16_t mask,
                                                  vuint8mf2x2_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei64_v_u8mf2x3_tumu(vbool16_t mask,
                                                  vuint8mf2x3_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei64_v_u8mf2x4_tumu(vbool16_t mask,
                                                  vuint8mf2x4_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei64_v_u8mf2x5_tumu(vbool16_t mask,
                                                  vuint8mf2x5_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei64_v_u8mf2x6_tumu(vbool16_t mask,
                                                  vuint8mf2x6_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei64_v_u8mf2x7_tumu(vbool16_t mask,
                                                  vuint8mf2x7_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei64_v_u8mf2x8_tumu(vbool16_t mask,
                                                  vuint8mf2x8_t maskedoff_tuple,
                                                  const uint8_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei64_v_u8m1x2_tumu(vbool8_t mask,
                                                vuint8m1x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei64_v_u8m1x3_tumu(vbool8_t mask,
                                                vuint8m1x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei64_v_u8m1x4_tumu(vbool8_t mask,
                                                vuint8m1x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei64_v_u8m1x5_tumu(vbool8_t mask,
                                                vuint8m1x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei64_v_u8m1x6_tumu(vbool8_t mask,
                                                vuint8m1x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei64_v_u8m1x7_tumu(vbool8_t mask,
                                                vuint8m1x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei64_v_u8m1x8_tumu(vbool8_t mask,
                                                vuint8m1x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei8_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei8_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei8_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei8_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei8_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei8_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei8_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei8_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei8_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei8_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei8_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei8_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei8_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei8_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei8_v_u16m1x2_tumu(vbool16_t mask,
                                                 vuint16m1x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei8_v_u16m1x3_tumu(vbool16_t mask,
                                                 vuint16m1x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei8_v_u16m1x4_tumu(vbool16_t mask,
                                                 vuint16m1x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei8_v_u16m1x5_tumu(vbool16_t mask,
                                                 vuint16m1x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei8_v_u16m1x6_tumu(vbool16_t mask,
                                                 vuint16m1x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei8_v_u16m1x7_tumu(vbool16_t mask,
                                                 vuint16m1x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei8_v_u16m1x8_tumu(vbool16_t mask,
                                                 vuint16m1x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei8_v_u16m2x2_tumu(vbool8_t mask,
                                                 vuint16m2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei8_v_u16m2x3_tumu(vbool8_t mask,
                                                 vuint16m2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei8_v_u16m2x4_tumu(vbool8_t mask,
                                                 vuint16m2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei8_v_u16m4x2_tumu(vbool4_t mask,
                                                 vuint16m4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei16_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei16_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei16_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei16_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei16_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei16_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei16_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei16_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei16_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei16_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei16_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei16_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei16_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei16_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei16_v_u16m1x2_tumu(vbool16_t mask,
                                                  vuint16m1x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei16_v_u16m1x3_tumu(vbool16_t mask,
                                                  vuint16m1x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei16_v_u16m1x4_tumu(vbool16_t mask,
                                                  vuint16m1x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei16_v_u16m1x5_tumu(vbool16_t mask,
                                                  vuint16m1x5_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei16_v_u16m1x6_tumu(vbool16_t mask,
                                                  vuint16m1x6_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei16_v_u16m1x7_tumu(vbool16_t mask,
                                                  vuint16m1x7_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei16_v_u16m1x8_tumu(vbool16_t mask,
                                                  vuint16m1x8_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei16_v_u16m2x2_tumu(vbool8_t mask,
                                                  vuint16m2x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei16_v_u16m2x3_tumu(vbool8_t mask,
                                                  vuint16m2x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei16_v_u16m2x4_tumu(vbool8_t mask,
                                                  vuint16m2x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei16_v_u16m4x2_tumu(vbool4_t mask,
                                                  vuint16m4x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint16m4_t bindex,
                                                  size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei32_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei32_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei32_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei32_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei32_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei32_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei32_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei32_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei32_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei32_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei32_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei32_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei32_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei32_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei32_v_u16m1x2_tumu(vbool16_t mask,
                                                  vuint16m1x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei32_v_u16m1x3_tumu(vbool16_t mask,
                                                  vuint16m1x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei32_v_u16m1x4_tumu(vbool16_t mask,
                                                  vuint16m1x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei32_v_u16m1x5_tumu(vbool16_t mask,
                                                  vuint16m1x5_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei32_v_u16m1x6_tumu(vbool16_t mask,
                                                  vuint16m1x6_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei32_v_u16m1x7_tumu(vbool16_t mask,
                                                  vuint16m1x7_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei32_v_u16m1x8_tumu(vbool16_t mask,
                                                  vuint16m1x8_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei32_v_u16m2x2_tumu(vbool8_t mask,
                                                  vuint16m2x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei32_v_u16m2x3_tumu(vbool8_t mask,
                                                  vuint16m2x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei32_v_u16m2x4_tumu(vbool8_t mask,
                                                  vuint16m2x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei32_v_u16m4x2_tumu(vbool4_t mask,
                                                  vuint16m4x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint32m8_t bindex,
                                                  size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei64_v_u16mf4x2_tumu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei64_v_u16mf4x3_tumu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei64_v_u16mf4x4_tumu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei64_v_u16mf4x5_tumu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei64_v_u16mf4x6_tumu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei64_v_u16mf4x7_tumu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei64_v_u16mf4x8_tumu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei64_v_u16mf2x2_tumu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei64_v_u16mf2x3_tumu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei64_v_u16mf2x4_tumu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei64_v_u16mf2x5_tumu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei64_v_u16mf2x6_tumu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei64_v_u16mf2x7_tumu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei64_v_u16mf2x8_tumu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei64_v_u16m1x2_tumu(vbool16_t mask,
                                                  vuint16m1x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei64_v_u16m1x3_tumu(vbool16_t mask,
                                                  vuint16m1x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei64_v_u16m1x4_tumu(vbool16_t mask,
                                                  vuint16m1x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei64_v_u16m1x5_tumu(vbool16_t mask,
                                                  vuint16m1x5_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei64_v_u16m1x6_tumu(vbool16_t mask,
                                                  vuint16m1x6_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei64_v_u16m1x7_tumu(vbool16_t mask,
                                                  vuint16m1x7_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei64_v_u16m1x8_tumu(vbool16_t mask,
                                                  vuint16m1x8_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei64_v_u16m2x2_tumu(vbool8_t mask,
                                                  vuint16m2x2_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei64_v_u16m2x3_tumu(vbool8_t mask,
                                                  vuint16m2x3_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei64_v_u16m2x4_tumu(vbool8_t mask,
                                                  vuint16m2x4_t maskedoff_tuple,
                                                  const uint16_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei8_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei8_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei8_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei8_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei8_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei8_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei8_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei8_v_u32m1x2_tumu(vbool32_t mask,
                                                 vuint32m1x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei8_v_u32m1x3_tumu(vbool32_t mask,
                                                 vuint32m1x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei8_v_u32m1x4_tumu(vbool32_t mask,
                                                 vuint32m1x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei8_v_u32m1x5_tumu(vbool32_t mask,
                                                 vuint32m1x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei8_v_u32m1x6_tumu(vbool32_t mask,
                                                 vuint32m1x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei8_v_u32m1x7_tumu(vbool32_t mask,
                                                 vuint32m1x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei8_v_u32m1x8_tumu(vbool32_t mask,
                                                 vuint32m1x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei8_v_u32m2x2_tumu(vbool16_t mask,
                                                 vuint32m2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei8_v_u32m2x3_tumu(vbool16_t mask,
                                                 vuint32m2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei8_v_u32m2x4_tumu(vbool16_t mask,
                                                 vuint32m2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei8_v_u32m4x2_tumu(vbool8_t mask,
                                                 vuint32m4x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei16_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei16_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei16_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei16_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei16_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei16_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei16_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei16_v_u32m1x2_tumu(vbool32_t mask,
                                                  vuint32m1x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei16_v_u32m1x3_tumu(vbool32_t mask,
                                                  vuint32m1x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei16_v_u32m1x4_tumu(vbool32_t mask,
                                                  vuint32m1x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei16_v_u32m1x5_tumu(vbool32_t mask,
                                                  vuint32m1x5_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei16_v_u32m1x6_tumu(vbool32_t mask,
                                                  vuint32m1x6_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei16_v_u32m1x7_tumu(vbool32_t mask,
                                                  vuint32m1x7_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei16_v_u32m1x8_tumu(vbool32_t mask,
                                                  vuint32m1x8_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei16_v_u32m2x2_tumu(vbool16_t mask,
                                                  vuint32m2x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei16_v_u32m2x3_tumu(vbool16_t mask,
                                                  vuint32m2x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei16_v_u32m2x4_tumu(vbool16_t mask,
                                                  vuint32m2x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei16_v_u32m4x2_tumu(vbool8_t mask,
                                                  vuint32m4x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint16m2_t bindex,
                                                  size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei32_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei32_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei32_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei32_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei32_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei32_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei32_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei32_v_u32m1x2_tumu(vbool32_t mask,
                                                  vuint32m1x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei32_v_u32m1x3_tumu(vbool32_t mask,
                                                  vuint32m1x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei32_v_u32m1x4_tumu(vbool32_t mask,
                                                  vuint32m1x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei32_v_u32m1x5_tumu(vbool32_t mask,
                                                  vuint32m1x5_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei32_v_u32m1x6_tumu(vbool32_t mask,
                                                  vuint32m1x6_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei32_v_u32m1x7_tumu(vbool32_t mask,
                                                  vuint32m1x7_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei32_v_u32m1x8_tumu(vbool32_t mask,
                                                  vuint32m1x8_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei32_v_u32m2x2_tumu(vbool16_t mask,
                                                  vuint32m2x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei32_v_u32m2x3_tumu(vbool16_t mask,
                                                  vuint32m2x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei32_v_u32m2x4_tumu(vbool16_t mask,
                                                  vuint32m2x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei32_v_u32m4x2_tumu(vbool8_t mask,
                                                  vuint32m4x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint32m4_t bindex,
                                                  size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei64_v_u32mf2x2_tumu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei64_v_u32mf2x3_tumu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei64_v_u32mf2x4_tumu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei64_v_u32mf2x5_tumu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei64_v_u32mf2x6_tumu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei64_v_u32mf2x7_tumu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei64_v_u32mf2x8_tumu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei64_v_u32m1x2_tumu(vbool32_t mask,
                                                  vuint32m1x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei64_v_u32m1x3_tumu(vbool32_t mask,
                                                  vuint32m1x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei64_v_u32m1x4_tumu(vbool32_t mask,
                                                  vuint32m1x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei64_v_u32m1x5_tumu(vbool32_t mask,
                                                  vuint32m1x5_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei64_v_u32m1x6_tumu(vbool32_t mask,
                                                  vuint32m1x6_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei64_v_u32m1x7_tumu(vbool32_t mask,
                                                  vuint32m1x7_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei64_v_u32m1x8_tumu(vbool32_t mask,
                                                  vuint32m1x8_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei64_v_u32m2x2_tumu(vbool16_t mask,
                                                  vuint32m2x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei64_v_u32m2x3_tumu(vbool16_t mask,
                                                  vuint32m2x3_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei64_v_u32m2x4_tumu(vbool16_t mask,
                                                  vuint32m2x4_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei64_v_u32m4x2_tumu(vbool8_t mask,
                                                  vuint32m4x2_t maskedoff_tuple,
                                                  const uint32_t *base,
                                                  vuint64m8_t bindex,
                                                  size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei8_v_u64m1x2_tumu(vbool64_t mask,
                                                 vuint64m1x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei8_v_u64m1x3_tumu(vbool64_t mask,
                                                 vuint64m1x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei8_v_u64m1x4_tumu(vbool64_t mask,
                                                 vuint64m1x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei8_v_u64m1x5_tumu(vbool64_t mask,
                                                 vuint64m1x5_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei8_v_u64m1x6_tumu(vbool64_t mask,
                                                 vuint64m1x6_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei8_v_u64m1x7_tumu(vbool64_t mask,
                                                 vuint64m1x7_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei8_v_u64m1x8_tumu(vbool64_t mask,
                                                 vuint64m1x8_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei8_v_u64m2x2_tumu(vbool32_t mask,
                                                 vuint64m2x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei8_v_u64m2x3_tumu(vbool32_t mask,
                                                 vuint64m2x3_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei8_v_u64m2x4_tumu(vbool32_t mask,
                                                 vuint64m2x4_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei8_v_u64m4x2_tumu(vbool16_t mask,
                                                 vuint64m4x2_t maskedoff_tuple,
                                                 const uint64_t *base,
                                                 vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei16_v_u64m1x2_tumu(vbool64_t mask,
                                                  vuint64m1x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei16_v_u64m1x3_tumu(vbool64_t mask,
                                                  vuint64m1x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei16_v_u64m1x4_tumu(vbool64_t mask,
                                                  vuint64m1x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei16_v_u64m1x5_tumu(vbool64_t mask,
                                                  vuint64m1x5_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei16_v_u64m1x6_tumu(vbool64_t mask,
                                                  vuint64m1x6_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei16_v_u64m1x7_tumu(vbool64_t mask,
                                                  vuint64m1x7_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei16_v_u64m1x8_tumu(vbool64_t mask,
                                                  vuint64m1x8_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf4_t bindex,
                                                  size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei16_v_u64m2x2_tumu(vbool32_t mask,
                                                  vuint64m2x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei16_v_u64m2x3_tumu(vbool32_t mask,
                                                  vuint64m2x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei16_v_u64m2x4_tumu(vbool32_t mask,
                                                  vuint64m2x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16mf2_t bindex,
                                                  size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei16_v_u64m4x2_tumu(vbool16_t mask,
                                                  vuint64m4x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint16m1_t bindex,
                                                  size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei32_v_u64m1x2_tumu(vbool64_t mask,
                                                  vuint64m1x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei32_v_u64m1x3_tumu(vbool64_t mask,
                                                  vuint64m1x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei32_v_u64m1x4_tumu(vbool64_t mask,
                                                  vuint64m1x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei32_v_u64m1x5_tumu(vbool64_t mask,
                                                  vuint64m1x5_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei32_v_u64m1x6_tumu(vbool64_t mask,
                                                  vuint64m1x6_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei32_v_u64m1x7_tumu(vbool64_t mask,
                                                  vuint64m1x7_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei32_v_u64m1x8_tumu(vbool64_t mask,
                                                  vuint64m1x8_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32mf2_t bindex,
                                                  size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei32_v_u64m2x2_tumu(vbool32_t mask,
                                                  vuint64m2x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei32_v_u64m2x3_tumu(vbool32_t mask,
                                                  vuint64m2x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei32_v_u64m2x4_tumu(vbool32_t mask,
                                                  vuint64m2x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m1_t bindex,
                                                  size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei32_v_u64m4x2_tumu(vbool16_t mask,
                                                  vuint64m4x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint32m2_t bindex,
                                                  size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei64_v_u64m1x2_tumu(vbool64_t mask,
                                                  vuint64m1x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei64_v_u64m1x3_tumu(vbool64_t mask,
                                                  vuint64m1x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei64_v_u64m1x4_tumu(vbool64_t mask,
                                                  vuint64m1x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei64_v_u64m1x5_tumu(vbool64_t mask,
                                                  vuint64m1x5_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei64_v_u64m1x6_tumu(vbool64_t mask,
                                                  vuint64m1x6_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei64_v_u64m1x7_tumu(vbool64_t mask,
                                                  vuint64m1x7_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei64_v_u64m1x8_tumu(vbool64_t mask,
                                                  vuint64m1x8_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m1_t bindex,
                                                  size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei64_v_u64m2x2_tumu(vbool32_t mask,
                                                  vuint64m2x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei64_v_u64m2x3_tumu(vbool32_t mask,
                                                  vuint64m2x3_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei64_v_u64m2x4_tumu(vbool32_t mask,
                                                  vuint64m2x4_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m2_t bindex,
                                                  size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei64_v_u64m4x2_tumu(vbool16_t mask,
                                                  vuint64m4x2_t maskedoff_tuple,
                                                  const uint64_t *base,
                                                  vuint64m4_t bindex,
                                                  size_t vl);
// masked functions
vfloat16mf4x2_t __riscv_vloxseg2ei8_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei8_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei8_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei8_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei8_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei8_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei8_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei8_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei8_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei8_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei8_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei8_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei8_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei8_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei8_v_f16m1x2_mu(vbool16_t mask,
                                                vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei8_v_f16m1x3_mu(vbool16_t mask,
                                                vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei8_v_f16m1x4_mu(vbool16_t mask,
                                                vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei8_v_f16m1x5_mu(vbool16_t mask,
                                                vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei8_v_f16m1x6_mu(vbool16_t mask,
                                                vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei8_v_f16m1x7_mu(vbool16_t mask,
                                                vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei8_v_f16m1x8_mu(vbool16_t mask,
                                                vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei8_v_f16m2x2_mu(vbool8_t mask,
                                                vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei8_v_f16m2x3_mu(vbool8_t mask,
                                                vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei8_v_f16m2x4_mu(vbool8_t mask,
                                                vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei8_v_f16m4x2_mu(vbool4_t mask,
                                                vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei16_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei16_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei16_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei16_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei16_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei16_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei16_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei16_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei16_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei16_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei16_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei16_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei16_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei16_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei16_v_f16m1x2_mu(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei16_v_f16m1x3_mu(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei16_v_f16m1x4_mu(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei16_v_f16m1x5_mu(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei16_v_f16m1x6_mu(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei16_v_f16m1x7_mu(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei16_v_f16m1x8_mu(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei16_v_f16m2x2_mu(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei16_v_f16m2x3_mu(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei16_v_f16m2x4_mu(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei16_v_f16m4x2_mu(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei32_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei32_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei32_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei32_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei32_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei32_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei32_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei32_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei32_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei32_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei32_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei32_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei32_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei32_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei32_v_f16m1x2_mu(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei32_v_f16m1x3_mu(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei32_v_f16m1x4_mu(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei32_v_f16m1x5_mu(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei32_v_f16m1x6_mu(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei32_v_f16m1x7_mu(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei32_v_f16m1x8_mu(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei32_v_f16m2x2_mu(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei32_v_f16m2x3_mu(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei32_v_f16m2x4_mu(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vloxseg2ei32_v_f16m4x2_mu(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vloxseg2ei64_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vloxseg3ei64_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vloxseg4ei64_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vloxseg5ei64_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vloxseg6ei64_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vloxseg7ei64_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vloxseg8ei64_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vloxseg2ei64_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vloxseg3ei64_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vloxseg4ei64_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vloxseg5ei64_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vloxseg6ei64_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vloxseg7ei64_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vloxseg8ei64_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vloxseg2ei64_v_f16m1x2_mu(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vloxseg3ei64_v_f16m1x3_mu(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vloxseg4ei64_v_f16m1x4_mu(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vloxseg5ei64_v_f16m1x5_mu(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vloxseg6ei64_v_f16m1x6_mu(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vloxseg7ei64_v_f16m1x7_mu(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vloxseg8ei64_v_f16m1x8_mu(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vloxseg2ei64_v_f16m2x2_mu(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vloxseg3ei64_v_f16m2x3_mu(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vloxseg4ei64_v_f16m2x4_mu(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei8_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei8_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei8_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei8_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei8_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei8_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei8_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei8_v_f32m1x2_mu(vbool32_t mask,
                                                vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei8_v_f32m1x3_mu(vbool32_t mask,
                                                vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei8_v_f32m1x4_mu(vbool32_t mask,
                                                vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei8_v_f32m1x5_mu(vbool32_t mask,
                                                vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei8_v_f32m1x6_mu(vbool32_t mask,
                                                vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei8_v_f32m1x7_mu(vbool32_t mask,
                                                vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei8_v_f32m1x8_mu(vbool32_t mask,
                                                vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei8_v_f32m2x2_mu(vbool16_t mask,
                                                vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei8_v_f32m2x3_mu(vbool16_t mask,
                                                vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei8_v_f32m2x4_mu(vbool16_t mask,
                                                vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei8_v_f32m4x2_mu(vbool8_t mask,
                                                vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei16_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei16_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei16_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei16_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei16_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei16_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei16_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei16_v_f32m1x2_mu(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei16_v_f32m1x3_mu(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei16_v_f32m1x4_mu(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei16_v_f32m1x5_mu(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei16_v_f32m1x6_mu(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei16_v_f32m1x7_mu(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei16_v_f32m1x8_mu(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei16_v_f32m2x2_mu(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei16_v_f32m2x3_mu(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei16_v_f32m2x4_mu(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei16_v_f32m4x2_mu(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei32_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei32_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei32_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei32_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei32_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei32_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei32_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei32_v_f32m1x2_mu(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei32_v_f32m1x3_mu(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei32_v_f32m1x4_mu(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei32_v_f32m1x5_mu(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei32_v_f32m1x6_mu(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei32_v_f32m1x7_mu(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei32_v_f32m1x8_mu(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei32_v_f32m2x2_mu(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei32_v_f32m2x3_mu(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei32_v_f32m2x4_mu(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei32_v_f32m4x2_mu(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vloxseg2ei64_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vloxseg3ei64_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vloxseg4ei64_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vloxseg5ei64_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vloxseg6ei64_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vloxseg7ei64_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vloxseg8ei64_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vloxseg2ei64_v_f32m1x2_mu(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vloxseg3ei64_v_f32m1x3_mu(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vloxseg4ei64_v_f32m1x4_mu(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vloxseg5ei64_v_f32m1x5_mu(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vloxseg6ei64_v_f32m1x6_mu(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vloxseg7ei64_v_f32m1x7_mu(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vloxseg8ei64_v_f32m1x8_mu(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vloxseg2ei64_v_f32m2x2_mu(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vloxseg3ei64_v_f32m2x3_mu(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vloxseg4ei64_v_f32m2x4_mu(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vloxseg2ei64_v_f32m4x2_mu(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei8_v_f64m1x2_mu(vbool64_t mask,
                                                vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei8_v_f64m1x3_mu(vbool64_t mask,
                                                vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei8_v_f64m1x4_mu(vbool64_t mask,
                                                vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei8_v_f64m1x5_mu(vbool64_t mask,
                                                vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei8_v_f64m1x6_mu(vbool64_t mask,
                                                vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei8_v_f64m1x7_mu(vbool64_t mask,
                                                vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei8_v_f64m1x8_mu(vbool64_t mask,
                                                vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei8_v_f64m2x2_mu(vbool32_t mask,
                                                vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei8_v_f64m2x3_mu(vbool32_t mask,
                                                vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei8_v_f64m2x4_mu(vbool32_t mask,
                                                vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei8_v_f64m4x2_mu(vbool16_t mask,
                                                vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei16_v_f64m1x2_mu(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei16_v_f64m1x3_mu(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei16_v_f64m1x4_mu(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei16_v_f64m1x5_mu(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei16_v_f64m1x6_mu(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei16_v_f64m1x7_mu(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei16_v_f64m1x8_mu(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei16_v_f64m2x2_mu(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei16_v_f64m2x3_mu(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei16_v_f64m2x4_mu(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei16_v_f64m4x2_mu(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei32_v_f64m1x2_mu(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei32_v_f64m1x3_mu(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei32_v_f64m1x4_mu(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei32_v_f64m1x5_mu(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei32_v_f64m1x6_mu(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei32_v_f64m1x7_mu(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei32_v_f64m1x8_mu(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei32_v_f64m2x2_mu(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei32_v_f64m2x3_mu(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei32_v_f64m2x4_mu(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei32_v_f64m4x2_mu(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vloxseg2ei64_v_f64m1x2_mu(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vloxseg3ei64_v_f64m1x3_mu(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vloxseg4ei64_v_f64m1x4_mu(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vloxseg5ei64_v_f64m1x5_mu(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vloxseg6ei64_v_f64m1x6_mu(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vloxseg7ei64_v_f64m1x7_mu(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vloxseg8ei64_v_f64m1x8_mu(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vloxseg2ei64_v_f64m2x2_mu(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vloxseg3ei64_v_f64m2x3_mu(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vloxseg4ei64_v_f64m2x4_mu(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vloxseg2ei64_v_f64m4x2_mu(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei8_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei8_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei8_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei8_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei8_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei8_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei8_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei8_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei8_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei8_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei8_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei8_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei8_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei8_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint8mf4_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei8_v_f16m1x2_mu(vbool16_t mask,
                                                vfloat16m1x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei8_v_f16m1x3_mu(vbool16_t mask,
                                                vfloat16m1x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei8_v_f16m1x4_mu(vbool16_t mask,
                                                vfloat16m1x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei8_v_f16m1x5_mu(vbool16_t mask,
                                                vfloat16m1x5_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei8_v_f16m1x6_mu(vbool16_t mask,
                                                vfloat16m1x6_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei8_v_f16m1x7_mu(vbool16_t mask,
                                                vfloat16m1x7_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei8_v_f16m1x8_mu(vbool16_t mask,
                                                vfloat16m1x8_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei8_v_f16m2x2_mu(vbool8_t mask,
                                                vfloat16m2x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei8_v_f16m2x3_mu(vbool8_t mask,
                                                vfloat16m2x3_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei8_v_f16m2x4_mu(vbool8_t mask,
                                                vfloat16m2x4_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei8_v_f16m4x2_mu(vbool4_t mask,
                                                vfloat16m4x2_t maskedoff_tuple,
                                                const float16_t *base,
                                                vuint8m2_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei16_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei16_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei16_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei16_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei16_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei16_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei16_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei16_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei16_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei16_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei16_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei16_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei16_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei16_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint16mf2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei16_v_f16m1x2_mu(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei16_v_f16m1x3_mu(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei16_v_f16m1x4_mu(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei16_v_f16m1x5_mu(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei16_v_f16m1x6_mu(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei16_v_f16m1x7_mu(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei16_v_f16m1x8_mu(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei16_v_f16m2x2_mu(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei16_v_f16m2x3_mu(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei16_v_f16m2x4_mu(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei16_v_f16m4x2_mu(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint16m4_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei32_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei32_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei32_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei32_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei32_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei32_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei32_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei32_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei32_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei32_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei32_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei32_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei32_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei32_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint32m1_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei32_v_f16m1x2_mu(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei32_v_f16m1x3_mu(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei32_v_f16m1x4_mu(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei32_v_f16m1x5_mu(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei32_v_f16m1x6_mu(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei32_v_f16m1x7_mu(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei32_v_f16m1x8_mu(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei32_v_f16m2x2_mu(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei32_v_f16m2x3_mu(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei32_v_f16m2x4_mu(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat16m4x2_t __riscv_vluxseg2ei32_v_f16m4x2_mu(vbool4_t mask,
                                                 vfloat16m4x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint32m8_t bindex, size_t vl);
vfloat16mf4x2_t __riscv_vluxseg2ei64_v_f16mf4x2_mu(
    vbool64_t mask, vfloat16mf4x2_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x3_t __riscv_vluxseg3ei64_v_f16mf4x3_mu(
    vbool64_t mask, vfloat16mf4x3_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x4_t __riscv_vluxseg4ei64_v_f16mf4x4_mu(
    vbool64_t mask, vfloat16mf4x4_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x5_t __riscv_vluxseg5ei64_v_f16mf4x5_mu(
    vbool64_t mask, vfloat16mf4x5_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x6_t __riscv_vluxseg6ei64_v_f16mf4x6_mu(
    vbool64_t mask, vfloat16mf4x6_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x7_t __riscv_vluxseg7ei64_v_f16mf4x7_mu(
    vbool64_t mask, vfloat16mf4x7_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf4x8_t __riscv_vluxseg8ei64_v_f16mf4x8_mu(
    vbool64_t mask, vfloat16mf4x8_t maskedoff_tuple, const float16_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat16mf2x2_t __riscv_vluxseg2ei64_v_f16mf2x2_mu(
    vbool32_t mask, vfloat16mf2x2_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x3_t __riscv_vluxseg3ei64_v_f16mf2x3_mu(
    vbool32_t mask, vfloat16mf2x3_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x4_t __riscv_vluxseg4ei64_v_f16mf2x4_mu(
    vbool32_t mask, vfloat16mf2x4_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x5_t __riscv_vluxseg5ei64_v_f16mf2x5_mu(
    vbool32_t mask, vfloat16mf2x5_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x6_t __riscv_vluxseg6ei64_v_f16mf2x6_mu(
    vbool32_t mask, vfloat16mf2x6_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x7_t __riscv_vluxseg7ei64_v_f16mf2x7_mu(
    vbool32_t mask, vfloat16mf2x7_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16mf2x8_t __riscv_vluxseg8ei64_v_f16mf2x8_mu(
    vbool32_t mask, vfloat16mf2x8_t maskedoff_tuple, const float16_t *base,
    vuint64m2_t bindex, size_t vl);
vfloat16m1x2_t __riscv_vluxseg2ei64_v_f16m1x2_mu(vbool16_t mask,
                                                 vfloat16m1x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x3_t __riscv_vluxseg3ei64_v_f16m1x3_mu(vbool16_t mask,
                                                 vfloat16m1x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x4_t __riscv_vluxseg4ei64_v_f16m1x4_mu(vbool16_t mask,
                                                 vfloat16m1x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x5_t __riscv_vluxseg5ei64_v_f16m1x5_mu(vbool16_t mask,
                                                 vfloat16m1x5_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x6_t __riscv_vluxseg6ei64_v_f16m1x6_mu(vbool16_t mask,
                                                 vfloat16m1x6_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x7_t __riscv_vluxseg7ei64_v_f16m1x7_mu(vbool16_t mask,
                                                 vfloat16m1x7_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m1x8_t __riscv_vluxseg8ei64_v_f16m1x8_mu(vbool16_t mask,
                                                 vfloat16m1x8_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat16m2x2_t __riscv_vluxseg2ei64_v_f16m2x2_mu(vbool8_t mask,
                                                 vfloat16m2x2_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x3_t __riscv_vluxseg3ei64_v_f16m2x3_mu(vbool8_t mask,
                                                 vfloat16m2x3_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat16m2x4_t __riscv_vluxseg4ei64_v_f16m2x4_mu(vbool8_t mask,
                                                 vfloat16m2x4_t maskedoff_tuple,
                                                 const float16_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei8_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei8_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei8_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei8_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei8_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei8_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei8_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint8mf8_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei8_v_f32m1x2_mu(vbool32_t mask,
                                                vfloat32m1x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei8_v_f32m1x3_mu(vbool32_t mask,
                                                vfloat32m1x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei8_v_f32m1x4_mu(vbool32_t mask,
                                                vfloat32m1x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei8_v_f32m1x5_mu(vbool32_t mask,
                                                vfloat32m1x5_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei8_v_f32m1x6_mu(vbool32_t mask,
                                                vfloat32m1x6_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei8_v_f32m1x7_mu(vbool32_t mask,
                                                vfloat32m1x7_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei8_v_f32m1x8_mu(vbool32_t mask,
                                                vfloat32m1x8_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei8_v_f32m2x2_mu(vbool16_t mask,
                                                vfloat32m2x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei8_v_f32m2x3_mu(vbool16_t mask,
                                                vfloat32m2x3_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei8_v_f32m2x4_mu(vbool16_t mask,
                                                vfloat32m2x4_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei8_v_f32m4x2_mu(vbool8_t mask,
                                                vfloat32m4x2_t maskedoff_tuple,
                                                const float32_t *base,
                                                vuint8m1_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei16_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei16_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei16_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei16_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei16_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei16_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei16_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint16mf4_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei16_v_f32m1x2_mu(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei16_v_f32m1x3_mu(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei16_v_f32m1x4_mu(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei16_v_f32m1x5_mu(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei16_v_f32m1x6_mu(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei16_v_f32m1x7_mu(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei16_v_f32m1x8_mu(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei16_v_f32m2x2_mu(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei16_v_f32m2x3_mu(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei16_v_f32m2x4_mu(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei16_v_f32m4x2_mu(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint16m2_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei32_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei32_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei32_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei32_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei32_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei32_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei32_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint32mf2_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei32_v_f32m1x2_mu(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei32_v_f32m1x3_mu(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei32_v_f32m1x4_mu(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei32_v_f32m1x5_mu(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei32_v_f32m1x6_mu(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei32_v_f32m1x7_mu(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei32_v_f32m1x8_mu(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei32_v_f32m2x2_mu(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei32_v_f32m2x3_mu(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei32_v_f32m2x4_mu(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei32_v_f32m4x2_mu(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint32m4_t bindex, size_t vl);
vfloat32mf2x2_t __riscv_vluxseg2ei64_v_f32mf2x2_mu(
    vbool64_t mask, vfloat32mf2x2_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x3_t __riscv_vluxseg3ei64_v_f32mf2x3_mu(
    vbool64_t mask, vfloat32mf2x3_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x4_t __riscv_vluxseg4ei64_v_f32mf2x4_mu(
    vbool64_t mask, vfloat32mf2x4_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x5_t __riscv_vluxseg5ei64_v_f32mf2x5_mu(
    vbool64_t mask, vfloat32mf2x5_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x6_t __riscv_vluxseg6ei64_v_f32mf2x6_mu(
    vbool64_t mask, vfloat32mf2x6_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x7_t __riscv_vluxseg7ei64_v_f32mf2x7_mu(
    vbool64_t mask, vfloat32mf2x7_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32mf2x8_t __riscv_vluxseg8ei64_v_f32mf2x8_mu(
    vbool64_t mask, vfloat32mf2x8_t maskedoff_tuple, const float32_t *base,
    vuint64m1_t bindex, size_t vl);
vfloat32m1x2_t __riscv_vluxseg2ei64_v_f32m1x2_mu(vbool32_t mask,
                                                 vfloat32m1x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x3_t __riscv_vluxseg3ei64_v_f32m1x3_mu(vbool32_t mask,
                                                 vfloat32m1x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x4_t __riscv_vluxseg4ei64_v_f32m1x4_mu(vbool32_t mask,
                                                 vfloat32m1x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x5_t __riscv_vluxseg5ei64_v_f32m1x5_mu(vbool32_t mask,
                                                 vfloat32m1x5_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x6_t __riscv_vluxseg6ei64_v_f32m1x6_mu(vbool32_t mask,
                                                 vfloat32m1x6_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x7_t __riscv_vluxseg7ei64_v_f32m1x7_mu(vbool32_t mask,
                                                 vfloat32m1x7_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m1x8_t __riscv_vluxseg8ei64_v_f32m1x8_mu(vbool32_t mask,
                                                 vfloat32m1x8_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat32m2x2_t __riscv_vluxseg2ei64_v_f32m2x2_mu(vbool16_t mask,
                                                 vfloat32m2x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x3_t __riscv_vluxseg3ei64_v_f32m2x3_mu(vbool16_t mask,
                                                 vfloat32m2x3_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m2x4_t __riscv_vluxseg4ei64_v_f32m2x4_mu(vbool16_t mask,
                                                 vfloat32m2x4_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vfloat32m4x2_t __riscv_vluxseg2ei64_v_f32m4x2_mu(vbool8_t mask,
                                                 vfloat32m4x2_t maskedoff_tuple,
                                                 const float32_t *base,
                                                 vuint64m8_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei8_v_f64m1x2_mu(vbool64_t mask,
                                                vfloat64m1x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei8_v_f64m1x3_mu(vbool64_t mask,
                                                vfloat64m1x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei8_v_f64m1x4_mu(vbool64_t mask,
                                                vfloat64m1x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei8_v_f64m1x5_mu(vbool64_t mask,
                                                vfloat64m1x5_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei8_v_f64m1x6_mu(vbool64_t mask,
                                                vfloat64m1x6_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei8_v_f64m1x7_mu(vbool64_t mask,
                                                vfloat64m1x7_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei8_v_f64m1x8_mu(vbool64_t mask,
                                                vfloat64m1x8_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei8_v_f64m2x2_mu(vbool32_t mask,
                                                vfloat64m2x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei8_v_f64m2x3_mu(vbool32_t mask,
                                                vfloat64m2x3_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei8_v_f64m2x4_mu(vbool32_t mask,
                                                vfloat64m2x4_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei8_v_f64m4x2_mu(vbool16_t mask,
                                                vfloat64m4x2_t maskedoff_tuple,
                                                const float64_t *base,
                                                vuint8mf2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei16_v_f64m1x2_mu(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei16_v_f64m1x3_mu(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei16_v_f64m1x4_mu(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei16_v_f64m1x5_mu(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei16_v_f64m1x6_mu(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei16_v_f64m1x7_mu(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei16_v_f64m1x8_mu(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei16_v_f64m2x2_mu(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei16_v_f64m2x3_mu(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei16_v_f64m2x4_mu(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei16_v_f64m4x2_mu(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint16m1_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei32_v_f64m1x2_mu(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei32_v_f64m1x3_mu(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei32_v_f64m1x4_mu(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei32_v_f64m1x5_mu(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei32_v_f64m1x6_mu(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei32_v_f64m1x7_mu(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei32_v_f64m1x8_mu(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei32_v_f64m2x2_mu(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei32_v_f64m2x3_mu(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei32_v_f64m2x4_mu(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei32_v_f64m4x2_mu(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint32m2_t bindex, size_t vl);
vfloat64m1x2_t __riscv_vluxseg2ei64_v_f64m1x2_mu(vbool64_t mask,
                                                 vfloat64m1x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x3_t __riscv_vluxseg3ei64_v_f64m1x3_mu(vbool64_t mask,
                                                 vfloat64m1x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x4_t __riscv_vluxseg4ei64_v_f64m1x4_mu(vbool64_t mask,
                                                 vfloat64m1x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x5_t __riscv_vluxseg5ei64_v_f64m1x5_mu(vbool64_t mask,
                                                 vfloat64m1x5_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x6_t __riscv_vluxseg6ei64_v_f64m1x6_mu(vbool64_t mask,
                                                 vfloat64m1x6_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x7_t __riscv_vluxseg7ei64_v_f64m1x7_mu(vbool64_t mask,
                                                 vfloat64m1x7_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m1x8_t __riscv_vluxseg8ei64_v_f64m1x8_mu(vbool64_t mask,
                                                 vfloat64m1x8_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vfloat64m2x2_t __riscv_vluxseg2ei64_v_f64m2x2_mu(vbool32_t mask,
                                                 vfloat64m2x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x3_t __riscv_vluxseg3ei64_v_f64m2x3_mu(vbool32_t mask,
                                                 vfloat64m2x3_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m2x4_t __riscv_vluxseg4ei64_v_f64m2x4_mu(vbool32_t mask,
                                                 vfloat64m2x4_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vfloat64m4x2_t __riscv_vluxseg2ei64_v_f64m4x2_mu(vbool16_t mask,
                                                 vfloat64m4x2_t maskedoff_tuple,
                                                 const float64_t *base,
                                                 vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei8_v_i8mf8x2_mu(vbool64_t mask,
                                              vint8mf8x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei8_v_i8mf8x3_mu(vbool64_t mask,
                                              vint8mf8x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei8_v_i8mf8x4_mu(vbool64_t mask,
                                              vint8mf8x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei8_v_i8mf8x5_mu(vbool64_t mask,
                                              vint8mf8x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei8_v_i8mf8x6_mu(vbool64_t mask,
                                              vint8mf8x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei8_v_i8mf8x7_mu(vbool64_t mask,
                                              vint8mf8x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei8_v_i8mf8x8_mu(vbool64_t mask,
                                              vint8mf8x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei8_v_i8mf4x2_mu(vbool32_t mask,
                                              vint8mf4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei8_v_i8mf4x3_mu(vbool32_t mask,
                                              vint8mf4x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei8_v_i8mf4x4_mu(vbool32_t mask,
                                              vint8mf4x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei8_v_i8mf4x5_mu(vbool32_t mask,
                                              vint8mf4x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei8_v_i8mf4x6_mu(vbool32_t mask,
                                              vint8mf4x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei8_v_i8mf4x7_mu(vbool32_t mask,
                                              vint8mf4x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei8_v_i8mf4x8_mu(vbool32_t mask,
                                              vint8mf4x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei8_v_i8mf2x2_mu(vbool16_t mask,
                                              vint8mf2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei8_v_i8mf2x3_mu(vbool16_t mask,
                                              vint8mf2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei8_v_i8mf2x4_mu(vbool16_t mask,
                                              vint8mf2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei8_v_i8mf2x5_mu(vbool16_t mask,
                                              vint8mf2x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei8_v_i8mf2x6_mu(vbool16_t mask,
                                              vint8mf2x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei8_v_i8mf2x7_mu(vbool16_t mask,
                                              vint8mf2x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei8_v_i8mf2x8_mu(vbool16_t mask,
                                              vint8mf2x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei8_v_i8m1x2_mu(vbool8_t mask,
                                            vint8m1x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei8_v_i8m1x3_mu(vbool8_t mask,
                                            vint8m1x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei8_v_i8m1x4_mu(vbool8_t mask,
                                            vint8m1x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei8_v_i8m1x5_mu(vbool8_t mask,
                                            vint8m1x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei8_v_i8m1x6_mu(vbool8_t mask,
                                            vint8m1x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei8_v_i8m1x7_mu(vbool8_t mask,
                                            vint8m1x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei8_v_i8m1x8_mu(vbool8_t mask,
                                            vint8m1x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei8_v_i8m2x2_mu(vbool4_t mask,
                                            vint8m2x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei8_v_i8m2x3_mu(vbool4_t mask,
                                            vint8m2x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei8_v_i8m2x4_mu(vbool4_t mask,
                                            vint8m2x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei8_v_i8m4x2_mu(vbool2_t mask,
                                            vint8m4x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei16_v_i8mf8x2_mu(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei16_v_i8mf8x3_mu(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei16_v_i8mf8x4_mu(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei16_v_i8mf8x5_mu(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei16_v_i8mf8x6_mu(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei16_v_i8mf8x7_mu(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei16_v_i8mf8x8_mu(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei16_v_i8mf4x2_mu(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei16_v_i8mf4x3_mu(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei16_v_i8mf4x4_mu(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei16_v_i8mf4x5_mu(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei16_v_i8mf4x6_mu(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei16_v_i8mf4x7_mu(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei16_v_i8mf4x8_mu(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei16_v_i8mf2x2_mu(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei16_v_i8mf2x3_mu(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei16_v_i8mf2x4_mu(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei16_v_i8mf2x5_mu(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei16_v_i8mf2x6_mu(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei16_v_i8mf2x7_mu(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei16_v_i8mf2x8_mu(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei16_v_i8m1x2_mu(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei16_v_i8m1x3_mu(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei16_v_i8m1x4_mu(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei16_v_i8m1x5_mu(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei16_v_i8m1x6_mu(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei16_v_i8m1x7_mu(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei16_v_i8m1x8_mu(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei16_v_i8m2x2_mu(vbool4_t mask,
                                             vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei16_v_i8m2x3_mu(vbool4_t mask,
                                             vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei16_v_i8m2x4_mu(vbool4_t mask,
                                             vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vloxseg2ei16_v_i8m4x2_mu(vbool2_t mask,
                                             vint8m4x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei32_v_i8mf8x2_mu(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei32_v_i8mf8x3_mu(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei32_v_i8mf8x4_mu(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei32_v_i8mf8x5_mu(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei32_v_i8mf8x6_mu(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei32_v_i8mf8x7_mu(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei32_v_i8mf8x8_mu(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei32_v_i8mf4x2_mu(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei32_v_i8mf4x3_mu(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei32_v_i8mf4x4_mu(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei32_v_i8mf4x5_mu(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei32_v_i8mf4x6_mu(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei32_v_i8mf4x7_mu(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei32_v_i8mf4x8_mu(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei32_v_i8mf2x2_mu(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei32_v_i8mf2x3_mu(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei32_v_i8mf2x4_mu(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei32_v_i8mf2x5_mu(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei32_v_i8mf2x6_mu(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei32_v_i8mf2x7_mu(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei32_v_i8mf2x8_mu(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei32_v_i8m1x2_mu(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei32_v_i8m1x3_mu(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei32_v_i8m1x4_mu(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei32_v_i8m1x5_mu(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei32_v_i8m1x6_mu(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei32_v_i8m1x7_mu(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei32_v_i8m1x8_mu(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vloxseg2ei32_v_i8m2x2_mu(vbool4_t mask,
                                             vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vloxseg3ei32_v_i8m2x3_mu(vbool4_t mask,
                                             vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vloxseg4ei32_v_i8m2x4_mu(vbool4_t mask,
                                             vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vloxseg2ei64_v_i8mf8x2_mu(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vloxseg3ei64_v_i8mf8x3_mu(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vloxseg4ei64_v_i8mf8x4_mu(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vloxseg5ei64_v_i8mf8x5_mu(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vloxseg6ei64_v_i8mf8x6_mu(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vloxseg7ei64_v_i8mf8x7_mu(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vloxseg8ei64_v_i8mf8x8_mu(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vloxseg2ei64_v_i8mf4x2_mu(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vloxseg3ei64_v_i8mf4x3_mu(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vloxseg4ei64_v_i8mf4x4_mu(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vloxseg5ei64_v_i8mf4x5_mu(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vloxseg6ei64_v_i8mf4x6_mu(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vloxseg7ei64_v_i8mf4x7_mu(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vloxseg8ei64_v_i8mf4x8_mu(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vloxseg2ei64_v_i8mf2x2_mu(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vloxseg3ei64_v_i8mf2x3_mu(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vloxseg4ei64_v_i8mf2x4_mu(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vloxseg5ei64_v_i8mf2x5_mu(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vloxseg6ei64_v_i8mf2x6_mu(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vloxseg7ei64_v_i8mf2x7_mu(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vloxseg8ei64_v_i8mf2x8_mu(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vloxseg2ei64_v_i8m1x2_mu(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vloxseg3ei64_v_i8m1x3_mu(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vloxseg4ei64_v_i8m1x4_mu(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vloxseg5ei64_v_i8m1x5_mu(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vloxseg6ei64_v_i8m1x6_mu(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vloxseg7ei64_v_i8m1x7_mu(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vloxseg8ei64_v_i8m1x8_mu(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei8_v_i16mf4x2_mu(vbool64_t mask,
                                                vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei8_v_i16mf4x3_mu(vbool64_t mask,
                                                vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei8_v_i16mf4x4_mu(vbool64_t mask,
                                                vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei8_v_i16mf4x5_mu(vbool64_t mask,
                                                vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei8_v_i16mf4x6_mu(vbool64_t mask,
                                                vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei8_v_i16mf4x7_mu(vbool64_t mask,
                                                vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei8_v_i16mf4x8_mu(vbool64_t mask,
                                                vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei8_v_i16mf2x2_mu(vbool32_t mask,
                                                vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei8_v_i16mf2x3_mu(vbool32_t mask,
                                                vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei8_v_i16mf2x4_mu(vbool32_t mask,
                                                vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei8_v_i16mf2x5_mu(vbool32_t mask,
                                                vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei8_v_i16mf2x6_mu(vbool32_t mask,
                                                vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei8_v_i16mf2x7_mu(vbool32_t mask,
                                                vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei8_v_i16mf2x8_mu(vbool32_t mask,
                                                vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei8_v_i16m1x2_mu(vbool16_t mask,
                                              vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei8_v_i16m1x3_mu(vbool16_t mask,
                                              vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei8_v_i16m1x4_mu(vbool16_t mask,
                                              vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei8_v_i16m1x5_mu(vbool16_t mask,
                                              vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei8_v_i16m1x6_mu(vbool16_t mask,
                                              vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei8_v_i16m1x7_mu(vbool16_t mask,
                                              vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei8_v_i16m1x8_mu(vbool16_t mask,
                                              vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei8_v_i16m2x2_mu(vbool8_t mask,
                                              vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei8_v_i16m2x3_mu(vbool8_t mask,
                                              vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei8_v_i16m2x4_mu(vbool8_t mask,
                                              vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei8_v_i16m4x2_mu(vbool4_t mask,
                                              vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei16_v_i16mf4x2_mu(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei16_v_i16mf4x3_mu(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei16_v_i16mf4x4_mu(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei16_v_i16mf4x5_mu(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei16_v_i16mf4x6_mu(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei16_v_i16mf4x7_mu(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei16_v_i16mf4x8_mu(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei16_v_i16mf2x2_mu(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei16_v_i16mf2x3_mu(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei16_v_i16mf2x4_mu(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei16_v_i16mf2x5_mu(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei16_v_i16mf2x6_mu(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei16_v_i16mf2x7_mu(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei16_v_i16mf2x8_mu(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16m1x2_t __riscv_vloxseg2ei16_v_i16m1x2_mu(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei16_v_i16m1x3_mu(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei16_v_i16m1x4_mu(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei16_v_i16m1x5_mu(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei16_v_i16m1x6_mu(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei16_v_i16m1x7_mu(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei16_v_i16m1x8_mu(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei16_v_i16m2x2_mu(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei16_v_i16m2x3_mu(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei16_v_i16m2x4_mu(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei16_v_i16m4x2_mu(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei32_v_i16mf4x2_mu(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei32_v_i16mf4x3_mu(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei32_v_i16mf4x4_mu(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei32_v_i16mf4x5_mu(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei32_v_i16mf4x6_mu(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei32_v_i16mf4x7_mu(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei32_v_i16mf4x8_mu(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei32_v_i16mf2x2_mu(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei32_v_i16mf2x3_mu(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei32_v_i16mf2x4_mu(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei32_v_i16mf2x5_mu(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei32_v_i16mf2x6_mu(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei32_v_i16mf2x7_mu(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei32_v_i16mf2x8_mu(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei32_v_i16m1x2_mu(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei32_v_i16m1x3_mu(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei32_v_i16m1x4_mu(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei32_v_i16m1x5_mu(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei32_v_i16m1x6_mu(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei32_v_i16m1x7_mu(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei32_v_i16m1x8_mu(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei32_v_i16m2x2_mu(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei32_v_i16m2x3_mu(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei32_v_i16m2x4_mu(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vloxseg2ei32_v_i16m4x2_mu(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vloxseg2ei64_v_i16mf4x2_mu(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x3_t __riscv_vloxseg3ei64_v_i16mf4x3_mu(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x4_t __riscv_vloxseg4ei64_v_i16mf4x4_mu(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x5_t __riscv_vloxseg5ei64_v_i16mf4x5_mu(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x6_t __riscv_vloxseg6ei64_v_i16mf4x6_mu(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x7_t __riscv_vloxseg7ei64_v_i16mf4x7_mu(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x8_t __riscv_vloxseg8ei64_v_i16mf4x8_mu(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf2x2_t __riscv_vloxseg2ei64_v_i16mf2x2_mu(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vloxseg3ei64_v_i16mf2x3_mu(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vloxseg4ei64_v_i16mf2x4_mu(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vloxseg5ei64_v_i16mf2x5_mu(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vloxseg6ei64_v_i16mf2x6_mu(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vloxseg7ei64_v_i16mf2x7_mu(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vloxseg8ei64_v_i16mf2x8_mu(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16m1x2_t __riscv_vloxseg2ei64_v_i16m1x2_mu(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vloxseg3ei64_v_i16m1x3_mu(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vloxseg4ei64_v_i16m1x4_mu(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vloxseg5ei64_v_i16m1x5_mu(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vloxseg6ei64_v_i16m1x6_mu(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vloxseg7ei64_v_i16m1x7_mu(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vloxseg8ei64_v_i16m1x8_mu(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vloxseg2ei64_v_i16m2x2_mu(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vloxseg3ei64_v_i16m2x3_mu(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vloxseg4ei64_v_i16m2x4_mu(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei8_v_i32mf2x2_mu(vbool64_t mask,
                                                vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei8_v_i32mf2x3_mu(vbool64_t mask,
                                                vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei8_v_i32mf2x4_mu(vbool64_t mask,
                                                vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei8_v_i32mf2x5_mu(vbool64_t mask,
                                                vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei8_v_i32mf2x6_mu(vbool64_t mask,
                                                vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei8_v_i32mf2x7_mu(vbool64_t mask,
                                                vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei8_v_i32mf2x8_mu(vbool64_t mask,
                                                vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei8_v_i32m1x2_mu(vbool32_t mask,
                                              vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei8_v_i32m1x3_mu(vbool32_t mask,
                                              vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei8_v_i32m1x4_mu(vbool32_t mask,
                                              vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei8_v_i32m1x5_mu(vbool32_t mask,
                                              vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei8_v_i32m1x6_mu(vbool32_t mask,
                                              vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei8_v_i32m1x7_mu(vbool32_t mask,
                                              vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei8_v_i32m1x8_mu(vbool32_t mask,
                                              vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei8_v_i32m2x2_mu(vbool16_t mask,
                                              vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei8_v_i32m2x3_mu(vbool16_t mask,
                                              vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei8_v_i32m2x4_mu(vbool16_t mask,
                                              vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei8_v_i32m4x2_mu(vbool8_t mask,
                                              vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei16_v_i32mf2x2_mu(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei16_v_i32mf2x3_mu(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei16_v_i32mf2x4_mu(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei16_v_i32mf2x5_mu(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei16_v_i32mf2x6_mu(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei16_v_i32mf2x7_mu(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei16_v_i32mf2x8_mu(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vloxseg2ei16_v_i32m1x2_mu(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei16_v_i32m1x3_mu(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei16_v_i32m1x4_mu(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei16_v_i32m1x5_mu(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei16_v_i32m1x6_mu(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei16_v_i32m1x7_mu(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei16_v_i32m1x8_mu(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei16_v_i32m2x2_mu(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei16_v_i32m2x3_mu(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei16_v_i32m2x4_mu(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei16_v_i32m4x2_mu(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei32_v_i32mf2x2_mu(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei32_v_i32mf2x3_mu(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei32_v_i32mf2x4_mu(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei32_v_i32mf2x5_mu(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei32_v_i32mf2x6_mu(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei32_v_i32mf2x7_mu(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei32_v_i32mf2x8_mu(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vloxseg2ei32_v_i32m1x2_mu(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei32_v_i32m1x3_mu(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei32_v_i32m1x4_mu(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei32_v_i32m1x5_mu(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei32_v_i32m1x6_mu(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei32_v_i32m1x7_mu(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei32_v_i32m1x8_mu(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei32_v_i32m2x2_mu(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei32_v_i32m2x3_mu(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei32_v_i32m2x4_mu(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei32_v_i32m4x2_mu(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vloxseg2ei64_v_i32mf2x2_mu(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x3_t __riscv_vloxseg3ei64_v_i32mf2x3_mu(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x4_t __riscv_vloxseg4ei64_v_i32mf2x4_mu(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x5_t __riscv_vloxseg5ei64_v_i32mf2x5_mu(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x6_t __riscv_vloxseg6ei64_v_i32mf2x6_mu(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x7_t __riscv_vloxseg7ei64_v_i32mf2x7_mu(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x8_t __riscv_vloxseg8ei64_v_i32mf2x8_mu(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32m1x2_t __riscv_vloxseg2ei64_v_i32m1x2_mu(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vloxseg3ei64_v_i32m1x3_mu(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vloxseg4ei64_v_i32m1x4_mu(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vloxseg5ei64_v_i32m1x5_mu(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vloxseg6ei64_v_i32m1x6_mu(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vloxseg7ei64_v_i32m1x7_mu(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vloxseg8ei64_v_i32m1x8_mu(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vloxseg2ei64_v_i32m2x2_mu(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vloxseg3ei64_v_i32m2x3_mu(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vloxseg4ei64_v_i32m2x4_mu(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vloxseg2ei64_v_i32m4x2_mu(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei8_v_i64m1x2_mu(vbool64_t mask,
                                              vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei8_v_i64m1x3_mu(vbool64_t mask,
                                              vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei8_v_i64m1x4_mu(vbool64_t mask,
                                              vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei8_v_i64m1x5_mu(vbool64_t mask,
                                              vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei8_v_i64m1x6_mu(vbool64_t mask,
                                              vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei8_v_i64m1x7_mu(vbool64_t mask,
                                              vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei8_v_i64m1x8_mu(vbool64_t mask,
                                              vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei8_v_i64m2x2_mu(vbool32_t mask,
                                              vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei8_v_i64m2x3_mu(vbool32_t mask,
                                              vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei8_v_i64m2x4_mu(vbool32_t mask,
                                              vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei8_v_i64m4x2_mu(vbool16_t mask,
                                              vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei16_v_i64m1x2_mu(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei16_v_i64m1x3_mu(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei16_v_i64m1x4_mu(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei16_v_i64m1x5_mu(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei16_v_i64m1x6_mu(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei16_v_i64m1x7_mu(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei16_v_i64m1x8_mu(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei16_v_i64m2x2_mu(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei16_v_i64m2x3_mu(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei16_v_i64m2x4_mu(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei16_v_i64m4x2_mu(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei32_v_i64m1x2_mu(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei32_v_i64m1x3_mu(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei32_v_i64m1x4_mu(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei32_v_i64m1x5_mu(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei32_v_i64m1x6_mu(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei32_v_i64m1x7_mu(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei32_v_i64m1x8_mu(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei32_v_i64m2x2_mu(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei32_v_i64m2x3_mu(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei32_v_i64m2x4_mu(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei32_v_i64m4x2_mu(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vloxseg2ei64_v_i64m1x2_mu(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vloxseg3ei64_v_i64m1x3_mu(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vloxseg4ei64_v_i64m1x4_mu(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vloxseg5ei64_v_i64m1x5_mu(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vloxseg6ei64_v_i64m1x6_mu(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vloxseg7ei64_v_i64m1x7_mu(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vloxseg8ei64_v_i64m1x8_mu(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vloxseg2ei64_v_i64m2x2_mu(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vloxseg3ei64_v_i64m2x3_mu(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vloxseg4ei64_v_i64m2x4_mu(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vloxseg2ei64_v_i64m4x2_mu(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei8_v_i8mf8x2_mu(vbool64_t mask,
                                              vint8mf8x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei8_v_i8mf8x3_mu(vbool64_t mask,
                                              vint8mf8x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei8_v_i8mf8x4_mu(vbool64_t mask,
                                              vint8mf8x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei8_v_i8mf8x5_mu(vbool64_t mask,
                                              vint8mf8x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei8_v_i8mf8x6_mu(vbool64_t mask,
                                              vint8mf8x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei8_v_i8mf8x7_mu(vbool64_t mask,
                                              vint8mf8x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei8_v_i8mf8x8_mu(vbool64_t mask,
                                              vint8mf8x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei8_v_i8mf4x2_mu(vbool32_t mask,
                                              vint8mf4x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei8_v_i8mf4x3_mu(vbool32_t mask,
                                              vint8mf4x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei8_v_i8mf4x4_mu(vbool32_t mask,
                                              vint8mf4x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei8_v_i8mf4x5_mu(vbool32_t mask,
                                              vint8mf4x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei8_v_i8mf4x6_mu(vbool32_t mask,
                                              vint8mf4x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei8_v_i8mf4x7_mu(vbool32_t mask,
                                              vint8mf4x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei8_v_i8mf4x8_mu(vbool32_t mask,
                                              vint8mf4x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei8_v_i8mf2x2_mu(vbool16_t mask,
                                              vint8mf2x2_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei8_v_i8mf2x3_mu(vbool16_t mask,
                                              vint8mf2x3_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei8_v_i8mf2x4_mu(vbool16_t mask,
                                              vint8mf2x4_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei8_v_i8mf2x5_mu(vbool16_t mask,
                                              vint8mf2x5_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei8_v_i8mf2x6_mu(vbool16_t mask,
                                              vint8mf2x6_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei8_v_i8mf2x7_mu(vbool16_t mask,
                                              vint8mf2x7_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei8_v_i8mf2x8_mu(vbool16_t mask,
                                              vint8mf2x8_t maskedoff_tuple,
                                              const int8_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei8_v_i8m1x2_mu(vbool8_t mask,
                                            vint8m1x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei8_v_i8m1x3_mu(vbool8_t mask,
                                            vint8m1x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei8_v_i8m1x4_mu(vbool8_t mask,
                                            vint8m1x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei8_v_i8m1x5_mu(vbool8_t mask,
                                            vint8m1x5_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei8_v_i8m1x6_mu(vbool8_t mask,
                                            vint8m1x6_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei8_v_i8m1x7_mu(vbool8_t mask,
                                            vint8m1x7_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei8_v_i8m1x8_mu(vbool8_t mask,
                                            vint8m1x8_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m1_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei8_v_i8m2x2_mu(vbool4_t mask,
                                            vint8m2x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei8_v_i8m2x3_mu(vbool4_t mask,
                                            vint8m2x3_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei8_v_i8m2x4_mu(vbool4_t mask,
                                            vint8m2x4_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m2_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei8_v_i8m4x2_mu(vbool2_t mask,
                                            vint8m4x2_t maskedoff_tuple,
                                            const int8_t *base,
                                            vuint8m4_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei16_v_i8mf8x2_mu(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei16_v_i8mf8x3_mu(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei16_v_i8mf8x4_mu(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei16_v_i8mf8x5_mu(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei16_v_i8mf8x6_mu(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei16_v_i8mf8x7_mu(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei16_v_i8mf8x8_mu(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei16_v_i8mf4x2_mu(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei16_v_i8mf4x3_mu(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei16_v_i8mf4x4_mu(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei16_v_i8mf4x5_mu(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei16_v_i8mf4x6_mu(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei16_v_i8mf4x7_mu(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei16_v_i8mf4x8_mu(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei16_v_i8mf2x2_mu(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei16_v_i8mf2x3_mu(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei16_v_i8mf2x4_mu(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei16_v_i8mf2x5_mu(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei16_v_i8mf2x6_mu(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei16_v_i8mf2x7_mu(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei16_v_i8mf2x8_mu(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei16_v_i8m1x2_mu(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei16_v_i8m1x3_mu(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei16_v_i8m1x4_mu(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei16_v_i8m1x5_mu(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei16_v_i8m1x6_mu(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei16_v_i8m1x7_mu(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei16_v_i8m1x8_mu(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m2_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei16_v_i8m2x2_mu(vbool4_t mask,
                                             vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei16_v_i8m2x3_mu(vbool4_t mask,
                                             vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei16_v_i8m2x4_mu(vbool4_t mask,
                                             vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m4_t bindex, size_t vl);
vint8m4x2_t __riscv_vluxseg2ei16_v_i8m4x2_mu(vbool2_t mask,
                                             vint8m4x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint16m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei32_v_i8mf8x2_mu(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei32_v_i8mf8x3_mu(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei32_v_i8mf8x4_mu(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei32_v_i8mf8x5_mu(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei32_v_i8mf8x6_mu(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei32_v_i8mf8x7_mu(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei32_v_i8mf8x8_mu(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei32_v_i8mf4x2_mu(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei32_v_i8mf4x3_mu(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei32_v_i8mf4x4_mu(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei32_v_i8mf4x5_mu(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei32_v_i8mf4x6_mu(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei32_v_i8mf4x7_mu(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei32_v_i8mf4x8_mu(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei32_v_i8mf2x2_mu(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei32_v_i8mf2x3_mu(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei32_v_i8mf2x4_mu(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei32_v_i8mf2x5_mu(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei32_v_i8mf2x6_mu(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei32_v_i8mf2x7_mu(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei32_v_i8mf2x8_mu(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei32_v_i8m1x2_mu(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei32_v_i8m1x3_mu(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei32_v_i8m1x4_mu(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei32_v_i8m1x5_mu(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei32_v_i8m1x6_mu(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei32_v_i8m1x7_mu(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei32_v_i8m1x8_mu(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m4_t bindex, size_t vl);
vint8m2x2_t __riscv_vluxseg2ei32_v_i8m2x2_mu(vbool4_t mask,
                                             vint8m2x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x3_t __riscv_vluxseg3ei32_v_i8m2x3_mu(vbool4_t mask,
                                             vint8m2x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8m2x4_t __riscv_vluxseg4ei32_v_i8m2x4_mu(vbool4_t mask,
                                             vint8m2x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint32m8_t bindex, size_t vl);
vint8mf8x2_t __riscv_vluxseg2ei64_v_i8mf8x2_mu(vbool64_t mask,
                                               vint8mf8x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x3_t __riscv_vluxseg3ei64_v_i8mf8x3_mu(vbool64_t mask,
                                               vint8mf8x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x4_t __riscv_vluxseg4ei64_v_i8mf8x4_mu(vbool64_t mask,
                                               vint8mf8x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x5_t __riscv_vluxseg5ei64_v_i8mf8x5_mu(vbool64_t mask,
                                               vint8mf8x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x6_t __riscv_vluxseg6ei64_v_i8mf8x6_mu(vbool64_t mask,
                                               vint8mf8x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x7_t __riscv_vluxseg7ei64_v_i8mf8x7_mu(vbool64_t mask,
                                               vint8mf8x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf8x8_t __riscv_vluxseg8ei64_v_i8mf8x8_mu(vbool64_t mask,
                                               vint8mf8x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint8mf4x2_t __riscv_vluxseg2ei64_v_i8mf4x2_mu(vbool32_t mask,
                                               vint8mf4x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x3_t __riscv_vluxseg3ei64_v_i8mf4x3_mu(vbool32_t mask,
                                               vint8mf4x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x4_t __riscv_vluxseg4ei64_v_i8mf4x4_mu(vbool32_t mask,
                                               vint8mf4x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x5_t __riscv_vluxseg5ei64_v_i8mf4x5_mu(vbool32_t mask,
                                               vint8mf4x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x6_t __riscv_vluxseg6ei64_v_i8mf4x6_mu(vbool32_t mask,
                                               vint8mf4x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x7_t __riscv_vluxseg7ei64_v_i8mf4x7_mu(vbool32_t mask,
                                               vint8mf4x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf4x8_t __riscv_vluxseg8ei64_v_i8mf4x8_mu(vbool32_t mask,
                                               vint8mf4x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint8mf2x2_t __riscv_vluxseg2ei64_v_i8mf2x2_mu(vbool16_t mask,
                                               vint8mf2x2_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x3_t __riscv_vluxseg3ei64_v_i8mf2x3_mu(vbool16_t mask,
                                               vint8mf2x3_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x4_t __riscv_vluxseg4ei64_v_i8mf2x4_mu(vbool16_t mask,
                                               vint8mf2x4_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x5_t __riscv_vluxseg5ei64_v_i8mf2x5_mu(vbool16_t mask,
                                               vint8mf2x5_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x6_t __riscv_vluxseg6ei64_v_i8mf2x6_mu(vbool16_t mask,
                                               vint8mf2x6_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x7_t __riscv_vluxseg7ei64_v_i8mf2x7_mu(vbool16_t mask,
                                               vint8mf2x7_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8mf2x8_t __riscv_vluxseg8ei64_v_i8mf2x8_mu(vbool16_t mask,
                                               vint8mf2x8_t maskedoff_tuple,
                                               const int8_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint8m1x2_t __riscv_vluxseg2ei64_v_i8m1x2_mu(vbool8_t mask,
                                             vint8m1x2_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x3_t __riscv_vluxseg3ei64_v_i8m1x3_mu(vbool8_t mask,
                                             vint8m1x3_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x4_t __riscv_vluxseg4ei64_v_i8m1x4_mu(vbool8_t mask,
                                             vint8m1x4_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x5_t __riscv_vluxseg5ei64_v_i8m1x5_mu(vbool8_t mask,
                                             vint8m1x5_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x6_t __riscv_vluxseg6ei64_v_i8m1x6_mu(vbool8_t mask,
                                             vint8m1x6_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x7_t __riscv_vluxseg7ei64_v_i8m1x7_mu(vbool8_t mask,
                                             vint8m1x7_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint8m1x8_t __riscv_vluxseg8ei64_v_i8m1x8_mu(vbool8_t mask,
                                             vint8m1x8_t maskedoff_tuple,
                                             const int8_t *base,
                                             vuint64m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei8_v_i16mf4x2_mu(vbool64_t mask,
                                                vint16mf4x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei8_v_i16mf4x3_mu(vbool64_t mask,
                                                vint16mf4x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei8_v_i16mf4x4_mu(vbool64_t mask,
                                                vint16mf4x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei8_v_i16mf4x5_mu(vbool64_t mask,
                                                vint16mf4x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei8_v_i16mf4x6_mu(vbool64_t mask,
                                                vint16mf4x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei8_v_i16mf4x7_mu(vbool64_t mask,
                                                vint16mf4x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei8_v_i16mf4x8_mu(vbool64_t mask,
                                                vint16mf4x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei8_v_i16mf2x2_mu(vbool32_t mask,
                                                vint16mf2x2_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei8_v_i16mf2x3_mu(vbool32_t mask,
                                                vint16mf2x3_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei8_v_i16mf2x4_mu(vbool32_t mask,
                                                vint16mf2x4_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei8_v_i16mf2x5_mu(vbool32_t mask,
                                                vint16mf2x5_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei8_v_i16mf2x6_mu(vbool32_t mask,
                                                vint16mf2x6_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei8_v_i16mf2x7_mu(vbool32_t mask,
                                                vint16mf2x7_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei8_v_i16mf2x8_mu(vbool32_t mask,
                                                vint16mf2x8_t maskedoff_tuple,
                                                const int16_t *base,
                                                vuint8mf4_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei8_v_i16m1x2_mu(vbool16_t mask,
                                              vint16m1x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei8_v_i16m1x3_mu(vbool16_t mask,
                                              vint16m1x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei8_v_i16m1x4_mu(vbool16_t mask,
                                              vint16m1x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei8_v_i16m1x5_mu(vbool16_t mask,
                                              vint16m1x5_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei8_v_i16m1x6_mu(vbool16_t mask,
                                              vint16m1x6_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei8_v_i16m1x7_mu(vbool16_t mask,
                                              vint16m1x7_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei8_v_i16m1x8_mu(vbool16_t mask,
                                              vint16m1x8_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei8_v_i16m2x2_mu(vbool8_t mask,
                                              vint16m2x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei8_v_i16m2x3_mu(vbool8_t mask,
                                              vint16m2x3_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei8_v_i16m2x4_mu(vbool8_t mask,
                                              vint16m2x4_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei8_v_i16m4x2_mu(vbool4_t mask,
                                              vint16m4x2_t maskedoff_tuple,
                                              const int16_t *base,
                                              vuint8m2_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei16_v_i16mf4x2_mu(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei16_v_i16mf4x3_mu(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei16_v_i16mf4x4_mu(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei16_v_i16mf4x5_mu(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei16_v_i16mf4x6_mu(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei16_v_i16mf4x7_mu(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei16_v_i16mf4x8_mu(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei16_v_i16mf2x2_mu(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei16_v_i16mf2x3_mu(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei16_v_i16mf2x4_mu(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei16_v_i16mf2x5_mu(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei16_v_i16mf2x6_mu(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei16_v_i16mf2x7_mu(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei16_v_i16mf2x8_mu(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint16mf2_t bindex,
                                                 size_t vl);
vint16m1x2_t __riscv_vluxseg2ei16_v_i16m1x2_mu(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei16_v_i16m1x3_mu(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei16_v_i16m1x4_mu(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei16_v_i16m1x5_mu(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei16_v_i16m1x6_mu(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei16_v_i16m1x7_mu(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei16_v_i16m1x8_mu(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei16_v_i16m2x2_mu(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei16_v_i16m2x3_mu(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei16_v_i16m2x4_mu(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei16_v_i16m4x2_mu(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint16m4_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei32_v_i16mf4x2_mu(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei32_v_i16mf4x3_mu(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei32_v_i16mf4x4_mu(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei32_v_i16mf4x5_mu(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei32_v_i16mf4x6_mu(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei32_v_i16mf4x7_mu(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei32_v_i16mf4x8_mu(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei32_v_i16mf2x2_mu(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei32_v_i16mf2x3_mu(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei32_v_i16mf2x4_mu(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei32_v_i16mf2x5_mu(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei32_v_i16mf2x6_mu(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei32_v_i16mf2x7_mu(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei32_v_i16mf2x8_mu(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint32m1_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei32_v_i16m1x2_mu(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei32_v_i16m1x3_mu(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei32_v_i16m1x4_mu(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei32_v_i16m1x5_mu(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei32_v_i16m1x6_mu(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei32_v_i16m1x7_mu(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei32_v_i16m1x8_mu(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei32_v_i16m2x2_mu(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei32_v_i16m2x3_mu(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei32_v_i16m2x4_mu(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint16m4x2_t __riscv_vluxseg2ei32_v_i16m4x2_mu(vbool4_t mask,
                                               vint16m4x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint32m8_t bindex, size_t vl);
vint16mf4x2_t __riscv_vluxseg2ei64_v_i16mf4x2_mu(vbool64_t mask,
                                                 vint16mf4x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x3_t __riscv_vluxseg3ei64_v_i16mf4x3_mu(vbool64_t mask,
                                                 vint16mf4x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x4_t __riscv_vluxseg4ei64_v_i16mf4x4_mu(vbool64_t mask,
                                                 vint16mf4x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x5_t __riscv_vluxseg5ei64_v_i16mf4x5_mu(vbool64_t mask,
                                                 vint16mf4x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x6_t __riscv_vluxseg6ei64_v_i16mf4x6_mu(vbool64_t mask,
                                                 vint16mf4x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x7_t __riscv_vluxseg7ei64_v_i16mf4x7_mu(vbool64_t mask,
                                                 vint16mf4x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf4x8_t __riscv_vluxseg8ei64_v_i16mf4x8_mu(vbool64_t mask,
                                                 vint16mf4x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint16mf2x2_t __riscv_vluxseg2ei64_v_i16mf2x2_mu(vbool32_t mask,
                                                 vint16mf2x2_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x3_t __riscv_vluxseg3ei64_v_i16mf2x3_mu(vbool32_t mask,
                                                 vint16mf2x3_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x4_t __riscv_vluxseg4ei64_v_i16mf2x4_mu(vbool32_t mask,
                                                 vint16mf2x4_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x5_t __riscv_vluxseg5ei64_v_i16mf2x5_mu(vbool32_t mask,
                                                 vint16mf2x5_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x6_t __riscv_vluxseg6ei64_v_i16mf2x6_mu(vbool32_t mask,
                                                 vint16mf2x6_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x7_t __riscv_vluxseg7ei64_v_i16mf2x7_mu(vbool32_t mask,
                                                 vint16mf2x7_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16mf2x8_t __riscv_vluxseg8ei64_v_i16mf2x8_mu(vbool32_t mask,
                                                 vint16mf2x8_t maskedoff_tuple,
                                                 const int16_t *base,
                                                 vuint64m2_t bindex, size_t vl);
vint16m1x2_t __riscv_vluxseg2ei64_v_i16m1x2_mu(vbool16_t mask,
                                               vint16m1x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x3_t __riscv_vluxseg3ei64_v_i16m1x3_mu(vbool16_t mask,
                                               vint16m1x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x4_t __riscv_vluxseg4ei64_v_i16m1x4_mu(vbool16_t mask,
                                               vint16m1x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x5_t __riscv_vluxseg5ei64_v_i16m1x5_mu(vbool16_t mask,
                                               vint16m1x5_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x6_t __riscv_vluxseg6ei64_v_i16m1x6_mu(vbool16_t mask,
                                               vint16m1x6_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x7_t __riscv_vluxseg7ei64_v_i16m1x7_mu(vbool16_t mask,
                                               vint16m1x7_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m1x8_t __riscv_vluxseg8ei64_v_i16m1x8_mu(vbool16_t mask,
                                               vint16m1x8_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint16m2x2_t __riscv_vluxseg2ei64_v_i16m2x2_mu(vbool8_t mask,
                                               vint16m2x2_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x3_t __riscv_vluxseg3ei64_v_i16m2x3_mu(vbool8_t mask,
                                               vint16m2x3_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint16m2x4_t __riscv_vluxseg4ei64_v_i16m2x4_mu(vbool8_t mask,
                                               vint16m2x4_t maskedoff_tuple,
                                               const int16_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei8_v_i32mf2x2_mu(vbool64_t mask,
                                                vint32mf2x2_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei8_v_i32mf2x3_mu(vbool64_t mask,
                                                vint32mf2x3_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei8_v_i32mf2x4_mu(vbool64_t mask,
                                                vint32mf2x4_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei8_v_i32mf2x5_mu(vbool64_t mask,
                                                vint32mf2x5_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei8_v_i32mf2x6_mu(vbool64_t mask,
                                                vint32mf2x6_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei8_v_i32mf2x7_mu(vbool64_t mask,
                                                vint32mf2x7_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei8_v_i32mf2x8_mu(vbool64_t mask,
                                                vint32mf2x8_t maskedoff_tuple,
                                                const int32_t *base,
                                                vuint8mf8_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei8_v_i32m1x2_mu(vbool32_t mask,
                                              vint32m1x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei8_v_i32m1x3_mu(vbool32_t mask,
                                              vint32m1x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei8_v_i32m1x4_mu(vbool32_t mask,
                                              vint32m1x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei8_v_i32m1x5_mu(vbool32_t mask,
                                              vint32m1x5_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei8_v_i32m1x6_mu(vbool32_t mask,
                                              vint32m1x6_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei8_v_i32m1x7_mu(vbool32_t mask,
                                              vint32m1x7_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei8_v_i32m1x8_mu(vbool32_t mask,
                                              vint32m1x8_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei8_v_i32m2x2_mu(vbool16_t mask,
                                              vint32m2x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei8_v_i32m2x3_mu(vbool16_t mask,
                                              vint32m2x3_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei8_v_i32m2x4_mu(vbool16_t mask,
                                              vint32m2x4_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei8_v_i32m4x2_mu(vbool8_t mask,
                                              vint32m4x2_t maskedoff_tuple,
                                              const int32_t *base,
                                              vuint8m1_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei16_v_i32mf2x2_mu(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei16_v_i32mf2x3_mu(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei16_v_i32mf2x4_mu(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei16_v_i32mf2x5_mu(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei16_v_i32mf2x6_mu(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei16_v_i32mf2x7_mu(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei16_v_i32mf2x8_mu(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint16mf4_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vluxseg2ei16_v_i32m1x2_mu(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei16_v_i32m1x3_mu(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei16_v_i32m1x4_mu(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei16_v_i32m1x5_mu(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei16_v_i32m1x6_mu(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei16_v_i32m1x7_mu(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei16_v_i32m1x8_mu(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei16_v_i32m2x2_mu(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei16_v_i32m2x3_mu(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei16_v_i32m2x4_mu(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei16_v_i32m4x2_mu(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint16m2_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei32_v_i32mf2x2_mu(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei32_v_i32mf2x3_mu(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei32_v_i32mf2x4_mu(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei32_v_i32mf2x5_mu(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei32_v_i32mf2x6_mu(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei32_v_i32mf2x7_mu(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei32_v_i32mf2x8_mu(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint32mf2_t bindex,
                                                 size_t vl);
vint32m1x2_t __riscv_vluxseg2ei32_v_i32m1x2_mu(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei32_v_i32m1x3_mu(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei32_v_i32m1x4_mu(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei32_v_i32m1x5_mu(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei32_v_i32m1x6_mu(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei32_v_i32m1x7_mu(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei32_v_i32m1x8_mu(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei32_v_i32m2x2_mu(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei32_v_i32m2x3_mu(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei32_v_i32m2x4_mu(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei32_v_i32m4x2_mu(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint32m4_t bindex, size_t vl);
vint32mf2x2_t __riscv_vluxseg2ei64_v_i32mf2x2_mu(vbool64_t mask,
                                                 vint32mf2x2_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x3_t __riscv_vluxseg3ei64_v_i32mf2x3_mu(vbool64_t mask,
                                                 vint32mf2x3_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x4_t __riscv_vluxseg4ei64_v_i32mf2x4_mu(vbool64_t mask,
                                                 vint32mf2x4_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x5_t __riscv_vluxseg5ei64_v_i32mf2x5_mu(vbool64_t mask,
                                                 vint32mf2x5_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x6_t __riscv_vluxseg6ei64_v_i32mf2x6_mu(vbool64_t mask,
                                                 vint32mf2x6_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x7_t __riscv_vluxseg7ei64_v_i32mf2x7_mu(vbool64_t mask,
                                                 vint32mf2x7_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32mf2x8_t __riscv_vluxseg8ei64_v_i32mf2x8_mu(vbool64_t mask,
                                                 vint32mf2x8_t maskedoff_tuple,
                                                 const int32_t *base,
                                                 vuint64m1_t bindex, size_t vl);
vint32m1x2_t __riscv_vluxseg2ei64_v_i32m1x2_mu(vbool32_t mask,
                                               vint32m1x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x3_t __riscv_vluxseg3ei64_v_i32m1x3_mu(vbool32_t mask,
                                               vint32m1x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x4_t __riscv_vluxseg4ei64_v_i32m1x4_mu(vbool32_t mask,
                                               vint32m1x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x5_t __riscv_vluxseg5ei64_v_i32m1x5_mu(vbool32_t mask,
                                               vint32m1x5_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x6_t __riscv_vluxseg6ei64_v_i32m1x6_mu(vbool32_t mask,
                                               vint32m1x6_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x7_t __riscv_vluxseg7ei64_v_i32m1x7_mu(vbool32_t mask,
                                               vint32m1x7_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m1x8_t __riscv_vluxseg8ei64_v_i32m1x8_mu(vbool32_t mask,
                                               vint32m1x8_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint32m2x2_t __riscv_vluxseg2ei64_v_i32m2x2_mu(vbool16_t mask,
                                               vint32m2x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x3_t __riscv_vluxseg3ei64_v_i32m2x3_mu(vbool16_t mask,
                                               vint32m2x3_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m2x4_t __riscv_vluxseg4ei64_v_i32m2x4_mu(vbool16_t mask,
                                               vint32m2x4_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m4_t bindex, size_t vl);
vint32m4x2_t __riscv_vluxseg2ei64_v_i32m4x2_mu(vbool8_t mask,
                                               vint32m4x2_t maskedoff_tuple,
                                               const int32_t *base,
                                               vuint64m8_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei8_v_i64m1x2_mu(vbool64_t mask,
                                              vint64m1x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei8_v_i64m1x3_mu(vbool64_t mask,
                                              vint64m1x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei8_v_i64m1x4_mu(vbool64_t mask,
                                              vint64m1x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei8_v_i64m1x5_mu(vbool64_t mask,
                                              vint64m1x5_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei8_v_i64m1x6_mu(vbool64_t mask,
                                              vint64m1x6_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei8_v_i64m1x7_mu(vbool64_t mask,
                                              vint64m1x7_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei8_v_i64m1x8_mu(vbool64_t mask,
                                              vint64m1x8_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf8_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei8_v_i64m2x2_mu(vbool32_t mask,
                                              vint64m2x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei8_v_i64m2x3_mu(vbool32_t mask,
                                              vint64m2x3_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei8_v_i64m2x4_mu(vbool32_t mask,
                                              vint64m2x4_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf4_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei8_v_i64m4x2_mu(vbool16_t mask,
                                              vint64m4x2_t maskedoff_tuple,
                                              const int64_t *base,
                                              vuint8mf2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei16_v_i64m1x2_mu(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei16_v_i64m1x3_mu(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei16_v_i64m1x4_mu(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei16_v_i64m1x5_mu(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei16_v_i64m1x6_mu(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei16_v_i64m1x7_mu(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei16_v_i64m1x8_mu(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf4_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei16_v_i64m2x2_mu(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei16_v_i64m2x3_mu(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei16_v_i64m2x4_mu(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16mf2_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei16_v_i64m4x2_mu(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint16m1_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei32_v_i64m1x2_mu(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei32_v_i64m1x3_mu(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei32_v_i64m1x4_mu(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei32_v_i64m1x5_mu(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei32_v_i64m1x6_mu(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei32_v_i64m1x7_mu(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei32_v_i64m1x8_mu(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32mf2_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei32_v_i64m2x2_mu(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei32_v_i64m2x3_mu(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei32_v_i64m2x4_mu(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m1_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei32_v_i64m4x2_mu(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint32m2_t bindex, size_t vl);
vint64m1x2_t __riscv_vluxseg2ei64_v_i64m1x2_mu(vbool64_t mask,
                                               vint64m1x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x3_t __riscv_vluxseg3ei64_v_i64m1x3_mu(vbool64_t mask,
                                               vint64m1x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x4_t __riscv_vluxseg4ei64_v_i64m1x4_mu(vbool64_t mask,
                                               vint64m1x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x5_t __riscv_vluxseg5ei64_v_i64m1x5_mu(vbool64_t mask,
                                               vint64m1x5_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x6_t __riscv_vluxseg6ei64_v_i64m1x6_mu(vbool64_t mask,
                                               vint64m1x6_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x7_t __riscv_vluxseg7ei64_v_i64m1x7_mu(vbool64_t mask,
                                               vint64m1x7_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m1x8_t __riscv_vluxseg8ei64_v_i64m1x8_mu(vbool64_t mask,
                                               vint64m1x8_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m1_t bindex, size_t vl);
vint64m2x2_t __riscv_vluxseg2ei64_v_i64m2x2_mu(vbool32_t mask,
                                               vint64m2x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x3_t __riscv_vluxseg3ei64_v_i64m2x3_mu(vbool32_t mask,
                                               vint64m2x3_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m2x4_t __riscv_vluxseg4ei64_v_i64m2x4_mu(vbool32_t mask,
                                               vint64m2x4_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m2_t bindex, size_t vl);
vint64m4x2_t __riscv_vluxseg2ei64_v_i64m4x2_mu(vbool16_t mask,
                                               vint64m4x2_t maskedoff_tuple,
                                               const int64_t *base,
                                               vuint64m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei8_v_u8mf8x2_mu(vbool64_t mask,
                                               vuint8mf8x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei8_v_u8mf8x3_mu(vbool64_t mask,
                                               vuint8mf8x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei8_v_u8mf8x4_mu(vbool64_t mask,
                                               vuint8mf8x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei8_v_u8mf8x5_mu(vbool64_t mask,
                                               vuint8mf8x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei8_v_u8mf8x6_mu(vbool64_t mask,
                                               vuint8mf8x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei8_v_u8mf8x7_mu(vbool64_t mask,
                                               vuint8mf8x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei8_v_u8mf8x8_mu(vbool64_t mask,
                                               vuint8mf8x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei8_v_u8mf4x2_mu(vbool32_t mask,
                                               vuint8mf4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei8_v_u8mf4x3_mu(vbool32_t mask,
                                               vuint8mf4x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei8_v_u8mf4x4_mu(vbool32_t mask,
                                               vuint8mf4x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei8_v_u8mf4x5_mu(vbool32_t mask,
                                               vuint8mf4x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei8_v_u8mf4x6_mu(vbool32_t mask,
                                               vuint8mf4x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei8_v_u8mf4x7_mu(vbool32_t mask,
                                               vuint8mf4x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei8_v_u8mf4x8_mu(vbool32_t mask,
                                               vuint8mf4x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei8_v_u8mf2x2_mu(vbool16_t mask,
                                               vuint8mf2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei8_v_u8mf2x3_mu(vbool16_t mask,
                                               vuint8mf2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei8_v_u8mf2x4_mu(vbool16_t mask,
                                               vuint8mf2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei8_v_u8mf2x5_mu(vbool16_t mask,
                                               vuint8mf2x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei8_v_u8mf2x6_mu(vbool16_t mask,
                                               vuint8mf2x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei8_v_u8mf2x7_mu(vbool16_t mask,
                                               vuint8mf2x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei8_v_u8mf2x8_mu(vbool16_t mask,
                                               vuint8mf2x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei8_v_u8m1x2_mu(vbool8_t mask,
                                             vuint8m1x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei8_v_u8m1x3_mu(vbool8_t mask,
                                             vuint8m1x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei8_v_u8m1x4_mu(vbool8_t mask,
                                             vuint8m1x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei8_v_u8m1x5_mu(vbool8_t mask,
                                             vuint8m1x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei8_v_u8m1x6_mu(vbool8_t mask,
                                             vuint8m1x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei8_v_u8m1x7_mu(vbool8_t mask,
                                             vuint8m1x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei8_v_u8m1x8_mu(vbool8_t mask,
                                             vuint8m1x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei8_v_u8m2x2_mu(vbool4_t mask,
                                             vuint8m2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei8_v_u8m2x3_mu(vbool4_t mask,
                                             vuint8m2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei8_v_u8m2x4_mu(vbool4_t mask,
                                             vuint8m2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei8_v_u8m4x2_mu(vbool2_t mask,
                                             vuint8m4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei16_v_u8mf8x2_mu(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei16_v_u8mf8x3_mu(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei16_v_u8mf8x4_mu(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei16_v_u8mf8x5_mu(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei16_v_u8mf8x6_mu(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei16_v_u8mf8x7_mu(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei16_v_u8mf8x8_mu(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei16_v_u8mf4x2_mu(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei16_v_u8mf4x3_mu(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei16_v_u8mf4x4_mu(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei16_v_u8mf4x5_mu(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei16_v_u8mf4x6_mu(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei16_v_u8mf4x7_mu(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei16_v_u8mf4x8_mu(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei16_v_u8mf2x2_mu(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei16_v_u8mf2x3_mu(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei16_v_u8mf2x4_mu(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei16_v_u8mf2x5_mu(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei16_v_u8mf2x6_mu(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei16_v_u8mf2x7_mu(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei16_v_u8mf2x8_mu(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei16_v_u8m1x2_mu(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei16_v_u8m1x3_mu(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei16_v_u8m1x4_mu(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei16_v_u8m1x5_mu(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei16_v_u8m1x6_mu(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei16_v_u8m1x7_mu(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei16_v_u8m1x8_mu(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei16_v_u8m2x2_mu(vbool4_t mask,
                                              vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei16_v_u8m2x3_mu(vbool4_t mask,
                                              vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei16_v_u8m2x4_mu(vbool4_t mask,
                                              vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vloxseg2ei16_v_u8m4x2_mu(vbool2_t mask,
                                              vuint8m4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei32_v_u8mf8x2_mu(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei32_v_u8mf8x3_mu(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei32_v_u8mf8x4_mu(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei32_v_u8mf8x5_mu(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei32_v_u8mf8x6_mu(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei32_v_u8mf8x7_mu(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei32_v_u8mf8x8_mu(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei32_v_u8mf4x2_mu(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei32_v_u8mf4x3_mu(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei32_v_u8mf4x4_mu(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei32_v_u8mf4x5_mu(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei32_v_u8mf4x6_mu(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei32_v_u8mf4x7_mu(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei32_v_u8mf4x8_mu(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei32_v_u8mf2x2_mu(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei32_v_u8mf2x3_mu(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei32_v_u8mf2x4_mu(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei32_v_u8mf2x5_mu(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei32_v_u8mf2x6_mu(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei32_v_u8mf2x7_mu(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei32_v_u8mf2x8_mu(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei32_v_u8m1x2_mu(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei32_v_u8m1x3_mu(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei32_v_u8m1x4_mu(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei32_v_u8m1x5_mu(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei32_v_u8m1x6_mu(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei32_v_u8m1x7_mu(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei32_v_u8m1x8_mu(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vloxseg2ei32_v_u8m2x2_mu(vbool4_t mask,
                                              vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vloxseg3ei32_v_u8m2x3_mu(vbool4_t mask,
                                              vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vloxseg4ei32_v_u8m2x4_mu(vbool4_t mask,
                                              vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vloxseg2ei64_v_u8mf8x2_mu(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vloxseg3ei64_v_u8mf8x3_mu(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vloxseg4ei64_v_u8mf8x4_mu(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vloxseg5ei64_v_u8mf8x5_mu(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vloxseg6ei64_v_u8mf8x6_mu(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vloxseg7ei64_v_u8mf8x7_mu(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vloxseg8ei64_v_u8mf8x8_mu(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vloxseg2ei64_v_u8mf4x2_mu(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vloxseg3ei64_v_u8mf4x3_mu(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vloxseg4ei64_v_u8mf4x4_mu(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vloxseg5ei64_v_u8mf4x5_mu(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vloxseg6ei64_v_u8mf4x6_mu(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vloxseg7ei64_v_u8mf4x7_mu(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vloxseg8ei64_v_u8mf4x8_mu(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vloxseg2ei64_v_u8mf2x2_mu(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vloxseg3ei64_v_u8mf2x3_mu(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vloxseg4ei64_v_u8mf2x4_mu(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vloxseg5ei64_v_u8mf2x5_mu(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vloxseg6ei64_v_u8mf2x6_mu(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vloxseg7ei64_v_u8mf2x7_mu(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vloxseg8ei64_v_u8mf2x8_mu(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8m1x2_t __riscv_vloxseg2ei64_v_u8m1x2_mu(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vloxseg3ei64_v_u8m1x3_mu(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vloxseg4ei64_v_u8m1x4_mu(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vloxseg5ei64_v_u8m1x5_mu(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vloxseg6ei64_v_u8m1x6_mu(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vloxseg7ei64_v_u8m1x7_mu(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vloxseg8ei64_v_u8m1x8_mu(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei8_v_u16mf4x2_mu(vbool64_t mask,
                                                 vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei8_v_u16mf4x3_mu(vbool64_t mask,
                                                 vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei8_v_u16mf4x4_mu(vbool64_t mask,
                                                 vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei8_v_u16mf4x5_mu(vbool64_t mask,
                                                 vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei8_v_u16mf4x6_mu(vbool64_t mask,
                                                 vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei8_v_u16mf4x7_mu(vbool64_t mask,
                                                 vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei8_v_u16mf4x8_mu(vbool64_t mask,
                                                 vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei8_v_u16mf2x2_mu(vbool32_t mask,
                                                 vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei8_v_u16mf2x3_mu(vbool32_t mask,
                                                 vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei8_v_u16mf2x4_mu(vbool32_t mask,
                                                 vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei8_v_u16mf2x5_mu(vbool32_t mask,
                                                 vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei8_v_u16mf2x6_mu(vbool32_t mask,
                                                 vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei8_v_u16mf2x7_mu(vbool32_t mask,
                                                 vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei8_v_u16mf2x8_mu(vbool32_t mask,
                                                 vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei8_v_u16m1x2_mu(vbool16_t mask,
                                               vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei8_v_u16m1x3_mu(vbool16_t mask,
                                               vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei8_v_u16m1x4_mu(vbool16_t mask,
                                               vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei8_v_u16m1x5_mu(vbool16_t mask,
                                               vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei8_v_u16m1x6_mu(vbool16_t mask,
                                               vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei8_v_u16m1x7_mu(vbool16_t mask,
                                               vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei8_v_u16m1x8_mu(vbool16_t mask,
                                               vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei8_v_u16m2x2_mu(vbool8_t mask,
                                               vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei8_v_u16m2x3_mu(vbool8_t mask,
                                               vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei8_v_u16m2x4_mu(vbool8_t mask,
                                               vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei8_v_u16m4x2_mu(vbool4_t mask,
                                               vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei16_v_u16mf4x2_mu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei16_v_u16mf4x3_mu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei16_v_u16mf4x4_mu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei16_v_u16mf4x5_mu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei16_v_u16mf4x6_mu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei16_v_u16mf4x7_mu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei16_v_u16mf4x8_mu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei16_v_u16mf2x2_mu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei16_v_u16mf2x3_mu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei16_v_u16mf2x4_mu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei16_v_u16mf2x5_mu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei16_v_u16mf2x6_mu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei16_v_u16mf2x7_mu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei16_v_u16mf2x8_mu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei16_v_u16m1x2_mu(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei16_v_u16m1x3_mu(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei16_v_u16m1x4_mu(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei16_v_u16m1x5_mu(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei16_v_u16m1x6_mu(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei16_v_u16m1x7_mu(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei16_v_u16m1x8_mu(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei16_v_u16m2x2_mu(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei16_v_u16m2x3_mu(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei16_v_u16m2x4_mu(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei16_v_u16m4x2_mu(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei32_v_u16mf4x2_mu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei32_v_u16mf4x3_mu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei32_v_u16mf4x4_mu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei32_v_u16mf4x5_mu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei32_v_u16mf4x6_mu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei32_v_u16mf4x7_mu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei32_v_u16mf4x8_mu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei32_v_u16mf2x2_mu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei32_v_u16mf2x3_mu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei32_v_u16mf2x4_mu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei32_v_u16mf2x5_mu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei32_v_u16mf2x6_mu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei32_v_u16mf2x7_mu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei32_v_u16mf2x8_mu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei32_v_u16m1x2_mu(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei32_v_u16m1x3_mu(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei32_v_u16m1x4_mu(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei32_v_u16m1x5_mu(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei32_v_u16m1x6_mu(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei32_v_u16m1x7_mu(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei32_v_u16m1x8_mu(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei32_v_u16m2x2_mu(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei32_v_u16m2x3_mu(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei32_v_u16m2x4_mu(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m4x2_t __riscv_vloxseg2ei32_v_u16m4x2_mu(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vloxseg2ei64_v_u16mf4x2_mu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vloxseg3ei64_v_u16mf4x3_mu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vloxseg4ei64_v_u16mf4x4_mu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vloxseg5ei64_v_u16mf4x5_mu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vloxseg6ei64_v_u16mf4x6_mu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vloxseg7ei64_v_u16mf4x7_mu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vloxseg8ei64_v_u16mf4x8_mu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vloxseg2ei64_v_u16mf2x2_mu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vloxseg3ei64_v_u16mf2x3_mu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vloxseg4ei64_v_u16mf2x4_mu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vloxseg5ei64_v_u16mf2x5_mu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vloxseg6ei64_v_u16mf2x6_mu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vloxseg7ei64_v_u16mf2x7_mu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vloxseg8ei64_v_u16mf2x8_mu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vloxseg2ei64_v_u16m1x2_mu(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x3_t __riscv_vloxseg3ei64_v_u16m1x3_mu(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x4_t __riscv_vloxseg4ei64_v_u16m1x4_mu(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x5_t __riscv_vloxseg5ei64_v_u16m1x5_mu(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x6_t __riscv_vloxseg6ei64_v_u16m1x6_mu(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x7_t __riscv_vloxseg7ei64_v_u16m1x7_mu(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x8_t __riscv_vloxseg8ei64_v_u16m1x8_mu(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m2x2_t __riscv_vloxseg2ei64_v_u16m2x2_mu(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x3_t __riscv_vloxseg3ei64_v_u16m2x3_mu(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x4_t __riscv_vloxseg4ei64_v_u16m2x4_mu(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei8_v_u32mf2x2_mu(vbool64_t mask,
                                                 vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei8_v_u32mf2x3_mu(vbool64_t mask,
                                                 vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei8_v_u32mf2x4_mu(vbool64_t mask,
                                                 vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei8_v_u32mf2x5_mu(vbool64_t mask,
                                                 vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei8_v_u32mf2x6_mu(vbool64_t mask,
                                                 vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei8_v_u32mf2x7_mu(vbool64_t mask,
                                                 vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei8_v_u32mf2x8_mu(vbool64_t mask,
                                                 vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei8_v_u32m1x2_mu(vbool32_t mask,
                                               vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei8_v_u32m1x3_mu(vbool32_t mask,
                                               vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei8_v_u32m1x4_mu(vbool32_t mask,
                                               vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei8_v_u32m1x5_mu(vbool32_t mask,
                                               vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei8_v_u32m1x6_mu(vbool32_t mask,
                                               vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei8_v_u32m1x7_mu(vbool32_t mask,
                                               vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei8_v_u32m1x8_mu(vbool32_t mask,
                                               vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei8_v_u32m2x2_mu(vbool16_t mask,
                                               vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei8_v_u32m2x3_mu(vbool16_t mask,
                                               vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei8_v_u32m2x4_mu(vbool16_t mask,
                                               vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei8_v_u32m4x2_mu(vbool8_t mask,
                                               vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei16_v_u32mf2x2_mu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei16_v_u32mf2x3_mu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei16_v_u32mf2x4_mu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei16_v_u32mf2x5_mu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei16_v_u32mf2x6_mu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei16_v_u32mf2x7_mu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei16_v_u32mf2x8_mu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei16_v_u32m1x2_mu(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei16_v_u32m1x3_mu(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei16_v_u32m1x4_mu(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei16_v_u32m1x5_mu(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei16_v_u32m1x6_mu(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei16_v_u32m1x7_mu(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei16_v_u32m1x8_mu(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei16_v_u32m2x2_mu(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei16_v_u32m2x3_mu(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei16_v_u32m2x4_mu(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei16_v_u32m4x2_mu(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei32_v_u32mf2x2_mu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei32_v_u32mf2x3_mu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei32_v_u32mf2x4_mu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei32_v_u32mf2x5_mu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei32_v_u32mf2x6_mu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei32_v_u32mf2x7_mu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei32_v_u32mf2x8_mu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei32_v_u32m1x2_mu(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei32_v_u32m1x3_mu(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei32_v_u32m1x4_mu(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei32_v_u32m1x5_mu(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei32_v_u32m1x6_mu(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei32_v_u32m1x7_mu(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei32_v_u32m1x8_mu(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei32_v_u32m2x2_mu(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei32_v_u32m2x3_mu(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei32_v_u32m2x4_mu(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei32_v_u32m4x2_mu(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vloxseg2ei64_v_u32mf2x2_mu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vloxseg3ei64_v_u32mf2x3_mu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vloxseg4ei64_v_u32mf2x4_mu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vloxseg5ei64_v_u32mf2x5_mu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vloxseg6ei64_v_u32mf2x6_mu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vloxseg7ei64_v_u32mf2x7_mu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vloxseg8ei64_v_u32mf2x8_mu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32m1x2_t __riscv_vloxseg2ei64_v_u32m1x2_mu(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vloxseg3ei64_v_u32m1x3_mu(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vloxseg4ei64_v_u32m1x4_mu(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vloxseg5ei64_v_u32m1x5_mu(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vloxseg6ei64_v_u32m1x6_mu(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vloxseg7ei64_v_u32m1x7_mu(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vloxseg8ei64_v_u32m1x8_mu(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vloxseg2ei64_v_u32m2x2_mu(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x3_t __riscv_vloxseg3ei64_v_u32m2x3_mu(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x4_t __riscv_vloxseg4ei64_v_u32m2x4_mu(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m4x2_t __riscv_vloxseg2ei64_v_u32m4x2_mu(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei8_v_u64m1x2_mu(vbool64_t mask,
                                               vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei8_v_u64m1x3_mu(vbool64_t mask,
                                               vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei8_v_u64m1x4_mu(vbool64_t mask,
                                               vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei8_v_u64m1x5_mu(vbool64_t mask,
                                               vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei8_v_u64m1x6_mu(vbool64_t mask,
                                               vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei8_v_u64m1x7_mu(vbool64_t mask,
                                               vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei8_v_u64m1x8_mu(vbool64_t mask,
                                               vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei8_v_u64m2x2_mu(vbool32_t mask,
                                               vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei8_v_u64m2x3_mu(vbool32_t mask,
                                               vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei8_v_u64m2x4_mu(vbool32_t mask,
                                               vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei8_v_u64m4x2_mu(vbool16_t mask,
                                               vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei16_v_u64m1x2_mu(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei16_v_u64m1x3_mu(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei16_v_u64m1x4_mu(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei16_v_u64m1x5_mu(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei16_v_u64m1x6_mu(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei16_v_u64m1x7_mu(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei16_v_u64m1x8_mu(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei16_v_u64m2x2_mu(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei16_v_u64m2x3_mu(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei16_v_u64m2x4_mu(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei16_v_u64m4x2_mu(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei32_v_u64m1x2_mu(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei32_v_u64m1x3_mu(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei32_v_u64m1x4_mu(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei32_v_u64m1x5_mu(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei32_v_u64m1x6_mu(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei32_v_u64m1x7_mu(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei32_v_u64m1x8_mu(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei32_v_u64m2x2_mu(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei32_v_u64m2x3_mu(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei32_v_u64m2x4_mu(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei32_v_u64m4x2_mu(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vloxseg2ei64_v_u64m1x2_mu(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x3_t __riscv_vloxseg3ei64_v_u64m1x3_mu(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x4_t __riscv_vloxseg4ei64_v_u64m1x4_mu(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x5_t __riscv_vloxseg5ei64_v_u64m1x5_mu(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x6_t __riscv_vloxseg6ei64_v_u64m1x6_mu(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x7_t __riscv_vloxseg7ei64_v_u64m1x7_mu(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x8_t __riscv_vloxseg8ei64_v_u64m1x8_mu(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m2x2_t __riscv_vloxseg2ei64_v_u64m2x2_mu(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vloxseg3ei64_v_u64m2x3_mu(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vloxseg4ei64_v_u64m2x4_mu(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vloxseg2ei64_v_u64m4x2_mu(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei8_v_u8mf8x2_mu(vbool64_t mask,
                                               vuint8mf8x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei8_v_u8mf8x3_mu(vbool64_t mask,
                                               vuint8mf8x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei8_v_u8mf8x4_mu(vbool64_t mask,
                                               vuint8mf8x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei8_v_u8mf8x5_mu(vbool64_t mask,
                                               vuint8mf8x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei8_v_u8mf8x6_mu(vbool64_t mask,
                                               vuint8mf8x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei8_v_u8mf8x7_mu(vbool64_t mask,
                                               vuint8mf8x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei8_v_u8mf8x8_mu(vbool64_t mask,
                                               vuint8mf8x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei8_v_u8mf4x2_mu(vbool32_t mask,
                                               vuint8mf4x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei8_v_u8mf4x3_mu(vbool32_t mask,
                                               vuint8mf4x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei8_v_u8mf4x4_mu(vbool32_t mask,
                                               vuint8mf4x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei8_v_u8mf4x5_mu(vbool32_t mask,
                                               vuint8mf4x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei8_v_u8mf4x6_mu(vbool32_t mask,
                                               vuint8mf4x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei8_v_u8mf4x7_mu(vbool32_t mask,
                                               vuint8mf4x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei8_v_u8mf4x8_mu(vbool32_t mask,
                                               vuint8mf4x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei8_v_u8mf2x2_mu(vbool16_t mask,
                                               vuint8mf2x2_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei8_v_u8mf2x3_mu(vbool16_t mask,
                                               vuint8mf2x3_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei8_v_u8mf2x4_mu(vbool16_t mask,
                                               vuint8mf2x4_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei8_v_u8mf2x5_mu(vbool16_t mask,
                                               vuint8mf2x5_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei8_v_u8mf2x6_mu(vbool16_t mask,
                                               vuint8mf2x6_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei8_v_u8mf2x7_mu(vbool16_t mask,
                                               vuint8mf2x7_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei8_v_u8mf2x8_mu(vbool16_t mask,
                                               vuint8mf2x8_t maskedoff_tuple,
                                               const uint8_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei8_v_u8m1x2_mu(vbool8_t mask,
                                             vuint8m1x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei8_v_u8m1x3_mu(vbool8_t mask,
                                             vuint8m1x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei8_v_u8m1x4_mu(vbool8_t mask,
                                             vuint8m1x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei8_v_u8m1x5_mu(vbool8_t mask,
                                             vuint8m1x5_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei8_v_u8m1x6_mu(vbool8_t mask,
                                             vuint8m1x6_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei8_v_u8m1x7_mu(vbool8_t mask,
                                             vuint8m1x7_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei8_v_u8m1x8_mu(vbool8_t mask,
                                             vuint8m1x8_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m1_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei8_v_u8m2x2_mu(vbool4_t mask,
                                             vuint8m2x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei8_v_u8m2x3_mu(vbool4_t mask,
                                             vuint8m2x3_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei8_v_u8m2x4_mu(vbool4_t mask,
                                             vuint8m2x4_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m2_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei8_v_u8m4x2_mu(vbool2_t mask,
                                             vuint8m4x2_t maskedoff_tuple,
                                             const uint8_t *base,
                                             vuint8m4_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei16_v_u8mf8x2_mu(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei16_v_u8mf8x3_mu(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei16_v_u8mf8x4_mu(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei16_v_u8mf8x5_mu(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei16_v_u8mf8x6_mu(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei16_v_u8mf8x7_mu(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei16_v_u8mf8x8_mu(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei16_v_u8mf4x2_mu(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei16_v_u8mf4x3_mu(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei16_v_u8mf4x4_mu(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei16_v_u8mf4x5_mu(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei16_v_u8mf4x6_mu(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei16_v_u8mf4x7_mu(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei16_v_u8mf4x8_mu(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei16_v_u8mf2x2_mu(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei16_v_u8mf2x3_mu(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei16_v_u8mf2x4_mu(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei16_v_u8mf2x5_mu(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei16_v_u8mf2x6_mu(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei16_v_u8mf2x7_mu(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei16_v_u8mf2x8_mu(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei16_v_u8m1x2_mu(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei16_v_u8m1x3_mu(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei16_v_u8m1x4_mu(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei16_v_u8m1x5_mu(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei16_v_u8m1x6_mu(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei16_v_u8m1x7_mu(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei16_v_u8m1x8_mu(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m2_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei16_v_u8m2x2_mu(vbool4_t mask,
                                              vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei16_v_u8m2x3_mu(vbool4_t mask,
                                              vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei16_v_u8m2x4_mu(vbool4_t mask,
                                              vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m4_t bindex, size_t vl);
vuint8m4x2_t __riscv_vluxseg2ei16_v_u8m4x2_mu(vbool2_t mask,
                                              vuint8m4x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint16m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei32_v_u8mf8x2_mu(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei32_v_u8mf8x3_mu(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei32_v_u8mf8x4_mu(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei32_v_u8mf8x5_mu(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei32_v_u8mf8x6_mu(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei32_v_u8mf8x7_mu(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei32_v_u8mf8x8_mu(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei32_v_u8mf4x2_mu(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei32_v_u8mf4x3_mu(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei32_v_u8mf4x4_mu(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei32_v_u8mf4x5_mu(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei32_v_u8mf4x6_mu(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei32_v_u8mf4x7_mu(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei32_v_u8mf4x8_mu(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei32_v_u8mf2x2_mu(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei32_v_u8mf2x3_mu(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei32_v_u8mf2x4_mu(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei32_v_u8mf2x5_mu(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei32_v_u8mf2x6_mu(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei32_v_u8mf2x7_mu(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei32_v_u8mf2x8_mu(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei32_v_u8m1x2_mu(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei32_v_u8m1x3_mu(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei32_v_u8m1x4_mu(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei32_v_u8m1x5_mu(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei32_v_u8m1x6_mu(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei32_v_u8m1x7_mu(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei32_v_u8m1x8_mu(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m4_t bindex, size_t vl);
vuint8m2x2_t __riscv_vluxseg2ei32_v_u8m2x2_mu(vbool4_t mask,
                                              vuint8m2x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x3_t __riscv_vluxseg3ei32_v_u8m2x3_mu(vbool4_t mask,
                                              vuint8m2x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8m2x4_t __riscv_vluxseg4ei32_v_u8m2x4_mu(vbool4_t mask,
                                              vuint8m2x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint32m8_t bindex, size_t vl);
vuint8mf8x2_t __riscv_vluxseg2ei64_v_u8mf8x2_mu(vbool64_t mask,
                                                vuint8mf8x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x3_t __riscv_vluxseg3ei64_v_u8mf8x3_mu(vbool64_t mask,
                                                vuint8mf8x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x4_t __riscv_vluxseg4ei64_v_u8mf8x4_mu(vbool64_t mask,
                                                vuint8mf8x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x5_t __riscv_vluxseg5ei64_v_u8mf8x5_mu(vbool64_t mask,
                                                vuint8mf8x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x6_t __riscv_vluxseg6ei64_v_u8mf8x6_mu(vbool64_t mask,
                                                vuint8mf8x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x7_t __riscv_vluxseg7ei64_v_u8mf8x7_mu(vbool64_t mask,
                                                vuint8mf8x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf8x8_t __riscv_vluxseg8ei64_v_u8mf8x8_mu(vbool64_t mask,
                                                vuint8mf8x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint8mf4x2_t __riscv_vluxseg2ei64_v_u8mf4x2_mu(vbool32_t mask,
                                                vuint8mf4x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x3_t __riscv_vluxseg3ei64_v_u8mf4x3_mu(vbool32_t mask,
                                                vuint8mf4x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x4_t __riscv_vluxseg4ei64_v_u8mf4x4_mu(vbool32_t mask,
                                                vuint8mf4x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x5_t __riscv_vluxseg5ei64_v_u8mf4x5_mu(vbool32_t mask,
                                                vuint8mf4x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x6_t __riscv_vluxseg6ei64_v_u8mf4x6_mu(vbool32_t mask,
                                                vuint8mf4x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x7_t __riscv_vluxseg7ei64_v_u8mf4x7_mu(vbool32_t mask,
                                                vuint8mf4x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf4x8_t __riscv_vluxseg8ei64_v_u8mf4x8_mu(vbool32_t mask,
                                                vuint8mf4x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint8mf2x2_t __riscv_vluxseg2ei64_v_u8mf2x2_mu(vbool16_t mask,
                                                vuint8mf2x2_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x3_t __riscv_vluxseg3ei64_v_u8mf2x3_mu(vbool16_t mask,
                                                vuint8mf2x3_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x4_t __riscv_vluxseg4ei64_v_u8mf2x4_mu(vbool16_t mask,
                                                vuint8mf2x4_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x5_t __riscv_vluxseg5ei64_v_u8mf2x5_mu(vbool16_t mask,
                                                vuint8mf2x5_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x6_t __riscv_vluxseg6ei64_v_u8mf2x6_mu(vbool16_t mask,
                                                vuint8mf2x6_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x7_t __riscv_vluxseg7ei64_v_u8mf2x7_mu(vbool16_t mask,
                                                vuint8mf2x7_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8mf2x8_t __riscv_vluxseg8ei64_v_u8mf2x8_mu(vbool16_t mask,
                                                vuint8mf2x8_t maskedoff_tuple,
                                                const uint8_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint8m1x2_t __riscv_vluxseg2ei64_v_u8m1x2_mu(vbool8_t mask,
                                              vuint8m1x2_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x3_t __riscv_vluxseg3ei64_v_u8m1x3_mu(vbool8_t mask,
                                              vuint8m1x3_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x4_t __riscv_vluxseg4ei64_v_u8m1x4_mu(vbool8_t mask,
                                              vuint8m1x4_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x5_t __riscv_vluxseg5ei64_v_u8m1x5_mu(vbool8_t mask,
                                              vuint8m1x5_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x6_t __riscv_vluxseg6ei64_v_u8m1x6_mu(vbool8_t mask,
                                              vuint8m1x6_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x7_t __riscv_vluxseg7ei64_v_u8m1x7_mu(vbool8_t mask,
                                              vuint8m1x7_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint8m1x8_t __riscv_vluxseg8ei64_v_u8m1x8_mu(vbool8_t mask,
                                              vuint8m1x8_t maskedoff_tuple,
                                              const uint8_t *base,
                                              vuint64m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei8_v_u16mf4x2_mu(vbool64_t mask,
                                                 vuint16mf4x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei8_v_u16mf4x3_mu(vbool64_t mask,
                                                 vuint16mf4x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei8_v_u16mf4x4_mu(vbool64_t mask,
                                                 vuint16mf4x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei8_v_u16mf4x5_mu(vbool64_t mask,
                                                 vuint16mf4x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei8_v_u16mf4x6_mu(vbool64_t mask,
                                                 vuint16mf4x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei8_v_u16mf4x7_mu(vbool64_t mask,
                                                 vuint16mf4x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei8_v_u16mf4x8_mu(vbool64_t mask,
                                                 vuint16mf4x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei8_v_u16mf2x2_mu(vbool32_t mask,
                                                 vuint16mf2x2_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei8_v_u16mf2x3_mu(vbool32_t mask,
                                                 vuint16mf2x3_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei8_v_u16mf2x4_mu(vbool32_t mask,
                                                 vuint16mf2x4_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei8_v_u16mf2x5_mu(vbool32_t mask,
                                                 vuint16mf2x5_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei8_v_u16mf2x6_mu(vbool32_t mask,
                                                 vuint16mf2x6_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei8_v_u16mf2x7_mu(vbool32_t mask,
                                                 vuint16mf2x7_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei8_v_u16mf2x8_mu(vbool32_t mask,
                                                 vuint16mf2x8_t maskedoff_tuple,
                                                 const uint16_t *base,
                                                 vuint8mf4_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei8_v_u16m1x2_mu(vbool16_t mask,
                                               vuint16m1x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei8_v_u16m1x3_mu(vbool16_t mask,
                                               vuint16m1x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei8_v_u16m1x4_mu(vbool16_t mask,
                                               vuint16m1x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei8_v_u16m1x5_mu(vbool16_t mask,
                                               vuint16m1x5_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei8_v_u16m1x6_mu(vbool16_t mask,
                                               vuint16m1x6_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei8_v_u16m1x7_mu(vbool16_t mask,
                                               vuint16m1x7_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei8_v_u16m1x8_mu(vbool16_t mask,
                                               vuint16m1x8_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei8_v_u16m2x2_mu(vbool8_t mask,
                                               vuint16m2x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei8_v_u16m2x3_mu(vbool8_t mask,
                                               vuint16m2x3_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei8_v_u16m2x4_mu(vbool8_t mask,
                                               vuint16m2x4_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei8_v_u16m4x2_mu(vbool4_t mask,
                                               vuint16m4x2_t maskedoff_tuple,
                                               const uint16_t *base,
                                               vuint8m2_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei16_v_u16mf4x2_mu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei16_v_u16mf4x3_mu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei16_v_u16mf4x4_mu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei16_v_u16mf4x5_mu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei16_v_u16mf4x6_mu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei16_v_u16mf4x7_mu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei16_v_u16mf4x8_mu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei16_v_u16mf2x2_mu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei16_v_u16mf2x3_mu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei16_v_u16mf2x4_mu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei16_v_u16mf2x5_mu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei16_v_u16mf2x6_mu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei16_v_u16mf2x7_mu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei16_v_u16mf2x8_mu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint16mf2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei16_v_u16m1x2_mu(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei16_v_u16m1x3_mu(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei16_v_u16m1x4_mu(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei16_v_u16m1x5_mu(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei16_v_u16m1x6_mu(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei16_v_u16m1x7_mu(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei16_v_u16m1x8_mu(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei16_v_u16m2x2_mu(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei16_v_u16m2x3_mu(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei16_v_u16m2x4_mu(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei16_v_u16m4x2_mu(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint16m4_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei32_v_u16mf4x2_mu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei32_v_u16mf4x3_mu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei32_v_u16mf4x4_mu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei32_v_u16mf4x5_mu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei32_v_u16mf4x6_mu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei32_v_u16mf4x7_mu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei32_v_u16mf4x8_mu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei32_v_u16mf2x2_mu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei32_v_u16mf2x3_mu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei32_v_u16mf2x4_mu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei32_v_u16mf2x5_mu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei32_v_u16mf2x6_mu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei32_v_u16mf2x7_mu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei32_v_u16mf2x8_mu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint32m1_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei32_v_u16m1x2_mu(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei32_v_u16m1x3_mu(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei32_v_u16m1x4_mu(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei32_v_u16m1x5_mu(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei32_v_u16m1x6_mu(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei32_v_u16m1x7_mu(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei32_v_u16m1x8_mu(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei32_v_u16m2x2_mu(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei32_v_u16m2x3_mu(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei32_v_u16m2x4_mu(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint16m4x2_t __riscv_vluxseg2ei32_v_u16m4x2_mu(vbool4_t mask,
                                                vuint16m4x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint32m8_t bindex, size_t vl);
vuint16mf4x2_t __riscv_vluxseg2ei64_v_u16mf4x2_mu(
    vbool64_t mask, vuint16mf4x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x3_t __riscv_vluxseg3ei64_v_u16mf4x3_mu(
    vbool64_t mask, vuint16mf4x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x4_t __riscv_vluxseg4ei64_v_u16mf4x4_mu(
    vbool64_t mask, vuint16mf4x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x5_t __riscv_vluxseg5ei64_v_u16mf4x5_mu(
    vbool64_t mask, vuint16mf4x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x6_t __riscv_vluxseg6ei64_v_u16mf4x6_mu(
    vbool64_t mask, vuint16mf4x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x7_t __riscv_vluxseg7ei64_v_u16mf4x7_mu(
    vbool64_t mask, vuint16mf4x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf4x8_t __riscv_vluxseg8ei64_v_u16mf4x8_mu(
    vbool64_t mask, vuint16mf4x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m1_t bindex, size_t vl);
vuint16mf2x2_t __riscv_vluxseg2ei64_v_u16mf2x2_mu(
    vbool32_t mask, vuint16mf2x2_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x3_t __riscv_vluxseg3ei64_v_u16mf2x3_mu(
    vbool32_t mask, vuint16mf2x3_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x4_t __riscv_vluxseg4ei64_v_u16mf2x4_mu(
    vbool32_t mask, vuint16mf2x4_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x5_t __riscv_vluxseg5ei64_v_u16mf2x5_mu(
    vbool32_t mask, vuint16mf2x5_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x6_t __riscv_vluxseg6ei64_v_u16mf2x6_mu(
    vbool32_t mask, vuint16mf2x6_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x7_t __riscv_vluxseg7ei64_v_u16mf2x7_mu(
    vbool32_t mask, vuint16mf2x7_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16mf2x8_t __riscv_vluxseg8ei64_v_u16mf2x8_mu(
    vbool32_t mask, vuint16mf2x8_t maskedoff_tuple, const uint16_t *base,
    vuint64m2_t bindex, size_t vl);
vuint16m1x2_t __riscv_vluxseg2ei64_v_u16m1x2_mu(vbool16_t mask,
                                                vuint16m1x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x3_t __riscv_vluxseg3ei64_v_u16m1x3_mu(vbool16_t mask,
                                                vuint16m1x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x4_t __riscv_vluxseg4ei64_v_u16m1x4_mu(vbool16_t mask,
                                                vuint16m1x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x5_t __riscv_vluxseg5ei64_v_u16m1x5_mu(vbool16_t mask,
                                                vuint16m1x5_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x6_t __riscv_vluxseg6ei64_v_u16m1x6_mu(vbool16_t mask,
                                                vuint16m1x6_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x7_t __riscv_vluxseg7ei64_v_u16m1x7_mu(vbool16_t mask,
                                                vuint16m1x7_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m1x8_t __riscv_vluxseg8ei64_v_u16m1x8_mu(vbool16_t mask,
                                                vuint16m1x8_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint16m2x2_t __riscv_vluxseg2ei64_v_u16m2x2_mu(vbool8_t mask,
                                                vuint16m2x2_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x3_t __riscv_vluxseg3ei64_v_u16m2x3_mu(vbool8_t mask,
                                                vuint16m2x3_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint16m2x4_t __riscv_vluxseg4ei64_v_u16m2x4_mu(vbool8_t mask,
                                                vuint16m2x4_t maskedoff_tuple,
                                                const uint16_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei8_v_u32mf2x2_mu(vbool64_t mask,
                                                 vuint32mf2x2_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei8_v_u32mf2x3_mu(vbool64_t mask,
                                                 vuint32mf2x3_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei8_v_u32mf2x4_mu(vbool64_t mask,
                                                 vuint32mf2x4_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei8_v_u32mf2x5_mu(vbool64_t mask,
                                                 vuint32mf2x5_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei8_v_u32mf2x6_mu(vbool64_t mask,
                                                 vuint32mf2x6_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei8_v_u32mf2x7_mu(vbool64_t mask,
                                                 vuint32mf2x7_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei8_v_u32mf2x8_mu(vbool64_t mask,
                                                 vuint32mf2x8_t maskedoff_tuple,
                                                 const uint32_t *base,
                                                 vuint8mf8_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei8_v_u32m1x2_mu(vbool32_t mask,
                                               vuint32m1x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei8_v_u32m1x3_mu(vbool32_t mask,
                                               vuint32m1x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei8_v_u32m1x4_mu(vbool32_t mask,
                                               vuint32m1x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei8_v_u32m1x5_mu(vbool32_t mask,
                                               vuint32m1x5_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei8_v_u32m1x6_mu(vbool32_t mask,
                                               vuint32m1x6_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei8_v_u32m1x7_mu(vbool32_t mask,
                                               vuint32m1x7_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei8_v_u32m1x8_mu(vbool32_t mask,
                                               vuint32m1x8_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei8_v_u32m2x2_mu(vbool16_t mask,
                                               vuint32m2x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei8_v_u32m2x3_mu(vbool16_t mask,
                                               vuint32m2x3_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei8_v_u32m2x4_mu(vbool16_t mask,
                                               vuint32m2x4_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei8_v_u32m4x2_mu(vbool8_t mask,
                                               vuint32m4x2_t maskedoff_tuple,
                                               const uint32_t *base,
                                               vuint8m1_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei16_v_u32mf2x2_mu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei16_v_u32mf2x3_mu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei16_v_u32mf2x4_mu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei16_v_u32mf2x5_mu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei16_v_u32mf2x6_mu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei16_v_u32mf2x7_mu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei16_v_u32mf2x8_mu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint16mf4_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei16_v_u32m1x2_mu(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei16_v_u32m1x3_mu(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei16_v_u32m1x4_mu(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei16_v_u32m1x5_mu(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei16_v_u32m1x6_mu(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei16_v_u32m1x7_mu(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei16_v_u32m1x8_mu(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei16_v_u32m2x2_mu(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei16_v_u32m2x3_mu(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei16_v_u32m2x4_mu(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei16_v_u32m4x2_mu(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint16m2_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei32_v_u32mf2x2_mu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei32_v_u32mf2x3_mu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei32_v_u32mf2x4_mu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei32_v_u32mf2x5_mu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei32_v_u32mf2x6_mu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei32_v_u32mf2x7_mu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei32_v_u32mf2x8_mu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint32mf2_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei32_v_u32m1x2_mu(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei32_v_u32m1x3_mu(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei32_v_u32m1x4_mu(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei32_v_u32m1x5_mu(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei32_v_u32m1x6_mu(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei32_v_u32m1x7_mu(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei32_v_u32m1x8_mu(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei32_v_u32m2x2_mu(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei32_v_u32m2x3_mu(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei32_v_u32m2x4_mu(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei32_v_u32m4x2_mu(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint32m4_t bindex, size_t vl);
vuint32mf2x2_t __riscv_vluxseg2ei64_v_u32mf2x2_mu(
    vbool64_t mask, vuint32mf2x2_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x3_t __riscv_vluxseg3ei64_v_u32mf2x3_mu(
    vbool64_t mask, vuint32mf2x3_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x4_t __riscv_vluxseg4ei64_v_u32mf2x4_mu(
    vbool64_t mask, vuint32mf2x4_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x5_t __riscv_vluxseg5ei64_v_u32mf2x5_mu(
    vbool64_t mask, vuint32mf2x5_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x6_t __riscv_vluxseg6ei64_v_u32mf2x6_mu(
    vbool64_t mask, vuint32mf2x6_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x7_t __riscv_vluxseg7ei64_v_u32mf2x7_mu(
    vbool64_t mask, vuint32mf2x7_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32mf2x8_t __riscv_vluxseg8ei64_v_u32mf2x8_mu(
    vbool64_t mask, vuint32mf2x8_t maskedoff_tuple, const uint32_t *base,
    vuint64m1_t bindex, size_t vl);
vuint32m1x2_t __riscv_vluxseg2ei64_v_u32m1x2_mu(vbool32_t mask,
                                                vuint32m1x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x3_t __riscv_vluxseg3ei64_v_u32m1x3_mu(vbool32_t mask,
                                                vuint32m1x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x4_t __riscv_vluxseg4ei64_v_u32m1x4_mu(vbool32_t mask,
                                                vuint32m1x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x5_t __riscv_vluxseg5ei64_v_u32m1x5_mu(vbool32_t mask,
                                                vuint32m1x5_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x6_t __riscv_vluxseg6ei64_v_u32m1x6_mu(vbool32_t mask,
                                                vuint32m1x6_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x7_t __riscv_vluxseg7ei64_v_u32m1x7_mu(vbool32_t mask,
                                                vuint32m1x7_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m1x8_t __riscv_vluxseg8ei64_v_u32m1x8_mu(vbool32_t mask,
                                                vuint32m1x8_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint32m2x2_t __riscv_vluxseg2ei64_v_u32m2x2_mu(vbool16_t mask,
                                                vuint32m2x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x3_t __riscv_vluxseg3ei64_v_u32m2x3_mu(vbool16_t mask,
                                                vuint32m2x3_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m2x4_t __riscv_vluxseg4ei64_v_u32m2x4_mu(vbool16_t mask,
                                                vuint32m2x4_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m4_t bindex, size_t vl);
vuint32m4x2_t __riscv_vluxseg2ei64_v_u32m4x2_mu(vbool8_t mask,
                                                vuint32m4x2_t maskedoff_tuple,
                                                const uint32_t *base,
                                                vuint64m8_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei8_v_u64m1x2_mu(vbool64_t mask,
                                               vuint64m1x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei8_v_u64m1x3_mu(vbool64_t mask,
                                               vuint64m1x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei8_v_u64m1x4_mu(vbool64_t mask,
                                               vuint64m1x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei8_v_u64m1x5_mu(vbool64_t mask,
                                               vuint64m1x5_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei8_v_u64m1x6_mu(vbool64_t mask,
                                               vuint64m1x6_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei8_v_u64m1x7_mu(vbool64_t mask,
                                               vuint64m1x7_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei8_v_u64m1x8_mu(vbool64_t mask,
                                               vuint64m1x8_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf8_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei8_v_u64m2x2_mu(vbool32_t mask,
                                               vuint64m2x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei8_v_u64m2x3_mu(vbool32_t mask,
                                               vuint64m2x3_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei8_v_u64m2x4_mu(vbool32_t mask,
                                               vuint64m2x4_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf4_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei8_v_u64m4x2_mu(vbool16_t mask,
                                               vuint64m4x2_t maskedoff_tuple,
                                               const uint64_t *base,
                                               vuint8mf2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei16_v_u64m1x2_mu(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei16_v_u64m1x3_mu(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei16_v_u64m1x4_mu(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei16_v_u64m1x5_mu(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei16_v_u64m1x6_mu(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei16_v_u64m1x7_mu(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei16_v_u64m1x8_mu(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf4_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei16_v_u64m2x2_mu(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei16_v_u64m2x3_mu(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei16_v_u64m2x4_mu(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16mf2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei16_v_u64m4x2_mu(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint16m1_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei32_v_u64m1x2_mu(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei32_v_u64m1x3_mu(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei32_v_u64m1x4_mu(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei32_v_u64m1x5_mu(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei32_v_u64m1x6_mu(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei32_v_u64m1x7_mu(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei32_v_u64m1x8_mu(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32mf2_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei32_v_u64m2x2_mu(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei32_v_u64m2x3_mu(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei32_v_u64m2x4_mu(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m1_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei32_v_u64m4x2_mu(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint32m2_t bindex, size_t vl);
vuint64m1x2_t __riscv_vluxseg2ei64_v_u64m1x2_mu(vbool64_t mask,
                                                vuint64m1x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x3_t __riscv_vluxseg3ei64_v_u64m1x3_mu(vbool64_t mask,
                                                vuint64m1x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x4_t __riscv_vluxseg4ei64_v_u64m1x4_mu(vbool64_t mask,
                                                vuint64m1x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x5_t __riscv_vluxseg5ei64_v_u64m1x5_mu(vbool64_t mask,
                                                vuint64m1x5_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x6_t __riscv_vluxseg6ei64_v_u64m1x6_mu(vbool64_t mask,
                                                vuint64m1x6_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x7_t __riscv_vluxseg7ei64_v_u64m1x7_mu(vbool64_t mask,
                                                vuint64m1x7_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m1x8_t __riscv_vluxseg8ei64_v_u64m1x8_mu(vbool64_t mask,
                                                vuint64m1x8_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m1_t bindex, size_t vl);
vuint64m2x2_t __riscv_vluxseg2ei64_v_u64m2x2_mu(vbool32_t mask,
                                                vuint64m2x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x3_t __riscv_vluxseg3ei64_v_u64m2x3_mu(vbool32_t mask,
                                                vuint64m2x3_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m2x4_t __riscv_vluxseg4ei64_v_u64m2x4_mu(vbool32_t mask,
                                                vuint64m2x4_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m2_t bindex, size_t vl);
vuint64m4x2_t __riscv_vluxseg2ei64_v_u64m4x2_mu(vbool16_t mask,
                                                vuint64m4x2_t maskedoff_tuple,
                                                const uint64_t *base,
                                                vuint64m4_t bindex, size_t vl);
----

[[policy-variant-vector-indexed-segment-store]]
=== Vector Indexed Segment Store Intrinsics
Intrinsics here don't have a policy variant.
