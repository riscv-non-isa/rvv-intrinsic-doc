
=== BFloat16 Vector Loads and Stores Intrinsics

[[policy-variant-overloadedbf16-vector-unit-stride-load]]
==== Vector Unit-Stride Load Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vle16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                size_t vl);
vbfloat16mf2_t __riscv_vle16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                size_t vl);
vbfloat16m1_t __riscv_vle16_tu(vbfloat16m1_t vd, const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_tu(vbfloat16m2_t vd, const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_tu(vbfloat16m4_t vd, const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_tu(vbfloat16m8_t vd, const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                  const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                  const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                 const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_mu(vbool16_t vm, vbfloat16m1_t vd,
                               const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_mu(vbool8_t vm, vbfloat16m2_t vd, const __bf16 *rs1,
                               size_t vl);
vbfloat16m4_t __riscv_vle16_mu(vbool4_t vm, vbfloat16m4_t vd, const __bf16 *rs1,
                               size_t vl);
vbfloat16m8_t __riscv_vle16_mu(vbool2_t vm, vbfloat16m8_t vd, const __bf16 *rs1,
                               size_t vl);
----

[[policy-variant-overloadedbf16-vector-unit-stride-store]]
==== Vector Unit-Stride Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-strided-load]]
==== Vector Strided Load Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vlse16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                 ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                 ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_mu(vbool16_t vm, vbfloat16m1_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_mu(vbool8_t vm, vbfloat16m2_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_mu(vbool4_t vm, vbfloat16m4_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_mu(vbool2_t vm, vbfloat16m8_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
----

[[policy-variant-overloadedvector-strided-store]]
==== Vector Strided Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-indexed-load]]
==== Vector Indexed Load Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vloxei16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                   vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vloxei16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                   vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vloxei16_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                  vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vloxei16_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                  vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vloxei16_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                  vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vloxei16_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                  vuint16m8_t rs2, size_t vl);
vbfloat16mf4_t __riscv_vluxei16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                   vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vluxei16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                   vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vluxei16_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                  vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vluxei16_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                  vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vluxei16_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                  vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vluxei16_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                  vuint16m8_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                    const __bf16 *rs1, vuint16mf4_t rs2,
                                    size_t vl);
vbfloat16mf2_t __riscv_vloxei16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                    const __bf16 *rs1, vuint16mf2_t rs2,
                                    size_t vl);
vbfloat16m1_t __riscv_vloxei16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                   const __bf16 *rs1, vuint16m1_t rs2,
                                   size_t vl);
vbfloat16m2_t __riscv_vloxei16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                   const __bf16 *rs1, vuint16m2_t rs2,
                                   size_t vl);
vbfloat16m4_t __riscv_vloxei16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                   const __bf16 *rs1, vuint16m4_t rs2,
                                   size_t vl);
vbfloat16m8_t __riscv_vloxei16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                   const __bf16 *rs1, vuint16m8_t rs2,
                                   size_t vl);
vbfloat16mf4_t __riscv_vluxei16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                    const __bf16 *rs1, vuint16mf4_t rs2,
                                    size_t vl);
vbfloat16mf2_t __riscv_vluxei16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                    const __bf16 *rs1, vuint16mf2_t rs2,
                                    size_t vl);
vbfloat16m1_t __riscv_vluxei16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                   const __bf16 *rs1, vuint16m1_t rs2,
                                   size_t vl);
vbfloat16m2_t __riscv_vluxei16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                   const __bf16 *rs1, vuint16m2_t rs2,
                                   size_t vl);
vbfloat16m4_t __riscv_vluxei16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                   const __bf16 *rs1, vuint16m4_t rs2,
                                   size_t vl);
vbfloat16m8_t __riscv_vluxei16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                   const __bf16 *rs1, vuint16m8_t rs2,
                                   size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                     const __bf16 *rs1, vuint16mf4_t rs2,
                                     size_t vl);
vbfloat16mf2_t __riscv_vloxei16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                     const __bf16 *rs1, vuint16mf2_t rs2,
                                     size_t vl);
vbfloat16m1_t __riscv_vloxei16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                    const __bf16 *rs1, vuint16m1_t rs2,
                                    size_t vl);
vbfloat16m2_t __riscv_vloxei16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                    const __bf16 *rs1, vuint16m2_t rs2,
                                    size_t vl);
vbfloat16m4_t __riscv_vloxei16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                    const __bf16 *rs1, vuint16m4_t rs2,
                                    size_t vl);
vbfloat16m8_t __riscv_vloxei16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                    const __bf16 *rs1, vuint16m8_t rs2,
                                    size_t vl);
vbfloat16mf4_t __riscv_vluxei16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                     const __bf16 *rs1, vuint16mf4_t rs2,
                                     size_t vl);
vbfloat16mf2_t __riscv_vluxei16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                     const __bf16 *rs1, vuint16mf2_t rs2,
                                     size_t vl);
vbfloat16m1_t __riscv_vluxei16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                    const __bf16 *rs1, vuint16m1_t rs2,
                                    size_t vl);
vbfloat16m2_t __riscv_vluxei16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                    const __bf16 *rs1, vuint16m2_t rs2,
                                    size_t vl);
vbfloat16m4_t __riscv_vluxei16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                    const __bf16 *rs1, vuint16m4_t rs2,
                                    size_t vl);
vbfloat16m8_t __riscv_vluxei16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                    const __bf16 *rs1, vuint16m8_t rs2,
                                    size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, vuint16mf4_t rs2,
                                   size_t vl);
vbfloat16mf2_t __riscv_vloxei16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, vuint16mf2_t rs2,
                                   size_t vl);
vbfloat16m1_t __riscv_vloxei16_mu(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, vuint16m1_t rs2,
                                  size_t vl);
vbfloat16m2_t __riscv_vloxei16_mu(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, vuint16m2_t rs2,
                                  size_t vl);
vbfloat16m4_t __riscv_vloxei16_mu(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, vuint16m4_t rs2,
                                  size_t vl);
vbfloat16m8_t __riscv_vloxei16_mu(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, vuint16m8_t rs2,
                                  size_t vl);
vbfloat16mf4_t __riscv_vluxei16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, vuint16mf4_t rs2,
                                   size_t vl);
vbfloat16mf2_t __riscv_vluxei16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, vuint16mf2_t rs2,
                                   size_t vl);
vbfloat16m1_t __riscv_vluxei16_mu(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, vuint16m1_t rs2,
                                  size_t vl);
vbfloat16m2_t __riscv_vluxei16_mu(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, vuint16m2_t rs2,
                                  size_t vl);
vbfloat16m4_t __riscv_vluxei16_mu(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, vuint16m4_t rs2,
                                  size_t vl);
vbfloat16m8_t __riscv_vluxei16_mu(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, vuint16m8_t rs2,
                                  size_t vl);
----

[[policy-variant-overloadedvector-indexed-store]]
==== Vector Indexed Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedunit-stride-fault-only-first-loads]]
==== Unit-stride Fault-Only-First Loads Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vle16ff_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                  size_t *new_vl, size_t vl);
vbfloat16mf2_t __riscv_vle16ff_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                  size_t *new_vl, size_t vl);
vbfloat16m1_t __riscv_vle16ff_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16mf2_t __riscv_vle16ff_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m1_t __riscv_vle16ff_tum(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_tum(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_tum(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_tum(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                    const __bf16 *rs1, size_t *new_vl,
                                    size_t vl);
vbfloat16mf2_t __riscv_vle16ff_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                    const __bf16 *rs1, size_t *new_vl,
                                    size_t vl);
vbfloat16m1_t __riscv_vle16ff_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m2_t __riscv_vle16ff_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m4_t __riscv_vle16ff_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m8_t __riscv_vle16ff_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16mf2_t __riscv_vle16ff_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m1_t __riscv_vle16ff_mu(vbool16_t vm, vbfloat16m1_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_mu(vbool8_t vm, vbfloat16m2_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_mu(vbool4_t vm, vbfloat16m4_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_mu(vbool2_t vm, vbfloat16m8_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
----
