
=== BFloat16 Vector Loads and Stores Intrinsics

[[bf16-vector-unit-stride-load]]
==== Vector Unit-Stride Load Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vle16_v_bf16mf4(const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_v_bf16mf2(const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_v_bf16m1(const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_v_bf16m2(const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_v_bf16m4(const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_v_bf16m8(const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                         size_t vl);
vbfloat16mf2_t __riscv_vle16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                         size_t vl);
vbfloat16m1_t __riscv_vle16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                       size_t vl);
vbfloat16m2_t __riscv_vle16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                       size_t vl);
vbfloat16m4_t __riscv_vle16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                       size_t vl);
vbfloat16m8_t __riscv_vle16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                       size_t vl);
----

[[bf16-vector-unit-stride-store]]
==== Vector Unit-Stride Store Intrinsics

[,c]
----
void __riscv_vse16_v_bf16mf4(__bf16 *rs1, vbfloat16mf4_t vs3, size_t vl);
void __riscv_vse16_v_bf16mf2(__bf16 *rs1, vbfloat16mf2_t vs3, size_t vl);
void __riscv_vse16_v_bf16m1(__bf16 *rs1, vbfloat16m1_t vs3, size_t vl);
void __riscv_vse16_v_bf16m2(__bf16 *rs1, vbfloat16m2_t vs3, size_t vl);
void __riscv_vse16_v_bf16m4(__bf16 *rs1, vbfloat16m4_t vs3, size_t vl);
void __riscv_vse16_v_bf16m8(__bf16 *rs1, vbfloat16m8_t vs3, size_t vl);
// masked functions
void __riscv_vse16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, vbfloat16mf4_t vs3,
                               size_t vl);
void __riscv_vse16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, vbfloat16mf2_t vs3,
                               size_t vl);
void __riscv_vse16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, vbfloat16m1_t vs3,
                              size_t vl);
void __riscv_vse16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, vbfloat16m2_t vs3,
                              size_t vl);
void __riscv_vse16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, vbfloat16m4_t vs3,
                              size_t vl);
void __riscv_vse16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, vbfloat16m8_t vs3,
                              size_t vl);
----

[[vector-strided-load]]
==== Vector Strided Load Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vlse16_v_bf16mf4(const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2_t __riscv_vlse16_v_bf16mf2(const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1_t __riscv_vlse16_v_bf16m1(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m2_t __riscv_vlse16_v_bf16m2(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m4_t __riscv_vlse16_v_bf16m4(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m8_t __riscv_vlse16_v_bf16m8(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                          ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                          ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
----

[[vector-strided-store]]
==== Vector Strided Store Intrinsics

[,c]
----
void __riscv_vsse16_v_bf16mf4(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4_t vs3,
                              size_t vl);
void __riscv_vsse16_v_bf16mf2(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2_t vs3,
                              size_t vl);
void __riscv_vsse16_v_bf16m1(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1_t vs3,
                             size_t vl);
void __riscv_vsse16_v_bf16m2(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m2_t vs3,
                             size_t vl);
void __riscv_vsse16_v_bf16m4(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m4_t vs3,
                             size_t vl);
void __riscv_vsse16_v_bf16m8(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m8_t vs3,
                             size_t vl);
// masked functions
void __riscv_vsse16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsse16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m1_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m2_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m4_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m8_t vs3, size_t vl);
----

[[vector-indexed-load]]
==== Vector Indexed Load Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vloxei16_v_bf16mf4(const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf2_t __riscv_vloxei16_v_bf16mf2(const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16m1_t __riscv_vloxei16_v_bf16m1(const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m2_t __riscv_vloxei16_v_bf16m2(const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m4_t __riscv_vloxei16_v_bf16m4(const __bf16 *rs1, vuint16m4_t rs2,
                                        size_t vl);
vbfloat16m8_t __riscv_vloxei16_v_bf16m8(const __bf16 *rs1, vuint16m8_t rs2,
                                        size_t vl);
vbfloat16mf4_t __riscv_vluxei16_v_bf16mf4(const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf2_t __riscv_vluxei16_v_bf16mf2(const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16m1_t __riscv_vluxei16_v_bf16m1(const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m2_t __riscv_vluxei16_v_bf16m2(const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m4_t __riscv_vluxei16_v_bf16m4(const __bf16 *rs1, vuint16m4_t rs2,
                                        size_t vl);
vbfloat16m8_t __riscv_vluxei16_v_bf16m8(const __bf16 *rs1, vuint16m8_t rs2,
                                        size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                            vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vloxei16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                            vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vloxei16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                          vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vloxei16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                          vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vloxei16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                          vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vloxei16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                          vuint16m8_t rs2, size_t vl);
vbfloat16mf4_t __riscv_vluxei16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                            vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vluxei16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                            vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vluxei16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                          vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vluxei16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                          vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vluxei16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                          vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vluxei16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                          vuint16m8_t rs2, size_t vl);
----

[[vector-indexed-store]]
==== Vector Indexed Store Intrinsics

[,c]
----
void __riscv_vsoxei16_v_bf16mf4(__bf16 *rs1, vuint16mf4_t rs2,
                                vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16mf2(__bf16 *rs1, vuint16mf2_t rs2,
                                vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m1(__bf16 *rs1, vuint16m1_t rs2, vbfloat16m1_t vs3,
                               size_t vl);
void __riscv_vsoxei16_v_bf16m2(__bf16 *rs1, vuint16m2_t rs2, vbfloat16m2_t vs3,
                               size_t vl);
void __riscv_vsoxei16_v_bf16m4(__bf16 *rs1, vuint16m4_t rs2, vbfloat16m4_t vs3,
                               size_t vl);
void __riscv_vsoxei16_v_bf16m8(__bf16 *rs1, vuint16m8_t rs2, vbfloat16m8_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16mf4(__bf16 *rs1, vuint16mf4_t rs2,
                                vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16mf2(__bf16 *rs1, vuint16mf2_t rs2,
                                vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m1(__bf16 *rs1, vuint16m1_t rs2, vbfloat16m1_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16m2(__bf16 *rs1, vuint16m2_t rs2, vbfloat16m2_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16m4(__bf16 *rs1, vuint16m4_t rs2, vbfloat16m4_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16m8(__bf16 *rs1, vuint16m8_t rs2, vbfloat16m8_t vs3,
                               size_t vl);
// masked functions
void __riscv_vsoxei16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, vuint16mf4_t rs2,
                                  vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, vuint16mf2_t rs2,
                                  vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, vuint16m1_t rs2,
                                 vbfloat16m1_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, vuint16m2_t rs2,
                                 vbfloat16m2_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, vuint16m4_t rs2,
                                 vbfloat16m4_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, vuint16m8_t rs2,
                                 vbfloat16m8_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, vuint16mf4_t rs2,
                                  vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, vuint16mf2_t rs2,
                                  vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, vuint16m1_t rs2,
                                 vbfloat16m1_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, vuint16m2_t rs2,
                                 vbfloat16m2_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, vuint16m4_t rs2,
                                 vbfloat16m4_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, vuint16m8_t rs2,
                                 vbfloat16m8_t vs3, size_t vl);
----

[[unit-stride-fault-only-first-loads]]
==== Unit-stride Fault-Only-First Loads Intrinsics

[,c]
----
vbfloat16mf4_t __riscv_vle16ff_v_bf16mf4(const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2_t __riscv_vle16ff_v_bf16mf2(const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1_t __riscv_vle16ff_v_bf16m1(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m2_t __riscv_vle16ff_v_bf16m2(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m4_t __riscv_vle16ff_v_bf16m4(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m8_t __riscv_vle16ff_v_bf16m8(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                           size_t *new_vl, size_t vl);
vbfloat16mf2_t __riscv_vle16ff_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                           size_t *new_vl, size_t vl);
vbfloat16m1_t __riscv_vle16ff_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
----
