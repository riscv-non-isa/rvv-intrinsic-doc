
=== Zvbb - Vector Bit-manipulation used in Cryptography

[[overloaded-]]
==== Vector Bit-manipulation used in Cryptography - Bitwise And-Not

[,c]
----
vuint8mf8_t __riscv_vandn(vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vandn(vuint8mf8_t vs2, uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn(vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vandn(vuint8mf4_t vs2, uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn(vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vandn(vuint8mf2_t vs2, uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn(vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn(vuint8m1_t vs2, uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn(vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn(vuint8m2_t vs2, uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn(vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn(vuint8m4_t vs2, uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn(vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn(vuint8m8_t vs2, uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn(vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vandn(vuint16mf4_t vs2, uint16_t rs1, size_t vl);
vuint16mf2_t __riscv_vandn(vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vandn(vuint16mf2_t vs2, uint16_t rs1, size_t vl);
vuint16m1_t __riscv_vandn(vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vandn(vuint16m1_t vs2, uint16_t rs1, size_t vl);
vuint16m2_t __riscv_vandn(vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vandn(vuint16m2_t vs2, uint16_t rs1, size_t vl);
vuint16m4_t __riscv_vandn(vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vandn(vuint16m4_t vs2, uint16_t rs1, size_t vl);
vuint16m8_t __riscv_vandn(vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vandn(vuint16m8_t vs2, uint16_t rs1, size_t vl);
vuint32mf2_t __riscv_vandn(vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vandn(vuint32mf2_t vs2, uint32_t rs1, size_t vl);
vuint32m1_t __riscv_vandn(vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vandn(vuint32m1_t vs2, uint32_t rs1, size_t vl);
vuint32m2_t __riscv_vandn(vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vandn(vuint32m2_t vs2, uint32_t rs1, size_t vl);
vuint32m4_t __riscv_vandn(vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vandn(vuint32m4_t vs2, uint32_t rs1, size_t vl);
vuint32m8_t __riscv_vandn(vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vandn(vuint32m8_t vs2, uint32_t rs1, size_t vl);
vuint64m1_t __riscv_vandn(vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vandn(vuint64m1_t vs2, uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vandn(vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vandn(vuint64m2_t vs2, uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vandn(vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vandn(vuint64m4_t vs2, uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vandn(vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vandn(vuint64m8_t vs2, uint64_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vandn(vbool64_t vm, vuint8mf8_t vs2, vuint8mf8_t vs1,
                          size_t vl);
vuint8mf8_t __riscv_vandn(vbool64_t vm, vuint8mf8_t vs2, uint8_t rs1,
                          size_t vl);
vuint8mf4_t __riscv_vandn(vbool32_t vm, vuint8mf4_t vs2, vuint8mf4_t vs1,
                          size_t vl);
vuint8mf4_t __riscv_vandn(vbool32_t vm, vuint8mf4_t vs2, uint8_t rs1,
                          size_t vl);
vuint8mf2_t __riscv_vandn(vbool16_t vm, vuint8mf2_t vs2, vuint8mf2_t vs1,
                          size_t vl);
vuint8mf2_t __riscv_vandn(vbool16_t vm, vuint8mf2_t vs2, uint8_t rs1,
                          size_t vl);
vuint8m1_t __riscv_vandn(vbool8_t vm, vuint8m1_t vs2, vuint8m1_t vs1,
                         size_t vl);
vuint8m1_t __riscv_vandn(vbool8_t vm, vuint8m1_t vs2, uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn(vbool4_t vm, vuint8m2_t vs2, vuint8m2_t vs1,
                         size_t vl);
vuint8m2_t __riscv_vandn(vbool4_t vm, vuint8m2_t vs2, uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn(vbool2_t vm, vuint8m4_t vs2, vuint8m4_t vs1,
                         size_t vl);
vuint8m4_t __riscv_vandn(vbool2_t vm, vuint8m4_t vs2, uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn(vbool1_t vm, vuint8m8_t vs2, vuint8m8_t vs1,
                         size_t vl);
vuint8m8_t __riscv_vandn(vbool1_t vm, vuint8m8_t vs2, uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn(vbool64_t vm, vuint16mf4_t vs2, vuint16mf4_t vs1,
                           size_t vl);
vuint16mf4_t __riscv_vandn(vbool64_t vm, vuint16mf4_t vs2, uint16_t rs1,
                           size_t vl);
vuint16mf2_t __riscv_vandn(vbool32_t vm, vuint16mf2_t vs2, vuint16mf2_t vs1,
                           size_t vl);
vuint16mf2_t __riscv_vandn(vbool32_t vm, vuint16mf2_t vs2, uint16_t rs1,
                           size_t vl);
vuint16m1_t __riscv_vandn(vbool16_t vm, vuint16m1_t vs2, vuint16m1_t vs1,
                          size_t vl);
vuint16m1_t __riscv_vandn(vbool16_t vm, vuint16m1_t vs2, uint16_t rs1,
                          size_t vl);
vuint16m2_t __riscv_vandn(vbool8_t vm, vuint16m2_t vs2, vuint16m2_t vs1,
                          size_t vl);
vuint16m2_t __riscv_vandn(vbool8_t vm, vuint16m2_t vs2, uint16_t rs1,
                          size_t vl);
vuint16m4_t __riscv_vandn(vbool4_t vm, vuint16m4_t vs2, vuint16m4_t vs1,
                          size_t vl);
vuint16m4_t __riscv_vandn(vbool4_t vm, vuint16m4_t vs2, uint16_t rs1,
                          size_t vl);
vuint16m8_t __riscv_vandn(vbool2_t vm, vuint16m8_t vs2, vuint16m8_t vs1,
                          size_t vl);
vuint16m8_t __riscv_vandn(vbool2_t vm, vuint16m8_t vs2, uint16_t rs1,
                          size_t vl);
vuint32mf2_t __riscv_vandn(vbool64_t vm, vuint32mf2_t vs2, vuint32mf2_t vs1,
                           size_t vl);
vuint32mf2_t __riscv_vandn(vbool64_t vm, vuint32mf2_t vs2, uint32_t rs1,
                           size_t vl);
vuint32m1_t __riscv_vandn(vbool32_t vm, vuint32m1_t vs2, vuint32m1_t vs1,
                          size_t vl);
vuint32m1_t __riscv_vandn(vbool32_t vm, vuint32m1_t vs2, uint32_t rs1,
                          size_t vl);
vuint32m2_t __riscv_vandn(vbool16_t vm, vuint32m2_t vs2, vuint32m2_t vs1,
                          size_t vl);
vuint32m2_t __riscv_vandn(vbool16_t vm, vuint32m2_t vs2, uint32_t rs1,
                          size_t vl);
vuint32m4_t __riscv_vandn(vbool8_t vm, vuint32m4_t vs2, vuint32m4_t vs1,
                          size_t vl);
vuint32m4_t __riscv_vandn(vbool8_t vm, vuint32m4_t vs2, uint32_t rs1,
                          size_t vl);
vuint32m8_t __riscv_vandn(vbool4_t vm, vuint32m8_t vs2, vuint32m8_t vs1,
                          size_t vl);
vuint32m8_t __riscv_vandn(vbool4_t vm, vuint32m8_t vs2, uint32_t rs1,
                          size_t vl);
vuint64m1_t __riscv_vandn(vbool64_t vm, vuint64m1_t vs2, vuint64m1_t vs1,
                          size_t vl);
vuint64m1_t __riscv_vandn(vbool64_t vm, vuint64m1_t vs2, uint64_t rs1,
                          size_t vl);
vuint64m2_t __riscv_vandn(vbool32_t vm, vuint64m2_t vs2, vuint64m2_t vs1,
                          size_t vl);
vuint64m2_t __riscv_vandn(vbool32_t vm, vuint64m2_t vs2, uint64_t rs1,
                          size_t vl);
vuint64m4_t __riscv_vandn(vbool16_t vm, vuint64m4_t vs2, vuint64m4_t vs1,
                          size_t vl);
vuint64m4_t __riscv_vandn(vbool16_t vm, vuint64m4_t vs2, uint64_t rs1,
                          size_t vl);
vuint64m8_t __riscv_vandn(vbool8_t vm, vuint64m8_t vs2, vuint64m8_t vs1,
                          size_t vl);
vuint64m8_t __riscv_vandn(vbool8_t vm, vuint64m8_t vs2, uint64_t rs1,
                          size_t vl);
----

[[overloaded-]]
==== Vector Basic Bit-manipulation - Reverse

[,c]
----
vuint8mf8_t __riscv_vbrev(vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev(vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev(vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev(vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev(vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev(vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev(vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev(vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev(vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev(vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev(vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev(vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev(vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev(vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev(vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev(vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev(vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev(vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev(vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev(vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev(vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev(vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8(vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8(vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8(vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8(vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8(vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8(vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8(vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8(vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8(vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8(vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8(vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8(vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8(vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8(vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8(vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8(vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8(vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8(vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8(vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8(vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8(vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8(vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8(vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8(vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8(vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8(vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vrev8(vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vrev8(vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vrev8(vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vrev8(vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8(vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8(vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8(vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8(vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8(vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8(vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8(vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8(vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8(vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8(vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8(vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8(vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8(vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8(vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vbrev(vbool64_t vm, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev(vbool32_t vm, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev(vbool16_t vm, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev(vbool8_t vm, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev(vbool4_t vm, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev(vbool2_t vm, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev(vbool1_t vm, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev(vbool64_t vm, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev(vbool32_t vm, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev(vbool16_t vm, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev(vbool8_t vm, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev(vbool4_t vm, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev(vbool2_t vm, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev(vbool64_t vm, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev(vbool32_t vm, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev(vbool16_t vm, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev(vbool8_t vm, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev(vbool4_t vm, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev(vbool64_t vm, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev(vbool32_t vm, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev(vbool16_t vm, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev(vbool8_t vm, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8(vbool64_t vm, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8(vbool32_t vm, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8(vbool16_t vm, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8(vbool8_t vm, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8(vbool4_t vm, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8(vbool2_t vm, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8(vbool1_t vm, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8(vbool64_t vm, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8(vbool32_t vm, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8(vbool16_t vm, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8(vbool8_t vm, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8(vbool4_t vm, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8(vbool2_t vm, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8(vbool64_t vm, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8(vbool32_t vm, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8(vbool16_t vm, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8(vbool8_t vm, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8(vbool4_t vm, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8(vbool64_t vm, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8(vbool32_t vm, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8(vbool16_t vm, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8(vbool8_t vm, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8(vbool64_t vm, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8(vbool32_t vm, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8(vbool16_t vm, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8(vbool8_t vm, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vrev8(vbool4_t vm, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vrev8(vbool2_t vm, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vrev8(vbool1_t vm, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vrev8(vbool64_t vm, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8(vbool32_t vm, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8(vbool16_t vm, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8(vbool8_t vm, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8(vbool4_t vm, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8(vbool2_t vm, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8(vbool64_t vm, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8(vbool32_t vm, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8(vbool16_t vm, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8(vbool8_t vm, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8(vbool4_t vm, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8(vbool64_t vm, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8(vbool32_t vm, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8(vbool16_t vm, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8(vbool8_t vm, vuint64m8_t vs2, size_t vl);
----

[[overloaded-]]
==== Vector Basic Bit-manipulation - Count Bits

[,c]
----
vuint8mf8_t __riscv_vclz(vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vclz(vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vclz(vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vclz(vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vclz(vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vclz(vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vclz(vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vclz(vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vclz(vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vclz(vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vclz(vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vclz(vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vclz(vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vclz(vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vclz(vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vclz(vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vclz(vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vclz(vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vclz(vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vclz(vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vclz(vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vclz(vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vctz(vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vctz(vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vctz(vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vctz(vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vctz(vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vctz(vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vctz(vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vctz(vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vctz(vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vctz(vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vctz(vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vctz(vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vctz(vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vctz(vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vctz(vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vctz(vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vctz(vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vctz(vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vctz(vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vctz(vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vctz(vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vctz(vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vclz(vbool64_t vm, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vclz(vbool32_t vm, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vclz(vbool16_t vm, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vclz(vbool8_t vm, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vclz(vbool4_t vm, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vclz(vbool2_t vm, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vclz(vbool1_t vm, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vclz(vbool64_t vm, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vclz(vbool32_t vm, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vclz(vbool16_t vm, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vclz(vbool8_t vm, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vclz(vbool4_t vm, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vclz(vbool2_t vm, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vclz(vbool64_t vm, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vclz(vbool32_t vm, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vclz(vbool16_t vm, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vclz(vbool8_t vm, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vclz(vbool4_t vm, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vclz(vbool64_t vm, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vclz(vbool32_t vm, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vclz(vbool16_t vm, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vclz(vbool8_t vm, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vctz(vbool64_t vm, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vctz(vbool32_t vm, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vctz(vbool16_t vm, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vctz(vbool8_t vm, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vctz(vbool4_t vm, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vctz(vbool2_t vm, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vctz(vbool1_t vm, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vctz(vbool64_t vm, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vctz(vbool32_t vm, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vctz(vbool16_t vm, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vctz(vbool8_t vm, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vctz(vbool4_t vm, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vctz(vbool2_t vm, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vctz(vbool64_t vm, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vctz(vbool32_t vm, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vctz(vbool16_t vm, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vctz(vbool8_t vm, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vctz(vbool4_t vm, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vctz(vbool64_t vm, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vctz(vbool32_t vm, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vctz(vbool16_t vm, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vctz(vbool8_t vm, vuint64m8_t vs2, size_t vl);
----

[[overloaded-]]
==== Vector Basic Bit-manipulation - Vector Population Count

[,c]
----
vuint8mf8_t __riscv_vcpop(vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vcpop(vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vcpop(vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vcpop(vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vcpop(vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vcpop(vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vcpop(vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vcpop(vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vcpop(vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vcpop(vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vcpop(vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vcpop(vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vcpop(vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vcpop(vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vcpop(vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vcpop(vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vcpop(vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vcpop(vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vcpop(vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vcpop(vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vcpop(vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vcpop(vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vcpop(vbool64_t vm, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vcpop(vbool32_t vm, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vcpop(vbool16_t vm, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vcpop(vbool8_t vm, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vcpop(vbool4_t vm, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vcpop(vbool2_t vm, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vcpop(vbool1_t vm, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vcpop(vbool64_t vm, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vcpop(vbool32_t vm, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vcpop(vbool16_t vm, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vcpop(vbool8_t vm, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vcpop(vbool4_t vm, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vcpop(vbool2_t vm, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vcpop(vbool64_t vm, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vcpop(vbool32_t vm, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vcpop(vbool16_t vm, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vcpop(vbool8_t vm, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vcpop(vbool4_t vm, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vcpop(vbool64_t vm, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vcpop(vbool32_t vm, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vcpop(vbool16_t vm, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vcpop(vbool8_t vm, vuint64m8_t vs2, size_t vl);
----

[[overloaded-]]
==== Vector Bit-manipulation used in Cryptography - Rotate

[,c]
----
vuint8mf8_t __riscv_vrol(vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vrol(vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol(vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vrol(vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol(vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vrol(vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol(vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol(vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol(vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol(vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol(vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol(vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol(vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol(vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol(vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vrol(vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vrol(vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vrol(vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vrol(vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vrol(vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol(vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vrol(vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol(vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vrol(vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol(vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vrol(vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol(vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vrol(vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vrol(vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vrol(vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol(vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vrol(vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol(vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vrol(vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol(vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vrol(vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol(vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vrol(vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol(vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vrol(vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol(vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vrol(vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol(vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vrol(vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror(vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vror(vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror(vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vror(vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror(vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vror(vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror(vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror(vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vror(vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror(vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vror(vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror(vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vror(vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror(vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror(vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vror(vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vror(vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vror(vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vror(vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vror(vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror(vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vror(vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror(vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vror(vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror(vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vror(vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror(vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vror(vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vror(vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vror(vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror(vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vror(vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror(vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vror(vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror(vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vror(vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror(vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vror(vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror(vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vror(vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror(vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vror(vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror(vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vror(vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vrol(vbool64_t vm, vuint8mf8_t vs2, vuint8mf8_t vs1,
                         size_t vl);
vuint8mf8_t __riscv_vrol(vbool64_t vm, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol(vbool32_t vm, vuint8mf4_t vs2, vuint8mf4_t vs1,
                         size_t vl);
vuint8mf4_t __riscv_vrol(vbool32_t vm, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol(vbool16_t vm, vuint8mf2_t vs2, vuint8mf2_t vs1,
                         size_t vl);
vuint8mf2_t __riscv_vrol(vbool16_t vm, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol(vbool8_t vm, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol(vbool8_t vm, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol(vbool4_t vm, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol(vbool4_t vm, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol(vbool2_t vm, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol(vbool2_t vm, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol(vbool1_t vm, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol(vbool1_t vm, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol(vbool64_t vm, vuint16mf4_t vs2, vuint16mf4_t vs1,
                          size_t vl);
vuint16mf4_t __riscv_vrol(vbool64_t vm, vuint16mf4_t vs2, size_t rs1,
                          size_t vl);
vuint16mf2_t __riscv_vrol(vbool32_t vm, vuint16mf2_t vs2, vuint16mf2_t vs1,
                          size_t vl);
vuint16mf2_t __riscv_vrol(vbool32_t vm, vuint16mf2_t vs2, size_t rs1,
                          size_t vl);
vuint16m1_t __riscv_vrol(vbool16_t vm, vuint16m1_t vs2, vuint16m1_t vs1,
                         size_t vl);
vuint16m1_t __riscv_vrol(vbool16_t vm, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol(vbool8_t vm, vuint16m2_t vs2, vuint16m2_t vs1,
                         size_t vl);
vuint16m2_t __riscv_vrol(vbool8_t vm, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol(vbool4_t vm, vuint16m4_t vs2, vuint16m4_t vs1,
                         size_t vl);
vuint16m4_t __riscv_vrol(vbool4_t vm, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol(vbool2_t vm, vuint16m8_t vs2, vuint16m8_t vs1,
                         size_t vl);
vuint16m8_t __riscv_vrol(vbool2_t vm, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol(vbool64_t vm, vuint32mf2_t vs2, vuint32mf2_t vs1,
                          size_t vl);
vuint32mf2_t __riscv_vrol(vbool64_t vm, vuint32mf2_t vs2, size_t rs1,
                          size_t vl);
vuint32m1_t __riscv_vrol(vbool32_t vm, vuint32m1_t vs2, vuint32m1_t vs1,
                         size_t vl);
vuint32m1_t __riscv_vrol(vbool32_t vm, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol(vbool16_t vm, vuint32m2_t vs2, vuint32m2_t vs1,
                         size_t vl);
vuint32m2_t __riscv_vrol(vbool16_t vm, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol(vbool8_t vm, vuint32m4_t vs2, vuint32m4_t vs1,
                         size_t vl);
vuint32m4_t __riscv_vrol(vbool8_t vm, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol(vbool4_t vm, vuint32m8_t vs2, vuint32m8_t vs1,
                         size_t vl);
vuint32m8_t __riscv_vrol(vbool4_t vm, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol(vbool64_t vm, vuint64m1_t vs2, vuint64m1_t vs1,
                         size_t vl);
vuint64m1_t __riscv_vrol(vbool64_t vm, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol(vbool32_t vm, vuint64m2_t vs2, vuint64m2_t vs1,
                         size_t vl);
vuint64m2_t __riscv_vrol(vbool32_t vm, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol(vbool16_t vm, vuint64m4_t vs2, vuint64m4_t vs1,
                         size_t vl);
vuint64m4_t __riscv_vrol(vbool16_t vm, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol(vbool8_t vm, vuint64m8_t vs2, vuint64m8_t vs1,
                         size_t vl);
vuint64m8_t __riscv_vrol(vbool8_t vm, vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror(vbool64_t vm, vuint8mf8_t vs2, vuint8mf8_t vs1,
                         size_t vl);
vuint8mf8_t __riscv_vror(vbool64_t vm, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror(vbool32_t vm, vuint8mf4_t vs2, vuint8mf4_t vs1,
                         size_t vl);
vuint8mf4_t __riscv_vror(vbool32_t vm, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror(vbool16_t vm, vuint8mf2_t vs2, vuint8mf2_t vs1,
                         size_t vl);
vuint8mf2_t __riscv_vror(vbool16_t vm, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror(vbool8_t vm, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror(vbool8_t vm, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vror(vbool4_t vm, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror(vbool4_t vm, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vror(vbool2_t vm, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror(vbool2_t vm, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vror(vbool1_t vm, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror(vbool1_t vm, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror(vbool64_t vm, vuint16mf4_t vs2, vuint16mf4_t vs1,
                          size_t vl);
vuint16mf4_t __riscv_vror(vbool64_t vm, vuint16mf4_t vs2, size_t rs1,
                          size_t vl);
vuint16mf2_t __riscv_vror(vbool32_t vm, vuint16mf2_t vs2, vuint16mf2_t vs1,
                          size_t vl);
vuint16mf2_t __riscv_vror(vbool32_t vm, vuint16mf2_t vs2, size_t rs1,
                          size_t vl);
vuint16m1_t __riscv_vror(vbool16_t vm, vuint16m1_t vs2, vuint16m1_t vs1,
                         size_t vl);
vuint16m1_t __riscv_vror(vbool16_t vm, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror(vbool8_t vm, vuint16m2_t vs2, vuint16m2_t vs1,
                         size_t vl);
vuint16m2_t __riscv_vror(vbool8_t vm, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror(vbool4_t vm, vuint16m4_t vs2, vuint16m4_t vs1,
                         size_t vl);
vuint16m4_t __riscv_vror(vbool4_t vm, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror(vbool2_t vm, vuint16m8_t vs2, vuint16m8_t vs1,
                         size_t vl);
vuint16m8_t __riscv_vror(vbool2_t vm, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror(vbool64_t vm, vuint32mf2_t vs2, vuint32mf2_t vs1,
                          size_t vl);
vuint32mf2_t __riscv_vror(vbool64_t vm, vuint32mf2_t vs2, size_t rs1,
                          size_t vl);
vuint32m1_t __riscv_vror(vbool32_t vm, vuint32m1_t vs2, vuint32m1_t vs1,
                         size_t vl);
vuint32m1_t __riscv_vror(vbool32_t vm, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror(vbool16_t vm, vuint32m2_t vs2, vuint32m2_t vs1,
                         size_t vl);
vuint32m2_t __riscv_vror(vbool16_t vm, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror(vbool8_t vm, vuint32m4_t vs2, vuint32m4_t vs1,
                         size_t vl);
vuint32m4_t __riscv_vror(vbool8_t vm, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror(vbool4_t vm, vuint32m8_t vs2, vuint32m8_t vs1,
                         size_t vl);
vuint32m8_t __riscv_vror(vbool4_t vm, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror(vbool64_t vm, vuint64m1_t vs2, vuint64m1_t vs1,
                         size_t vl);
vuint64m1_t __riscv_vror(vbool64_t vm, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror(vbool32_t vm, vuint64m2_t vs2, vuint64m2_t vs1,
                         size_t vl);
vuint64m2_t __riscv_vror(vbool32_t vm, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror(vbool16_t vm, vuint64m4_t vs2, vuint64m4_t vs1,
                         size_t vl);
vuint64m4_t __riscv_vror(vbool16_t vm, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror(vbool8_t vm, vuint64m8_t vs2, vuint64m8_t vs1,
                         size_t vl);
vuint64m8_t __riscv_vror(vbool8_t vm, vuint64m8_t vs2, size_t rs1, size_t vl);
----

[[overloaded-]]
==== Vector Basic Bit-manipulation used - Widening Shift

[,c]
----
vuint16mf4_t __riscv_vwsll(vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint16mf4_t __riscv_vwsll(vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vwsll(vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint16mf2_t __riscv_vwsll(vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vwsll(vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint16m1_t __riscv_vwsll(vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll(vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint16m2_t __riscv_vwsll(vuint8m1_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll(vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint16m4_t __riscv_vwsll(vuint8m2_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll(vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint16m8_t __riscv_vwsll(vuint8m4_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll(vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint32mf2_t __riscv_vwsll(vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vwsll(vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vwsll(vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll(vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint32m2_t __riscv_vwsll(vuint16m1_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll(vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint32m4_t __riscv_vwsll(vuint16m2_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll(vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint32m8_t __riscv_vwsll(vuint16m4_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll(vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint64m1_t __riscv_vwsll(vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll(vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint64m2_t __riscv_vwsll(vuint32m1_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll(vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint64m4_t __riscv_vwsll(vuint32m2_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll(vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint64m8_t __riscv_vwsll(vuint32m4_t vs2, size_t rs1, size_t vl);
// masked functions
vuint16mf4_t __riscv_vwsll(vbool64_t vm, vuint8mf8_t vs2, vuint8mf8_t vs1,
                           size_t vl);
vuint16mf4_t __riscv_vwsll(vbool64_t vm, vuint8mf8_t vs2, size_t rs1,
                           size_t vl);
vuint16mf2_t __riscv_vwsll(vbool32_t vm, vuint8mf4_t vs2, vuint8mf4_t vs1,
                           size_t vl);
vuint16mf2_t __riscv_vwsll(vbool32_t vm, vuint8mf4_t vs2, size_t rs1,
                           size_t vl);
vuint16m1_t __riscv_vwsll(vbool16_t vm, vuint8mf2_t vs2, vuint8mf2_t vs1,
                          size_t vl);
vuint16m1_t __riscv_vwsll(vbool16_t vm, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll(vbool8_t vm, vuint8m1_t vs2, vuint8m1_t vs1,
                          size_t vl);
vuint16m2_t __riscv_vwsll(vbool8_t vm, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll(vbool4_t vm, vuint8m2_t vs2, vuint8m2_t vs1,
                          size_t vl);
vuint16m4_t __riscv_vwsll(vbool4_t vm, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll(vbool2_t vm, vuint8m4_t vs2, vuint8m4_t vs1,
                          size_t vl);
vuint16m8_t __riscv_vwsll(vbool2_t vm, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll(vbool64_t vm, vuint16mf4_t vs2, vuint16mf4_t vs1,
                           size_t vl);
vuint32mf2_t __riscv_vwsll(vbool64_t vm, vuint16mf4_t vs2, size_t rs1,
                           size_t vl);
vuint32m1_t __riscv_vwsll(vbool32_t vm, vuint16mf2_t vs2, vuint16mf2_t vs1,
                          size_t vl);
vuint32m1_t __riscv_vwsll(vbool32_t vm, vuint16mf2_t vs2, size_t rs1,
                          size_t vl);
vuint32m2_t __riscv_vwsll(vbool16_t vm, vuint16m1_t vs2, vuint16m1_t vs1,
                          size_t vl);
vuint32m2_t __riscv_vwsll(vbool16_t vm, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll(vbool8_t vm, vuint16m2_t vs2, vuint16m2_t vs1,
                          size_t vl);
vuint32m4_t __riscv_vwsll(vbool8_t vm, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll(vbool4_t vm, vuint16m4_t vs2, vuint16m4_t vs1,
                          size_t vl);
vuint32m8_t __riscv_vwsll(vbool4_t vm, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll(vbool64_t vm, vuint32mf2_t vs2, vuint32mf2_t vs1,
                          size_t vl);
vuint64m1_t __riscv_vwsll(vbool64_t vm, vuint32mf2_t vs2, size_t rs1,
                          size_t vl);
vuint64m2_t __riscv_vwsll(vbool32_t vm, vuint32m1_t vs2, vuint32m1_t vs1,
                          size_t vl);
vuint64m2_t __riscv_vwsll(vbool32_t vm, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll(vbool16_t vm, vuint32m2_t vs2, vuint32m2_t vs1,
                          size_t vl);
vuint64m4_t __riscv_vwsll(vbool16_t vm, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll(vbool8_t vm, vuint32m4_t vs2, vuint32m4_t vs1,
                          size_t vl);
vuint64m8_t __riscv_vwsll(vbool8_t vm, vuint32m4_t vs2, size_t rs1, size_t vl);
----
