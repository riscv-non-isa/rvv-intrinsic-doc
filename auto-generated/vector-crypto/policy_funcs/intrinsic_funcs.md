
## Zvbb - Vector Bit-manipulation used in Cryptography:

### [Vector Bit-manipulation used in Cryptography - Bitwise And-Not]():

**Prototypes:**
``` C
vuint8mf8_t __riscv_vandn_vv_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, uint16_t rs1, size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, uint16_t rs1, size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, uint16_t rs1, size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, uint16_t rs1, size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, uint16_t rs1, size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, uint16_t rs1, size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, uint32_t rs1, size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, uint32_t rs1, size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, uint32_t rs1, size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, uint32_t rs1, size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, uint32_t rs1, size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, uint64_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vandn_vv_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, uint16_t rs1, size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, uint16_t rs1, size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, uint16_t rs1, size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, uint16_t rs1, size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, uint16_t rs1, size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, uint16_t rs1, size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, uint32_t rs1, size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, uint32_t rs1, size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, uint32_t rs1, size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, uint32_t rs1, size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, uint32_t rs1, size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, uint64_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vandn_vv_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, uint16_t rs1, size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, uint16_t rs1, size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, uint16_t rs1, size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, uint16_t rs1, size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, uint16_t rs1, size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, uint16_t rs1, size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, uint32_t rs1, size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, uint32_t rs1, size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, uint32_t rs1, size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, uint32_t rs1, size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, uint32_t rs1, size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, uint64_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vandn_vv_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, uint16_t rs1, size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, uint16_t rs1, size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, uint16_t rs1, size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, uint16_t rs1, size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, uint16_t rs1, size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, uint16_t rs1, size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, uint32_t rs1, size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, uint32_t rs1, size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, uint32_t rs1, size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, uint32_t rs1, size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, uint32_t rs1, size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, uint64_t rs1, size_t vl);
```

### [Vector Bit-manipulation used in Cryptography - Reverse Bits]():

**Prototypes:**
``` C
vuint8mf8_t __riscv_vbrev_v_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vbrev_v_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vbrev_v_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vbrev_v_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t vl);
```

### [Vector Bit-manipulation used in Cryptography - Count Bits]():
This operation don't have Policy Intrinsic Functions.

### [Vector Bit-manipulation used in Cryptography - Rotate]():

**Prototypes:**
``` C
vuint8mf8_t __riscv_vrol_vv_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_tu (vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_tu (vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_tu (vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_tu (vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_tu (vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_tu (vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_tu (vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vrol_vv_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_tum (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vrol_vv_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_tumu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vrol_vv_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, size_t rs1, size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, size_t rs1, size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, size_t rs1, size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_mu (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t vs2, size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
```

### [Vector Bit-manipulation used in Cryptography - Shift]():

**Prototypes:**
``` C
vuint16mf4_t __riscv_vwsll_vv_u16mf4_tu (vuint16mf4_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_tu (vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_tu (vuint16mf2_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_tu (vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_tu (vuint16m1_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_tu (vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_tu (vuint16m2_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_tu (vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_tu (vuint16m4_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_tu (vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_tu (vuint16m8_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_tu (vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_tu (vuint32mf2_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_tu (vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_tu (vuint32m1_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_tu (vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_tu (vuint32m2_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_tu (vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_tu (vuint32m4_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_tu (vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_tu (vuint32m8_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_tu (vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_tu (vuint64m1_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_tu (vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_tu (vuint64m2_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_tu (vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_tu (vuint64m4_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_tu (vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_tu (vuint64m8_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_tu (vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint16mf4_t __riscv_vwsll_vv_u16mf4_tum (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_tum (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_tum (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_tum (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_tum (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_tum (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_tum (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_tum (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_tum (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_tum (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_tum (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_tum (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_tum (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_tum (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_tum (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_tum (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_tum (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_tum (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_tum (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_tum (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_tum (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint16mf4_t __riscv_vwsll_vv_u16mf4_tumu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_tumu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_tumu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_tumu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_tumu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_tumu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_tumu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_tumu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_tumu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_tumu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_tumu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_tumu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_tumu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_tumu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_tumu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_tumu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_tumu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_tumu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_tumu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_tumu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_tumu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint16mf4_t __riscv_vwsll_vv_u16mf4_mu (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t vs2, vuint8mf8_t vs1, size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_mu (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t vs2, vuint8mf4_t vs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_mu (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t vs2, vuint8mf2_t vs1, size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_mu (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t vs2, vuint8m1_t vs1, size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_mu (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t vs2, vuint8m2_t vs1, size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_mu (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t vs2, vuint8m4_t vs1, size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_mu (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_mu (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_mu (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_mu (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t vs2, vuint16m1_t vs1, size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_mu (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t vs2, vuint16m2_t vs1, size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_mu (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t vs2, vuint16m4_t vs1, size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_mu (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_mu (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t vs2, vuint32mf2_t vs1, size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_mu (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_mu (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t vs2, vuint32m1_t vs1, size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_mu (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_mu (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t vs2, vuint32m2_t vs1, size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_mu (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_mu (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t vs2, vuint32m4_t vs1, size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_mu (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t vs2, size_t rs1, size_t vl);
```
