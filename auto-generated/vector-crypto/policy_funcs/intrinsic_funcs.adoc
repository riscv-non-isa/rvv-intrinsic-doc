
=== Zvbb - Vector Bit-manipulation used in Cryptography

[[policy-variant-]]
==== Vector Bit-manipulation used in Cryptography - Bitwise And-Not

[,c]
----
vuint8mf8_t __riscv_vandn_vv_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                      vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                      uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                      vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                      uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                      vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                      uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2,
                                    vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, uint8_t rs1,
                                    size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2,
                                    vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, uint8_t rs1,
                                    size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2,
                                    vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, uint8_t rs1,
                                    size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2,
                                    vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, uint8_t rs1,
                                    size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                        vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                        uint16_t rs1, size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                        vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                        uint16_t rs1, size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                      vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                      uint16_t rs1, size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                      vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                      uint16_t rs1, size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                      vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                      uint16_t rs1, size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                      vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                      uint16_t rs1, size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        uint32_t rs1, size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      uint32_t rs1, size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      uint32_t rs1, size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      uint32_t rs1, size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      uint32_t rs1, size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                      vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                      uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                      vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                      uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                      vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                      uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                      vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                      uint64_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vandn_vv_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, vuint8mf8_t vs1,
                                       size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, vuint8mf4_t vs1,
                                       size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, vuint8mf2_t vs1,
                                       size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, vuint16mf4_t vs1,
                                         size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, uint16_t rs1,
                                         size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, vuint16mf2_t vs1,
                                         size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, uint16_t rs1,
                                         size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, vuint16m1_t vs1,
                                       size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, uint16_t rs1,
                                       size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, vuint16m2_t vs1,
                                       size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, uint16_t rs1,
                                       size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, vuint16m4_t vs1,
                                       size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, uint16_t rs1,
                                       size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, vuint16m8_t vs1,
                                       size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, uint16_t rs1,
                                       size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, vuint32mf2_t vs1,
                                         size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, uint32_t rs1,
                                         size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, vuint32m1_t vs1,
                                       size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, uint32_t rs1,
                                       size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, vuint32m2_t vs1,
                                       size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, uint32_t rs1,
                                       size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, vuint32m4_t vs1,
                                       size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, uint32_t rs1,
                                       size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, vuint32m8_t vs1,
                                       size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, uint32_t rs1,
                                       size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, vuint64m1_t vs1,
                                       size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, uint64_t rs1,
                                       size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, vuint64m2_t vs1,
                                       size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, uint64_t rs1,
                                       size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, vuint64m4_t vs1,
                                       size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, uint64_t rs1,
                                       size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, vuint64m8_t vs1,
                                       size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, uint64_t rs1,
                                       size_t vl);
// masked functions
vuint8mf8_t __riscv_vandn_vv_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                        vuint8mf8_t vs2, vuint8mf8_t vs1,
                                        size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                        vuint8mf8_t vs2, uint8_t rs1,
                                        size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                        vuint8mf4_t vs2, vuint8mf4_t vs1,
                                        size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                        vuint8mf4_t vs2, uint8_t rs1,
                                        size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                        vuint8mf2_t vs2, vuint8mf2_t vs1,
                                        size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                        vuint8mf2_t vs2, uint8_t rs1,
                                        size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_tumu(vbool8_t vm, vuint8m1_t vd,
                                      vuint8m1_t vs2, vuint8m1_t vs1,
                                      size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_tumu(vbool8_t vm, vuint8m1_t vd,
                                      vuint8m1_t vs2, uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_tumu(vbool4_t vm, vuint8m2_t vd,
                                      vuint8m2_t vs2, vuint8m2_t vs1,
                                      size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_tumu(vbool4_t vm, vuint8m2_t vd,
                                      vuint8m2_t vs2, uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_tumu(vbool2_t vm, vuint8m4_t vd,
                                      vuint8m4_t vs2, vuint8m4_t vs1,
                                      size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_tumu(vbool2_t vm, vuint8m4_t vd,
                                      vuint8m4_t vs2, uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_tumu(vbool1_t vm, vuint8m8_t vd,
                                      vuint8m8_t vs2, vuint8m8_t vs1,
                                      size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_tumu(vbool1_t vm, vuint8m8_t vd,
                                      vuint8m8_t vs2, uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                          vuint16mf4_t vs2, vuint16mf4_t vs1,
                                          size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                          vuint16mf4_t vs2, uint16_t rs1,
                                          size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                          vuint16mf2_t vs2, vuint16mf2_t vs1,
                                          size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                          vuint16mf2_t vs2, uint16_t rs1,
                                          size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                        vuint16m1_t vs2, vuint16m1_t vs1,
                                        size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                        vuint16m1_t vs2, uint16_t rs1,
                                        size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                        vuint16m2_t vs2, vuint16m2_t vs1,
                                        size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                        vuint16m2_t vs2, uint16_t rs1,
                                        size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                        vuint16m4_t vs2, vuint16m4_t vs1,
                                        size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                        vuint16m4_t vs2, uint16_t rs1,
                                        size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                        vuint16m8_t vs2, vuint16m8_t vs1,
                                        size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                        vuint16m8_t vs2, uint16_t rs1,
                                        size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                          vuint32mf2_t vs2, vuint32mf2_t vs1,
                                          size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                          vuint32mf2_t vs2, uint32_t rs1,
                                          size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                        vuint32m1_t vs2, vuint32m1_t vs1,
                                        size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                        vuint32m1_t vs2, uint32_t rs1,
                                        size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                        vuint32m2_t vs2, vuint32m2_t vs1,
                                        size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                        vuint32m2_t vs2, uint32_t rs1,
                                        size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                        vuint32m4_t vs2, vuint32m4_t vs1,
                                        size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                        vuint32m4_t vs2, uint32_t rs1,
                                        size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                        vuint32m8_t vs2, vuint32m8_t vs1,
                                        size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                        vuint32m8_t vs2, uint32_t rs1,
                                        size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                        vuint64m1_t vs2, vuint64m1_t vs1,
                                        size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                        vuint64m1_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                        vuint64m2_t vs2, vuint64m2_t vs1,
                                        size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                        vuint64m2_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                        vuint64m4_t vs2, vuint64m4_t vs1,
                                        size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                        vuint64m4_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                        vuint64m8_t vs2, vuint64m8_t vs1,
                                        size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                        vuint64m8_t vs2, uint64_t rs1,
                                        size_t vl);
// masked functions
vuint8mf8_t __riscv_vandn_vv_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, vuint8mf8_t vs1,
                                      size_t vl);
vuint8mf8_t __riscv_vandn_vx_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, uint8_t rs1, size_t vl);
vuint8mf4_t __riscv_vandn_vv_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, vuint8mf4_t vs1,
                                      size_t vl);
vuint8mf4_t __riscv_vandn_vx_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, uint8_t rs1, size_t vl);
vuint8mf2_t __riscv_vandn_vv_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, vuint8mf2_t vs1,
                                      size_t vl);
vuint8mf2_t __riscv_vandn_vx_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, uint8_t rs1, size_t vl);
vuint8m1_t __riscv_vandn_vv_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vandn_vx_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    uint8_t rs1, size_t vl);
vuint8m2_t __riscv_vandn_vv_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vandn_vx_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    uint8_t rs1, size_t vl);
vuint8m4_t __riscv_vandn_vv_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vandn_vx_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    uint8_t rs1, size_t vl);
vuint8m8_t __riscv_vandn_vv_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vandn_vx_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    uint8_t rs1, size_t vl);
vuint16mf4_t __riscv_vandn_vv_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, vuint16mf4_t vs1,
                                        size_t vl);
vuint16mf4_t __riscv_vandn_vx_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, uint16_t rs1,
                                        size_t vl);
vuint16mf2_t __riscv_vandn_vv_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, vuint16mf2_t vs1,
                                        size_t vl);
vuint16mf2_t __riscv_vandn_vx_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, uint16_t rs1,
                                        size_t vl);
vuint16m1_t __riscv_vandn_vv_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, vuint16m1_t vs1,
                                      size_t vl);
vuint16m1_t __riscv_vandn_vx_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, uint16_t rs1, size_t vl);
vuint16m2_t __riscv_vandn_vv_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, vuint16m2_t vs1,
                                      size_t vl);
vuint16m2_t __riscv_vandn_vx_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, uint16_t rs1, size_t vl);
vuint16m4_t __riscv_vandn_vv_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, vuint16m4_t vs1,
                                      size_t vl);
vuint16m4_t __riscv_vandn_vx_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, uint16_t rs1, size_t vl);
vuint16m8_t __riscv_vandn_vv_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, vuint16m8_t vs1,
                                      size_t vl);
vuint16m8_t __riscv_vandn_vx_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, uint16_t rs1, size_t vl);
vuint32mf2_t __riscv_vandn_vv_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, vuint32mf2_t vs1,
                                        size_t vl);
vuint32mf2_t __riscv_vandn_vx_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, uint32_t rs1,
                                        size_t vl);
vuint32m1_t __riscv_vandn_vv_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, vuint32m1_t vs1,
                                      size_t vl);
vuint32m1_t __riscv_vandn_vx_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, uint32_t rs1, size_t vl);
vuint32m2_t __riscv_vandn_vv_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, vuint32m2_t vs1,
                                      size_t vl);
vuint32m2_t __riscv_vandn_vx_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, uint32_t rs1, size_t vl);
vuint32m4_t __riscv_vandn_vv_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, vuint32m4_t vs1,
                                      size_t vl);
vuint32m4_t __riscv_vandn_vx_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, uint32_t rs1, size_t vl);
vuint32m8_t __riscv_vandn_vv_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, vuint32m8_t vs1,
                                      size_t vl);
vuint32m8_t __riscv_vandn_vx_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, uint32_t rs1, size_t vl);
vuint64m1_t __riscv_vandn_vv_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, vuint64m1_t vs1,
                                      size_t vl);
vuint64m1_t __riscv_vandn_vx_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vandn_vv_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, vuint64m2_t vs1,
                                      size_t vl);
vuint64m2_t __riscv_vandn_vx_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vandn_vv_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, vuint64m4_t vs1,
                                      size_t vl);
vuint64m4_t __riscv_vandn_vx_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vandn_vv_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, vuint64m8_t vs1,
                                      size_t vl);
vuint64m8_t __riscv_vandn_vx_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, uint64_t rs1, size_t vl);
----

[[policy-variant-]]
==== Vector Basic Bit-manipulation - Reverse

[,c]
----
vuint8mf8_t __riscv_vbrev_v_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                     size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                     size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                     size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                       size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                       size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                     size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                     size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                     size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                     size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                       size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                     size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                     size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                     size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                     size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                     size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                     size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                     size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                     size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                      size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                      size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                      size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                        size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                        size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                      size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                      size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                      size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                      size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                      size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                      size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                      size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                      size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                     size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                     size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                     size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                       size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                       size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                     size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                     size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                     size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                     size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                       size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                     size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                     size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                     size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                     size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                     size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                     size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                     size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                     size_t vl);
// masked functions
vuint8mf8_t __riscv_vbrev_v_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vbrev_v_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                        vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                        vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                        vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_tumu(vbool8_t vm, vuint8m1_t vd,
                                      vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_tumu(vbool4_t vm, vuint8m2_t vd,
                                      vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_tumu(vbool2_t vm, vuint8m4_t vd,
                                      vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_tumu(vbool1_t vm, vuint8m8_t vd,
                                      vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                          vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                          vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                        vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                        vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                        vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                        vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                          vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                        vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                        vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                        vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                        vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                        vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                        vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                        vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                        vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vbrev_v_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev_v_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev_v_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev_v_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   size_t vl);
vuint8m2_t __riscv_vbrev_v_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   size_t vl);
vuint8m4_t __riscv_vbrev_v_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   size_t vl);
vuint8m8_t __riscv_vbrev_v_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   size_t vl);
vuint16mf4_t __riscv_vbrev_v_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev_v_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev_v_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev_v_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev_v_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev_v_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev_v_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev_v_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev_v_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev_v_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev_v_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev_v_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev_v_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev_v_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev_v_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vbrev8_v_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vbrev8_v_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vbrev8_v_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vbrev8_v_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t vl);
vuint8m2_t __riscv_vbrev8_v_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t vl);
vuint8m4_t __riscv_vbrev8_v_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t vl);
vuint8m8_t __riscv_vbrev8_v_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t vl);
vuint16mf4_t __riscv_vbrev8_v_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vbrev8_v_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vbrev8_v_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vbrev8_v_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vbrev8_v_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vbrev8_v_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vbrev8_v_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vbrev8_v_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vbrev8_v_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vbrev8_v_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vbrev8_v_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vbrev8_v_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vbrev8_v_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vbrev8_v_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vbrev8_v_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vrev8_v_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vrev8_v_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vrev8_v_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vrev8_v_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   size_t vl);
vuint8m2_t __riscv_vrev8_v_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   size_t vl);
vuint8m4_t __riscv_vrev8_v_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   size_t vl);
vuint8m8_t __riscv_vrev8_v_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   size_t vl);
vuint16mf4_t __riscv_vrev8_v_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vrev8_v_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vrev8_v_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vrev8_v_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vrev8_v_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vrev8_v_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vrev8_v_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vrev8_v_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vrev8_v_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vrev8_v_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vrev8_v_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vrev8_v_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vrev8_v_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vrev8_v_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vrev8_v_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, size_t vl);
----

[[policy-variant-]]
==== Vector Basic Bit-manipulation - Count Bits

[,c]
----
vuint8mf8_t __riscv_vclz_v_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vclz_v_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vclz_v_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vclz_v_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vclz_v_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vclz_v_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vclz_v_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vclz_v_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                      size_t vl);
vuint16mf2_t __riscv_vclz_v_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                      size_t vl);
vuint16m1_t __riscv_vclz_v_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vclz_v_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vclz_v_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vclz_v_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vclz_v_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                      size_t vl);
vuint32m1_t __riscv_vclz_v_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vclz_v_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vclz_v_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vclz_v_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vclz_v_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vclz_v_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vclz_v_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vclz_v_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vctz_v_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vctz_v_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vctz_v_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vctz_v_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vctz_v_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vctz_v_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vctz_v_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vctz_v_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                      size_t vl);
vuint16mf2_t __riscv_vctz_v_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                      size_t vl);
vuint16m1_t __riscv_vctz_v_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vctz_v_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vctz_v_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vctz_v_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vctz_v_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                      size_t vl);
vuint32m1_t __riscv_vctz_v_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vctz_v_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vctz_v_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vctz_v_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vctz_v_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vctz_v_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vctz_v_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vctz_v_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vclz_v_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vclz_v_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vclz_v_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vclz_v_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   size_t vl);
vuint8m2_t __riscv_vclz_v_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   size_t vl);
vuint8m4_t __riscv_vclz_v_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   size_t vl);
vuint8m8_t __riscv_vclz_v_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   size_t vl);
vuint16mf4_t __riscv_vclz_v_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vclz_v_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vclz_v_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vclz_v_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vclz_v_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vclz_v_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vclz_v_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vclz_v_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vclz_v_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vclz_v_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vclz_v_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vclz_v_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vclz_v_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vclz_v_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vclz_v_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vctz_v_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vctz_v_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vctz_v_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vctz_v_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   size_t vl);
vuint8m2_t __riscv_vctz_v_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   size_t vl);
vuint8m4_t __riscv_vctz_v_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   size_t vl);
vuint8m8_t __riscv_vctz_v_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   size_t vl);
vuint16mf4_t __riscv_vctz_v_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vctz_v_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vctz_v_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vctz_v_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vctz_v_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vctz_v_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vctz_v_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vctz_v_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vctz_v_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vctz_v_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vctz_v_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vctz_v_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vctz_v_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vctz_v_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vctz_v_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vclz_v_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vclz_v_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vclz_v_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vclz_v_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t vl);
vuint8m2_t __riscv_vclz_v_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t vl);
vuint8m4_t __riscv_vclz_v_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t vl);
vuint8m8_t __riscv_vclz_v_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t vl);
vuint16mf4_t __riscv_vclz_v_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vclz_v_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vclz_v_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vclz_v_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vclz_v_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vclz_v_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vclz_v_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vclz_v_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vclz_v_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vclz_v_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vclz_v_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vclz_v_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vclz_v_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vclz_v_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vclz_v_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vctz_v_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vctz_v_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vctz_v_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vctz_v_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t vl);
vuint8m2_t __riscv_vctz_v_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t vl);
vuint8m4_t __riscv_vctz_v_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t vl);
vuint8m8_t __riscv_vctz_v_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t vl);
vuint16mf4_t __riscv_vctz_v_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vctz_v_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vctz_v_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vctz_v_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vctz_v_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vctz_v_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vctz_v_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vctz_v_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vctz_v_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vctz_v_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vctz_v_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vctz_v_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vctz_v_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vctz_v_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vctz_v_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vclz_v_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                    vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vclz_v_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                    vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vclz_v_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                    vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vclz_v_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                  size_t vl);
vuint8m2_t __riscv_vclz_v_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                  size_t vl);
vuint8m4_t __riscv_vclz_v_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                  size_t vl);
vuint8m8_t __riscv_vclz_v_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                  size_t vl);
vuint16mf4_t __riscv_vclz_v_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                      vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vclz_v_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                      vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vclz_v_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                    vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vclz_v_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                    vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vclz_v_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                    vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vclz_v_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                    vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vclz_v_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                      vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vclz_v_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                    vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vclz_v_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                    vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vclz_v_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                    vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vclz_v_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                    vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vclz_v_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                    vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vclz_v_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                    vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vclz_v_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                    vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vclz_v_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                    vuint64m8_t vs2, size_t vl);
vuint8mf8_t __riscv_vctz_v_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                    vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vctz_v_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                    vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vctz_v_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                    vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vctz_v_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                  size_t vl);
vuint8m2_t __riscv_vctz_v_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                  size_t vl);
vuint8m4_t __riscv_vctz_v_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                  size_t vl);
vuint8m8_t __riscv_vctz_v_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                  size_t vl);
vuint16mf4_t __riscv_vctz_v_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                      vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vctz_v_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                      vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vctz_v_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                    vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vctz_v_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                    vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vctz_v_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                    vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vctz_v_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                    vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vctz_v_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                      vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vctz_v_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                    vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vctz_v_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                    vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vctz_v_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                    vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vctz_v_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                    vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vctz_v_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                    vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vctz_v_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                    vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vctz_v_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                    vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vctz_v_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                    vuint64m8_t vs2, size_t vl);
----

[[policy-variant-]]
==== Vector Basic Bit-manipulation - Vector Population Count

[,c]
----
vuint8mf8_t __riscv_vcpop_v_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                     size_t vl);
vuint8mf4_t __riscv_vcpop_v_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                     size_t vl);
vuint8mf2_t __riscv_vcpop_v_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                     size_t vl);
vuint8m1_t __riscv_vcpop_v_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t vl);
vuint8m2_t __riscv_vcpop_v_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t vl);
vuint8m4_t __riscv_vcpop_v_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t vl);
vuint8m8_t __riscv_vcpop_v_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t vl);
vuint16mf4_t __riscv_vcpop_v_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                       size_t vl);
vuint16mf2_t __riscv_vcpop_v_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                       size_t vl);
vuint16m1_t __riscv_vcpop_v_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                     size_t vl);
vuint16m2_t __riscv_vcpop_v_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                     size_t vl);
vuint16m4_t __riscv_vcpop_v_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                     size_t vl);
vuint16m8_t __riscv_vcpop_v_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                     size_t vl);
vuint32mf2_t __riscv_vcpop_v_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                       size_t vl);
vuint32m1_t __riscv_vcpop_v_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                     size_t vl);
vuint32m2_t __riscv_vcpop_v_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                     size_t vl);
vuint32m4_t __riscv_vcpop_v_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                     size_t vl);
vuint32m8_t __riscv_vcpop_v_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                     size_t vl);
vuint64m1_t __riscv_vcpop_v_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                     size_t vl);
vuint64m2_t __riscv_vcpop_v_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                     size_t vl);
vuint64m4_t __riscv_vcpop_v_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                     size_t vl);
vuint64m8_t __riscv_vcpop_v_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                     size_t vl);
// masked functions
vuint8mf8_t __riscv_vcpop_v_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vcpop_v_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vcpop_v_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vcpop_v_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t vl);
vuint8m2_t __riscv_vcpop_v_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t vl);
vuint8m4_t __riscv_vcpop_v_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t vl);
vuint8m8_t __riscv_vcpop_v_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t vl);
vuint16mf4_t __riscv_vcpop_v_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vcpop_v_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vcpop_v_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vcpop_v_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vcpop_v_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vcpop_v_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vcpop_v_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vcpop_v_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vcpop_v_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vcpop_v_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vcpop_v_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vcpop_v_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vcpop_v_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vcpop_v_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vcpop_v_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vcpop_v_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vcpop_v_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vcpop_v_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vcpop_v_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     size_t vl);
vuint8m2_t __riscv_vcpop_v_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     size_t vl);
vuint8m4_t __riscv_vcpop_v_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     size_t vl);
vuint8m8_t __riscv_vcpop_v_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     size_t vl);
vuint16mf4_t __riscv_vcpop_v_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vcpop_v_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vcpop_v_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vcpop_v_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vcpop_v_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vcpop_v_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vcpop_v_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vcpop_v_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vcpop_v_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vcpop_v_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vcpop_v_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vcpop_v_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vcpop_v_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vcpop_v_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vcpop_v_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, size_t vl);
// masked functions
vuint8mf8_t __riscv_vcpop_v_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, size_t vl);
vuint8mf4_t __riscv_vcpop_v_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, size_t vl);
vuint8mf2_t __riscv_vcpop_v_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, size_t vl);
vuint8m1_t __riscv_vcpop_v_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   size_t vl);
vuint8m2_t __riscv_vcpop_v_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   size_t vl);
vuint8m4_t __riscv_vcpop_v_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   size_t vl);
vuint8m8_t __riscv_vcpop_v_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   size_t vl);
vuint16mf4_t __riscv_vcpop_v_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, size_t vl);
vuint16mf2_t __riscv_vcpop_v_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, size_t vl);
vuint16m1_t __riscv_vcpop_v_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, size_t vl);
vuint16m2_t __riscv_vcpop_v_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, size_t vl);
vuint16m4_t __riscv_vcpop_v_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, size_t vl);
vuint16m8_t __riscv_vcpop_v_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_vcpop_v_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vcpop_v_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, size_t vl);
vuint32m2_t __riscv_vcpop_v_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, size_t vl);
vuint32m4_t __riscv_vcpop_v_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, size_t vl);
vuint32m8_t __riscv_vcpop_v_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, size_t vl);
vuint64m1_t __riscv_vcpop_v_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, size_t vl);
vuint64m2_t __riscv_vcpop_v_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, size_t vl);
vuint64m4_t __riscv_vcpop_v_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, size_t vl);
vuint64m8_t __riscv_vcpop_v_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, size_t vl);
----

[[policy-variant-]]
==== Vector Bit-manipulation used in Cryptography - Rotate

[,c]
----
vuint8mf8_t __riscv_vrol_vv_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                     vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                     size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                     vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                     size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                     vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                     size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2,
                                   vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t rs1,
                                   size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2,
                                   vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t rs1,
                                   size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2,
                                   vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t rs1,
                                   size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2,
                                   vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t rs1,
                                   size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                       vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                       size_t rs1, size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                       vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                       size_t rs1, size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                     vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                     size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                     vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                     size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                     vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                     size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                     vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                     size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                       vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                       size_t rs1, size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                     vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                     size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                     vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                     size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                     vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                     size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                     vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                     size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                     vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                     size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                     vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                     size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                     vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                     size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                     vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                     size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                     vuint8mf8_t vs1, size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_tu(vuint8mf8_t vd, vuint8mf8_t vs2,
                                     size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                     vuint8mf4_t vs1, size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_tu(vuint8mf4_t vd, vuint8mf4_t vs2,
                                     size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                     vuint8mf2_t vs1, size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_tu(vuint8mf2_t vd, vuint8mf2_t vs2,
                                     size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2,
                                   vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_tu(vuint8m1_t vd, vuint8m1_t vs2, size_t rs1,
                                   size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2,
                                   vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_tu(vuint8m2_t vd, vuint8m2_t vs2, size_t rs1,
                                   size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2,
                                   vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_tu(vuint8m4_t vd, vuint8m4_t vs2, size_t rs1,
                                   size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2,
                                   vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_tu(vuint8m8_t vd, vuint8m8_t vs2, size_t rs1,
                                   size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                       vuint16mf4_t vs1, size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_tu(vuint16mf4_t vd, vuint16mf4_t vs2,
                                       size_t rs1, size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                       vuint16mf2_t vs1, size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_tu(vuint16mf2_t vd, vuint16mf2_t vs2,
                                       size_t rs1, size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                     vuint16m1_t vs1, size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_tu(vuint16m1_t vd, vuint16m1_t vs2,
                                     size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                     vuint16m2_t vs1, size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_tu(vuint16m2_t vd, vuint16m2_t vs2,
                                     size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                     vuint16m4_t vs1, size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_tu(vuint16m4_t vd, vuint16m4_t vs2,
                                     size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                     vuint16m8_t vs1, size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_tu(vuint16m8_t vd, vuint16m8_t vs2,
                                     size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                       vuint32mf2_t vs1, size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                       size_t rs1, size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                     vuint32m1_t vs1, size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                     size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                     vuint32m2_t vs1, size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                     size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                     vuint32m4_t vs1, size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                     size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                     vuint32m8_t vs1, size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                     size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                     vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                     size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                     vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                     size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                     vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                     size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                     vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                     size_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vrol_vv_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, vuint8mf8_t vs1,
                                      size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, vuint8mf4_t vs1,
                                      size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, vuint8mf2_t vs1,
                                      size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, vuint16mf4_t vs1,
                                        size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t rs1,
                                        size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, vuint16mf2_t vs1,
                                        size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t rs1,
                                        size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, vuint16m1_t vs1,
                                      size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, vuint16m2_t vs1,
                                      size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, vuint16m4_t vs1,
                                      size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, vuint16m8_t vs1,
                                      size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, vuint32mf2_t vs1,
                                        size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t rs1,
                                        size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, vuint32m1_t vs1,
                                      size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, vuint32m2_t vs1,
                                      size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, vuint32m4_t vs1,
                                      size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, vuint32m8_t vs1,
                                      size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, vuint64m1_t vs1,
                                      size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, vuint64m2_t vs1,
                                      size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, vuint64m4_t vs1,
                                      size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, vuint64m8_t vs1,
                                      size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, vuint8mf8_t vs1,
                                      size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_tum(vbool64_t vm, vuint8mf8_t vd,
                                      vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, vuint8mf4_t vs1,
                                      size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_tum(vbool32_t vm, vuint8mf4_t vd,
                                      vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, vuint8mf2_t vs1,
                                      size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_tum(vbool16_t vm, vuint8mf2_t vd,
                                      vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_tum(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                    size_t rs1, size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_tum(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                    size_t rs1, size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_tum(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                    size_t rs1, size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_tum(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                    size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, vuint16mf4_t vs1,
                                        size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                        vuint16mf4_t vs2, size_t rs1,
                                        size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, vuint16mf2_t vs1,
                                        size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                        vuint16mf2_t vs2, size_t rs1,
                                        size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, vuint16m1_t vs1,
                                      size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                      vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, vuint16m2_t vs1,
                                      size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                      vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, vuint16m4_t vs1,
                                      size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                      vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, vuint16m8_t vs1,
                                      size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                      vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, vuint32mf2_t vs1,
                                        size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                        vuint32mf2_t vs2, size_t rs1,
                                        size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, vuint32m1_t vs1,
                                      size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                      vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, vuint32m2_t vs1,
                                      size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                      vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, vuint32m4_t vs1,
                                      size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                      vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, vuint32m8_t vs1,
                                      size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                      vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, vuint64m1_t vs1,
                                      size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                      vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, vuint64m2_t vs1,
                                      size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                      vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, vuint64m4_t vs1,
                                      size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                      vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, vuint64m8_t vs1,
                                      size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                      vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vrol_vv_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, vuint8mf8_t vs1,
                                       size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, vuint8mf4_t vs1,
                                       size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, vuint8mf2_t vs1,
                                       size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, vuint16mf4_t vs1,
                                         size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, size_t rs1,
                                         size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, vuint16mf2_t vs1,
                                         size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, size_t rs1,
                                         size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, vuint16m1_t vs1,
                                       size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, vuint16m2_t vs1,
                                       size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, vuint16m4_t vs1,
                                       size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, vuint16m8_t vs1,
                                       size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, vuint32mf2_t vs1,
                                         size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, size_t rs1,
                                         size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, vuint32m1_t vs1,
                                       size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, vuint32m2_t vs1,
                                       size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, vuint32m4_t vs1,
                                       size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, vuint32m8_t vs1,
                                       size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, vuint64m1_t vs1,
                                       size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, vuint64m2_t vs1,
                                       size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, vuint64m4_t vs1,
                                       size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, vuint64m8_t vs1,
                                       size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, vuint8mf8_t vs1,
                                       size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                       vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, vuint8mf4_t vs1,
                                       size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_tumu(vbool32_t vm, vuint8mf4_t vd,
                                       vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, vuint8mf2_t vs1,
                                       size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_tumu(vbool16_t vm, vuint8mf2_t vd,
                                       vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_tumu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                     size_t rs1, size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_tumu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                     size_t rs1, size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_tumu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                     size_t rs1, size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_tumu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                     size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, vuint16mf4_t vs1,
                                         size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                         vuint16mf4_t vs2, size_t rs1,
                                         size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, vuint16mf2_t vs1,
                                         size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                         vuint16mf2_t vs2, size_t rs1,
                                         size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, vuint16m1_t vs1,
                                       size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                       vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, vuint16m2_t vs1,
                                       size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                       vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, vuint16m4_t vs1,
                                       size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                       vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, vuint16m8_t vs1,
                                       size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                       vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, vuint32mf2_t vs1,
                                         size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                         vuint32mf2_t vs2, size_t rs1,
                                         size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, vuint32m1_t vs1,
                                       size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                       vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, vuint32m2_t vs1,
                                       size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                       vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, vuint32m4_t vs1,
                                       size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                       vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, vuint32m8_t vs1,
                                       size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                       vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, vuint64m1_t vs1,
                                       size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, vuint64m2_t vs1,
                                       size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, vuint64m4_t vs1,
                                       size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, vuint64m8_t vs1,
                                       size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, size_t rs1, size_t vl);
// masked functions
vuint8mf8_t __riscv_vrol_vv_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, vuint8mf8_t vs1,
                                     size_t vl);
vuint8mf8_t __riscv_vrol_vx_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vrol_vv_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, vuint8mf4_t vs1,
                                     size_t vl);
vuint8mf4_t __riscv_vrol_vx_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vrol_vv_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, vuint8mf2_t vs1,
                                     size_t vl);
vuint8mf2_t __riscv_vrol_vx_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vrol_vv_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vrol_vx_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   size_t rs1, size_t vl);
vuint8m2_t __riscv_vrol_vv_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vrol_vx_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   size_t rs1, size_t vl);
vuint8m4_t __riscv_vrol_vv_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vrol_vx_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   size_t rs1, size_t vl);
vuint8m8_t __riscv_vrol_vv_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vrol_vx_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   size_t rs1, size_t vl);
vuint16mf4_t __riscv_vrol_vv_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, vuint16mf4_t vs1,
                                       size_t vl);
vuint16mf4_t __riscv_vrol_vx_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vrol_vv_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, vuint16mf2_t vs1,
                                       size_t vl);
vuint16mf2_t __riscv_vrol_vx_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vrol_vv_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, vuint16m1_t vs1,
                                     size_t vl);
vuint16m1_t __riscv_vrol_vx_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vrol_vv_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, vuint16m2_t vs1,
                                     size_t vl);
vuint16m2_t __riscv_vrol_vx_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vrol_vv_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, vuint16m4_t vs1,
                                     size_t vl);
vuint16m4_t __riscv_vrol_vx_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vrol_vv_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, vuint16m8_t vs1,
                                     size_t vl);
vuint16m8_t __riscv_vrol_vx_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vrol_vv_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, vuint32mf2_t vs1,
                                       size_t vl);
vuint32mf2_t __riscv_vrol_vx_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vrol_vv_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, vuint32m1_t vs1,
                                     size_t vl);
vuint32m1_t __riscv_vrol_vx_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vrol_vv_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, vuint32m2_t vs1,
                                     size_t vl);
vuint32m2_t __riscv_vrol_vx_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vrol_vv_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, vuint32m4_t vs1,
                                     size_t vl);
vuint32m4_t __riscv_vrol_vx_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vrol_vv_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, vuint32m8_t vs1,
                                     size_t vl);
vuint32m8_t __riscv_vrol_vx_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vrol_vv_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, vuint64m1_t vs1,
                                     size_t vl);
vuint64m1_t __riscv_vrol_vx_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vrol_vv_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, vuint64m2_t vs1,
                                     size_t vl);
vuint64m2_t __riscv_vrol_vx_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vrol_vv_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, vuint64m4_t vs1,
                                     size_t vl);
vuint64m4_t __riscv_vrol_vx_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vrol_vv_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, vuint64m8_t vs1,
                                     size_t vl);
vuint64m8_t __riscv_vrol_vx_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, size_t rs1, size_t vl);
vuint8mf8_t __riscv_vror_vv_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, vuint8mf8_t vs1,
                                     size_t vl);
vuint8mf8_t __riscv_vror_vx_u8mf8_mu(vbool64_t vm, vuint8mf8_t vd,
                                     vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint8mf4_t __riscv_vror_vv_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, vuint8mf4_t vs1,
                                     size_t vl);
vuint8mf4_t __riscv_vror_vx_u8mf4_mu(vbool32_t vm, vuint8mf4_t vd,
                                     vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint8mf2_t __riscv_vror_vv_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, vuint8mf2_t vs1,
                                     size_t vl);
vuint8mf2_t __riscv_vror_vx_u8mf2_mu(vbool16_t vm, vuint8mf2_t vd,
                                     vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint8m1_t __riscv_vror_vv_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   vuint8m1_t vs1, size_t vl);
vuint8m1_t __riscv_vror_vx_u8m1_mu(vbool8_t vm, vuint8m1_t vd, vuint8m1_t vs2,
                                   size_t rs1, size_t vl);
vuint8m2_t __riscv_vror_vv_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   vuint8m2_t vs1, size_t vl);
vuint8m2_t __riscv_vror_vx_u8m2_mu(vbool4_t vm, vuint8m2_t vd, vuint8m2_t vs2,
                                   size_t rs1, size_t vl);
vuint8m4_t __riscv_vror_vv_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   vuint8m4_t vs1, size_t vl);
vuint8m4_t __riscv_vror_vx_u8m4_mu(vbool2_t vm, vuint8m4_t vd, vuint8m4_t vs2,
                                   size_t rs1, size_t vl);
vuint8m8_t __riscv_vror_vv_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   vuint8m8_t vs1, size_t vl);
vuint8m8_t __riscv_vror_vx_u8m8_mu(vbool1_t vm, vuint8m8_t vd, vuint8m8_t vs2,
                                   size_t rs1, size_t vl);
vuint16mf4_t __riscv_vror_vv_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, vuint16mf4_t vs1,
                                       size_t vl);
vuint16mf4_t __riscv_vror_vx_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                       vuint16mf4_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vror_vv_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, vuint16mf2_t vs1,
                                       size_t vl);
vuint16mf2_t __riscv_vror_vx_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                       vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vror_vv_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, vuint16m1_t vs1,
                                     size_t vl);
vuint16m1_t __riscv_vror_vx_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                     vuint16m1_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vror_vv_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, vuint16m2_t vs1,
                                     size_t vl);
vuint16m2_t __riscv_vror_vx_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                     vuint16m2_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vror_vv_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, vuint16m4_t vs1,
                                     size_t vl);
vuint16m4_t __riscv_vror_vx_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                     vuint16m4_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vror_vv_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, vuint16m8_t vs1,
                                     size_t vl);
vuint16m8_t __riscv_vror_vx_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                     vuint16m8_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vror_vv_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, vuint32mf2_t vs1,
                                       size_t vl);
vuint32mf2_t __riscv_vror_vx_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                       vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint32m1_t __riscv_vror_vv_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, vuint32m1_t vs1,
                                     size_t vl);
vuint32m1_t __riscv_vror_vx_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                     vuint32m1_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vror_vv_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, vuint32m2_t vs1,
                                     size_t vl);
vuint32m2_t __riscv_vror_vx_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                     vuint32m2_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vror_vv_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, vuint32m4_t vs1,
                                     size_t vl);
vuint32m4_t __riscv_vror_vx_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                     vuint32m4_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vror_vv_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, vuint32m8_t vs1,
                                     size_t vl);
vuint32m8_t __riscv_vror_vx_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                     vuint32m8_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vror_vv_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, vuint64m1_t vs1,
                                     size_t vl);
vuint64m1_t __riscv_vror_vx_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                     vuint64m1_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vror_vv_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, vuint64m2_t vs1,
                                     size_t vl);
vuint64m2_t __riscv_vror_vx_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                     vuint64m2_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vror_vv_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, vuint64m4_t vs1,
                                     size_t vl);
vuint64m4_t __riscv_vror_vx_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                     vuint64m4_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vror_vv_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, vuint64m8_t vs1,
                                     size_t vl);
vuint64m8_t __riscv_vror_vx_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                     vuint64m8_t vs2, size_t rs1, size_t vl);
----

[[policy-variant-]]
==== Vector Basic Bit-manipulation used - Widening Shift

[,c]
----
vuint16mf4_t __riscv_vwsll_vv_u16mf4_tu(vuint16mf4_t vd, vuint8mf8_t vs2,
                                        vuint8mf8_t vs1, size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_tu(vuint16mf4_t vd, vuint8mf8_t vs2,
                                        size_t rs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_tu(vuint16mf2_t vd, vuint8mf4_t vs2,
                                        vuint8mf4_t vs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_tu(vuint16mf2_t vd, vuint8mf4_t vs2,
                                        size_t rs1, size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_tu(vuint16m1_t vd, vuint8mf2_t vs2,
                                      vuint8mf2_t vs1, size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_tu(vuint16m1_t vd, vuint8mf2_t vs2,
                                      size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_tu(vuint16m2_t vd, vuint8m1_t vs2,
                                      vuint8m1_t vs1, size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_tu(vuint16m2_t vd, vuint8m1_t vs2,
                                      size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_tu(vuint16m4_t vd, vuint8m2_t vs2,
                                      vuint8m2_t vs1, size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_tu(vuint16m4_t vd, vuint8m2_t vs2,
                                      size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_tu(vuint16m8_t vd, vuint8m4_t vs2,
                                      vuint8m4_t vs1, size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_tu(vuint16m8_t vd, vuint8m4_t vs2,
                                      size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_tu(vuint32mf2_t vd, vuint16mf4_t vs2,
                                        vuint16mf4_t vs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_tu(vuint32mf2_t vd, vuint16mf4_t vs2,
                                        size_t rs1, size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_tu(vuint32m1_t vd, vuint16mf2_t vs2,
                                      vuint16mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_tu(vuint32m1_t vd, vuint16mf2_t vs2,
                                      size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_tu(vuint32m2_t vd, vuint16m1_t vs2,
                                      vuint16m1_t vs1, size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_tu(vuint32m2_t vd, vuint16m1_t vs2,
                                      size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_tu(vuint32m4_t vd, vuint16m2_t vs2,
                                      vuint16m2_t vs1, size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_tu(vuint32m4_t vd, vuint16m2_t vs2,
                                      size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_tu(vuint32m8_t vd, vuint16m4_t vs2,
                                      vuint16m4_t vs1, size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_tu(vuint32m8_t vd, vuint16m4_t vs2,
                                      size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_tu(vuint64m1_t vd, vuint32mf2_t vs2,
                                      vuint32mf2_t vs1, size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_tu(vuint64m1_t vd, vuint32mf2_t vs2,
                                      size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_tu(vuint64m2_t vd, vuint32m1_t vs2,
                                      vuint32m1_t vs1, size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_tu(vuint64m2_t vd, vuint32m1_t vs2,
                                      size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_tu(vuint64m4_t vd, vuint32m2_t vs2,
                                      vuint32m2_t vs1, size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_tu(vuint64m4_t vd, vuint32m2_t vs2,
                                      size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_tu(vuint64m8_t vd, vuint32m4_t vs2,
                                      vuint32m4_t vs1, size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_tu(vuint64m8_t vd, vuint32m4_t vs2,
                                      size_t rs1, size_t vl);
// masked functions
vuint16mf4_t __riscv_vwsll_vv_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                         vuint8mf8_t vs2, vuint8mf8_t vs1,
                                         size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_tum(vbool64_t vm, vuint16mf4_t vd,
                                         vuint8mf8_t vs2, size_t rs1,
                                         size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                         vuint8mf4_t vs2, vuint8mf4_t vs1,
                                         size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_tum(vbool32_t vm, vuint16mf2_t vd,
                                         vuint8mf4_t vs2, size_t rs1,
                                         size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                       vuint8mf2_t vs2, vuint8mf2_t vs1,
                                       size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_tum(vbool16_t vm, vuint16m1_t vd,
                                       vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                       vuint8m1_t vs2, vuint8m1_t vs1,
                                       size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_tum(vbool8_t vm, vuint16m2_t vd,
                                       vuint8m1_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                       vuint8m2_t vs2, vuint8m2_t vs1,
                                       size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_tum(vbool4_t vm, vuint16m4_t vd,
                                       vuint8m2_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                       vuint8m4_t vs2, vuint8m4_t vs1,
                                       size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_tum(vbool2_t vm, vuint16m8_t vd,
                                       vuint8m4_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                         vuint16mf4_t vs2, vuint16mf4_t vs1,
                                         size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_tum(vbool64_t vm, vuint32mf2_t vd,
                                         vuint16mf4_t vs2, size_t rs1,
                                         size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                       vuint16mf2_t vs2, vuint16mf2_t vs1,
                                       size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_tum(vbool32_t vm, vuint32m1_t vd,
                                       vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                       vuint16m1_t vs2, vuint16m1_t vs1,
                                       size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_tum(vbool16_t vm, vuint32m2_t vd,
                                       vuint16m1_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                       vuint16m2_t vs2, vuint16m2_t vs1,
                                       size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_tum(vbool8_t vm, vuint32m4_t vd,
                                       vuint16m2_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                       vuint16m4_t vs2, vuint16m4_t vs1,
                                       size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_tum(vbool4_t vm, vuint32m8_t vd,
                                       vuint16m4_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                       vuint32mf2_t vs2, vuint32mf2_t vs1,
                                       size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                       vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                       vuint32m1_t vs2, vuint32m1_t vs1,
                                       size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                       vuint32m1_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                       vuint32m2_t vs2, vuint32m2_t vs1,
                                       size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                       vuint32m2_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                       vuint32m4_t vs2, vuint32m4_t vs1,
                                       size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                       vuint32m4_t vs2, size_t rs1, size_t vl);
// masked functions
vuint16mf4_t __riscv_vwsll_vv_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                          vuint8mf8_t vs2, vuint8mf8_t vs1,
                                          size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_tumu(vbool64_t vm, vuint16mf4_t vd,
                                          vuint8mf8_t vs2, size_t rs1,
                                          size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                          vuint8mf4_t vs2, vuint8mf4_t vs1,
                                          size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_tumu(vbool32_t vm, vuint16mf2_t vd,
                                          vuint8mf4_t vs2, size_t rs1,
                                          size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                        vuint8mf2_t vs2, vuint8mf2_t vs1,
                                        size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_tumu(vbool16_t vm, vuint16m1_t vd,
                                        vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                        vuint8m1_t vs2, vuint8m1_t vs1,
                                        size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_tumu(vbool8_t vm, vuint16m2_t vd,
                                        vuint8m1_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                        vuint8m2_t vs2, vuint8m2_t vs1,
                                        size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_tumu(vbool4_t vm, vuint16m4_t vd,
                                        vuint8m2_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                        vuint8m4_t vs2, vuint8m4_t vs1,
                                        size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_tumu(vbool2_t vm, vuint16m8_t vd,
                                        vuint8m4_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                          vuint16mf4_t vs2, vuint16mf4_t vs1,
                                          size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_tumu(vbool64_t vm, vuint32mf2_t vd,
                                          vuint16mf4_t vs2, size_t rs1,
                                          size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                        vuint16mf2_t vs2, vuint16mf2_t vs1,
                                        size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_tumu(vbool32_t vm, vuint32m1_t vd,
                                        vuint16mf2_t vs2, size_t rs1,
                                        size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                        vuint16m1_t vs2, vuint16m1_t vs1,
                                        size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_tumu(vbool16_t vm, vuint32m2_t vd,
                                        vuint16m1_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                        vuint16m2_t vs2, vuint16m2_t vs1,
                                        size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_tumu(vbool8_t vm, vuint32m4_t vd,
                                        vuint16m2_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                        vuint16m4_t vs2, vuint16m4_t vs1,
                                        size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_tumu(vbool4_t vm, vuint32m8_t vd,
                                        vuint16m4_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                        vuint32mf2_t vs2, vuint32mf2_t vs1,
                                        size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                        vuint32mf2_t vs2, size_t rs1,
                                        size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                        vuint32m1_t vs2, vuint32m1_t vs1,
                                        size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                        vuint32m1_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                        vuint32m2_t vs2, vuint32m2_t vs1,
                                        size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                        vuint32m2_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                        vuint32m4_t vs2, vuint32m4_t vs1,
                                        size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                        vuint32m4_t vs2, size_t rs1, size_t vl);
// masked functions
vuint16mf4_t __riscv_vwsll_vv_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                        vuint8mf8_t vs2, vuint8mf8_t vs1,
                                        size_t vl);
vuint16mf4_t __riscv_vwsll_vx_u16mf4_mu(vbool64_t vm, vuint16mf4_t vd,
                                        vuint8mf8_t vs2, size_t rs1, size_t vl);
vuint16mf2_t __riscv_vwsll_vv_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                        vuint8mf4_t vs2, vuint8mf4_t vs1,
                                        size_t vl);
vuint16mf2_t __riscv_vwsll_vx_u16mf2_mu(vbool32_t vm, vuint16mf2_t vd,
                                        vuint8mf4_t vs2, size_t rs1, size_t vl);
vuint16m1_t __riscv_vwsll_vv_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                      vuint8mf2_t vs2, vuint8mf2_t vs1,
                                      size_t vl);
vuint16m1_t __riscv_vwsll_vx_u16m1_mu(vbool16_t vm, vuint16m1_t vd,
                                      vuint8mf2_t vs2, size_t rs1, size_t vl);
vuint16m2_t __riscv_vwsll_vv_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                      vuint8m1_t vs2, vuint8m1_t vs1,
                                      size_t vl);
vuint16m2_t __riscv_vwsll_vx_u16m2_mu(vbool8_t vm, vuint16m2_t vd,
                                      vuint8m1_t vs2, size_t rs1, size_t vl);
vuint16m4_t __riscv_vwsll_vv_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                      vuint8m2_t vs2, vuint8m2_t vs1,
                                      size_t vl);
vuint16m4_t __riscv_vwsll_vx_u16m4_mu(vbool4_t vm, vuint16m4_t vd,
                                      vuint8m2_t vs2, size_t rs1, size_t vl);
vuint16m8_t __riscv_vwsll_vv_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                      vuint8m4_t vs2, vuint8m4_t vs1,
                                      size_t vl);
vuint16m8_t __riscv_vwsll_vx_u16m8_mu(vbool2_t vm, vuint16m8_t vd,
                                      vuint8m4_t vs2, size_t rs1, size_t vl);
vuint32mf2_t __riscv_vwsll_vv_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                        vuint16mf4_t vs2, vuint16mf4_t vs1,
                                        size_t vl);
vuint32mf2_t __riscv_vwsll_vx_u32mf2_mu(vbool64_t vm, vuint32mf2_t vd,
                                        vuint16mf4_t vs2, size_t rs1,
                                        size_t vl);
vuint32m1_t __riscv_vwsll_vv_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                      vuint16mf2_t vs2, vuint16mf2_t vs1,
                                      size_t vl);
vuint32m1_t __riscv_vwsll_vx_u32m1_mu(vbool32_t vm, vuint32m1_t vd,
                                      vuint16mf2_t vs2, size_t rs1, size_t vl);
vuint32m2_t __riscv_vwsll_vv_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                      vuint16m1_t vs2, vuint16m1_t vs1,
                                      size_t vl);
vuint32m2_t __riscv_vwsll_vx_u32m2_mu(vbool16_t vm, vuint32m2_t vd,
                                      vuint16m1_t vs2, size_t rs1, size_t vl);
vuint32m4_t __riscv_vwsll_vv_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                      vuint16m2_t vs2, vuint16m2_t vs1,
                                      size_t vl);
vuint32m4_t __riscv_vwsll_vx_u32m4_mu(vbool8_t vm, vuint32m4_t vd,
                                      vuint16m2_t vs2, size_t rs1, size_t vl);
vuint32m8_t __riscv_vwsll_vv_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                      vuint16m4_t vs2, vuint16m4_t vs1,
                                      size_t vl);
vuint32m8_t __riscv_vwsll_vx_u32m8_mu(vbool4_t vm, vuint32m8_t vd,
                                      vuint16m4_t vs2, size_t rs1, size_t vl);
vuint64m1_t __riscv_vwsll_vv_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                      vuint32mf2_t vs2, vuint32mf2_t vs1,
                                      size_t vl);
vuint64m1_t __riscv_vwsll_vx_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                      vuint32mf2_t vs2, size_t rs1, size_t vl);
vuint64m2_t __riscv_vwsll_vv_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                      vuint32m1_t vs2, vuint32m1_t vs1,
                                      size_t vl);
vuint64m2_t __riscv_vwsll_vx_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                      vuint32m1_t vs2, size_t rs1, size_t vl);
vuint64m4_t __riscv_vwsll_vv_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                      vuint32m2_t vs2, vuint32m2_t vs1,
                                      size_t vl);
vuint64m4_t __riscv_vwsll_vx_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                      vuint32m2_t vs2, size_t rs1, size_t vl);
vuint64m8_t __riscv_vwsll_vv_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                      vuint32m4_t vs2, vuint32m4_t vs1,
                                      size_t vl);
vuint64m8_t __riscv_vwsll_vx_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                      vuint32m4_t vs2, size_t rs1, size_t vl);
----

=== Zvbc - Vector Carryless Multiplication

[[policy-variant-]]
==== Vector Carryless Multiplication

[,c]
----
vuint64m1_t __riscv_vclmul_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                       vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vclmul_vx_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                       uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vclmul_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                       vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vclmul_vx_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                       uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vclmul_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                       vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vclmul_vx_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                       uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vclmul_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                       vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vclmul_vx_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                       uint64_t rs1, size_t vl);
vuint64m1_t __riscv_vclmulh_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                        vuint64m1_t vs1, size_t vl);
vuint64m1_t __riscv_vclmulh_vx_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                        uint64_t rs1, size_t vl);
vuint64m2_t __riscv_vclmulh_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                        vuint64m2_t vs1, size_t vl);
vuint64m2_t __riscv_vclmulh_vx_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                        uint64_t rs1, size_t vl);
vuint64m4_t __riscv_vclmulh_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                        vuint64m4_t vs1, size_t vl);
vuint64m4_t __riscv_vclmulh_vx_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                        uint64_t rs1, size_t vl);
vuint64m8_t __riscv_vclmulh_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                        vuint64m8_t vs1, size_t vl);
vuint64m8_t __riscv_vclmulh_vx_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                        uint64_t rs1, size_t vl);
// masked functions
vuint64m1_t __riscv_vclmul_vv_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                        vuint64m1_t vs2, vuint64m1_t vs1,
                                        size_t vl);
vuint64m1_t __riscv_vclmul_vx_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                        vuint64m1_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m2_t __riscv_vclmul_vv_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                        vuint64m2_t vs2, vuint64m2_t vs1,
                                        size_t vl);
vuint64m2_t __riscv_vclmul_vx_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                        vuint64m2_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m4_t __riscv_vclmul_vv_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                        vuint64m4_t vs2, vuint64m4_t vs1,
                                        size_t vl);
vuint64m4_t __riscv_vclmul_vx_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                        vuint64m4_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m8_t __riscv_vclmul_vv_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                        vuint64m8_t vs2, vuint64m8_t vs1,
                                        size_t vl);
vuint64m8_t __riscv_vclmul_vx_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                        vuint64m8_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m1_t __riscv_vclmulh_vv_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                         vuint64m1_t vs2, vuint64m1_t vs1,
                                         size_t vl);
vuint64m1_t __riscv_vclmulh_vx_u64m1_tum(vbool64_t vm, vuint64m1_t vd,
                                         vuint64m1_t vs2, uint64_t rs1,
                                         size_t vl);
vuint64m2_t __riscv_vclmulh_vv_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                         vuint64m2_t vs2, vuint64m2_t vs1,
                                         size_t vl);
vuint64m2_t __riscv_vclmulh_vx_u64m2_tum(vbool32_t vm, vuint64m2_t vd,
                                         vuint64m2_t vs2, uint64_t rs1,
                                         size_t vl);
vuint64m4_t __riscv_vclmulh_vv_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                         vuint64m4_t vs2, vuint64m4_t vs1,
                                         size_t vl);
vuint64m4_t __riscv_vclmulh_vx_u64m4_tum(vbool16_t vm, vuint64m4_t vd,
                                         vuint64m4_t vs2, uint64_t rs1,
                                         size_t vl);
vuint64m8_t __riscv_vclmulh_vv_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                         vuint64m8_t vs2, vuint64m8_t vs1,
                                         size_t vl);
vuint64m8_t __riscv_vclmulh_vx_u64m8_tum(vbool8_t vm, vuint64m8_t vd,
                                         vuint64m8_t vs2, uint64_t rs1,
                                         size_t vl);
// masked functions
vuint64m1_t __riscv_vclmul_vv_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                         vuint64m1_t vs2, vuint64m1_t vs1,
                                         size_t vl);
vuint64m1_t __riscv_vclmul_vx_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                         vuint64m1_t vs2, uint64_t rs1,
                                         size_t vl);
vuint64m2_t __riscv_vclmul_vv_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                         vuint64m2_t vs2, vuint64m2_t vs1,
                                         size_t vl);
vuint64m2_t __riscv_vclmul_vx_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                         vuint64m2_t vs2, uint64_t rs1,
                                         size_t vl);
vuint64m4_t __riscv_vclmul_vv_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                         vuint64m4_t vs2, vuint64m4_t vs1,
                                         size_t vl);
vuint64m4_t __riscv_vclmul_vx_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                         vuint64m4_t vs2, uint64_t rs1,
                                         size_t vl);
vuint64m8_t __riscv_vclmul_vv_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                         vuint64m8_t vs2, vuint64m8_t vs1,
                                         size_t vl);
vuint64m8_t __riscv_vclmul_vx_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                         vuint64m8_t vs2, uint64_t rs1,
                                         size_t vl);
vuint64m1_t __riscv_vclmulh_vv_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                          vuint64m1_t vs2, vuint64m1_t vs1,
                                          size_t vl);
vuint64m1_t __riscv_vclmulh_vx_u64m1_tumu(vbool64_t vm, vuint64m1_t vd,
                                          vuint64m1_t vs2, uint64_t rs1,
                                          size_t vl);
vuint64m2_t __riscv_vclmulh_vv_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                          vuint64m2_t vs2, vuint64m2_t vs1,
                                          size_t vl);
vuint64m2_t __riscv_vclmulh_vx_u64m2_tumu(vbool32_t vm, vuint64m2_t vd,
                                          vuint64m2_t vs2, uint64_t rs1,
                                          size_t vl);
vuint64m4_t __riscv_vclmulh_vv_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                          vuint64m4_t vs2, vuint64m4_t vs1,
                                          size_t vl);
vuint64m4_t __riscv_vclmulh_vx_u64m4_tumu(vbool16_t vm, vuint64m4_t vd,
                                          vuint64m4_t vs2, uint64_t rs1,
                                          size_t vl);
vuint64m8_t __riscv_vclmulh_vv_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                          vuint64m8_t vs2, vuint64m8_t vs1,
                                          size_t vl);
vuint64m8_t __riscv_vclmulh_vx_u64m8_tumu(vbool8_t vm, vuint64m8_t vd,
                                          vuint64m8_t vs2, uint64_t rs1,
                                          size_t vl);
// masked functions
vuint64m1_t __riscv_vclmul_vv_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, vuint64m1_t vs1,
                                       size_t vl);
vuint64m1_t __riscv_vclmul_vx_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                       vuint64m1_t vs2, uint64_t rs1,
                                       size_t vl);
vuint64m2_t __riscv_vclmul_vv_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, vuint64m2_t vs1,
                                       size_t vl);
vuint64m2_t __riscv_vclmul_vx_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                       vuint64m2_t vs2, uint64_t rs1,
                                       size_t vl);
vuint64m4_t __riscv_vclmul_vv_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, vuint64m4_t vs1,
                                       size_t vl);
vuint64m4_t __riscv_vclmul_vx_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                       vuint64m4_t vs2, uint64_t rs1,
                                       size_t vl);
vuint64m8_t __riscv_vclmul_vv_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, vuint64m8_t vs1,
                                       size_t vl);
vuint64m8_t __riscv_vclmul_vx_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                       vuint64m8_t vs2, uint64_t rs1,
                                       size_t vl);
vuint64m1_t __riscv_vclmulh_vv_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                        vuint64m1_t vs2, vuint64m1_t vs1,
                                        size_t vl);
vuint64m1_t __riscv_vclmulh_vx_u64m1_mu(vbool64_t vm, vuint64m1_t vd,
                                        vuint64m1_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m2_t __riscv_vclmulh_vv_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                        vuint64m2_t vs2, vuint64m2_t vs1,
                                        size_t vl);
vuint64m2_t __riscv_vclmulh_vx_u64m2_mu(vbool32_t vm, vuint64m2_t vd,
                                        vuint64m2_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m4_t __riscv_vclmulh_vv_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                        vuint64m4_t vs2, vuint64m4_t vs1,
                                        size_t vl);
vuint64m4_t __riscv_vclmulh_vx_u64m4_mu(vbool16_t vm, vuint64m4_t vd,
                                        vuint64m4_t vs2, uint64_t rs1,
                                        size_t vl);
vuint64m8_t __riscv_vclmulh_vv_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                        vuint64m8_t vs2, vuint64m8_t vs1,
                                        size_t vl);
vuint64m8_t __riscv_vclmulh_vx_u64m8_mu(vbool8_t vm, vuint64m8_t vd,
                                        vuint64m8_t vs2, uint64_t rs1,
                                        size_t vl);
----

=== Zvkg - Vector GCM/GMAC

[[policy-variant-]]
==== Vector GCM/GMAC

[,c]
----
vuint32mf2_t __riscv_vghsh_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        vuint32mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vghsh_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      vuint32m1_t vs1, size_t vl);
vuint32m2_t __riscv_vghsh_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      vuint32m2_t vs1, size_t vl);
vuint32m4_t __riscv_vghsh_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      vuint32m4_t vs1, size_t vl);
vuint32m8_t __riscv_vghsh_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      vuint32m8_t vs1, size_t vl);
vuint32mf2_t __riscv_vgmul_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        size_t vl);
vuint32m1_t __riscv_vgmul_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      size_t vl);
vuint32m2_t __riscv_vgmul_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      size_t vl);
vuint32m4_t __riscv_vgmul_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      size_t vl);
vuint32m8_t __riscv_vgmul_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      size_t vl);
----

=== Zvkned - NIST Suite: Vector AES Block Cipher

[[policy-variant-]]
==== Vector AES Encryption

[,c]
----
vuint32mf2_t __riscv_vaesef_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                         size_t vl);
vuint32mf2_t __riscv_vaesef_vs_u32mf2_u32mf2_tu(vuint32mf2_t vd,
                                                vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vaesef_vs_u32mf2_u32m1_tu(vuint32m1_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m2_t __riscv_vaesef_vs_u32mf2_u32m2_tu(vuint32m2_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m4_t __riscv_vaesef_vs_u32mf2_u32m4_tu(vuint32m4_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m8_t __riscv_vaesef_vs_u32mf2_u32m8_tu(vuint32m8_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m1_t __riscv_vaesef_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                       size_t vl);
vuint32m1_t __riscv_vaesef_vs_u32m1_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesef_vs_u32m1_u32m2_tu(vuint32m2_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesef_vs_u32m1_u32m4_tu(vuint32m4_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesef_vs_u32m1_u32m8_tu(vuint32m8_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesef_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                       size_t vl);
vuint32m2_t __riscv_vaesef_vs_u32m2_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesef_vs_u32m2_u32m4_tu(vuint32m4_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesef_vs_u32m2_u32m8_tu(vuint32m8_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesef_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                       size_t vl);
vuint32m4_t __riscv_vaesef_vs_u32m4_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesef_vs_u32m4_u32m8_tu(vuint32m8_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesef_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                       size_t vl);
vuint32mf2_t __riscv_vaesem_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                         size_t vl);
vuint32mf2_t __riscv_vaesem_vs_u32mf2_u32mf2_tu(vuint32mf2_t vd,
                                                vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vaesem_vs_u32mf2_u32m1_tu(vuint32m1_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m2_t __riscv_vaesem_vs_u32mf2_u32m2_tu(vuint32m2_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m4_t __riscv_vaesem_vs_u32mf2_u32m4_tu(vuint32m4_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m8_t __riscv_vaesem_vs_u32mf2_u32m8_tu(vuint32m8_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m1_t __riscv_vaesem_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                       size_t vl);
vuint32m1_t __riscv_vaesem_vs_u32m1_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesem_vs_u32m1_u32m2_tu(vuint32m2_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesem_vs_u32m1_u32m4_tu(vuint32m4_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesem_vs_u32m1_u32m8_tu(vuint32m8_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesem_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                       size_t vl);
vuint32m2_t __riscv_vaesem_vs_u32m2_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesem_vs_u32m2_u32m4_tu(vuint32m4_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesem_vs_u32m2_u32m8_tu(vuint32m8_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesem_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                       size_t vl);
vuint32m4_t __riscv_vaesem_vs_u32m4_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesem_vs_u32m4_u32m8_tu(vuint32m8_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesem_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                       size_t vl);
----

[[policy-variant-]]
==== Vector AES Decryption

[,c]
----
vuint32mf2_t __riscv_vaesdf_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                         size_t vl);
vuint32mf2_t __riscv_vaesdf_vs_u32mf2_u32mf2_tu(vuint32mf2_t vd,
                                                vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vaesdf_vs_u32mf2_u32m1_tu(vuint32m1_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m2_t __riscv_vaesdf_vs_u32mf2_u32m2_tu(vuint32m2_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m4_t __riscv_vaesdf_vs_u32mf2_u32m4_tu(vuint32m4_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m8_t __riscv_vaesdf_vs_u32mf2_u32m8_tu(vuint32m8_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m1_t __riscv_vaesdf_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                       size_t vl);
vuint32m1_t __riscv_vaesdf_vs_u32m1_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesdf_vs_u32m1_u32m2_tu(vuint32m2_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesdf_vs_u32m1_u32m4_tu(vuint32m4_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdf_vs_u32m1_u32m8_tu(vuint32m8_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesdf_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                       size_t vl);
vuint32m2_t __riscv_vaesdf_vs_u32m2_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesdf_vs_u32m2_u32m4_tu(vuint32m4_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdf_vs_u32m2_u32m8_tu(vuint32m8_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesdf_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                       size_t vl);
vuint32m4_t __riscv_vaesdf_vs_u32m4_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdf_vs_u32m4_u32m8_tu(vuint32m8_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdf_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                       size_t vl);
vuint32mf2_t __riscv_vaesdm_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                         size_t vl);
vuint32mf2_t __riscv_vaesdm_vs_u32mf2_u32mf2_tu(vuint32mf2_t vd,
                                                vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vaesdm_vs_u32mf2_u32m1_tu(vuint32m1_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m2_t __riscv_vaesdm_vs_u32mf2_u32m2_tu(vuint32m2_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m4_t __riscv_vaesdm_vs_u32mf2_u32m4_tu(vuint32m4_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m8_t __riscv_vaesdm_vs_u32mf2_u32m8_tu(vuint32m8_t vd, vuint32mf2_t vs2,
                                              size_t vl);
vuint32m1_t __riscv_vaesdm_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                       size_t vl);
vuint32m1_t __riscv_vaesdm_vs_u32m1_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesdm_vs_u32m1_u32m2_tu(vuint32m2_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesdm_vs_u32m1_u32m4_tu(vuint32m4_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdm_vs_u32m1_u32m8_tu(vuint32m8_t vd, vuint32m1_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesdm_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                       size_t vl);
vuint32m2_t __riscv_vaesdm_vs_u32m2_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesdm_vs_u32m2_u32m4_tu(vuint32m4_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdm_vs_u32m2_u32m8_tu(vuint32m8_t vd, vuint32m2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesdm_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                       size_t vl);
vuint32m4_t __riscv_vaesdm_vs_u32m4_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdm_vs_u32m4_u32m8_tu(vuint32m8_t vd, vuint32m4_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesdm_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                       size_t vl);
----

[[policy-variant-]]
==== Vector AES-128 Forward KeySchedule generation

[,c]
----
vuint32mf2_t __riscv_vaeskf1_vi_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                          size_t uimm, size_t vl);
vuint32m1_t __riscv_vaeskf1_vi_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                        size_t uimm, size_t vl);
vuint32m2_t __riscv_vaeskf1_vi_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                        size_t uimm, size_t vl);
vuint32m4_t __riscv_vaeskf1_vi_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                        size_t uimm, size_t vl);
vuint32m8_t __riscv_vaeskf1_vi_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                        size_t uimm, size_t vl);
vuint32mf2_t __riscv_vaeskf2_vi_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                          size_t uimm, size_t vl);
vuint32m1_t __riscv_vaeskf2_vi_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                        size_t uimm, size_t vl);
vuint32m2_t __riscv_vaeskf2_vi_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                        size_t uimm, size_t vl);
vuint32m4_t __riscv_vaeskf2_vi_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                        size_t uimm, size_t vl);
vuint32m8_t __riscv_vaeskf2_vi_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                        size_t uimm, size_t vl);
----

[[policy-variant-]]
==== Vector AES round zero

[,c]
----
vuint32mf2_t __riscv_vaesz_vs_u32mf2_u32mf2_tu(vuint32mf2_t vd,
                                               vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vaesz_vs_u32mf2_u32m1_tu(vuint32m1_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vaesz_vs_u32mf2_u32m2_tu(vuint32m2_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vaesz_vs_u32mf2_u32m4_tu(vuint32m4_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vaesz_vs_u32mf2_u32m8_tu(vuint32m8_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m1_t __riscv_vaesz_vs_u32m1_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m2_t __riscv_vaesz_vs_u32m1_u32m2_tu(vuint32m2_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m4_t __riscv_vaesz_vs_u32m1_u32m4_tu(vuint32m4_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m8_t __riscv_vaesz_vs_u32m1_u32m8_tu(vuint32m8_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m2_t __riscv_vaesz_vs_u32m2_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                            size_t vl);
vuint32m4_t __riscv_vaesz_vs_u32m2_u32m4_tu(vuint32m4_t vd, vuint32m2_t vs2,
                                            size_t vl);
vuint32m8_t __riscv_vaesz_vs_u32m2_u32m8_tu(vuint32m8_t vd, vuint32m2_t vs2,
                                            size_t vl);
vuint32m4_t __riscv_vaesz_vs_u32m4_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                            size_t vl);
vuint32m8_t __riscv_vaesz_vs_u32m4_u32m8_tu(vuint32m8_t vd, vuint32m4_t vs2,
                                            size_t vl);
----

=== Zvknh - NIST Suite: Vector SHA-2 Secure Hash

[[policy-variant-]]
==== Vector SHA-2 message schedule

[,c]
----
vuint32mf2_t __riscv_vsha2ms_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                          vuint32mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vsha2ms_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                        vuint32m1_t vs1, size_t vl);
vuint32m2_t __riscv_vsha2ms_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                        vuint32m2_t vs1, size_t vl);
vuint32m4_t __riscv_vsha2ms_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                        vuint32m4_t vs1, size_t vl);
vuint32m8_t __riscv_vsha2ms_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                        vuint32m8_t vs1, size_t vl);
vuint64m1_t __riscv_vsha2ms_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                        vuint64m1_t vs1, size_t vl);
vuint64m2_t __riscv_vsha2ms_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                        vuint64m2_t vs1, size_t vl);
vuint64m4_t __riscv_vsha2ms_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                        vuint64m4_t vs1, size_t vl);
vuint64m8_t __riscv_vsha2ms_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                        vuint64m8_t vs1, size_t vl);
----

[[policy-variant-]]
==== Vector SHA-2 two rounds of compression

[,c]
----
vuint32mf2_t __riscv_vsha2ch_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                          vuint32mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vsha2ch_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                        vuint32m1_t vs1, size_t vl);
vuint32m2_t __riscv_vsha2ch_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                        vuint32m2_t vs1, size_t vl);
vuint32m4_t __riscv_vsha2ch_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                        vuint32m4_t vs1, size_t vl);
vuint32m8_t __riscv_vsha2ch_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                        vuint32m8_t vs1, size_t vl);
vuint64m1_t __riscv_vsha2ch_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                        vuint64m1_t vs1, size_t vl);
vuint64m2_t __riscv_vsha2ch_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                        vuint64m2_t vs1, size_t vl);
vuint64m4_t __riscv_vsha2ch_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                        vuint64m4_t vs1, size_t vl);
vuint64m8_t __riscv_vsha2ch_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                        vuint64m8_t vs1, size_t vl);
vuint32mf2_t __riscv_vsha2cl_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                          vuint32mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vsha2cl_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                        vuint32m1_t vs1, size_t vl);
vuint32m2_t __riscv_vsha2cl_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                        vuint32m2_t vs1, size_t vl);
vuint32m4_t __riscv_vsha2cl_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                        vuint32m4_t vs1, size_t vl);
vuint32m8_t __riscv_vsha2cl_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                        vuint32m8_t vs1, size_t vl);
vuint64m1_t __riscv_vsha2cl_vv_u64m1_tu(vuint64m1_t vd, vuint64m1_t vs2,
                                        vuint64m1_t vs1, size_t vl);
vuint64m2_t __riscv_vsha2cl_vv_u64m2_tu(vuint64m2_t vd, vuint64m2_t vs2,
                                        vuint64m2_t vs1, size_t vl);
vuint64m4_t __riscv_vsha2cl_vv_u64m4_tu(vuint64m4_t vd, vuint64m4_t vs2,
                                        vuint64m4_t vs1, size_t vl);
vuint64m8_t __riscv_vsha2cl_vv_u64m8_tu(vuint64m8_t vd, vuint64m8_t vs2,
                                        vuint64m8_t vs1, size_t vl);
----

=== Zvksed - ShangMi Suite: SM4 Block Cipher

[[policy-variant-]]
==== Vector SM4 KeyExpansion

[,c]
----
vuint32mf2_t __riscv_vsm4k_vi_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        size_t uimm, size_t vl);
vuint32m1_t __riscv_vsm4k_vi_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      size_t uimm, size_t vl);
vuint32m2_t __riscv_vsm4k_vi_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      size_t uimm, size_t vl);
vuint32m4_t __riscv_vsm4k_vi_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      size_t uimm, size_t vl);
vuint32m8_t __riscv_vsm4k_vi_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      size_t uimm, size_t vl);
----

[[policy-variant-]]
==== Vector SM4 Rounds

[,c]
----
vuint32mf2_t __riscv_vsm4r_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        size_t vl);
vuint32mf2_t __riscv_vsm4r_vs_u32mf2_u32mf2_tu(vuint32mf2_t vd,
                                               vuint32mf2_t vs2, size_t vl);
vuint32m1_t __riscv_vsm4r_vs_u32mf2_u32m1_tu(vuint32m1_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m2_t __riscv_vsm4r_vs_u32mf2_u32m2_tu(vuint32m2_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m4_t __riscv_vsm4r_vs_u32mf2_u32m4_tu(vuint32m4_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m8_t __riscv_vsm4r_vs_u32mf2_u32m8_tu(vuint32m8_t vd, vuint32mf2_t vs2,
                                             size_t vl);
vuint32m1_t __riscv_vsm4r_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      size_t vl);
vuint32m1_t __riscv_vsm4r_vs_u32m1_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m2_t __riscv_vsm4r_vs_u32m1_u32m2_tu(vuint32m2_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m4_t __riscv_vsm4r_vs_u32m1_u32m4_tu(vuint32m4_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m8_t __riscv_vsm4r_vs_u32m1_u32m8_tu(vuint32m8_t vd, vuint32m1_t vs2,
                                            size_t vl);
vuint32m2_t __riscv_vsm4r_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      size_t vl);
vuint32m2_t __riscv_vsm4r_vs_u32m2_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                            size_t vl);
vuint32m4_t __riscv_vsm4r_vs_u32m2_u32m4_tu(vuint32m4_t vd, vuint32m2_t vs2,
                                            size_t vl);
vuint32m8_t __riscv_vsm4r_vs_u32m2_u32m8_tu(vuint32m8_t vd, vuint32m2_t vs2,
                                            size_t vl);
vuint32m4_t __riscv_vsm4r_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      size_t vl);
vuint32m4_t __riscv_vsm4r_vs_u32m4_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                            size_t vl);
vuint32m8_t __riscv_vsm4r_vs_u32m4_u32m8_tu(vuint32m8_t vd, vuint32m4_t vs2,
                                            size_t vl);
vuint32m8_t __riscv_vsm4r_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      size_t vl);
----

=== Zvksh - ShangMi Suite: SM3 Secure Hash

[[policy-variant-]]
==== Vector SM3 Message Expansion

[,c]
----
vuint32mf2_t __riscv_vsm3me_vv_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                         vuint32mf2_t vs1, size_t vl);
vuint32m1_t __riscv_vsm3me_vv_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                       vuint32m1_t vs1, size_t vl);
vuint32m2_t __riscv_vsm3me_vv_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                       vuint32m2_t vs1, size_t vl);
vuint32m4_t __riscv_vsm3me_vv_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                       vuint32m4_t vs1, size_t vl);
vuint32m8_t __riscv_vsm3me_vv_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                       vuint32m8_t vs1, size_t vl);
----

[[policy-variant-]]
==== Vector SM3 Compression

[,c]
----
vuint32mf2_t __riscv_vsm3c_vi_u32mf2_tu(vuint32mf2_t vd, vuint32mf2_t vs2,
                                        size_t uimm, size_t vl);
vuint32m1_t __riscv_vsm3c_vi_u32m1_tu(vuint32m1_t vd, vuint32m1_t vs2,
                                      size_t uimm, size_t vl);
vuint32m2_t __riscv_vsm3c_vi_u32m2_tu(vuint32m2_t vd, vuint32m2_t vs2,
                                      size_t uimm, size_t vl);
vuint32m4_t __riscv_vsm3c_vi_u32m4_tu(vuint32m4_t vd, vuint32m4_t vs2,
                                      size_t uimm, size_t vl);
vuint32m8_t __riscv_vsm3c_vi_u32m8_tu(vuint32m8_t vd, vuint32m8_t vs2,
                                      size_t uimm, size_t vl);
----
