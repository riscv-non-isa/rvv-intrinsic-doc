<!--NOTE: This file is generated by rvv_intrinsic_gen.py-->

## Vector Permutation Functions:

### [Integer and Floating-Point Scalar Move Functions](../rvv-intrinsic-api.md#171-integer-scalar-move-operations):

**Prototypes:**
``` C
int8_t vmv_x_s (vint8mf8_t src);
vint8mf8_t vmv_s_x (vint8mf8_t dst, int8_t src);
int8_t vmv_x_s (vint8mf4_t src);
vint8mf4_t vmv_s_x (vint8mf4_t dst, int8_t src);
int8_t vmv_x_s (vint8mf2_t src);
vint8mf2_t vmv_s_x (vint8mf2_t dst, int8_t src);
int8_t vmv_x_s (vint8m1_t src);
vint8m1_t vmv_s_x (vint8m1_t dst, int8_t src);
int8_t vmv_x_s (vint8m2_t src);
vint8m2_t vmv_s_x (vint8m2_t dst, int8_t src);
int8_t vmv_x_s (vint8m4_t src);
vint8m4_t vmv_s_x (vint8m4_t dst, int8_t src);
int8_t vmv_x_s (vint8m8_t src);
vint8m8_t vmv_s_x (vint8m8_t dst, int8_t src);
int16_t vmv_x_s (vint16mf4_t src);
vint16mf4_t vmv_s_x (vint16mf4_t dst, int16_t src);
int16_t vmv_x_s (vint16mf2_t src);
vint16mf2_t vmv_s_x (vint16mf2_t dst, int16_t src);
int16_t vmv_x_s (vint16m1_t src);
vint16m1_t vmv_s_x (vint16m1_t dst, int16_t src);
int16_t vmv_x_s (vint16m2_t src);
vint16m2_t vmv_s_x (vint16m2_t dst, int16_t src);
int16_t vmv_x_s (vint16m4_t src);
vint16m4_t vmv_s_x (vint16m4_t dst, int16_t src);
int16_t vmv_x_s (vint16m8_t src);
vint16m8_t vmv_s_x (vint16m8_t dst, int16_t src);
int32_t vmv_x_s (vint32mf2_t src);
vint32mf2_t vmv_s_x (vint32mf2_t dst, int32_t src);
int32_t vmv_x_s (vint32m1_t src);
vint32m1_t vmv_s_x (vint32m1_t dst, int32_t src);
int32_t vmv_x_s (vint32m2_t src);
vint32m2_t vmv_s_x (vint32m2_t dst, int32_t src);
int32_t vmv_x_s (vint32m4_t src);
vint32m4_t vmv_s_x (vint32m4_t dst, int32_t src);
int32_t vmv_x_s (vint32m8_t src);
vint32m8_t vmv_s_x (vint32m8_t dst, int32_t src);
int64_t vmv_x_s (vint64m1_t src);
vint64m1_t vmv_s_x (vint64m1_t dst, int64_t src);
int64_t vmv_x_s (vint64m2_t src);
vint64m2_t vmv_s_x (vint64m2_t dst, int64_t src);
int64_t vmv_x_s (vint64m4_t src);
vint64m4_t vmv_s_x (vint64m4_t dst, int64_t src);
int64_t vmv_x_s (vint64m8_t src);
vint64m8_t vmv_s_x (vint64m8_t dst, int64_t src);
uint8_t vmv_x_s (vuint8mf8_t src);
vuint8mf8_t vmv_s_x (vuint8mf8_t dst, uint8_t src);
uint8_t vmv_x_s (vuint8mf4_t src);
vuint8mf4_t vmv_s_x (vuint8mf4_t dst, uint8_t src);
uint8_t vmv_x_s (vuint8mf2_t src);
vuint8mf2_t vmv_s_x (vuint8mf2_t dst, uint8_t src);
uint8_t vmv_x_s (vuint8m1_t src);
vuint8m1_t vmv_s_x (vuint8m1_t dst, uint8_t src);
uint8_t vmv_x_s (vuint8m2_t src);
vuint8m2_t vmv_s_x (vuint8m2_t dst, uint8_t src);
uint8_t vmv_x_s (vuint8m4_t src);
vuint8m4_t vmv_s_x (vuint8m4_t dst, uint8_t src);
uint8_t vmv_x_s (vuint8m8_t src);
vuint8m8_t vmv_s_x (vuint8m8_t dst, uint8_t src);
uint16_t vmv_x_s (vuint16mf4_t src);
vuint16mf4_t vmv_s_x (vuint16mf4_t dst, uint16_t src);
uint16_t vmv_x_s (vuint16mf2_t src);
vuint16mf2_t vmv_s_x (vuint16mf2_t dst, uint16_t src);
uint16_t vmv_x_s (vuint16m1_t src);
vuint16m1_t vmv_s_x (vuint16m1_t dst, uint16_t src);
uint16_t vmv_x_s (vuint16m2_t src);
vuint16m2_t vmv_s_x (vuint16m2_t dst, uint16_t src);
uint16_t vmv_x_s (vuint16m4_t src);
vuint16m4_t vmv_s_x (vuint16m4_t dst, uint16_t src);
uint16_t vmv_x_s (vuint16m8_t src);
vuint16m8_t vmv_s_x (vuint16m8_t dst, uint16_t src);
uint32_t vmv_x_s (vuint32mf2_t src);
vuint32mf2_t vmv_s_x (vuint32mf2_t dst, uint32_t src);
uint32_t vmv_x_s (vuint32m1_t src);
vuint32m1_t vmv_s_x (vuint32m1_t dst, uint32_t src);
uint32_t vmv_x_s (vuint32m2_t src);
vuint32m2_t vmv_s_x (vuint32m2_t dst, uint32_t src);
uint32_t vmv_x_s (vuint32m4_t src);
vuint32m4_t vmv_s_x (vuint32m4_t dst, uint32_t src);
uint32_t vmv_x_s (vuint32m8_t src);
vuint32m8_t vmv_s_x (vuint32m8_t dst, uint32_t src);
uint64_t vmv_x_s (vuint64m1_t src);
vuint64m1_t vmv_s_x (vuint64m1_t dst, uint64_t src);
uint64_t vmv_x_s (vuint64m2_t src);
vuint64m2_t vmv_s_x (vuint64m2_t dst, uint64_t src);
uint64_t vmv_x_s (vuint64m4_t src);
vuint64m4_t vmv_s_x (vuint64m4_t dst, uint64_t src);
uint64_t vmv_x_s (vuint64m8_t src);
vuint64m8_t vmv_s_x (vuint64m8_t dst, uint64_t src);
float16_t vfmv_f_s (vfloat16mf4_t src);
vfloat16mf4_t vfmv_s_f (vfloat16mf4_t dst, float16_t src);
float16_t vfmv_f_s (vfloat16mf2_t src);
vfloat16mf2_t vfmv_s_f (vfloat16mf2_t dst, float16_t src);
float16_t vfmv_f_s (vfloat16m1_t src);
vfloat16m1_t vfmv_s_f (vfloat16m1_t dst, float16_t src);
float16_t vfmv_f_s (vfloat16m2_t src);
vfloat16m2_t vfmv_s_f (vfloat16m2_t dst, float16_t src);
float16_t vfmv_f_s (vfloat16m4_t src);
vfloat16m4_t vfmv_s_f (vfloat16m4_t dst, float16_t src);
float16_t vfmv_f_s (vfloat16m8_t src);
vfloat16m8_t vfmv_s_f (vfloat16m8_t dst, float16_t src);
float32_t vfmv_f_s (vfloat32mf2_t src);
vfloat32mf2_t vfmv_s_f (vfloat32mf2_t dst, float32_t src);
float32_t vfmv_f_s (vfloat32m1_t src);
vfloat32m1_t vfmv_s_f (vfloat32m1_t dst, float32_t src);
float32_t vfmv_f_s (vfloat32m2_t src);
vfloat32m2_t vfmv_s_f (vfloat32m2_t dst, float32_t src);
float32_t vfmv_f_s (vfloat32m4_t src);
vfloat32m4_t vfmv_s_f (vfloat32m4_t dst, float32_t src);
float32_t vfmv_f_s (vfloat32m8_t src);
vfloat32m8_t vfmv_s_f (vfloat32m8_t dst, float32_t src);
float64_t vfmv_f_s (vfloat64m1_t src);
vfloat64m1_t vfmv_s_f (vfloat64m1_t dst, float64_t src);
float64_t vfmv_f_s (vfloat64m2_t src);
vfloat64m2_t vfmv_s_f (vfloat64m2_t dst, float64_t src);
float64_t vfmv_f_s (vfloat64m4_t src);
vfloat64m4_t vfmv_s_f (vfloat64m4_t dst, float64_t src);
float64_t vfmv_f_s (vfloat64m8_t src);
vfloat64m8_t vfmv_s_f (vfloat64m8_t dst, float64_t src);
```
### [Vector Slideup and Slidedown Functions](../rvv-intrinsic-api.md#173-vector-slide-operations):

**Prototypes:**
``` C
vint8mf8_t vslideup (vint8mf8_t dst, vint8mf8_t src, size_t offset);
vint8mf4_t vslideup (vint8mf4_t dst, vint8mf4_t src, size_t offset);
vint8mf2_t vslideup (vint8mf2_t dst, vint8mf2_t src, size_t offset);
vint8m1_t vslideup (vint8m1_t dst, vint8m1_t src, size_t offset);
vint8m2_t vslideup (vint8m2_t dst, vint8m2_t src, size_t offset);
vint8m4_t vslideup (vint8m4_t dst, vint8m4_t src, size_t offset);
vint8m8_t vslideup (vint8m8_t dst, vint8m8_t src, size_t offset);
vint16mf4_t vslideup (vint16mf4_t dst, vint16mf4_t src, size_t offset);
vint16mf2_t vslideup (vint16mf2_t dst, vint16mf2_t src, size_t offset);
vint16m1_t vslideup (vint16m1_t dst, vint16m1_t src, size_t offset);
vint16m2_t vslideup (vint16m2_t dst, vint16m2_t src, size_t offset);
vint16m4_t vslideup (vint16m4_t dst, vint16m4_t src, size_t offset);
vint16m8_t vslideup (vint16m8_t dst, vint16m8_t src, size_t offset);
vint32mf2_t vslideup (vint32mf2_t dst, vint32mf2_t src, size_t offset);
vint32m1_t vslideup (vint32m1_t dst, vint32m1_t src, size_t offset);
vint32m2_t vslideup (vint32m2_t dst, vint32m2_t src, size_t offset);
vint32m4_t vslideup (vint32m4_t dst, vint32m4_t src, size_t offset);
vint32m8_t vslideup (vint32m8_t dst, vint32m8_t src, size_t offset);
vint64m1_t vslideup (vint64m1_t dst, vint64m1_t src, size_t offset);
vint64m2_t vslideup (vint64m2_t dst, vint64m2_t src, size_t offset);
vint64m4_t vslideup (vint64m4_t dst, vint64m4_t src, size_t offset);
vint64m8_t vslideup (vint64m8_t dst, vint64m8_t src, size_t offset);
vuint8mf8_t vslideup (vuint8mf8_t dst, vuint8mf8_t src, size_t offset);
vuint8mf4_t vslideup (vuint8mf4_t dst, vuint8mf4_t src, size_t offset);
vuint8mf2_t vslideup (vuint8mf2_t dst, vuint8mf2_t src, size_t offset);
vuint8m1_t vslideup (vuint8m1_t dst, vuint8m1_t src, size_t offset);
vuint8m2_t vslideup (vuint8m2_t dst, vuint8m2_t src, size_t offset);
vuint8m4_t vslideup (vuint8m4_t dst, vuint8m4_t src, size_t offset);
vuint8m8_t vslideup (vuint8m8_t dst, vuint8m8_t src, size_t offset);
vuint16mf4_t vslideup (vuint16mf4_t dst, vuint16mf4_t src, size_t offset);
vuint16mf2_t vslideup (vuint16mf2_t dst, vuint16mf2_t src, size_t offset);
vuint16m1_t vslideup (vuint16m1_t dst, vuint16m1_t src, size_t offset);
vuint16m2_t vslideup (vuint16m2_t dst, vuint16m2_t src, size_t offset);
vuint16m4_t vslideup (vuint16m4_t dst, vuint16m4_t src, size_t offset);
vuint16m8_t vslideup (vuint16m8_t dst, vuint16m8_t src, size_t offset);
vuint32mf2_t vslideup (vuint32mf2_t dst, vuint32mf2_t src, size_t offset);
vuint32m1_t vslideup (vuint32m1_t dst, vuint32m1_t src, size_t offset);
vuint32m2_t vslideup (vuint32m2_t dst, vuint32m2_t src, size_t offset);
vuint32m4_t vslideup (vuint32m4_t dst, vuint32m4_t src, size_t offset);
vuint32m8_t vslideup (vuint32m8_t dst, vuint32m8_t src, size_t offset);
vuint64m1_t vslideup (vuint64m1_t dst, vuint64m1_t src, size_t offset);
vuint64m2_t vslideup (vuint64m2_t dst, vuint64m2_t src, size_t offset);
vuint64m4_t vslideup (vuint64m4_t dst, vuint64m4_t src, size_t offset);
vuint64m8_t vslideup (vuint64m8_t dst, vuint64m8_t src, size_t offset);
vfloat16mf4_t vslideup (vfloat16mf4_t dst, vfloat16mf4_t src, size_t offset);
vfloat16mf2_t vslideup (vfloat16mf2_t dst, vfloat16mf2_t src, size_t offset);
vfloat16m1_t vslideup (vfloat16m1_t dst, vfloat16m1_t src, size_t offset);
vfloat16m2_t vslideup (vfloat16m2_t dst, vfloat16m2_t src, size_t offset);
vfloat16m4_t vslideup (vfloat16m4_t dst, vfloat16m4_t src, size_t offset);
vfloat16m8_t vslideup (vfloat16m8_t dst, vfloat16m8_t src, size_t offset);
vfloat32mf2_t vslideup (vfloat32mf2_t dst, vfloat32mf2_t src, size_t offset);
vfloat32m1_t vslideup (vfloat32m1_t dst, vfloat32m1_t src, size_t offset);
vfloat32m2_t vslideup (vfloat32m2_t dst, vfloat32m2_t src, size_t offset);
vfloat32m4_t vslideup (vfloat32m4_t dst, vfloat32m4_t src, size_t offset);
vfloat32m8_t vslideup (vfloat32m8_t dst, vfloat32m8_t src, size_t offset);
vfloat64m1_t vslideup (vfloat64m1_t dst, vfloat64m1_t src, size_t offset);
vfloat64m2_t vslideup (vfloat64m2_t dst, vfloat64m2_t src, size_t offset);
vfloat64m4_t vslideup (vfloat64m4_t dst, vfloat64m4_t src, size_t offset);
vfloat64m8_t vslideup (vfloat64m8_t dst, vfloat64m8_t src, size_t offset);
vint8mf8_t vslidedown (vint8mf8_t dst, vint8mf8_t src, size_t offset);
vint8mf4_t vslidedown (vint8mf4_t dst, vint8mf4_t src, size_t offset);
vint8mf2_t vslidedown (vint8mf2_t dst, vint8mf2_t src, size_t offset);
vint8m1_t vslidedown (vint8m1_t dst, vint8m1_t src, size_t offset);
vint8m2_t vslidedown (vint8m2_t dst, vint8m2_t src, size_t offset);
vint8m4_t vslidedown (vint8m4_t dst, vint8m4_t src, size_t offset);
vint8m8_t vslidedown (vint8m8_t dst, vint8m8_t src, size_t offset);
vint16mf4_t vslidedown (vint16mf4_t dst, vint16mf4_t src, size_t offset);
vint16mf2_t vslidedown (vint16mf2_t dst, vint16mf2_t src, size_t offset);
vint16m1_t vslidedown (vint16m1_t dst, vint16m1_t src, size_t offset);
vint16m2_t vslidedown (vint16m2_t dst, vint16m2_t src, size_t offset);
vint16m4_t vslidedown (vint16m4_t dst, vint16m4_t src, size_t offset);
vint16m8_t vslidedown (vint16m8_t dst, vint16m8_t src, size_t offset);
vint32mf2_t vslidedown (vint32mf2_t dst, vint32mf2_t src, size_t offset);
vint32m1_t vslidedown (vint32m1_t dst, vint32m1_t src, size_t offset);
vint32m2_t vslidedown (vint32m2_t dst, vint32m2_t src, size_t offset);
vint32m4_t vslidedown (vint32m4_t dst, vint32m4_t src, size_t offset);
vint32m8_t vslidedown (vint32m8_t dst, vint32m8_t src, size_t offset);
vint64m1_t vslidedown (vint64m1_t dst, vint64m1_t src, size_t offset);
vint64m2_t vslidedown (vint64m2_t dst, vint64m2_t src, size_t offset);
vint64m4_t vslidedown (vint64m4_t dst, vint64m4_t src, size_t offset);
vint64m8_t vslidedown (vint64m8_t dst, vint64m8_t src, size_t offset);
vuint8mf8_t vslidedown (vuint8mf8_t dst, vuint8mf8_t src, size_t offset);
vuint8mf4_t vslidedown (vuint8mf4_t dst, vuint8mf4_t src, size_t offset);
vuint8mf2_t vslidedown (vuint8mf2_t dst, vuint8mf2_t src, size_t offset);
vuint8m1_t vslidedown (vuint8m1_t dst, vuint8m1_t src, size_t offset);
vuint8m2_t vslidedown (vuint8m2_t dst, vuint8m2_t src, size_t offset);
vuint8m4_t vslidedown (vuint8m4_t dst, vuint8m4_t src, size_t offset);
vuint8m8_t vslidedown (vuint8m8_t dst, vuint8m8_t src, size_t offset);
vuint16mf4_t vslidedown (vuint16mf4_t dst, vuint16mf4_t src, size_t offset);
vuint16mf2_t vslidedown (vuint16mf2_t dst, vuint16mf2_t src, size_t offset);
vuint16m1_t vslidedown (vuint16m1_t dst, vuint16m1_t src, size_t offset);
vuint16m2_t vslidedown (vuint16m2_t dst, vuint16m2_t src, size_t offset);
vuint16m4_t vslidedown (vuint16m4_t dst, vuint16m4_t src, size_t offset);
vuint16m8_t vslidedown (vuint16m8_t dst, vuint16m8_t src, size_t offset);
vuint32mf2_t vslidedown (vuint32mf2_t dst, vuint32mf2_t src, size_t offset);
vuint32m1_t vslidedown (vuint32m1_t dst, vuint32m1_t src, size_t offset);
vuint32m2_t vslidedown (vuint32m2_t dst, vuint32m2_t src, size_t offset);
vuint32m4_t vslidedown (vuint32m4_t dst, vuint32m4_t src, size_t offset);
vuint32m8_t vslidedown (vuint32m8_t dst, vuint32m8_t src, size_t offset);
vuint64m1_t vslidedown (vuint64m1_t dst, vuint64m1_t src, size_t offset);
vuint64m2_t vslidedown (vuint64m2_t dst, vuint64m2_t src, size_t offset);
vuint64m4_t vslidedown (vuint64m4_t dst, vuint64m4_t src, size_t offset);
vuint64m8_t vslidedown (vuint64m8_t dst, vuint64m8_t src, size_t offset);
vfloat16mf4_t vslidedown (vfloat16mf4_t dst, vfloat16mf4_t src, size_t offset);
vfloat16mf2_t vslidedown (vfloat16mf2_t dst, vfloat16mf2_t src, size_t offset);
vfloat16m1_t vslidedown (vfloat16m1_t dst, vfloat16m1_t src, size_t offset);
vfloat16m2_t vslidedown (vfloat16m2_t dst, vfloat16m2_t src, size_t offset);
vfloat16m4_t vslidedown (vfloat16m4_t dst, vfloat16m4_t src, size_t offset);
vfloat16m8_t vslidedown (vfloat16m8_t dst, vfloat16m8_t src, size_t offset);
vfloat32mf2_t vslidedown (vfloat32mf2_t dst, vfloat32mf2_t src, size_t offset);
vfloat32m1_t vslidedown (vfloat32m1_t dst, vfloat32m1_t src, size_t offset);
vfloat32m2_t vslidedown (vfloat32m2_t dst, vfloat32m2_t src, size_t offset);
vfloat32m4_t vslidedown (vfloat32m4_t dst, vfloat32m4_t src, size_t offset);
vfloat32m8_t vslidedown (vfloat32m8_t dst, vfloat32m8_t src, size_t offset);
vfloat64m1_t vslidedown (vfloat64m1_t dst, vfloat64m1_t src, size_t offset);
vfloat64m2_t vslidedown (vfloat64m2_t dst, vfloat64m2_t src, size_t offset);
vfloat64m4_t vslidedown (vfloat64m4_t dst, vfloat64m4_t src, size_t offset);
vfloat64m8_t vslidedown (vfloat64m8_t dst, vfloat64m8_t src, size_t offset);
// masked functions
vint8mf8_t vslideup_m (vbool64_t mask, vint8mf8_t dst, vint8mf8_t src, size_t offset);
vint8mf4_t vslideup_m (vbool32_t mask, vint8mf4_t dst, vint8mf4_t src, size_t offset);
vint8mf2_t vslideup_m (vbool16_t mask, vint8mf2_t dst, vint8mf2_t src, size_t offset);
vint8m1_t vslideup_m (vbool8_t mask, vint8m1_t dst, vint8m1_t src, size_t offset);
vint8m2_t vslideup_m (vbool4_t mask, vint8m2_t dst, vint8m2_t src, size_t offset);
vint8m4_t vslideup_m (vbool2_t mask, vint8m4_t dst, vint8m4_t src, size_t offset);
vint8m8_t vslideup_m (vbool1_t mask, vint8m8_t dst, vint8m8_t src, size_t offset);
vint16mf4_t vslideup_m (vbool64_t mask, vint16mf4_t dst, vint16mf4_t src, size_t offset);
vint16mf2_t vslideup_m (vbool32_t mask, vint16mf2_t dst, vint16mf2_t src, size_t offset);
vint16m1_t vslideup_m (vbool16_t mask, vint16m1_t dst, vint16m1_t src, size_t offset);
vint16m2_t vslideup_m (vbool8_t mask, vint16m2_t dst, vint16m2_t src, size_t offset);
vint16m4_t vslideup_m (vbool4_t mask, vint16m4_t dst, vint16m4_t src, size_t offset);
vint16m8_t vslideup_m (vbool2_t mask, vint16m8_t dst, vint16m8_t src, size_t offset);
vint32mf2_t vslideup_m (vbool64_t mask, vint32mf2_t dst, vint32mf2_t src, size_t offset);
vint32m1_t vslideup_m (vbool32_t mask, vint32m1_t dst, vint32m1_t src, size_t offset);
vint32m2_t vslideup_m (vbool16_t mask, vint32m2_t dst, vint32m2_t src, size_t offset);
vint32m4_t vslideup_m (vbool8_t mask, vint32m4_t dst, vint32m4_t src, size_t offset);
vint32m8_t vslideup_m (vbool4_t mask, vint32m8_t dst, vint32m8_t src, size_t offset);
vint64m1_t vslideup_m (vbool64_t mask, vint64m1_t dst, vint64m1_t src, size_t offset);
vint64m2_t vslideup_m (vbool32_t mask, vint64m2_t dst, vint64m2_t src, size_t offset);
vint64m4_t vslideup_m (vbool16_t mask, vint64m4_t dst, vint64m4_t src, size_t offset);
vint64m8_t vslideup_m (vbool8_t mask, vint64m8_t dst, vint64m8_t src, size_t offset);
vuint8mf8_t vslideup_m (vbool64_t mask, vuint8mf8_t dst, vuint8mf8_t src, size_t offset);
vuint8mf4_t vslideup_m (vbool32_t mask, vuint8mf4_t dst, vuint8mf4_t src, size_t offset);
vuint8mf2_t vslideup_m (vbool16_t mask, vuint8mf2_t dst, vuint8mf2_t src, size_t offset);
vuint8m1_t vslideup_m (vbool8_t mask, vuint8m1_t dst, vuint8m1_t src, size_t offset);
vuint8m2_t vslideup_m (vbool4_t mask, vuint8m2_t dst, vuint8m2_t src, size_t offset);
vuint8m4_t vslideup_m (vbool2_t mask, vuint8m4_t dst, vuint8m4_t src, size_t offset);
vuint8m8_t vslideup_m (vbool1_t mask, vuint8m8_t dst, vuint8m8_t src, size_t offset);
vuint16mf4_t vslideup_m (vbool64_t mask, vuint16mf4_t dst, vuint16mf4_t src, size_t offset);
vuint16mf2_t vslideup_m (vbool32_t mask, vuint16mf2_t dst, vuint16mf2_t src, size_t offset);
vuint16m1_t vslideup_m (vbool16_t mask, vuint16m1_t dst, vuint16m1_t src, size_t offset);
vuint16m2_t vslideup_m (vbool8_t mask, vuint16m2_t dst, vuint16m2_t src, size_t offset);
vuint16m4_t vslideup_m (vbool4_t mask, vuint16m4_t dst, vuint16m4_t src, size_t offset);
vuint16m8_t vslideup_m (vbool2_t mask, vuint16m8_t dst, vuint16m8_t src, size_t offset);
vuint32mf2_t vslideup_m (vbool64_t mask, vuint32mf2_t dst, vuint32mf2_t src, size_t offset);
vuint32m1_t vslideup_m (vbool32_t mask, vuint32m1_t dst, vuint32m1_t src, size_t offset);
vuint32m2_t vslideup_m (vbool16_t mask, vuint32m2_t dst, vuint32m2_t src, size_t offset);
vuint32m4_t vslideup_m (vbool8_t mask, vuint32m4_t dst, vuint32m4_t src, size_t offset);
vuint32m8_t vslideup_m (vbool4_t mask, vuint32m8_t dst, vuint32m8_t src, size_t offset);
vuint64m1_t vslideup_m (vbool64_t mask, vuint64m1_t dst, vuint64m1_t src, size_t offset);
vuint64m2_t vslideup_m (vbool32_t mask, vuint64m2_t dst, vuint64m2_t src, size_t offset);
vuint64m4_t vslideup_m (vbool16_t mask, vuint64m4_t dst, vuint64m4_t src, size_t offset);
vuint64m8_t vslideup_m (vbool8_t mask, vuint64m8_t dst, vuint64m8_t src, size_t offset);
vfloat16mf4_t vslideup_m (vbool64_t mask, vfloat16mf4_t dst, vfloat16mf4_t src, size_t offset);
vfloat16mf2_t vslideup_m (vbool32_t mask, vfloat16mf2_t dst, vfloat16mf2_t src, size_t offset);
vfloat16m1_t vslideup_m (vbool16_t mask, vfloat16m1_t dst, vfloat16m1_t src, size_t offset);
vfloat16m2_t vslideup_m (vbool8_t mask, vfloat16m2_t dst, vfloat16m2_t src, size_t offset);
vfloat16m4_t vslideup_m (vbool4_t mask, vfloat16m4_t dst, vfloat16m4_t src, size_t offset);
vfloat16m8_t vslideup_m (vbool2_t mask, vfloat16m8_t dst, vfloat16m8_t src, size_t offset);
vfloat32mf2_t vslideup_m (vbool64_t mask, vfloat32mf2_t dst, vfloat32mf2_t src, size_t offset);
vfloat32m1_t vslideup_m (vbool32_t mask, vfloat32m1_t dst, vfloat32m1_t src, size_t offset);
vfloat32m2_t vslideup_m (vbool16_t mask, vfloat32m2_t dst, vfloat32m2_t src, size_t offset);
vfloat32m4_t vslideup_m (vbool8_t mask, vfloat32m4_t dst, vfloat32m4_t src, size_t offset);
vfloat32m8_t vslideup_m (vbool4_t mask, vfloat32m8_t dst, vfloat32m8_t src, size_t offset);
vfloat64m1_t vslideup_m (vbool64_t mask, vfloat64m1_t dst, vfloat64m1_t src, size_t offset);
vfloat64m2_t vslideup_m (vbool32_t mask, vfloat64m2_t dst, vfloat64m2_t src, size_t offset);
vfloat64m4_t vslideup_m (vbool16_t mask, vfloat64m4_t dst, vfloat64m4_t src, size_t offset);
vfloat64m8_t vslideup_m (vbool8_t mask, vfloat64m8_t dst, vfloat64m8_t src, size_t offset);
vint8mf8_t vslidedown_m (vbool64_t mask, vint8mf8_t dst, vint8mf8_t src, size_t offset);
vint8mf4_t vslidedown_m (vbool32_t mask, vint8mf4_t dst, vint8mf4_t src, size_t offset);
vint8mf2_t vslidedown_m (vbool16_t mask, vint8mf2_t dst, vint8mf2_t src, size_t offset);
vint8m1_t vslidedown_m (vbool8_t mask, vint8m1_t dst, vint8m1_t src, size_t offset);
vint8m2_t vslidedown_m (vbool4_t mask, vint8m2_t dst, vint8m2_t src, size_t offset);
vint8m4_t vslidedown_m (vbool2_t mask, vint8m4_t dst, vint8m4_t src, size_t offset);
vint8m8_t vslidedown_m (vbool1_t mask, vint8m8_t dst, vint8m8_t src, size_t offset);
vint16mf4_t vslidedown_m (vbool64_t mask, vint16mf4_t dst, vint16mf4_t src, size_t offset);
vint16mf2_t vslidedown_m (vbool32_t mask, vint16mf2_t dst, vint16mf2_t src, size_t offset);
vint16m1_t vslidedown_m (vbool16_t mask, vint16m1_t dst, vint16m1_t src, size_t offset);
vint16m2_t vslidedown_m (vbool8_t mask, vint16m2_t dst, vint16m2_t src, size_t offset);
vint16m4_t vslidedown_m (vbool4_t mask, vint16m4_t dst, vint16m4_t src, size_t offset);
vint16m8_t vslidedown_m (vbool2_t mask, vint16m8_t dst, vint16m8_t src, size_t offset);
vint32mf2_t vslidedown_m (vbool64_t mask, vint32mf2_t dst, vint32mf2_t src, size_t offset);
vint32m1_t vslidedown_m (vbool32_t mask, vint32m1_t dst, vint32m1_t src, size_t offset);
vint32m2_t vslidedown_m (vbool16_t mask, vint32m2_t dst, vint32m2_t src, size_t offset);
vint32m4_t vslidedown_m (vbool8_t mask, vint32m4_t dst, vint32m4_t src, size_t offset);
vint32m8_t vslidedown_m (vbool4_t mask, vint32m8_t dst, vint32m8_t src, size_t offset);
vint64m1_t vslidedown_m (vbool64_t mask, vint64m1_t dst, vint64m1_t src, size_t offset);
vint64m2_t vslidedown_m (vbool32_t mask, vint64m2_t dst, vint64m2_t src, size_t offset);
vint64m4_t vslidedown_m (vbool16_t mask, vint64m4_t dst, vint64m4_t src, size_t offset);
vint64m8_t vslidedown_m (vbool8_t mask, vint64m8_t dst, vint64m8_t src, size_t offset);
vuint8mf8_t vslidedown_m (vbool64_t mask, vuint8mf8_t dst, vuint8mf8_t src, size_t offset);
vuint8mf4_t vslidedown_m (vbool32_t mask, vuint8mf4_t dst, vuint8mf4_t src, size_t offset);
vuint8mf2_t vslidedown_m (vbool16_t mask, vuint8mf2_t dst, vuint8mf2_t src, size_t offset);
vuint8m1_t vslidedown_m (vbool8_t mask, vuint8m1_t dst, vuint8m1_t src, size_t offset);
vuint8m2_t vslidedown_m (vbool4_t mask, vuint8m2_t dst, vuint8m2_t src, size_t offset);
vuint8m4_t vslidedown_m (vbool2_t mask, vuint8m4_t dst, vuint8m4_t src, size_t offset);
vuint8m8_t vslidedown_m (vbool1_t mask, vuint8m8_t dst, vuint8m8_t src, size_t offset);
vuint16mf4_t vslidedown_m (vbool64_t mask, vuint16mf4_t dst, vuint16mf4_t src, size_t offset);
vuint16mf2_t vslidedown_m (vbool32_t mask, vuint16mf2_t dst, vuint16mf2_t src, size_t offset);
vuint16m1_t vslidedown_m (vbool16_t mask, vuint16m1_t dst, vuint16m1_t src, size_t offset);
vuint16m2_t vslidedown_m (vbool8_t mask, vuint16m2_t dst, vuint16m2_t src, size_t offset);
vuint16m4_t vslidedown_m (vbool4_t mask, vuint16m4_t dst, vuint16m4_t src, size_t offset);
vuint16m8_t vslidedown_m (vbool2_t mask, vuint16m8_t dst, vuint16m8_t src, size_t offset);
vuint32mf2_t vslidedown_m (vbool64_t mask, vuint32mf2_t dst, vuint32mf2_t src, size_t offset);
vuint32m1_t vslidedown_m (vbool32_t mask, vuint32m1_t dst, vuint32m1_t src, size_t offset);
vuint32m2_t vslidedown_m (vbool16_t mask, vuint32m2_t dst, vuint32m2_t src, size_t offset);
vuint32m4_t vslidedown_m (vbool8_t mask, vuint32m4_t dst, vuint32m4_t src, size_t offset);
vuint32m8_t vslidedown_m (vbool4_t mask, vuint32m8_t dst, vuint32m8_t src, size_t offset);
vuint64m1_t vslidedown_m (vbool64_t mask, vuint64m1_t dst, vuint64m1_t src, size_t offset);
vuint64m2_t vslidedown_m (vbool32_t mask, vuint64m2_t dst, vuint64m2_t src, size_t offset);
vuint64m4_t vslidedown_m (vbool16_t mask, vuint64m4_t dst, vuint64m4_t src, size_t offset);
vuint64m8_t vslidedown_m (vbool8_t mask, vuint64m8_t dst, vuint64m8_t src, size_t offset);
vfloat16mf4_t vslidedown_m (vbool64_t mask, vfloat16mf4_t dst, vfloat16mf4_t src, size_t offset);
vfloat16mf2_t vslidedown_m (vbool32_t mask, vfloat16mf2_t dst, vfloat16mf2_t src, size_t offset);
vfloat16m1_t vslidedown_m (vbool16_t mask, vfloat16m1_t dst, vfloat16m1_t src, size_t offset);
vfloat16m2_t vslidedown_m (vbool8_t mask, vfloat16m2_t dst, vfloat16m2_t src, size_t offset);
vfloat16m4_t vslidedown_m (vbool4_t mask, vfloat16m4_t dst, vfloat16m4_t src, size_t offset);
vfloat16m8_t vslidedown_m (vbool2_t mask, vfloat16m8_t dst, vfloat16m8_t src, size_t offset);
vfloat32mf2_t vslidedown_m (vbool64_t mask, vfloat32mf2_t dst, vfloat32mf2_t src, size_t offset);
vfloat32m1_t vslidedown_m (vbool32_t mask, vfloat32m1_t dst, vfloat32m1_t src, size_t offset);
vfloat32m2_t vslidedown_m (vbool16_t mask, vfloat32m2_t dst, vfloat32m2_t src, size_t offset);
vfloat32m4_t vslidedown_m (vbool8_t mask, vfloat32m4_t dst, vfloat32m4_t src, size_t offset);
vfloat32m8_t vslidedown_m (vbool4_t mask, vfloat32m8_t dst, vfloat32m8_t src, size_t offset);
vfloat64m1_t vslidedown_m (vbool64_t mask, vfloat64m1_t dst, vfloat64m1_t src, size_t offset);
vfloat64m2_t vslidedown_m (vbool32_t mask, vfloat64m2_t dst, vfloat64m2_t src, size_t offset);
vfloat64m4_t vslidedown_m (vbool16_t mask, vfloat64m4_t dst, vfloat64m4_t src, size_t offset);
vfloat64m8_t vslidedown_m (vbool8_t mask, vfloat64m8_t dst, vfloat64m8_t src, size_t offset);
```
### [Vector Slide1up and Slide1down Functions](../rvv-intrinsic-api.md#173-vector-slide1up-and-slide1down-functions):

**Prototypes:**
``` C
vint8mf8_t vslide1up (vint8mf8_t src, int8_t value);
vint8mf4_t vslide1up (vint8mf4_t src, int8_t value);
vint8mf2_t vslide1up (vint8mf2_t src, int8_t value);
vint8m1_t vslide1up (vint8m1_t src, int8_t value);
vint8m2_t vslide1up (vint8m2_t src, int8_t value);
vint8m4_t vslide1up (vint8m4_t src, int8_t value);
vint8m8_t vslide1up (vint8m8_t src, int8_t value);
vint16mf4_t vslide1up (vint16mf4_t src, int16_t value);
vint16mf2_t vslide1up (vint16mf2_t src, int16_t value);
vint16m1_t vslide1up (vint16m1_t src, int16_t value);
vint16m2_t vslide1up (vint16m2_t src, int16_t value);
vint16m4_t vslide1up (vint16m4_t src, int16_t value);
vint16m8_t vslide1up (vint16m8_t src, int16_t value);
vint32mf2_t vslide1up (vint32mf2_t src, int32_t value);
vint32m1_t vslide1up (vint32m1_t src, int32_t value);
vint32m2_t vslide1up (vint32m2_t src, int32_t value);
vint32m4_t vslide1up (vint32m4_t src, int32_t value);
vint32m8_t vslide1up (vint32m8_t src, int32_t value);
vint64m1_t vslide1up (vint64m1_t src, int64_t value);
vint64m2_t vslide1up (vint64m2_t src, int64_t value);
vint64m4_t vslide1up (vint64m4_t src, int64_t value);
vint64m8_t vslide1up (vint64m8_t src, int64_t value);
vuint8mf8_t vslide1up (vuint8mf8_t src, uint8_t value);
vuint8mf4_t vslide1up (vuint8mf4_t src, uint8_t value);
vuint8mf2_t vslide1up (vuint8mf2_t src, uint8_t value);
vuint8m1_t vslide1up (vuint8m1_t src, uint8_t value);
vuint8m2_t vslide1up (vuint8m2_t src, uint8_t value);
vuint8m4_t vslide1up (vuint8m4_t src, uint8_t value);
vuint8m8_t vslide1up (vuint8m8_t src, uint8_t value);
vuint16mf4_t vslide1up (vuint16mf4_t src, uint16_t value);
vuint16mf2_t vslide1up (vuint16mf2_t src, uint16_t value);
vuint16m1_t vslide1up (vuint16m1_t src, uint16_t value);
vuint16m2_t vslide1up (vuint16m2_t src, uint16_t value);
vuint16m4_t vslide1up (vuint16m4_t src, uint16_t value);
vuint16m8_t vslide1up (vuint16m8_t src, uint16_t value);
vuint32mf2_t vslide1up (vuint32mf2_t src, uint32_t value);
vuint32m1_t vslide1up (vuint32m1_t src, uint32_t value);
vuint32m2_t vslide1up (vuint32m2_t src, uint32_t value);
vuint32m4_t vslide1up (vuint32m4_t src, uint32_t value);
vuint32m8_t vslide1up (vuint32m8_t src, uint32_t value);
vuint64m1_t vslide1up (vuint64m1_t src, uint64_t value);
vuint64m2_t vslide1up (vuint64m2_t src, uint64_t value);
vuint64m4_t vslide1up (vuint64m4_t src, uint64_t value);
vuint64m8_t vslide1up (vuint64m8_t src, uint64_t value);
vfloat16mf4_t vfslide1up (vfloat16mf4_t src, float16_t value);
vfloat16mf2_t vfslide1up (vfloat16mf2_t src, float16_t value);
vfloat16m1_t vfslide1up (vfloat16m1_t src, float16_t value);
vfloat16m2_t vfslide1up (vfloat16m2_t src, float16_t value);
vfloat16m4_t vfslide1up (vfloat16m4_t src, float16_t value);
vfloat16m8_t vfslide1up (vfloat16m8_t src, float16_t value);
vfloat32mf2_t vfslide1up (vfloat32mf2_t src, float32_t value);
vfloat32m1_t vfslide1up (vfloat32m1_t src, float32_t value);
vfloat32m2_t vfslide1up (vfloat32m2_t src, float32_t value);
vfloat32m4_t vfslide1up (vfloat32m4_t src, float32_t value);
vfloat32m8_t vfslide1up (vfloat32m8_t src, float32_t value);
vfloat64m1_t vfslide1up (vfloat64m1_t src, float64_t value);
vfloat64m2_t vfslide1up (vfloat64m2_t src, float64_t value);
vfloat64m4_t vfslide1up (vfloat64m4_t src, float64_t value);
vfloat64m8_t vfslide1up (vfloat64m8_t src, float64_t value);
vint8mf8_t vslide1down (vint8mf8_t src, int8_t value);
vint8mf4_t vslide1down (vint8mf4_t src, int8_t value);
vint8mf2_t vslide1down (vint8mf2_t src, int8_t value);
vint8m1_t vslide1down (vint8m1_t src, int8_t value);
vint8m2_t vslide1down (vint8m2_t src, int8_t value);
vint8m4_t vslide1down (vint8m4_t src, int8_t value);
vint8m8_t vslide1down (vint8m8_t src, int8_t value);
vint16mf4_t vslide1down (vint16mf4_t src, int16_t value);
vint16mf2_t vslide1down (vint16mf2_t src, int16_t value);
vint16m1_t vslide1down (vint16m1_t src, int16_t value);
vint16m2_t vslide1down (vint16m2_t src, int16_t value);
vint16m4_t vslide1down (vint16m4_t src, int16_t value);
vint16m8_t vslide1down (vint16m8_t src, int16_t value);
vint32mf2_t vslide1down (vint32mf2_t src, int32_t value);
vint32m1_t vslide1down (vint32m1_t src, int32_t value);
vint32m2_t vslide1down (vint32m2_t src, int32_t value);
vint32m4_t vslide1down (vint32m4_t src, int32_t value);
vint32m8_t vslide1down (vint32m8_t src, int32_t value);
vint64m1_t vslide1down (vint64m1_t src, int64_t value);
vint64m2_t vslide1down (vint64m2_t src, int64_t value);
vint64m4_t vslide1down (vint64m4_t src, int64_t value);
vint64m8_t vslide1down (vint64m8_t src, int64_t value);
vuint8mf8_t vslide1down (vuint8mf8_t src, uint8_t value);
vuint8mf4_t vslide1down (vuint8mf4_t src, uint8_t value);
vuint8mf2_t vslide1down (vuint8mf2_t src, uint8_t value);
vuint8m1_t vslide1down (vuint8m1_t src, uint8_t value);
vuint8m2_t vslide1down (vuint8m2_t src, uint8_t value);
vuint8m4_t vslide1down (vuint8m4_t src, uint8_t value);
vuint8m8_t vslide1down (vuint8m8_t src, uint8_t value);
vuint16mf4_t vslide1down (vuint16mf4_t src, uint16_t value);
vuint16mf2_t vslide1down (vuint16mf2_t src, uint16_t value);
vuint16m1_t vslide1down (vuint16m1_t src, uint16_t value);
vuint16m2_t vslide1down (vuint16m2_t src, uint16_t value);
vuint16m4_t vslide1down (vuint16m4_t src, uint16_t value);
vuint16m8_t vslide1down (vuint16m8_t src, uint16_t value);
vuint32mf2_t vslide1down (vuint32mf2_t src, uint32_t value);
vuint32m1_t vslide1down (vuint32m1_t src, uint32_t value);
vuint32m2_t vslide1down (vuint32m2_t src, uint32_t value);
vuint32m4_t vslide1down (vuint32m4_t src, uint32_t value);
vuint32m8_t vslide1down (vuint32m8_t src, uint32_t value);
vuint64m1_t vslide1down (vuint64m1_t src, uint64_t value);
vuint64m2_t vslide1down (vuint64m2_t src, uint64_t value);
vuint64m4_t vslide1down (vuint64m4_t src, uint64_t value);
vuint64m8_t vslide1down (vuint64m8_t src, uint64_t value);
vfloat16mf4_t vfslide1down (vfloat16mf4_t src, float16_t value);
vfloat16mf2_t vfslide1down (vfloat16mf2_t src, float16_t value);
vfloat16m1_t vfslide1down (vfloat16m1_t src, float16_t value);
vfloat16m2_t vfslide1down (vfloat16m2_t src, float16_t value);
vfloat16m4_t vfslide1down (vfloat16m4_t src, float16_t value);
vfloat16m8_t vfslide1down (vfloat16m8_t src, float16_t value);
vfloat32mf2_t vfslide1down (vfloat32mf2_t src, float32_t value);
vfloat32m1_t vfslide1down (vfloat32m1_t src, float32_t value);
vfloat32m2_t vfslide1down (vfloat32m2_t src, float32_t value);
vfloat32m4_t vfslide1down (vfloat32m4_t src, float32_t value);
vfloat32m8_t vfslide1down (vfloat32m8_t src, float32_t value);
vfloat64m1_t vfslide1down (vfloat64m1_t src, float64_t value);
vfloat64m2_t vfslide1down (vfloat64m2_t src, float64_t value);
vfloat64m4_t vfslide1down (vfloat64m4_t src, float64_t value);
vfloat64m8_t vfslide1down (vfloat64m8_t src, float64_t value);
// masked functions
vint8mf8_t vslide1up_m (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value);
vint8mf4_t vslide1up_m (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value);
vint8mf2_t vslide1up_m (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value);
vint8m1_t vslide1up_m (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value);
vint8m2_t vslide1up_m (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value);
vint8m4_t vslide1up_m (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value);
vint8m8_t vslide1up_m (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value);
vint16mf4_t vslide1up_m (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value);
vint16mf2_t vslide1up_m (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value);
vint16m1_t vslide1up_m (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value);
vint16m2_t vslide1up_m (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value);
vint16m4_t vslide1up_m (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value);
vint16m8_t vslide1up_m (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value);
vint32mf2_t vslide1up_m (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value);
vint32m1_t vslide1up_m (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value);
vint32m2_t vslide1up_m (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value);
vint32m4_t vslide1up_m (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value);
vint32m8_t vslide1up_m (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value);
vint64m1_t vslide1up_m (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value);
vint64m2_t vslide1up_m (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value);
vint64m4_t vslide1up_m (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value);
vint64m8_t vslide1up_m (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value);
vuint8mf8_t vslide1up_m (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value);
vuint8mf4_t vslide1up_m (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value);
vuint8mf2_t vslide1up_m (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value);
vuint8m1_t vslide1up_m (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value);
vuint8m2_t vslide1up_m (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value);
vuint8m4_t vslide1up_m (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value);
vuint8m8_t vslide1up_m (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value);
vuint16mf4_t vslide1up_m (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value);
vuint16mf2_t vslide1up_m (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value);
vuint16m1_t vslide1up_m (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value);
vuint16m2_t vslide1up_m (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value);
vuint16m4_t vslide1up_m (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value);
vuint16m8_t vslide1up_m (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value);
vuint32mf2_t vslide1up_m (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value);
vuint32m1_t vslide1up_m (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value);
vuint32m2_t vslide1up_m (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value);
vuint32m4_t vslide1up_m (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value);
vuint32m8_t vslide1up_m (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value);
vuint64m1_t vslide1up_m (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value);
vuint64m2_t vslide1up_m (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value);
vuint64m4_t vslide1up_m (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value);
vuint64m8_t vslide1up_m (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value);
vfloat16mf4_t vfslide1up_m (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value);
vfloat16mf2_t vfslide1up_m (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value);
vfloat16m1_t vfslide1up_m (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value);
vfloat16m2_t vfslide1up_m (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value);
vfloat16m4_t vfslide1up_m (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value);
vfloat16m8_t vfslide1up_m (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value);
vfloat32mf2_t vfslide1up_m (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value);
vfloat32m1_t vfslide1up_m (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value);
vfloat32m2_t vfslide1up_m (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value);
vfloat32m4_t vfslide1up_m (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value);
vfloat32m8_t vfslide1up_m (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value);
vfloat64m1_t vfslide1up_m (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value);
vfloat64m2_t vfslide1up_m (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value);
vfloat64m4_t vfslide1up_m (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value);
vfloat64m8_t vfslide1up_m (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value);
vint8mf8_t vslide1down_m (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t src, int8_t value);
vint8mf4_t vslide1down_m (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t src, int8_t value);
vint8mf2_t vslide1down_m (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t src, int8_t value);
vint8m1_t vslide1down_m (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t src, int8_t value);
vint8m2_t vslide1down_m (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t src, int8_t value);
vint8m4_t vslide1down_m (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t src, int8_t value);
vint8m8_t vslide1down_m (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t src, int8_t value);
vint16mf4_t vslide1down_m (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t src, int16_t value);
vint16mf2_t vslide1down_m (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t src, int16_t value);
vint16m1_t vslide1down_m (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t src, int16_t value);
vint16m2_t vslide1down_m (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t src, int16_t value);
vint16m4_t vslide1down_m (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t src, int16_t value);
vint16m8_t vslide1down_m (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t src, int16_t value);
vint32mf2_t vslide1down_m (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t src, int32_t value);
vint32m1_t vslide1down_m (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t src, int32_t value);
vint32m2_t vslide1down_m (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t src, int32_t value);
vint32m4_t vslide1down_m (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t src, int32_t value);
vint32m8_t vslide1down_m (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t src, int32_t value);
vint64m1_t vslide1down_m (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t src, int64_t value);
vint64m2_t vslide1down_m (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t src, int64_t value);
vint64m4_t vslide1down_m (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t src, int64_t value);
vint64m8_t vslide1down_m (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t src, int64_t value);
vuint8mf8_t vslide1down_m (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t src, uint8_t value);
vuint8mf4_t vslide1down_m (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t src, uint8_t value);
vuint8mf2_t vslide1down_m (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t src, uint8_t value);
vuint8m1_t vslide1down_m (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t src, uint8_t value);
vuint8m2_t vslide1down_m (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t src, uint8_t value);
vuint8m4_t vslide1down_m (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t src, uint8_t value);
vuint8m8_t vslide1down_m (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t src, uint8_t value);
vuint16mf4_t vslide1down_m (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t src, uint16_t value);
vuint16mf2_t vslide1down_m (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t src, uint16_t value);
vuint16m1_t vslide1down_m (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t src, uint16_t value);
vuint16m2_t vslide1down_m (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t src, uint16_t value);
vuint16m4_t vslide1down_m (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t src, uint16_t value);
vuint16m8_t vslide1down_m (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t src, uint16_t value);
vuint32mf2_t vslide1down_m (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t src, uint32_t value);
vuint32m1_t vslide1down_m (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t src, uint32_t value);
vuint32m2_t vslide1down_m (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t src, uint32_t value);
vuint32m4_t vslide1down_m (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t src, uint32_t value);
vuint32m8_t vslide1down_m (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t src, uint32_t value);
vuint64m1_t vslide1down_m (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t src, uint64_t value);
vuint64m2_t vslide1down_m (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t src, uint64_t value);
vuint64m4_t vslide1down_m (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t src, uint64_t value);
vuint64m8_t vslide1down_m (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t src, uint64_t value);
vfloat16mf4_t vfslide1down_m (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t src, float16_t value);
vfloat16mf2_t vfslide1down_m (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t src, float16_t value);
vfloat16m1_t vfslide1down_m (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t src, float16_t value);
vfloat16m2_t vfslide1down_m (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t src, float16_t value);
vfloat16m4_t vfslide1down_m (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t src, float16_t value);
vfloat16m8_t vfslide1down_m (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t src, float16_t value);
vfloat32mf2_t vfslide1down_m (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t src, float32_t value);
vfloat32m1_t vfslide1down_m (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t src, float32_t value);
vfloat32m2_t vfslide1down_m (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t src, float32_t value);
vfloat32m4_t vfslide1down_m (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t src, float32_t value);
vfloat32m8_t vfslide1down_m (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t src, float32_t value);
vfloat64m1_t vfslide1down_m (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t src, float64_t value);
vfloat64m2_t vfslide1down_m (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t src, float64_t value);
vfloat64m4_t vfslide1down_m (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t src, float64_t value);
vfloat64m8_t vfslide1down_m (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t src, float64_t value);
```
### [Vector Register Gather Functions](../rvv-intrinsic-api.md#174-vector-register-gather-operations):

**Prototypes:**
``` C
vint8mf8_t vrgather (vint8mf8_t op1, vuint8mf8_t op2);
vint8mf8_t vrgather (vint8mf8_t op1, size_t op2);
vint8mf4_t vrgather (vint8mf4_t op1, vuint8mf4_t op2);
vint8mf4_t vrgather (vint8mf4_t op1, size_t op2);
vint8mf2_t vrgather (vint8mf2_t op1, vuint8mf2_t op2);
vint8mf2_t vrgather (vint8mf2_t op1, size_t op2);
vint8m1_t vrgather (vint8m1_t op1, vuint8m1_t op2);
vint8m1_t vrgather (vint8m1_t op1, size_t op2);
vint8m2_t vrgather (vint8m2_t op1, vuint8m2_t op2);
vint8m2_t vrgather (vint8m2_t op1, size_t op2);
vint8m4_t vrgather (vint8m4_t op1, vuint8m4_t op2);
vint8m4_t vrgather (vint8m4_t op1, size_t op2);
vint8m8_t vrgather (vint8m8_t op1, vuint8m8_t op2);
vint8m8_t vrgather (vint8m8_t op1, size_t op2);
vint16mf4_t vrgather (vint16mf4_t op1, vuint16mf4_t op2);
vint16mf4_t vrgather (vint16mf4_t op1, size_t op2);
vint16mf2_t vrgather (vint16mf2_t op1, vuint16mf2_t op2);
vint16mf2_t vrgather (vint16mf2_t op1, size_t op2);
vint16m1_t vrgather (vint16m1_t op1, vuint16m1_t op2);
vint16m1_t vrgather (vint16m1_t op1, size_t op2);
vint16m2_t vrgather (vint16m2_t op1, vuint16m2_t op2);
vint16m2_t vrgather (vint16m2_t op1, size_t op2);
vint16m4_t vrgather (vint16m4_t op1, vuint16m4_t op2);
vint16m4_t vrgather (vint16m4_t op1, size_t op2);
vint16m8_t vrgather (vint16m8_t op1, vuint16m8_t op2);
vint16m8_t vrgather (vint16m8_t op1, size_t op2);
vint32mf2_t vrgather (vint32mf2_t op1, vuint32mf2_t op2);
vint32mf2_t vrgather (vint32mf2_t op1, size_t op2);
vint32m1_t vrgather (vint32m1_t op1, vuint32m1_t op2);
vint32m1_t vrgather (vint32m1_t op1, size_t op2);
vint32m2_t vrgather (vint32m2_t op1, vuint32m2_t op2);
vint32m2_t vrgather (vint32m2_t op1, size_t op2);
vint32m4_t vrgather (vint32m4_t op1, vuint32m4_t op2);
vint32m4_t vrgather (vint32m4_t op1, size_t op2);
vint32m8_t vrgather (vint32m8_t op1, vuint32m8_t op2);
vint32m8_t vrgather (vint32m8_t op1, size_t op2);
vint64m1_t vrgather (vint64m1_t op1, vuint64m1_t op2);
vint64m1_t vrgather (vint64m1_t op1, size_t op2);
vint64m2_t vrgather (vint64m2_t op1, vuint64m2_t op2);
vint64m2_t vrgather (vint64m2_t op1, size_t op2);
vint64m4_t vrgather (vint64m4_t op1, vuint64m4_t op2);
vint64m4_t vrgather (vint64m4_t op1, size_t op2);
vint64m8_t vrgather (vint64m8_t op1, vuint64m8_t op2);
vint64m8_t vrgather (vint64m8_t op1, size_t op2);
vuint8mf8_t vrgather (vuint8mf8_t op1, vuint8mf8_t op2);
vuint8mf8_t vrgather (vuint8mf8_t op1, size_t op2);
vuint8mf4_t vrgather (vuint8mf4_t op1, vuint8mf4_t op2);
vuint8mf4_t vrgather (vuint8mf4_t op1, size_t op2);
vuint8mf2_t vrgather (vuint8mf2_t op1, vuint8mf2_t op2);
vuint8mf2_t vrgather (vuint8mf2_t op1, size_t op2);
vuint8m1_t vrgather (vuint8m1_t op1, vuint8m1_t op2);
vuint8m1_t vrgather (vuint8m1_t op1, size_t op2);
vuint8m2_t vrgather (vuint8m2_t op1, vuint8m2_t op2);
vuint8m2_t vrgather (vuint8m2_t op1, size_t op2);
vuint8m4_t vrgather (vuint8m4_t op1, vuint8m4_t op2);
vuint8m4_t vrgather (vuint8m4_t op1, size_t op2);
vuint8m8_t vrgather (vuint8m8_t op1, vuint8m8_t op2);
vuint8m8_t vrgather (vuint8m8_t op1, size_t op2);
vuint16mf4_t vrgather (vuint16mf4_t op1, vuint16mf4_t op2);
vuint16mf4_t vrgather (vuint16mf4_t op1, size_t op2);
vuint16mf2_t vrgather (vuint16mf2_t op1, vuint16mf2_t op2);
vuint16mf2_t vrgather (vuint16mf2_t op1, size_t op2);
vuint16m1_t vrgather (vuint16m1_t op1, vuint16m1_t op2);
vuint16m1_t vrgather (vuint16m1_t op1, size_t op2);
vuint16m2_t vrgather (vuint16m2_t op1, vuint16m2_t op2);
vuint16m2_t vrgather (vuint16m2_t op1, size_t op2);
vuint16m4_t vrgather (vuint16m4_t op1, vuint16m4_t op2);
vuint16m4_t vrgather (vuint16m4_t op1, size_t op2);
vuint16m8_t vrgather (vuint16m8_t op1, vuint16m8_t op2);
vuint16m8_t vrgather (vuint16m8_t op1, size_t op2);
vuint32mf2_t vrgather (vuint32mf2_t op1, vuint32mf2_t op2);
vuint32mf2_t vrgather (vuint32mf2_t op1, size_t op2);
vuint32m1_t vrgather (vuint32m1_t op1, vuint32m1_t op2);
vuint32m1_t vrgather (vuint32m1_t op1, size_t op2);
vuint32m2_t vrgather (vuint32m2_t op1, vuint32m2_t op2);
vuint32m2_t vrgather (vuint32m2_t op1, size_t op2);
vuint32m4_t vrgather (vuint32m4_t op1, vuint32m4_t op2);
vuint32m4_t vrgather (vuint32m4_t op1, size_t op2);
vuint32m8_t vrgather (vuint32m8_t op1, vuint32m8_t op2);
vuint32m8_t vrgather (vuint32m8_t op1, size_t op2);
vuint64m1_t vrgather (vuint64m1_t op1, vuint64m1_t op2);
vuint64m1_t vrgather (vuint64m1_t op1, size_t op2);
vuint64m2_t vrgather (vuint64m2_t op1, vuint64m2_t op2);
vuint64m2_t vrgather (vuint64m2_t op1, size_t op2);
vuint64m4_t vrgather (vuint64m4_t op1, vuint64m4_t op2);
vuint64m4_t vrgather (vuint64m4_t op1, size_t op2);
vuint64m8_t vrgather (vuint64m8_t op1, vuint64m8_t op2);
vuint64m8_t vrgather (vuint64m8_t op1, size_t op2);
vfloat16mf4_t vrgather (vfloat16mf4_t op1, vuint16mf4_t op2);
vfloat16mf4_t vrgather (vfloat16mf4_t op1, size_t op2);
vfloat16mf2_t vrgather (vfloat16mf2_t op1, vuint16mf2_t op2);
vfloat16mf2_t vrgather (vfloat16mf2_t op1, size_t op2);
vfloat16m1_t vrgather (vfloat16m1_t op1, vuint16m1_t op2);
vfloat16m1_t vrgather (vfloat16m1_t op1, size_t op2);
vfloat16m2_t vrgather (vfloat16m2_t op1, vuint16m2_t op2);
vfloat16m2_t vrgather (vfloat16m2_t op1, size_t op2);
vfloat16m4_t vrgather (vfloat16m4_t op1, vuint16m4_t op2);
vfloat16m4_t vrgather (vfloat16m4_t op1, size_t op2);
vfloat16m8_t vrgather (vfloat16m8_t op1, vuint16m8_t op2);
vfloat16m8_t vrgather (vfloat16m8_t op1, size_t op2);
vfloat32mf2_t vrgather (vfloat32mf2_t op1, vuint32mf2_t op2);
vfloat32mf2_t vrgather (vfloat32mf2_t op1, size_t op2);
vfloat32m1_t vrgather (vfloat32m1_t op1, vuint32m1_t op2);
vfloat32m1_t vrgather (vfloat32m1_t op1, size_t op2);
vfloat32m2_t vrgather (vfloat32m2_t op1, vuint32m2_t op2);
vfloat32m2_t vrgather (vfloat32m2_t op1, size_t op2);
vfloat32m4_t vrgather (vfloat32m4_t op1, vuint32m4_t op2);
vfloat32m4_t vrgather (vfloat32m4_t op1, size_t op2);
vfloat32m8_t vrgather (vfloat32m8_t op1, vuint32m8_t op2);
vfloat32m8_t vrgather (vfloat32m8_t op1, size_t op2);
vfloat64m1_t vrgather (vfloat64m1_t op1, vuint64m1_t op2);
vfloat64m1_t vrgather (vfloat64m1_t op1, size_t op2);
vfloat64m2_t vrgather (vfloat64m2_t op1, vuint64m2_t op2);
vfloat64m2_t vrgather (vfloat64m2_t op1, size_t op2);
vfloat64m4_t vrgather (vfloat64m4_t op1, vuint64m4_t op2);
vfloat64m4_t vrgather (vfloat64m4_t op1, size_t op2);
vfloat64m8_t vrgather (vfloat64m8_t op1, vuint64m8_t op2);
vfloat64m8_t vrgather (vfloat64m8_t op1, size_t op2);
vint8mf8_t vrgatherei16 (vint8mf8_t op1, vuint16mf4_t op2);
vint8mf4_t vrgatherei16 (vint8mf4_t op1, vuint16mf2_t op2);
vint8mf2_t vrgatherei16 (vint8mf2_t op1, vuint16m1_t op2);
vint8m1_t vrgatherei16 (vint8m1_t op1, vuint16m2_t op2);
vint8m2_t vrgatherei16 (vint8m2_t op1, vuint16m4_t op2);
vint8m4_t vrgatherei16 (vint8m4_t op1, vuint16m8_t op2);
vint16mf4_t vrgatherei16 (vint16mf4_t op1, vuint16mf4_t op2);
vint16mf2_t vrgatherei16 (vint16mf2_t op1, vuint16mf2_t op2);
vint16m1_t vrgatherei16 (vint16m1_t op1, vuint16m1_t op2);
vint16m2_t vrgatherei16 (vint16m2_t op1, vuint16m2_t op2);
vint16m4_t vrgatherei16 (vint16m4_t op1, vuint16m4_t op2);
vint16m8_t vrgatherei16 (vint16m8_t op1, vuint16m8_t op2);
vint32mf2_t vrgatherei16 (vint32mf2_t op1, vuint16mf4_t op2);
vint32m1_t vrgatherei16 (vint32m1_t op1, vuint16mf2_t op2);
vint32m2_t vrgatherei16 (vint32m2_t op1, vuint16m1_t op2);
vint32m4_t vrgatherei16 (vint32m4_t op1, vuint16m2_t op2);
vint32m8_t vrgatherei16 (vint32m8_t op1, vuint16m4_t op2);
vint64m1_t vrgatherei16 (vint64m1_t op1, vuint16mf4_t op2);
vint64m2_t vrgatherei16 (vint64m2_t op1, vuint16mf2_t op2);
vint64m4_t vrgatherei16 (vint64m4_t op1, vuint16m1_t op2);
vint64m8_t vrgatherei16 (vint64m8_t op1, vuint16m2_t op2);
vuint8mf8_t vrgatherei16 (vuint8mf8_t op1, vuint16mf4_t op2);
vuint8mf4_t vrgatherei16 (vuint8mf4_t op1, vuint16mf2_t op2);
vuint8mf2_t vrgatherei16 (vuint8mf2_t op1, vuint16m1_t op2);
vuint8m1_t vrgatherei16 (vuint8m1_t op1, vuint16m2_t op2);
vuint8m2_t vrgatherei16 (vuint8m2_t op1, vuint16m4_t op2);
vuint8m4_t vrgatherei16 (vuint8m4_t op1, vuint16m8_t op2);
vuint16mf4_t vrgatherei16 (vuint16mf4_t op1, vuint16mf4_t op2);
vuint16mf2_t vrgatherei16 (vuint16mf2_t op1, vuint16mf2_t op2);
vuint16m1_t vrgatherei16 (vuint16m1_t op1, vuint16m1_t op2);
vuint16m2_t vrgatherei16 (vuint16m2_t op1, vuint16m2_t op2);
vuint16m4_t vrgatherei16 (vuint16m4_t op1, vuint16m4_t op2);
vuint16m8_t vrgatherei16 (vuint16m8_t op1, vuint16m8_t op2);
vuint32mf2_t vrgatherei16 (vuint32mf2_t op1, vuint16mf4_t op2);
vuint32m1_t vrgatherei16 (vuint32m1_t op1, vuint16mf2_t op2);
vuint32m2_t vrgatherei16 (vuint32m2_t op1, vuint16m1_t op2);
vuint32m4_t vrgatherei16 (vuint32m4_t op1, vuint16m2_t op2);
vuint32m8_t vrgatherei16 (vuint32m8_t op1, vuint16m4_t op2);
vuint64m1_t vrgatherei16 (vuint64m1_t op1, vuint16mf4_t op2);
vuint64m2_t vrgatherei16 (vuint64m2_t op1, vuint16mf2_t op2);
vuint64m4_t vrgatherei16 (vuint64m4_t op1, vuint16m1_t op2);
vuint64m8_t vrgatherei16 (vuint64m8_t op1, vuint16m2_t op2);
vfloat16mf4_t vrgatherei16 (vfloat16mf4_t op1, vuint16mf4_t op2);
vfloat16mf2_t vrgatherei16 (vfloat16mf2_t op1, vuint16mf2_t op2);
vfloat16m1_t vrgatherei16 (vfloat16m1_t op1, vuint16m1_t op2);
vfloat16m2_t vrgatherei16 (vfloat16m2_t op1, vuint16m2_t op2);
vfloat16m4_t vrgatherei16 (vfloat16m4_t op1, vuint16m4_t op2);
vfloat16m8_t vrgatherei16 (vfloat16m8_t op1, vuint16m8_t op2);
vfloat32mf2_t vrgatherei16 (vfloat32mf2_t op1, vuint16mf4_t op2);
vfloat32m1_t vrgatherei16 (vfloat32m1_t op1, vuint16mf2_t op2);
vfloat32m2_t vrgatherei16 (vfloat32m2_t op1, vuint16m1_t op2);
vfloat32m4_t vrgatherei16 (vfloat32m4_t op1, vuint16m2_t op2);
vfloat32m8_t vrgatherei16 (vfloat32m8_t op1, vuint16m4_t op2);
vfloat64m1_t vrgatherei16 (vfloat64m1_t op1, vuint16mf4_t op2);
vfloat64m2_t vrgatherei16 (vfloat64m2_t op1, vuint16mf2_t op2);
vfloat64m4_t vrgatherei16 (vfloat64m4_t op1, vuint16m1_t op2);
vfloat64m8_t vrgatherei16 (vfloat64m8_t op1, vuint16m2_t op2);
// masked functions
vint8mf8_t vrgather_m (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint8mf8_t op2);
vint8mf8_t vrgather_m (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, size_t op2);
vint8mf4_t vrgather_m (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint8mf4_t op2);
vint8mf4_t vrgather_m (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, size_t op2);
vint8mf2_t vrgather_m (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint8mf2_t op2);
vint8mf2_t vrgather_m (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, size_t op2);
vint8m1_t vrgather_m (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint8m1_t op2);
vint8m1_t vrgather_m (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, size_t op2);
vint8m2_t vrgather_m (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint8m2_t op2);
vint8m2_t vrgather_m (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, size_t op2);
vint8m4_t vrgather_m (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint8m4_t op2);
vint8m4_t vrgather_m (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, size_t op2);
vint8m8_t vrgather_m (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, vuint8m8_t op2);
vint8m8_t vrgather_m (vbool1_t mask, vint8m8_t maskedoff, vint8m8_t op1, size_t op2);
vint16mf4_t vrgather_m (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t op2);
vint16mf4_t vrgather_m (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, size_t op2);
vint16mf2_t vrgather_m (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t op2);
vint16mf2_t vrgather_m (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, size_t op2);
vint16m1_t vrgather_m (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2);
vint16m1_t vrgather_m (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, size_t op2);
vint16m2_t vrgather_m (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2);
vint16m2_t vrgather_m (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, size_t op2);
vint16m4_t vrgather_m (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2);
vint16m4_t vrgather_m (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, size_t op2);
vint16m8_t vrgather_m (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2);
vint16m8_t vrgather_m (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, size_t op2);
vint32mf2_t vrgather_m (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint32mf2_t op2);
vint32mf2_t vrgather_m (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, size_t op2);
vint32m1_t vrgather_m (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint32m1_t op2);
vint32m1_t vrgather_m (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, size_t op2);
vint32m2_t vrgather_m (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint32m2_t op2);
vint32m2_t vrgather_m (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, size_t op2);
vint32m4_t vrgather_m (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint32m4_t op2);
vint32m4_t vrgather_m (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, size_t op2);
vint32m8_t vrgather_m (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint32m8_t op2);
vint32m8_t vrgather_m (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, size_t op2);
vint64m1_t vrgather_m (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint64m1_t op2);
vint64m1_t vrgather_m (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, size_t op2);
vint64m2_t vrgather_m (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint64m2_t op2);
vint64m2_t vrgather_m (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, size_t op2);
vint64m4_t vrgather_m (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint64m4_t op2);
vint64m4_t vrgather_m (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, size_t op2);
vint64m8_t vrgather_m (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint64m8_t op2);
vint64m8_t vrgather_m (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, size_t op2);
vuint8mf8_t vrgather_m (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint8mf8_t op2);
vuint8mf8_t vrgather_m (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, size_t op2);
vuint8mf4_t vrgather_m (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint8mf4_t op2);
vuint8mf4_t vrgather_m (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, size_t op2);
vuint8mf2_t vrgather_m (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint8mf2_t op2);
vuint8mf2_t vrgather_m (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, size_t op2);
vuint8m1_t vrgather_m (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint8m1_t op2);
vuint8m1_t vrgather_m (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, size_t op2);
vuint8m2_t vrgather_m (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint8m2_t op2);
vuint8m2_t vrgather_m (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, size_t op2);
vuint8m4_t vrgather_m (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint8m4_t op2);
vuint8m4_t vrgather_m (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, size_t op2);
vuint8m8_t vrgather_m (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, vuint8m8_t op2);
vuint8m8_t vrgather_m (vbool1_t mask, vuint8m8_t maskedoff, vuint8m8_t op1, size_t op2);
vuint16mf4_t vrgather_m (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t op2);
vuint16mf4_t vrgather_m (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, size_t op2);
vuint16mf2_t vrgather_m (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t op2);
vuint16mf2_t vrgather_m (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, size_t op2);
vuint16m1_t vrgather_m (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2);
vuint16m1_t vrgather_m (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, size_t op2);
vuint16m2_t vrgather_m (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2);
vuint16m2_t vrgather_m (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, size_t op2);
vuint16m4_t vrgather_m (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2);
vuint16m4_t vrgather_m (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, size_t op2);
vuint16m8_t vrgather_m (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2);
vuint16m8_t vrgather_m (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, size_t op2);
vuint32mf2_t vrgather_m (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint32mf2_t op2);
vuint32mf2_t vrgather_m (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, size_t op2);
vuint32m1_t vrgather_m (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint32m1_t op2);
vuint32m1_t vrgather_m (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, size_t op2);
vuint32m2_t vrgather_m (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint32m2_t op2);
vuint32m2_t vrgather_m (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, size_t op2);
vuint32m4_t vrgather_m (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint32m4_t op2);
vuint32m4_t vrgather_m (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, size_t op2);
vuint32m8_t vrgather_m (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint32m8_t op2);
vuint32m8_t vrgather_m (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, size_t op2);
vuint64m1_t vrgather_m (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint64m1_t op2);
vuint64m1_t vrgather_m (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, size_t op2);
vuint64m2_t vrgather_m (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint64m2_t op2);
vuint64m2_t vrgather_m (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, size_t op2);
vuint64m4_t vrgather_m (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint64m4_t op2);
vuint64m4_t vrgather_m (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, size_t op2);
vuint64m8_t vrgather_m (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint64m8_t op2);
vuint64m8_t vrgather_m (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, size_t op2);
vfloat16mf4_t vrgather_m (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t op2);
vfloat16mf4_t vrgather_m (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, size_t op2);
vfloat16mf2_t vrgather_m (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t op2);
vfloat16mf2_t vrgather_m (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, size_t op2);
vfloat16m1_t vrgather_m (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t op2);
vfloat16m1_t vrgather_m (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, size_t op2);
vfloat16m2_t vrgather_m (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t op2);
vfloat16m2_t vrgather_m (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, size_t op2);
vfloat16m4_t vrgather_m (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t op2);
vfloat16m4_t vrgather_m (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, size_t op2);
vfloat16m8_t vrgather_m (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t op2);
vfloat16m8_t vrgather_m (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, size_t op2);
vfloat32mf2_t vrgather_m (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint32mf2_t op2);
vfloat32mf2_t vrgather_m (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, size_t op2);
vfloat32m1_t vrgather_m (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint32m1_t op2);
vfloat32m1_t vrgather_m (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, size_t op2);
vfloat32m2_t vrgather_m (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint32m2_t op2);
vfloat32m2_t vrgather_m (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, size_t op2);
vfloat32m4_t vrgather_m (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint32m4_t op2);
vfloat32m4_t vrgather_m (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, size_t op2);
vfloat32m8_t vrgather_m (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint32m8_t op2);
vfloat32m8_t vrgather_m (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, size_t op2);
vfloat64m1_t vrgather_m (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint64m1_t op2);
vfloat64m1_t vrgather_m (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, size_t op2);
vfloat64m2_t vrgather_m (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint64m2_t op2);
vfloat64m2_t vrgather_m (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, size_t op2);
vfloat64m4_t vrgather_m (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint64m4_t op2);
vfloat64m4_t vrgather_m (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, size_t op2);
vfloat64m8_t vrgather_m (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint64m8_t op2);
vfloat64m8_t vrgather_m (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, size_t op2);
vint8mf8_t vrgatherei16_m (vbool64_t mask, vint8mf8_t maskedoff, vint8mf8_t op1, vuint16mf4_t op2);
vint8mf4_t vrgatherei16_m (vbool32_t mask, vint8mf4_t maskedoff, vint8mf4_t op1, vuint16mf2_t op2);
vint8mf2_t vrgatherei16_m (vbool16_t mask, vint8mf2_t maskedoff, vint8mf2_t op1, vuint16m1_t op2);
vint8m1_t vrgatherei16_m (vbool8_t mask, vint8m1_t maskedoff, vint8m1_t op1, vuint16m2_t op2);
vint8m2_t vrgatherei16_m (vbool4_t mask, vint8m2_t maskedoff, vint8m2_t op1, vuint16m4_t op2);
vint8m4_t vrgatherei16_m (vbool2_t mask, vint8m4_t maskedoff, vint8m4_t op1, vuint16m8_t op2);
vint16mf4_t vrgatherei16_m (vbool64_t mask, vint16mf4_t maskedoff, vint16mf4_t op1, vuint16mf4_t op2);
vint16mf2_t vrgatherei16_m (vbool32_t mask, vint16mf2_t maskedoff, vint16mf2_t op1, vuint16mf2_t op2);
vint16m1_t vrgatherei16_m (vbool16_t mask, vint16m1_t maskedoff, vint16m1_t op1, vuint16m1_t op2);
vint16m2_t vrgatherei16_m (vbool8_t mask, vint16m2_t maskedoff, vint16m2_t op1, vuint16m2_t op2);
vint16m4_t vrgatherei16_m (vbool4_t mask, vint16m4_t maskedoff, vint16m4_t op1, vuint16m4_t op2);
vint16m8_t vrgatherei16_m (vbool2_t mask, vint16m8_t maskedoff, vint16m8_t op1, vuint16m8_t op2);
vint32mf2_t vrgatherei16_m (vbool64_t mask, vint32mf2_t maskedoff, vint32mf2_t op1, vuint16mf4_t op2);
vint32m1_t vrgatherei16_m (vbool32_t mask, vint32m1_t maskedoff, vint32m1_t op1, vuint16mf2_t op2);
vint32m2_t vrgatherei16_m (vbool16_t mask, vint32m2_t maskedoff, vint32m2_t op1, vuint16m1_t op2);
vint32m4_t vrgatherei16_m (vbool8_t mask, vint32m4_t maskedoff, vint32m4_t op1, vuint16m2_t op2);
vint32m8_t vrgatherei16_m (vbool4_t mask, vint32m8_t maskedoff, vint32m8_t op1, vuint16m4_t op2);
vint64m1_t vrgatherei16_m (vbool64_t mask, vint64m1_t maskedoff, vint64m1_t op1, vuint16mf4_t op2);
vint64m2_t vrgatherei16_m (vbool32_t mask, vint64m2_t maskedoff, vint64m2_t op1, vuint16mf2_t op2);
vint64m4_t vrgatherei16_m (vbool16_t mask, vint64m4_t maskedoff, vint64m4_t op1, vuint16m1_t op2);
vint64m8_t vrgatherei16_m (vbool8_t mask, vint64m8_t maskedoff, vint64m8_t op1, vuint16m2_t op2);
vuint8mf8_t vrgatherei16_m (vbool64_t mask, vuint8mf8_t maskedoff, vuint8mf8_t op1, vuint16mf4_t op2);
vuint8mf4_t vrgatherei16_m (vbool32_t mask, vuint8mf4_t maskedoff, vuint8mf4_t op1, vuint16mf2_t op2);
vuint8mf2_t vrgatherei16_m (vbool16_t mask, vuint8mf2_t maskedoff, vuint8mf2_t op1, vuint16m1_t op2);
vuint8m1_t vrgatherei16_m (vbool8_t mask, vuint8m1_t maskedoff, vuint8m1_t op1, vuint16m2_t op2);
vuint8m2_t vrgatherei16_m (vbool4_t mask, vuint8m2_t maskedoff, vuint8m2_t op1, vuint16m4_t op2);
vuint8m4_t vrgatherei16_m (vbool2_t mask, vuint8m4_t maskedoff, vuint8m4_t op1, vuint16m8_t op2);
vuint16mf4_t vrgatherei16_m (vbool64_t mask, vuint16mf4_t maskedoff, vuint16mf4_t op1, vuint16mf4_t op2);
vuint16mf2_t vrgatherei16_m (vbool32_t mask, vuint16mf2_t maskedoff, vuint16mf2_t op1, vuint16mf2_t op2);
vuint16m1_t vrgatherei16_m (vbool16_t mask, vuint16m1_t maskedoff, vuint16m1_t op1, vuint16m1_t op2);
vuint16m2_t vrgatherei16_m (vbool8_t mask, vuint16m2_t maskedoff, vuint16m2_t op1, vuint16m2_t op2);
vuint16m4_t vrgatherei16_m (vbool4_t mask, vuint16m4_t maskedoff, vuint16m4_t op1, vuint16m4_t op2);
vuint16m8_t vrgatherei16_m (vbool2_t mask, vuint16m8_t maskedoff, vuint16m8_t op1, vuint16m8_t op2);
vuint32mf2_t vrgatherei16_m (vbool64_t mask, vuint32mf2_t maskedoff, vuint32mf2_t op1, vuint16mf4_t op2);
vuint32m1_t vrgatherei16_m (vbool32_t mask, vuint32m1_t maskedoff, vuint32m1_t op1, vuint16mf2_t op2);
vuint32m2_t vrgatherei16_m (vbool16_t mask, vuint32m2_t maskedoff, vuint32m2_t op1, vuint16m1_t op2);
vuint32m4_t vrgatherei16_m (vbool8_t mask, vuint32m4_t maskedoff, vuint32m4_t op1, vuint16m2_t op2);
vuint32m8_t vrgatherei16_m (vbool4_t mask, vuint32m8_t maskedoff, vuint32m8_t op1, vuint16m4_t op2);
vuint64m1_t vrgatherei16_m (vbool64_t mask, vuint64m1_t maskedoff, vuint64m1_t op1, vuint16mf4_t op2);
vuint64m2_t vrgatherei16_m (vbool32_t mask, vuint64m2_t maskedoff, vuint64m2_t op1, vuint16mf2_t op2);
vuint64m4_t vrgatherei16_m (vbool16_t mask, vuint64m4_t maskedoff, vuint64m4_t op1, vuint16m1_t op2);
vuint64m8_t vrgatherei16_m (vbool8_t mask, vuint64m8_t maskedoff, vuint64m8_t op1, vuint16m2_t op2);
vfloat16mf4_t vrgatherei16_m (vbool64_t mask, vfloat16mf4_t maskedoff, vfloat16mf4_t op1, vuint16mf4_t op2);
vfloat16mf2_t vrgatherei16_m (vbool32_t mask, vfloat16mf2_t maskedoff, vfloat16mf2_t op1, vuint16mf2_t op2);
vfloat16m1_t vrgatherei16_m (vbool16_t mask, vfloat16m1_t maskedoff, vfloat16m1_t op1, vuint16m1_t op2);
vfloat16m2_t vrgatherei16_m (vbool8_t mask, vfloat16m2_t maskedoff, vfloat16m2_t op1, vuint16m2_t op2);
vfloat16m4_t vrgatherei16_m (vbool4_t mask, vfloat16m4_t maskedoff, vfloat16m4_t op1, vuint16m4_t op2);
vfloat16m8_t vrgatherei16_m (vbool2_t mask, vfloat16m8_t maskedoff, vfloat16m8_t op1, vuint16m8_t op2);
vfloat32mf2_t vrgatherei16_m (vbool64_t mask, vfloat32mf2_t maskedoff, vfloat32mf2_t op1, vuint16mf4_t op2);
vfloat32m1_t vrgatherei16_m (vbool32_t mask, vfloat32m1_t maskedoff, vfloat32m1_t op1, vuint16mf2_t op2);
vfloat32m2_t vrgatherei16_m (vbool16_t mask, vfloat32m2_t maskedoff, vfloat32m2_t op1, vuint16m1_t op2);
vfloat32m4_t vrgatherei16_m (vbool8_t mask, vfloat32m4_t maskedoff, vfloat32m4_t op1, vuint16m2_t op2);
vfloat32m8_t vrgatherei16_m (vbool4_t mask, vfloat32m8_t maskedoff, vfloat32m8_t op1, vuint16m4_t op2);
vfloat64m1_t vrgatherei16_m (vbool64_t mask, vfloat64m1_t maskedoff, vfloat64m1_t op1, vuint16mf4_t op2);
vfloat64m2_t vrgatherei16_m (vbool32_t mask, vfloat64m2_t maskedoff, vfloat64m2_t op1, vuint16mf2_t op2);
vfloat64m4_t vrgatherei16_m (vbool16_t mask, vfloat64m4_t maskedoff, vfloat64m4_t op1, vuint16m1_t op2);
vfloat64m8_t vrgatherei16_m (vbool8_t mask, vfloat64m8_t maskedoff, vfloat64m8_t op1, vuint16m2_t op2);
```
### [Vector Compress Functions](../rvv-intrinsic-api.md#175-vector-compress-operations):

**Prototypes:**
``` C
vint8mf8_t vcompress (vbool64_t mask, vint8mf8_t dest, vint8mf8_t src);
vint8mf4_t vcompress (vbool32_t mask, vint8mf4_t dest, vint8mf4_t src);
vint8mf2_t vcompress (vbool16_t mask, vint8mf2_t dest, vint8mf2_t src);
vint8m1_t vcompress (vbool8_t mask, vint8m1_t dest, vint8m1_t src);
vint8m2_t vcompress (vbool4_t mask, vint8m2_t dest, vint8m2_t src);
vint8m4_t vcompress (vbool2_t mask, vint8m4_t dest, vint8m4_t src);
vint8m8_t vcompress (vbool1_t mask, vint8m8_t dest, vint8m8_t src);
vint16mf4_t vcompress (vbool64_t mask, vint16mf4_t dest, vint16mf4_t src);
vint16mf2_t vcompress (vbool32_t mask, vint16mf2_t dest, vint16mf2_t src);
vint16m1_t vcompress (vbool16_t mask, vint16m1_t dest, vint16m1_t src);
vint16m2_t vcompress (vbool8_t mask, vint16m2_t dest, vint16m2_t src);
vint16m4_t vcompress (vbool4_t mask, vint16m4_t dest, vint16m4_t src);
vint16m8_t vcompress (vbool2_t mask, vint16m8_t dest, vint16m8_t src);
vint32mf2_t vcompress (vbool64_t mask, vint32mf2_t dest, vint32mf2_t src);
vint32m1_t vcompress (vbool32_t mask, vint32m1_t dest, vint32m1_t src);
vint32m2_t vcompress (vbool16_t mask, vint32m2_t dest, vint32m2_t src);
vint32m4_t vcompress (vbool8_t mask, vint32m4_t dest, vint32m4_t src);
vint32m8_t vcompress (vbool4_t mask, vint32m8_t dest, vint32m8_t src);
vint64m1_t vcompress (vbool64_t mask, vint64m1_t dest, vint64m1_t src);
vint64m2_t vcompress (vbool32_t mask, vint64m2_t dest, vint64m2_t src);
vint64m4_t vcompress (vbool16_t mask, vint64m4_t dest, vint64m4_t src);
vint64m8_t vcompress (vbool8_t mask, vint64m8_t dest, vint64m8_t src);
vuint8mf8_t vcompress (vbool64_t mask, vuint8mf8_t dest, vuint8mf8_t src);
vuint8mf4_t vcompress (vbool32_t mask, vuint8mf4_t dest, vuint8mf4_t src);
vuint8mf2_t vcompress (vbool16_t mask, vuint8mf2_t dest, vuint8mf2_t src);
vuint8m1_t vcompress (vbool8_t mask, vuint8m1_t dest, vuint8m1_t src);
vuint8m2_t vcompress (vbool4_t mask, vuint8m2_t dest, vuint8m2_t src);
vuint8m4_t vcompress (vbool2_t mask, vuint8m4_t dest, vuint8m4_t src);
vuint8m8_t vcompress (vbool1_t mask, vuint8m8_t dest, vuint8m8_t src);
vuint16mf4_t vcompress (vbool64_t mask, vuint16mf4_t dest, vuint16mf4_t src);
vuint16mf2_t vcompress (vbool32_t mask, vuint16mf2_t dest, vuint16mf2_t src);
vuint16m1_t vcompress (vbool16_t mask, vuint16m1_t dest, vuint16m1_t src);
vuint16m2_t vcompress (vbool8_t mask, vuint16m2_t dest, vuint16m2_t src);
vuint16m4_t vcompress (vbool4_t mask, vuint16m4_t dest, vuint16m4_t src);
vuint16m8_t vcompress (vbool2_t mask, vuint16m8_t dest, vuint16m8_t src);
vuint32mf2_t vcompress (vbool64_t mask, vuint32mf2_t dest, vuint32mf2_t src);
vuint32m1_t vcompress (vbool32_t mask, vuint32m1_t dest, vuint32m1_t src);
vuint32m2_t vcompress (vbool16_t mask, vuint32m2_t dest, vuint32m2_t src);
vuint32m4_t vcompress (vbool8_t mask, vuint32m4_t dest, vuint32m4_t src);
vuint32m8_t vcompress (vbool4_t mask, vuint32m8_t dest, vuint32m8_t src);
vuint64m1_t vcompress (vbool64_t mask, vuint64m1_t dest, vuint64m1_t src);
vuint64m2_t vcompress (vbool32_t mask, vuint64m2_t dest, vuint64m2_t src);
vuint64m4_t vcompress (vbool16_t mask, vuint64m4_t dest, vuint64m4_t src);
vuint64m8_t vcompress (vbool8_t mask, vuint64m8_t dest, vuint64m8_t src);
vfloat16mf4_t vcompress (vbool64_t mask, vfloat16mf4_t dest, vfloat16mf4_t src);
vfloat16mf2_t vcompress (vbool32_t mask, vfloat16mf2_t dest, vfloat16mf2_t src);
vfloat16m1_t vcompress (vbool16_t mask, vfloat16m1_t dest, vfloat16m1_t src);
vfloat16m2_t vcompress (vbool8_t mask, vfloat16m2_t dest, vfloat16m2_t src);
vfloat16m4_t vcompress (vbool4_t mask, vfloat16m4_t dest, vfloat16m4_t src);
vfloat16m8_t vcompress (vbool2_t mask, vfloat16m8_t dest, vfloat16m8_t src);
vfloat32mf2_t vcompress (vbool64_t mask, vfloat32mf2_t dest, vfloat32mf2_t src);
vfloat32m1_t vcompress (vbool32_t mask, vfloat32m1_t dest, vfloat32m1_t src);
vfloat32m2_t vcompress (vbool16_t mask, vfloat32m2_t dest, vfloat32m2_t src);
vfloat32m4_t vcompress (vbool8_t mask, vfloat32m4_t dest, vfloat32m4_t src);
vfloat32m8_t vcompress (vbool4_t mask, vfloat32m8_t dest, vfloat32m8_t src);
vfloat64m1_t vcompress (vbool64_t mask, vfloat64m1_t dest, vfloat64m1_t src);
vfloat64m2_t vcompress (vbool32_t mask, vfloat64m2_t dest, vfloat64m2_t src);
vfloat64m4_t vcompress (vbool16_t mask, vfloat64m4_t dest, vfloat64m4_t src);
vfloat64m8_t vcompress (vbool8_t mask, vfloat64m8_t dest, vfloat64m8_t src);
```